Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
The 15 Most Popular Data Science and Machine Learning Articles on Analytics Vidhya in 2018,Learn everything about Analytics|Introduction|Topics Covered in this Article|Machine Learning and Deep Learning  The Ultimate Duo|Business Intelligence and Data Visualization|Careers in Data Science|Natural Language Processing|Podcasts|End Notes,"A Comprehensive Guide to build a Recommendation Engine from Scratch (in Python)|24 Ultimate Data Science Projects To Boost Your Knowledge and Skills (& can be accessed freely)|Understanding and Building an Object Detection Model from Scratch in Python|A Comprehensive Guide to Ensemble Learning (with Python codes)|25 Open Datasets for Deep Learning Every Data Scientist Must Work With|The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes)|Intermediate Tableau Guide for Data Science and Business Intelligence Professionals|A Step-by-Step Guide to learn Advanced Tableau  for Data Science and Business Intelligence Professionals|The Most Comprehensive Data Science & Machine Learning Interview Guide Youll Ever Need|13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them|Want to Become a Data Engineer? Heres a Comprehensive List of Resources to get Started|Ultimate Guide to deal with Text Data (using Python)  for Data Scientists & Engineers|Building a FAQ Chatbot in Python  The Future of Information Searching|Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python|10 Data Science, Machine Learning and AI Podcasts You Must Listen To|Share this:|Like this:|Related Articles|The Ultimate Learning Path to Become a Data Scientist and Master Machine Learning in 2019|The 25 Best Data Science and Machine Learning GitHub Repositories from 2018|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What is the one thing you enjoy most about Analytics Vidhya? The most popular answer we receive (and have received since Kunal transformed his idea into reality) is the content we publish. Our content is the one thing take pride in, and 2018saw us take our high-quality content to a whole new level.We launched multiple top-quality and popular training courses, published knowledge-rich machine learning and deep learning articles and guides, and saw our blog visits cross 2.5 million per month. A HUGE thanks to our community for supporting us and for your insatiable interest in this field!As we draw the curtains on a wonderful 2018, we wanted to share the best of the year with our wonderful community. This article is part of that series and looks at the articles which you, dear reader, enjoyed the most. Do check out our other look-back articles so far:In this collection, I have summarized each article and categorized them according to their respective domains. Each article also contains a summary of what the content is about. If you have any other articles you found particularly useful, we would love to hear about it. Do let us know in the comments box below.And now without any further ado, lets take a look at the top articles published on Analytics Vidhya in 2018!Recommendation techniques have been around for decades (if not centuries). And the rise of machine learning has certainly accelerated the process of improving these techniques. We no longer have to rely on intuition and manual monitoring of behavior  combine the data with the right technique and voila! You have an extremely effective and profitable combination.This article is one of the most comprehensive guides youll find anywhere on this topic. It covers thevarious types of recommendation engine algorithms and fundamentals of creating them in Python. Pulkit first explains what recommendation engines are and how they work. He then takes a case study in Python (using the popular MovieLens dataset) and uses that to explain how to build certain models. The two major techniques he has focused on are collaborative filtering and matrix factorization.Once youve built your recommendation engine, how do you evaluate it? How can we tell whether its working as we had planned? Pulkit rounds off the guide by answering this question by showcasing six different evaluation techniques we can leverage to validate our model.This is one of Analytics Vidhyas most popular articles of all time. Originally published in 2016, our team updated it with the latest datasets from across industries. The datasets have been divided into three career levels  each level catering to the different stages of where you might be in your career:And the icing on the cake? Every project has a tutorial associated with it! So whether you want to learn from scratch, or are stuck at some point, or simply want to evaluate your results with a benchmark score, you can always bookmark and come back to that tutorial.Object detection really took flight in 2018. It helps self-driving cars safely navigate through traffic, spots violent behavior in a crowded place, assists sports teams analyze and build scouting reports, ensures proper quality control of parts in manufacturing, among many, many other things. And these are just scratching the surface of what object detection technology can do!In this article, Faizan Shaikh first explains what object detection is, before diving into the different approaches one can use to solve an object detection problem. He starts from the very basic approach of dividing the image into different parts and using an image classifier on each. The article then builds on that and improves with each step, eventually showcasing how deep learning can be used to build an end-to-end object detection model.If this topic fascinates you, and youre looking for a place to start your deep learning journey, I recommend checking out the awesome Computer Vision using Deep Learning course.Ensemble learning comes into the picture once weve mastered the basic machine learning algorithms. Its a fascinating concept and has been spectacularly well explained in this article. There are plenty of examples to help break down complex topics into easy-to-digest ideas.And because of the comprehensive nature of this guide, Aishwarya guides us through plenty of techniques  bagging, boosting, Random Forest, LightGBM, CatBoost, among others. A treasure trove of information all in one place!You will often come across this approach in hackathons  its a proven method of climbing up the leaderboard.Whats the best way to learn and ingrain a concept? Learning the theory is a good start, but learning by doing is when we truly understand how that technique works. And thats especially true of a field thats as vast as deep learning.Theres no shortage of datasets to hone your skills  but where should you start? Which datasets are the best at building your profile? And can you get domain specific datasets which will help you get acquainted with that line of work? To help you out, we scoured the internet and hand-picked the top 25 open deep learning datasets.These datasets are divided into three categories:So pick your interest and get started today!Ah, the curse of dimensionality. We all appreciate more data, it always helps to have a large enough training set. But as most data scientists will testify, having too much data can end up being quite a headache. What should you do when youre faced with a dataset that has 1000s of variables? Its not possible to analyze each variable at a granular level.Thats where dimensionality reduction techniques play such a vital role. Reducing the number of features without losing (too much) information is something we all strive for. Dimensionality reduction is quite a powerful way of doing this, as Pulkit shows in this comprehensive article. Check out the 12 (yes 12!) techniques he discusses along with their implementation in Python, including Principal Component Analysis (PCA), Factor Analysis and t-SNE.Tableau is such a wonderful tool for analyzing the data at hand. But its not just limited to producing beautiful visualizations  you can perform Excel-like tasks in it as well. Tableaus extended functionality really puts the intelligence in BI.This article is aimed towards users who are familiar with the basic functionality of Tableau and wish to expand their knowledge of the tool. The author covers topics like joins, data blending, performing calculations, analyzing and understanding parameters, among other topics. Its a beautifully illustrated article that will make you want to power up Tableau!You can go through the Tableau Beginners Guide first in case you need a quick refresher.This guide is the logical next step after youve gone through the intermediate article. We move beyond the Show Me feature of Tableau and explore advanced graphs here. As Pavleen puts it so eloquently  there is something exciting and enrapturing about the grandeur of these advanced charts.The different charts covered in this article  Motion, Bump, Donut, Waterfall and Pareto. Additionally, you will be introduced to the concept of R programming in Tableau. This can come in really handy when youre looking to combine data science with BI!I had a lot of fun putting this guide together. Interviews are often the biggest stumbling block aspiring data scientists face, and getting through them requires a combination of certain skills. Cracking these interviews becomes even more challenging if youre coming from a non-technical background (like me).What kind of questions are usually asked? What does the interviewer look for? Whats the right combination of technical and soft skills required? These can be daunting..only if youre not prepared. And that was the idea behind writing this lengthy and detailed guide.This comprehensive post covers multiple topics with plenty of resources, including data science and machine learning questions, tool specific quizzes, a variety of case studies, puzzles, guesstimates, and even a couple of really inspiring stories to point you towards the finishing line!Aspiring data scientists make tons of mistakes in their haste to break into the field. Ive made plenty of them as well, and have penned down the 13 most common ones I have seen others experience. Trust me, becoming a data scientist is a tough path to take, and youre not alone in making these mistakes.Learning from someone elses mistakes can also be a career-defining experience. Hence I have also provided a list of resources along with each point with the aim of helping you overcome these obstacles and accelerating your journey towards the promised land of data science.Weve been talking primarily about data scientists so far. But the field of data science has a variety of other roles to offer, and the hottest one right now is that of a data engineer. Theyre overlooked in all the data scientist hype going around, but are very crucial cogs in any DS project.There is currently no single structured path which one can follow to become a data engineer. You learn on the job, no tow ways about it. My hope is that this article will help provide a different option. There are tons of free resources here, including ebooks, video courses, text based article, etc.Once we understand what a data engineer is and how the role is different from that of a data scientist, we dive straight into the various aspects you need to know in order to make this role your own. I have also mentioned a few data engineering certifications that are respected within the data science community.This is one guide you NEED to read. This is the essential NLP beginners guide, which starts with some basic concepts and gradually builds towards more advanced techniques like Bag of Words and word embeddings. There are quite a number of ways you can approach a text data problem and you will understand these different methods here.Feature extraction, preprocessing and advanced techniques  these are all covered in terms of text data. Each technique is showcased using Python code and an open dataset so you can code along as you learn.You can also check out the comprehensive Natural Language Processing using Python course to get started with your own NLP career.2018 was the year chatbots peaked. They were the most common application of Natural Language Processing (NLP) to hit the market. Understandably, more and more folks want to learn how to build one. Well, youve come to the right place!This articles explores how to build a chatbot in Python byextracting information related to the recently introducedGoods and Services Tax (GST) in India. A GST-FAQ bot! The author has used the RASA-NLU library to build this bot.This is a very important topic  both for beginners as well as advanced NLP users. The ULMFiT framework was developed by Sebastian Ruder and Jeremy Howard and it has paved the way for other transfer learning libraries since. The article is more for folks who are familiar with basic NLP techniques and are looking to expand their portfolio.Prateek Joshi takes a streamlined path to introduce us to the world of transfer learning, ULMFiT and finally, how to implement these concepts in Python. As Sebastian Ruder said, NLPs ImageNet moment has arrived, and its time for you to jump on the wagon.Podcasts are a great medium of consuming information on the go. Not all of us have the time to read through articles. Podcasts have done an excellent job of filling that gap and keeping us up-to-date with the latest developments in machine learning. This collection of the top 10 podcasts went viral at the time of publication and has been at the top ever since.We also launched our own podcast series this year called DataHack Radio. DHR features top leaders and practitioners in the data science and machine learning industry and caters to all levels of the data science community.Its available onSoundCloud,iTunesand of course,our own site!Once again, a massive shout out to our community for their continued support and interest in data science. Lets work together to make 2019 an even better and bigger year, and promise to keep our hunger for learning intact and well fuelled! See you next year.",https://www.analyticsvidhya.com/blog/2018/12/most-popular-articles-analytics-vidhya-2018/
The 25 Best Data Science and Machine Learning GitHub Repositories from 2018,Learn everything about Analytics|Introduction|Topics we will cover in this article|Tools and Frameworks|Computer Vision|GANs|Other Deep Learning Projects|Natural Language Processing (NLP)|Automated Machine Learning (AutoML)|Reinforcement Learning|End Notes,"ML.NET|TensorFlow.js|PyTorch 1.0|Papers with Code|Facebooks Detectron|NVIDIAs vid2vid Technique|Training a Model on the ImageNet Dataset in 18 Minutes|Comprehensive Collection of Object Detection Papers|Facebooks DensePose|Everybody Dance Now  Pose Estimation|Deep Painterly Harmonization|Image Outpainting|Visualizing and Understanding GANs|GANimation|NVIDIAs FastPhotoStyle|NVIDIAs WaveGlow|AstroNet|VisualDL  Visualizing Deep Learning Models|Googles BERT|MatchZoo|NLP Progress|Auto Keras|Googles AdaNet|DeepMimic|Reinforcement Learning Notebooks|Share this:|Like this:|Related Articles|The 15 Most Popular Data Science and Machine Learning Articles on Analytics Vidhya in 2018|A Comprehensive Tutorial to learn Convolutional Neural Networks from Scratch (deeplearning.ai Course #4)|
Pranav Dar
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats the best platform for hosting your code, collaborating with team members, and also acts as an online resume to showcase your coding skills? Ask any data scientist, and theyll point you towards GitHub. It has been a truly revolutionary platform in recent years and has changed the landscape of how we host and even do coding.But thats not all. It acts as a learning tool as well. How, you ask? Ill give you a hint  open source!The worlds leading tech companies open source their projects on GitHub by releasing the code behind their popular algorithms. 2018 saw a huge spike in such releases, with the likes of Google and Facebook leading the way. The best part about these releases is that the researchers behind the code also provide pretrained models so folks like you and I dont have to waste time building difficult models from scratch.Additionally, we regularly see the top trending repositories aimed towards coders and developers  this includes resources like cheatsheets, video links, e-books, research paper links, among other things. No matter which level you are at in your professional career (beginner, established or advanced), you will always find something new to learn on GitHub.2018 was a transcendent one in a lot of data science sub-fields, as we will shortly see. Natural Language Processing (NLP) was easily the most talked about domain within the community with the likes of ULMFiT and BERT being open-sourced. In my quest to bring the best to our awesome community, I ran a monthly series throughout the year where I hand-picked the top 5 projects every data scientist should know about. You can check out the entire collection below:There will be some overlap here with my article covering the biggest breakthroughs in AI and ML in 2018. Do check out that article as well  it is essentially a list of all the major developments I feel everyone in this field needs to know about. As a bonus, there are predictions from experts as well  not something you want to miss. And now, get ready to explore new projects in your quest to attain data science stardom in 2019 and scroll down! Simply click on each project title to head over to the code repository on GitHub.Lets get the ball rolling with a look at the top projects in terms of tools, libraries and frameworks. Since we are speaking about a software repository platform, it feels right to open things with this section.Technology is advancing rapidly and computational costs are lower than ever, so were being treated to one massive release after another. Can we call this the golden age of coding in machine learning? That is an open question, but one thing we can all agree on  its a great time to be a programmer in data science. In this section (and the article overall), I have tried to diversify the languages as much as possible, but Python inevitably ruled the roost. How about all you .NET developers wanting to learn a bit of machine learning to complement your existing skills? Heres the perfect repository to get that idea started! ML.NET, a Microsoft project, is an open-source machine learning framework that allows you design and develop models in .NET.You can even integrate existing ML models into your application, all without requiring explicit knowledge of how ML models are developed. ML.NET is actually used in multiple Microsoft products, like Windows, Bing Search, MS Office, among others.ML.NET runs on Windows, Linux and MacOS.Machine learning in the browser! A fictional thought a few years back, a stunning reality now. A lot of us in this field are welded to our favorite IDEs, but TensorFlow.js has the potential to change your habits. Its become a very popular release since its release earlier this year and continues to amaze with its flexibility.As the repository states, there are primarily three major features of TensorFlow.js:If youre familiar with Keras, the high-level layers API will seem quite familiar. There are plenty of examples available on the GitHub repository, so check those out to quicken your learning curve.What a year it has been for PyTorch. It has won the hearts and now projects of data scientists and ML researchers around the globe. It is easy to grasp, flexible, and is already being implemented across high profile researches (as youll see later in this article). The latest version (v1.0) already powers many Facebook products and services at scale, including performing 6 billion text translations a day. If youve been wondering when to start dabbling with PyTorch, the time is NOW.If youre new to this field, ensure you check out Faizan Shaikhs guide to getting started with PyTorch.While not strictly a tool or framework, this repository is a gold mine for all data scientists. Most of us struggle with reading through a paper and then implementing it (at least I do). There are a lot of moving parts that dont seem to work on our machines.And thats where Papers with Code comes in. As the name suggests, they have a code implementation of all the major papers that have been released in the last 6 years or so. It is a mind-blowing collection that you will find yourself fawning over. They have even added code from papers presented at NIPS (NeurIPS) 2018. Get yourself over there now!Thanks to falling computational costs and a surge of breakthroughs from the top researchers (something tells me those two might be linked), deep learning is accessible to more people than ever before. And within deep learning, computer vision projects are ubiquitous  most of the repositories youll see in this section will cover one computer vision technique or another. It is simply the hottest field in deep learning right now and will continue to be so for the foreseeable future. Whether its object detection or pose estimation, theres a repository for seemingly all computer vision tasks. Never a better time to get acquainted with these developments  a lot of job openings might come your way soon.Detectron made a HUGE splash when it was launched in early 2018. Developed by Facebooks AI Research team (FAIR), it implements state-of-the-art object detection frameworks. It is (surprise, surprise) written in Python and has helped enable multiple projects, including DensePose (which we will talk about soon).This repository contains the code and over 70 pretrained models. Too good an opportunity pass up, wouldt you agree?Object detection in images is awesome, but what about doing it in videos? And not just that, can we extend this concept and translate the style of one video to another? Yes, we can! It is a really cool concept and NVIDIA have been generous enough to release the PyTorch implementation for you to play around with.The repository contains videos of how the technique looks, the full research paper, and of course the code. The Cityscapes dataset, available publicly post registration, is used in NVIDIAs examples. One of my favorite projects from 2018.Training a deep learning model in 18 minutes? While not having access to high-end computational resources? Believe me, its already been done. Fast.ais Jeremy Howard and his team of students built a model on the popular ImageNet dataset that even outperformed Googles approach.I encourage you to at least go through this project to get a sense of how these researchers structured their code. Not everyone has access to multiple GPUs (or even one) so this was quite a win for the minnows.Another research paper collection repository! Its always helpful to know how your subject of choice has evolved over a span of multiple years, and this one-stop shop will help you do just that for object detection. Its a comprehensive collection of papers from 2014 till date, and even include code wherever possible.The above image shows how object detection frameworks have evolved and transformed in the last five years. Quite fascinating, isnt it? Theres even a 2019 entry included, so you have quite a lot of catching up to do.Lets turn our attention to the field of pose detection. I came across this concept this year itself and have been fascinated with it ever since. That above image captures the essence of this repository  dense human pose estimation in the wild.The code to train and evaluate your own DensePose-RCNN model is included here. There are notebooks available as well to visualize the DensePose COCO dataset. Pretty good place to kick off your pose estimation learning.The above image (taken from a video) really piqued my interest.I covered the release of the research paperback in August and have continued to be in awe of this technique. This technique enables us to transfer the motion between human objects in different videos. The video I mentioned is available within the repository  it will blow your mind!This repository further contains the PyTorch implementation of this approach. The amount of intricate details this approach is capable of picking up and replicating is incredible.Im sure most of you must have come across a GAN application (even if you perhaps didnt realize it at the time). GANs, or Generative Adversarial Networks, were introduced by Ian Goodfellow back in 2014 and have caught fire since. They specilize in performing creative tasks, especially artistic ones. Check out this amazing introductory guide by Faizan Shaikh to the world of GANs, along with an implementation in Python.We saw a plethora of GAN based projects in 2018 and hence I wanted to create a separate section for this.Lets start off with one of my favorites. I want you to take a moment to just admire the above images. Can you tell which one was done by a human and which one by a machine? I certainly couldnt. Here, the first frame is the input image (original) and the third frame has been generated by this technique.Amazing, right? The algorithm adds an external object of your choosing to any image and manages to make it look like nothing touched it. Make sure you check out the code and try to implement it on a different set of images yourself. Its really, really fun.What if I gave you an image and asked you to extend the boundaries by imagining what it would look like when the entire scene was captured? You would understandably turn to some image editing software. But heres the awesome news  you can achieve it in a few lines of code!This project is a Keras implementation of Stanfords Image Outpainting paper (incredibly cool and illustrated paper  this is how most research papers should be!).You can either build a model from scratch or use the one provided by this repositorys author. Deep learning wonders never cease to amaze.If you havent got a handle on GANs yet, try out this project. Pioneered by researchers from MITs CSAIL division, it helped you visualize and understand GANs. You can explore what your GAN model has learned by inspecting and manipulating its neurons.I would like to point you towards the official MIT project page, which has plenty of resources to get you familiar with the concept, including a video demo.This algorithm enables you to change the facial expression of any person in an image. Its as exciting as it is concerning. The images above inside the green border at the originals, the rest have been generated by GANimation.The link contains a beginners guide, data preparation resources, prerequisites, and the Python code. As the author mentioned, do NOT use it for immoral purposes.This project is quite similar to the Deep Painterly Harmonization one we saw earlier. But it deserved a mention given it came from NVIDIA themselves. As you can see in the image above, the FastPhotoStyle algorithm requires two inputs  a style photo and a content photo. The algorithm then works in one of two ways to generate the output  it either uses photorealistic image stylization code or uses semantic label maps.The computer vision field has the potential to overshadow other work in deep learning but I wanted to highlight a few projects outside it.Audio processing is another field where deep learning has started to make its mark. Its not just limited to generating music, you can do tasks like audio classification, fingerprinting, segmentation, tagging, etc. There is a lot thats still yet to be explored and who knows, perhaps you could use these projects to pioneer your way to the top.Here are two intuitive articles to help you get acquainted with this line of work:And here comes NVIDIA again. WaveGlow is a flow-based network capable of generating really high quality audio. It is essentially a single network for speech synthesis.This repository includes a PyTorch implementation of WaveGlow along with a pre-trained model which you can download. The researchers have also listed down the steps you can follow if you want to train your own model from scratch.Want to discover your own planet? That might perhaps be overstating things a bit, but this AstroNet repository will definitely get you close. The Google Brain team discovered two new planets in December 2017 by applying AstroNet. Its a deep neural network meant for working with astronomical data. It goes to show the far-ranging applications of machine learning and was a truly monumental development.And now the team behind the technology has open sourced the entire code (hint: the model is based on CNNs!) that powers AstroNet.Who doesnt love visualizations? But it can get a tad bit intimidating to imagine how a deep learning model works  there are too many moving parts involved. But VisualDL does a great job mitigating those challenges by designing specific deep learning jobs.VisualDL currently supports the below components for visualizing jobs (you can see examples of each in the repository):Surprised to see NLP so down in this list? Thats primarily because I covered almost all the major open source releases in this article. I highly recommend checking out that list to stay on top of your NLP game. The frameworks I have mentioned here include ULMFiT, Googles BERT, ELMo, and Facebooks PyText. I will briefly mention BERT and a couple of other respositories here as I found them very helpful.I couldnt possibly let this section pass by without mentioning BERT. Google AIs release has smashed records on its way to winning the hearts of NLP enthusiasts and experts alike.Following ULMFiT and ELMo, BERT really blew away the competition with its performance. It obtained state-of-the-art results on 11 NLP tasks.Apart from the official Google repository I have linked to above, a PyTorch implementation of BERT is worth checking out. Whether it marks a new era of not in NLP we will soon find out.It often helps to know how well your model is performing against a certain benchmark. For NLP, and specifically deep text matching models, I have found the MatchZoo toolkit quite reliable.Potential tasks related to MatchZoo include:MatchZoo 2.0 is currently under development so expect to see a lot more being added to this already useful toolkit.This repository was created by none other than Sebastian Ruder. The aim of this project is to track the latest progress in NLP. This includes both datasets and state-of-the-art models.Any NLP technique youve ever wanted to know more about  theres a good chance itll already be present here. The repository covers both traditional and core NLP tasks such as reading comprehension and parts-of-speech tagging. Its mandatory to star/bookmark this repository if youre even vaguely interested in this field.What an year for AutoML. With industries look to integrate machine learning into their core mission, the need to data science specialists continues to grow. There is currently a massive gap between the demand and the supply. This gap could potentially be filled by AutoML tools.These tools are designed for those people who do not have data science expertise. While there are certainly some incredible tools out there, most of them are priced significantly higher than most individuals can afford. So our amazing open source community came to the rescue in 2018, with two high profile releases.This made quite a splash upon its release a few months ago. And why wouldnt it? Deep learning has been long considered a very specialist field, so a library that can automate most tasks came as a welcome sign. Quoting from their official site, The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background.You can install this library from pip:The repository contains a simple example to give you a sense of how the whole thing works. Youre welcome, deep learning enthusiasts. AdaNet is a framework for automatically learning high-quality models without requiring programming expertise. Since its a Google invention, the framework is based on TensorFlow. You can build ensemble models using AdaNet, and even extend its use to training a neural network.The GitHub page contains the code, an example, the API documentation, and other things to get your hands dirty. Trust me, AutoML is the next big thing in our field.Since I already covered a few reinforcement learning releases in my 2018 overview article, I will keep this section fairly brief. My hope in including a RL section where I can is to foster a discussion among our community and to hopefully accelerate research in this field.First, make sure you check out OpenAIs Spinning Up repository, an exhaustive educational resource for beginners. Then head over to Googles Dopamine page. It is a research framework for accelerating research in this still nascent field. Now lets look at a couple of other resources as well.If you follow a few researchers on social media, you must have come across the above images in video form. A stick human running across a terrain, or trying to stand up, or some such sort. That, dear reader, is reinforcement learning in action.Here is a signature example of it  a framework to train a simulated humanoid to imitate multiple motion skills. You can get the code, examples, and a step-by-step run-through on the above link.This repository is a collection of reinforcement learning algorithms from Richard Sutton and Andrew Bartos book and other research papers. These algorithms are presented in the form of Python notebooks.As the author of this repo mentioned, you will only truly learn if you implement the learning as you go along. Its a complex topic, and giving up or reading the resources like a storybook will lead you nowhere.And that bring us to the end of our journey for 2018. What a year! It was a joyful ride putting this article together and I learned a lot of new stuff along the way.I would love to hear your feedback on this article. Which repository have you used? Which one did you find the most useful? And which one(s) did I miss out on? Use the comments section below and let me know.",https://www.analyticsvidhya.com/blog/2018/12/best-data-science-machine-learning-projects-github/
A Comprehensive Tutorial to learn Convolutional Neural Networks from Scratch (deeplearning.ai Course #4),Learn everything about Analytics|Introduction|Table of Contents||Course Structure|Course 4: Convolutional Neural Network|Week 1: Foundations of Convolutional Neural Networks|Module 2: Deep Convolutional Models: Case Studies|Module 3: Object Detection|Module 4: Special Applications: Face Recognition & Neural Style transfer|End Notes,"Computer Vision|Edge Detection Example|More Edge Detection|Padding|Strided Convolutions|Convolutions Over Volume|One Layer of a Convolutional Network|Simple Convolutional Network Example|Pooling Layers|CNN Example|Why Convolutions?|Classic Networks|ResNet|Why ResNets Work?|Networks in Networks and 11 Convolutions|The Motivation Behind Inception Networks|Inception Networks|Practical advice for using ConvNets|Part 1: Face Recognition|What is face recognition?|One-Shot Learning|Siamese Network|Triplet Loss|Face Verification and Binary Classification|Part 2: Neural Style Transfer|What is neural style transfer?|What are deep ConvNets learning?|Cost Function|Content Cost Function|Style Cost Function|Share this:|Like this:|Related Articles|The 25 Best Data Science and Machine Learning GitHub Repositories from 2018|DataHack Radio #14: Quantum Computing and Quantum Machine Learning with Dr. Mandaar Pande|
Pulkit Sharma
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you had to pick one deep learning technique for computer vision from the plethora of options out there, which one would you go for? For a lot of folks, including myself, convolutional neural network is the default answer.But what is a convolutional neural network and why has it suddenly become so popular? Well, thats what well find out in this article! CNNs have become the go-to method for solving any image data challenge. Their use is being extended to video analytics as well but well keep the scope to image processing for now. Any data that has spatial relationships is ripe for applying CNN  lets just keep that in mind for now.In the previous articles in this series, we learned the key to deep learning  understanding how neural networks work. We saw how using deep neural networks on very large images increases the computation and memory cost. To combat this obstacle, we will see how convolutions and convolutional neural networks help us to bring down these factors and generate better results.So welcome to part 3 of our deeplearning.ai course series (deep learning specialization) taught by the great Andrew Ng. In addition to exploring how a convolutional neural network (ConvNet) works, well also look at different architectures of a ConvNet and how we can build an object detection model using YOLO. Finally, well tie our learnings together to understand where we can apply these concepts in real-life applications (like facial recognition and neural style transfer).I highly recommend going through the first two parts before diving into this guide:The previous articles of this series covered the basics of deep learning and neural networks. We also learned how to improve the performance of a deep neural network using techniques likehyperparameter tuning, regularization and optimization.So where to next? Lets turn our focus to the concept ofConvolutional Neural Networks.Course #4 of the deep learning specialization is divided into 4 modules:Ready? Good, because we are diving straight into module 1!The objectives behind the first module of the course 4 are:Some of the computer vision problems which we will be solving in this article are:One major problem with computer vision problems is that the input data can get really big. Suppose an image is of the size 68 X 68 X 3. The input feature dimension then becomes 12,288. This will be even bigger if we have larger images (say, of size 720 X 720 X 3). Now, if we pass such a big input to a neural network, the number of parameters will swell up to a HUGE number (depending on the number of hidden layers and hidden units). This will result in more computational and memory requirements  not something most of us can deal with.In the previous article, we saw that the early layers of a neural network detect edges from an image. Deeper layers might be able to detect the cause of the objects and even more deeper layers might detect the cause of complete objects (like a persons face).In this section, we will focus on how the edges can be detected from an image. Suppose we are given the below image:As you can see, there are many vertical and horizontal edges in the image. The first thing to do is to detect these edges:But how do we detect these edges? To illustrate this, lets take a 6 X 6 grayscale image (i.e. only one channel):Next, we convolve this 6 X 6 matrix with a 3 X 3 filter:After the convolution, we will get a 4 X 4 image. The first element of the 4 X 4 matrix will be calculated as:So, we take the first 3 X 3 matrix from the 6 X 6 image and multiply it with the filter. Now, the first element of the 4 X 4 output will be the sum of the element-wise product of these values, i.e. 3*1 + 0 + 1*-1 + 1*1 + 5*0 + 8*-1 + 2*1 + 7*0 + 2*-1 = -5. To calculate the second element of the 4 X 4 output, we will shift our filter one step towards the right and again get the sum of the element-wise product:Similarly, we will convolve over the entire image and get a 4 X 4 output:So, convolving a 6 X 6 input with a 3 X 3 filter gave us an output of 4 X 4. Consider one more example:Note: Higher pixel values represent the brighter portion of the image and the lower pixel values represent the darker portions. This is how we can detect a vertical edge in an image.The type of filter that we choose helps to detect the vertical or horizontal edges. We can use the following filters to detect different edges:Some of the commonly used filters are:The Sobel filter puts a little bit more weight on the central pixels. Instead of using these filters, we can create our own as well and treat them as a parameter which the model will learn using backpropagation.We have seen that convolving an input of 6 X 6 dimension with a 3 X 3 filter results in 4 X 4 output. We can generalize it and say that if the input is n X n and the filter size is f X f, then the output size will be (n-f+1) X (n-f+1):There are primarily two disadvantages here:To overcome these issues, we can pad the image with an additional border, i.e., we add one pixel all around the edges. This means that the input will be an 8 X 8 matrix (instead of a 6 X 6 matrix). Applying convolution of 3 X 3 on it will result in a 6 X 6 matrix which is the original shape of the image. This is where padding comes to the fore:There are two common choices for padding:We now know how to use padded convolution. This way we dont lose a lot of information and the image does not shrink either. Next, we will look at how to implement strided convolutions.Suppose we choose a stride of 2. So, while convoluting through the image, we will take two steps  both in the horizontal and vertical directions separately. The dimensions for stride s will be:Stride helps to reduce the size of the image, a particularly useful feature.Suppose, instead of a 2-D image, we have a 3-D input image of shape 6 X 6 X 3. How will we apply convolution on this image? We will use a 3 X 3 X 3 filter instead of a 3 X 3 filter. Lets look at an example:The dimensions above represent the height, width and channels in the input and filter. Keep in mind that the number of channels in the input and filter should be same. This will result in an output of 4 X 4. Lets understand it visually:Since there are three channels in the input, the filter will consequently also have three channels. After convolution, the output shape is a 4 X 4 matrix. So, the first element of the output is the sum of the element-wise product of the first 27 values from the input (9 values from each channel) and the 27 values from the filter. After that we convolve over the entire image.Instead of using just a single filter, we can use multiple filters as well. How do we do that? Lets say the first filter will detect vertical edges and the second filter will detect horizontal edges from the image. If we use multiple filters, the output dimension will change. So, instead of having a 4 X 4 output as in the above example, we would have a 4 X 4 X 2 output (if we have used 2 filters):Generalized dimensions can be given as:Here, nc is the number of channels in the input and filter, while nc is the number of filters.Once we get an output after convolving over the entire image using a filter, we add a bias term to those outputs and finally apply an activation function to generate activations. This is one layer of a convolutional network. Recall that the equation for one forward pass is given by:z[1] = w[1]*a[0] + b[1]
a[1] = g(z[1])In our case, input (6 X 6 X 3) is a[0]and filters (3 X 3 X 3) are the weights w[1]. These activations from layer 1 act as the input for layer 2, and so on. Clearly, the number of parameters in case of convolutional neural networks is independent of the size of the image. It essentially depends on the filter size. Suppose we have 10 filters, each of shape 3 X 3 X 3. What will be the number of parameters in that layer? Lets try to solve this:No matter how big the image is, the parameters only depend on the filter size. Awesome, isnt it? Lets have a look at the summary of notations for a convolution layer:Lets combine all the concepts we have learned so far and look at a convolutional network example.This is how a typical convolutional network looks like:We take an input image (size = 39 X 39 X 3 in our case), convolve it with 10 filters of size 3 X 3, and take the stride as 1 and no padding. This will give us an output of 37 X 37 X 10. We convolve this output further and get an output of 7 X 7 X 40 as shown above. Finally, we take all these numbers (7 X 7 X 40 = 1960), unroll them into a large vector, and pass them to a classifier that will make predictions. This is a microcosm of how a convolutional network works.There are a number of hyperparameters that we can tweak while building a convolutional network. These include the number of filters, size of filters, stride to be used, padding, etc. We will look at each of these in detail later in this article. Just keep in mind that as we go deeper into the network, the size of the image shrinks whereas the number of channels usually increases.In a convolutional network (ConvNet), there are basically three types of layers:Lets understand the pooling layer in the next section.Pooling layers are generally used to reduce the size of the inputs and hence speed up the computation. Consider a 4 X 4 matrix as shown below:Applying max pooling on this matrix will result in a 2 X 2 output:For every consecutive 2 X 2 block, we take the max number. Here, we have applied a filter of size 2 and a stride of 2. These are the hyperparameters for the pooling layer. Apart from max pooling, we can also apply average pooling where, instead of taking the max of the numbers, we take their average. In summary, the hyperparameters for a pooling layer are:If the input of the pooling layer isnh X nw X nc, then the output will be [{(nh  f) / s + 1} X {(nw  f) / s + 1} X nc].Well take things up a notch now. Lets look at how a convolution neural network with convolutional and pooling layer works. Suppose we have an input of shape 32 X 32 X 3:There are a combination of convolution and pooling layers at the beginning, a few fully connected layers at the end and finally a softmax classifier to classify the input into various categories. There are a lot of hyperparameters in this network which we have to specify as well.Generally, we take the set of hyperparameters which have been used in proven research and they end up doing well. As seen in the above example, the height and width of the input shrinks as we go deeper into the network (from 32 X 32 to 5 X 5) and the number of channels increases (from 3 to 10).All of these concepts and techniques bring up a very fundamental question  why convolutions? Why not something else?There are primarily two major advantages of using convolutional layers over using just fully connected layers:Consider the below example:If we would have used just the fully connected layer, the number of parameters would be = 32*32*3*28*28*6, which is nearly equal to 14 million! Makes no sense, right?If we see the number of parameters in case of a convolutional layer, it will be = (5*5 + 1) * 6 (if there are 6 filters), which is equal to 156. Convolutional layers reduce the number of parameters and speed up the training of the model significantly.In convolutions, we share the parameters while convolving through the input. The intuition behind this is that a feature detector, which is helpful in one part of the image, is probably also useful in another part of the image. So a single filter is convolved over the entire input and hence the parameters are shared.The second advantage of convolution is the sparsity of connections. For each layer, each output value depends on a small number of inputs, instead of taking into account all the inputs.The objective behind the second module of course 4 are:In this section, we will look at the following popular networks:We will also see how ResNet works and finally go through a case study of an inception neural network.LeNet-5Lets start with LeNet-5:It takes a grayscale image as input. Once we pass it through a combination of convolution and pooling layers, the output will be passed through fully connected layers and classified into corresponding classes. The total number of parameters in LeNet-5 are:AlexNetAn illustrated summary of AlexNet is given below:This network is similar to LeNet-5 with just more convolution and pooling layers:VGG-16The underlying idea behind VGG-16 was to use a much simpler network where the focus is on having convolution layers that have 3 X 3 filters with a stride of 1 (and always using the same padding). The max pool layer is used after each convolution layer with a filter size of 2 and a stride of 2. Lets look at the architecture of VGG-16:As it is a bigger network, the number of parameters are also more.These are three classic architectures. Next, well look at more advanced architecture starting with ResNet.Training very deep networks can lead to problems like vanishing and exploding gradients. How do we deal with these issues? We can use skip connections where we take activations from one layer and feed it to another layer that is even more deeper in the network. There are residual blocks in ResNet which help in training deeper networks.Residual BlocksThe general flow to calculate activations from different layers can be given as:This is how we calculate the activations a[l+2] using the activations a[l] and then a[l+1]. a[l] needs to go through all these steps to generate a[l+2]:In a residual network, we make a change in this path. We take the activations a[l] and pass them directly to the second layer:So, the activations a[l+2] will be:a[l+2] = g(z[l+2] + a[l])The residual network can be shown as:The benefit of training a residual network is that even if we train deeper networks, the training error does not increase. Whereas in case of a plain network, the training error first decreasesas we train a deeper network and then starts to rapidly increase:We now have an overview of how ResNet works. But why does it perform so well? Lets find out!In order to make a good model, we first have to make sure that its performance on the training data is good. Thats the first test and there really is no point in moving forward if our model fails here. We have seen earlier that training deeper networks using a plain network increases the training error after a point of time. But while training a residual network, this isnt the case. Even when we build a deeper residual network, the training error generally does not increase.The equation to calculate activation using a residual block is given by:a[l+2] = g(z[l+2] + a[l])
a[l+2] = g(w[l+2] * a[l+1] + b[l+2] + a[l])Now, say w[l+2] = 0 and the bias b[l+2] is also 0, then:a[l+2] = g(a[l])It is fairly easy to calculate a[l+2] knowing just the value of a[l]. As per the research paper, ResNet is given by:Lets see how a 1 X 1 convolution can be helpful. Suppose we have a 28 X 28 X 192 input and we apply a 1 X 1 convolution using 32 filters. So, the output will be 28 X 28 X 32:The basic idea of using 1 X 1 convolution is to reduce the number of channels from the image. A couple of points to keep in mind:While designing a convolutional neural network, we have to decide the filter size. Should it be a 1 X 1 filter, or a 3 X 3 filter, or a 5 X 5? Inception does all of that for us! Lets see how it works.Suppose we have a 28 X 28 X 192 input volume. Instead of choosing what filter size to use, or whether to use convolution layer or pooling layer, inception uses all of them and stacks all the outputs:A good question to ask here  why are we using all these filters instead of using just a single filter size, say 5 X 5? Lets look at how many computations would arise if we would have used only a 5 X 5 filter on our input:Number of multiplies = 28 * 28 * 32 * 5 * 5 * 192 = 120 million! Can you imagine how expensive performing all of these will be?Now, lets look at the computations a 1 X 1 convolution and then a 5 X 5 convolution will give us:Number of multiplies for first convolution = 28 * 28 * 16 * 1 * 1 * 192 = 2.4 million
Number of multiplies for second convolution = 28 * 28 * 32 * 5 * 5 * 16 = 10 million
Total number of multiplies = 12.4 millionA significant reduction. This is the key idea behind inception.This is how an inception block looks:We stack all the outputs together. Also, we apply a 1 X 1 convolution before applying 3 X 3 and 5 X 5 convolutions in order to reduce the computations. An inception model is the combination of these inception blocks repeated at different locations, some fully connected layer at the end, and a softmax classifier to output the classes.Now that we have understood how different ConvNets work, its important to gain a practical perspective around all of this.Building your own model from scratch can be a tedious and cumbersome process. Also, it is quite a task to reproduce a research paper on your own (trust me, I am speaking from experience!). In many cases, we also face issues like lack of data availability, etc. We can design a pretty decent model by simply following the below tips and tricks:With this, we come to the end of the second module. We saw some classical ConvNets, their structure and gained valuable practical tips on how to use these networks.The objectives behind the third module are:I have covered most of the concepts in this comprehensive article. I highly recommend going through it to learn the concepts of YOLO. For your reference, Ill summarize how YOLO works:It also applies Intersection over Union (IoU) and Non-Max Suppression to generate more accurate bounding boxes and minimize the chance of the same object being detected multiple times.In the final module of this course, we will look at some special applications of CNNs, such as face recognition and neural style transfer.The objective behind the final module is to discover how CNNs can be applied to multiple fields, including art generation and facial recognition.Face recognition is probably the most widely used application in computer vision. It seems to be everywhere I look these days  from my own smartphone to airport lounges, its becoming an integral part of our daily activities.In this section, we will discuss various concepts of face recognition, like one-shot learning, siamese network, and many more.In face recognition literature, there are majorly two terminologies which are discussed the most:In face verification, we pass the image and its corresponding name or ID as the input. For a new image, we want our model to verify whether the image is that of the claimed person. This is also called one-to-one mapping where we just want to know if the image is of the same person.Face recognition is where we have a database of a certain number of people with their facial images and corresponding IDs. When our model gets a new image, it has to match the input image with all the images available in the database and return an ID. It is a one-to-k mapping (k being the number of people) where we compare an input image with all the k people present in the database.One potential obstacle we usually encounter in a face recognition task is the problem a lack of training data. This is where we have only a single image of a persons face and we have to recognize new images using that. Since deep learning isnt exactly known for working well with one training example, you can imagine how this presents a challenge.One-shot learning is where we learn to recognize the person from just one example. Training a CNN to learn the representations of a face is not a good idea when we have less images. The model simply would not be able to learn the features of the face. If a new user joins the database, we have to retrain the entire network. Quite a conundrum, isnt it? So instead of using a ConvNet, we try to learn a similarity function:d(img1,img2) = degree of difference between imagesWe train a neural network to learn a function that takes two images as input and outputs the degree of difference between these two images. So, if two images are of the same person, the output will be a small number, and vice versa. We can define a threshold and if the degree is less than that threshold, we can safely say that the images are of the same person.We will use a Siamese network to learn the function which we defined earlier:d(img1,img2) = degree of difference between imagesSuppose we have two images, x(1) and x(2), and we pass both of them to the same ConvNet. Instead of generating the classes for these images, we extract the features by removing the final softmax layer. So, the last layer will be a fully connected layer having, say 128 neurons:Here, f(x(1)) and f(x(2)) are the encodings of images x(1) and x(2) respectively. So,d(x(1),x(2)) = || f(x(1))  f(x(2)) ||22We train the model in such a way that if x(i) and x(j) are images of the same person, || f(x(i))  f(x(j)) ||2 will be small and if x(i) and x(j) are images of different people, || f(x(i))  f(x(j)) ||2 will be large. This is the architecture of a Siamese network.Next up, we will learn the loss function that we should use to improve a models performance.In order to define a triplet loss, we take an anchor image, a positive image and a negative image. A positive image is the image of the same person thats present in the anchor image, while a negative image is the image of a different person. Since we are looking at three images at the same time, its called a triplet loss. We will use A for anchor image, P for positive image and N for negative image.So, for given A, P and N, we want:|| f(A)  f(P) ||2 <= || f(A)  f(N) ||2
|| f(A)  f(P) ||2  || f(A)  f(N) ||2 <= 0If the model outputs zero for both || f(A)  f(P) ||2 and || f(A)  f(N) ||2, the above equation will be satisfied. The model might be trained in a way such that both the terms are always 0. This will inevitably affect the performance of the model. How do we overcome this? We need to slightly modify the above equation and add a term , also known as the margin:|| f(A)  f(P) ||2  || f(A)  f(N) ||2 +  <= 0The loss function can thus be defined as:L(A,P,N) = max(|| f(A)  f(P) ||2  || f(A)  f(N) ||2 + , 0)Similarly, the cost function for a set of people can be defined as:Our aim is to minimize this cost function in order to improve our models performance. Apart with using triplet loss, we can treat face recognition as a binary classification problem.Instead of using triplet loss to learn the parameters and recognize faces, we can solve it by translating our problem into a binary classification one. We first use a Siamese network to compute the embeddings for the images and then pass these embeddings to a logistic regression, where the target will be 1 if both the embeddings are of the same person and 0 if they are of different people:The final output of the logistic regression is:Here,  is the sigmoid function. Hence, we treat it as a supervised learning problem and pass different sets of combinations. Each combination can have two images with their corresponding target being 1 if both images are of the same person and 0 if they are of different people.In the final section of this course, well discuss a very intriguing application of computer vision, i.e., neural style transfer.Lets understand the concept of neural style transfer using a simple example. Suppose we want to recreate a given image in the style of another image. Here, the input image is called as the content image while the image in which we want our input to be recreated is known as the style image:Neural style transfer allows us to create a new image which is the content image drawn in the fashion of the style image:Awesome, right?! For the sake of this article, we will be denoting the content image as C, the style image as S and the generated image as G. In order to perform neural style transfer, well need to extract features from different layers of our ConvNet.Before diving deeper into neural style transfer, lets first visually understand what the deeper layers of a ConvNet are really doing. Lets say weve trained a convolution neural network on a 224 X 224 X 3 input image:To visualize each hidden layer of the network, we first pick a unit in layer 1, find 9 patches that maximize the activations of that unit, and repeat it for other units. The first hidden layer looks for relatively simpler features, such as edges, or a particular shade of color. The image compresses as we go deeper into the network. The hidden unit of a CNNs deeper layer looks at a larger region of the image.As we move deeper, the model learns complex relations:This is what the shallow and deeper layers of a CNN are computing. We will use this learning to build a neural style transfer algorithm.First, lets look at the cost function needed to build a neural style transfer algorithm. Minimizing this cost function will help in getting a better generated image (G). Defining a cost function:J(G) = *JContent(C,G) + *JStyle(S,G)Here, the content cost function ensures that the generated image has the same content as that of the content image whereas the generated cost function is tasked with making sure that the generated image is of the style image fashion.Below are the steps for generating the image using the content and style images:Suppose the content and style images we have are:First, we initialize the generated image:After applying gradient descent and updating G multiple times, we get something like this:Not bad! This is the outline of a neural style transfer algorithm. Its important to understand both the content cost function and the style cost function in detail for maximizing our algorithms output.Suppose we use the lth layer to define the content cost function of a neural style transfer algorithm. Generally, the layer which is neither too shallow nor too deep is chosen as the lth layer for the content cost function. We use a pretrained ConvNet and take the activations of its lth layer for both the content image as well as the generated image and compare how similar their content is. With me so far?Now, we compare the activations of the lth layer. For the content and generated images, these are a[l](C) and a[l](G) respectively. If both these activations are similar, we can say that the images have similar content. Thus, the cost function can be defined as follows:JContent(C,G) =  * || a[l](C)  a[l](G) ||2We try to minimize this cost function and update the activations in order to get similar content. Next, we will define the style cost function to make sure that the style of the generated image is similar to the style image.Suppose we pass an image to a pretrained ConvNet:We take the activations from the lth layer to measure the style. We define the style as the correlation between activations across channels of that layer. Lets say that the lth layer looks like this:We want to know how correlated the activations are across different channels:ai,j,k = activations at (i,j,k)Here, i is the height, j is the width, and k is the channel number. We can create a correlation matrix which provides a clear picture of the correlation between the activations from every channel of the lth layer:where k and k ranges from 1 to nc[l]. This matrix is called a style matrix. If the activations are correlated, Gkk will be large, and vice versa. S denotes that this matrix is for the style image. Similarly, we can create a style matrix for the generated image:Using these two matrices, we define a style cost function:This style cost function is for a single layer. We can generalize it for all the layers of the network:Finally, we can combine the content and style cost function to get the overall cost function:J(G) = *JContent(C,G) + *JStyle(S,G)And there you go! Quite a ride through the world of CNNs, wasnt it?We have learned a lot about CNNs in this article (far more than I did in any one place!). We have seen how a ConvNet works, the various building blocks of a ConvNet, itsvarious architectures and how they can be used for image recognition applications. Finally, we have also learned how YOLO can be used for detecting objects in an image before diving into two really fascinating applications of computer vision  face recognition and neural style transfer.Do share your throughts with me regarding what you learned from this article. Have you used CNNs before? If yes, feel free to share your experience with me  it always helps to learn from each other.",https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/
DataHack Radio #14: Quantum Computing and Quantum Machine Learning with Dr. Mandaar Pande,Learn everything about Analytics|Introduction|Dr. Mandaar Pandes Background|Interest in Quantum Computing and Quantum Machine Learning|What are the key characteristics one needs to have in order to learn Quantum Computing?|But what is Quantum Computing in the first place?|Can we put a Timeline on when Quantum Computing will become Mainstream?|Impact of Quantum Computing on Machine Learning|End Notes,"Share this:|Related Articles|A Comprehensive Tutorial to learn Convolutional Neural Networks from Scratch (deeplearning.ai Course #4)|A Technical Overview of AI & ML (NLP, Computer Vision, Reinforcement Learning) in 2018 & Trends for 2019|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Quantum computing and quantum machine learning  most of us have come across these concepts at some point without getting the opportunity to delve deeper. But what if I told you that these could potentially disrupt the way we see and use technology?We are joined by Dr. Mandaar Pande in episode #14 of the DataHack Radio podcast, where he navigates us through the wonderfully complex world of quantum computing. Heres a mind-blowing fact to give you a taste of what to expect:The number of bits in a 300 qubit quantum computer will be more than the known atoms in the universe.  Dr. Mandaar PandeBefore I personally met Dr. Mandaar at DataHack Summit 2018 (where he also spoke on this subject), I only had a vague sense of what quantum computers are and the gigantic amount of power they can process. But as youll soon find out, theres a lot more that goes on behind the scenes that one might never have thought of.I have briefly covered the main topics discussed in this episode but the true joy and knowledge lies in listening to Dr. Mandaar himself.Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms:Dr. Mandaar holds a Ph.D degree in theoretical physics from the University of Hyderabad, with a specialization in non-linear optics. After completing his Ph.D in 1994, he took up a post as a lecturer at BITS Pilani for the next four years in the EEE department (electrical engineering).Experienced folks will recall that it was in the late 90s when IT started picking up steam in India, and Dr. Mandaar decided to take the plunge and explore other avenues outside of academia. He joined Tech Mahindra in 1998 as part of the modeling and simulation centre. There, he workedin the capacity of Group Head and Principal Consultant in the area of PerformanceEngineering and Management.Following a 12 year stint at Tech Mahindra, he spent 7 years at Wipro  first as a Lead Architect, then as the Global Practice Head for Performance Engineering  Quality Engineering and Testing. Dr. Mandaars experience in this field, as you can tell, is incredibly rich and not many folks come close to rivalling his experience and know-how.But for Dr. Mandaar, it felt inevitable that he would return to academia at some point and he joined Symbiosis as the professor of IT last year.So where does his interest and passion for quantum computing and machine learning fit into the picture? Well, towards his final years with Wipro, he got some exposure to the digital way of working (and data science was a big part of that). He built up an interest while working there, and kept that up during his transition back to academia with Symbiosis.His Ph.D in quantum optics obviously helped while he pursued quantum computing. But what is this field exactly? And how does it tie into machine learning? Lets hear that from Dr. Mandaar himself:Quantum Computing is a field that is at the intersection of quantum physics, information science, as well as function theory. And one of the largest applications of quantum computing in the near future is going to be quantum machine learning.This is a tough one, and a question I have been wondering about ever since I heard about this subject. Below are the two key points Dr. Mandaar mentioned:Theres a lot more to it, but if you dont possess a solid base in these two aspects, its going to be next to impossible to make headway.The current version of digital devices we use, like computers and smartphones, use metal chips in them (which are based on integrated circuits that are in turn based on transistors). Whatever computing we do today, including the data we capture and the analysis we perform, is done using the 2 bits we see in these transistors  0 and 1. Any algorithm that we write eventually gets broken down into 0 and 1 at the machine level.At the most fundamental level, the physical principles that govern the nature in quantum computing follow the laws of quantum mechanics. So then what is quantum mechanics? Its a theory in physics that describes nature at the smallest level (atoms and molecules). At this really basic level, these particles behave very differently.Now, there are a couple of things that are very important to understand:This is just a taste of what Dr. Mandaar described in the podcast. He broke down this complex topic into easy-to-digest bits of information using examples. If youve ever wondered how quantum computers work from the ground up, this section will feel like youve hit the jackpot.The general perception is that it might take up to 15-20 years before we see quantum computing becoming mainstream and getting democratized.But as Dr. Mandaar pointed out, these things are not written in stone. A big breakthrough has the potential to throw the doors wide open and accelerate the timeline (as we have seen with deep learning and AI in recent times). But until that happens, quantum computing will remain the realm of the big organisations like IBM.According to Dr. Mandaar, most of the classic machine learning and deep learning algorithms have a quantum equivalent now. He took the example of a perceptron, which is also available as a quantum perceptron. Further, Dr. Mandaar mentioned four ways how one can look at Quantum ML:That was a power-packed episode! There was a lot Dr. Mandaar told us about a subject most of us have vaguely heard of in our professional careers. Apart from the points Ive mentioned here, the conversation also covered topics like challenges in quantum computing, the areas where classical computers could potentially be better than quantum computers, a very specific list of requirements one needs to learn in order to gain traction in this field, what a quantum internet is (really cool concept!), among other things.",https://www.analyticsvidhya.com/blog/2018/12/datahack-radio-quantum-machine-learning/
"A Technical Overview of AI & ML (NLP, Computer Vision, Reinforcement Learning) in 2018 & Trends for 2019",Learn everything about Analytics|Introduction|Areas well cover in this article|Natural Language Processing (NLP)|Computer Vision|Tools and Libraries|Reinforcement Learning|AI for Good  A Move Towards Ethical AI|End Notes,"ULMFiT|ELMo|Googles BERT|Facebooks PyText|Google Duplex|NLP Trends to Expect in 2019||The Release of BigGANs|Fast.ais Model being Trained on ImageNet in 18 Minutes|NVIDIAs vid2vid technique|Computer Vision Trends to Expect in 2019|PyTorch 1.0|AutoML  Automated Machine Learning|TensorFlow.js  Deep Learning in the Browser|AutoML Trends to Expect in 2019|OpenAIs Spinning Up in Deep Reinforcement Learning|Dopamine by Google|Reinforcement Learning Trends to Expect in 2019|Campaigns by Google and Microsoft|How GDPR has Changed the Game|Ethical AI Trends to Expect in 2019|Share this:|Like this:|Related Articles|DataHack Radio #14: Quantum Computing and Quantum Machine Learning with Dr. Mandaar Pande|A Comprehensive Guide to Digital Marketing and Analytics Every Data Science Professional Must Read|
Pranav Dar
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The last few years have been a dream run for Artificial Intelligence enthusiasts and machine learning professionals. These technologies have evolved from being a niche to becoming mainstream, and are impacting millions of lives today. Countries now have dedicated AI ministers and budgets to make sure they stay relevant in this race.The same has been true for a data science professional. A few years back  you would have been comfortable knowing a few tools and techniques. Not anymore! There is so much happening in this domain and so much to keep pace with  it feels mind boggling at times.This is why I thought of taking a step back and looking at the developments in some of the key areas in Artificial Intelligence from a data science practitioners perspective. What were these breakthroughs? What happened in 2018 and what can be expected in 2019? Read this article to find out!P.S. As with any forecasts, these are my takes. These are based on me trying to connect the dots. If you have a different perspective  I would love to hear it. Do let me know what you think might change in 2019.Making machines parse words and sentences has always seemed like a dream. There are way too many nuances and aspects of a language that even humans struggle to grasp at times. But 2018 has truly been a watershed moment for NLP.We saw one remarkable breakthrough after another  ULMFiT, ELMO, OpenAIs Transformer and Googles BERT to name a few. The successful application of transfer learning (the art of being able to apply pretrained models to data) to NLP tasks has blown open the door to potentially unlimited applications. Our podcast with SebastianRuder further cemented our belief in how far his field has traversed in recent times. As a side note, thats amust-listen podcast for all NLP enthusiasts.Lets look at some of these key developments in a bit more detail. And if youre looking to learn the ropes in NLP and are looking for a place to get started, make sure you head over to this NLP using Python course. Its as good a place as any to start your text-fuelled journey!Designed by Sebastian Ruder and fast.ais Jeremy Howard, ULMFiT was the first framework that got the NLP transfer learning party started this year. For the uninitiated, it stands for Universal Language Model Fine-Tuning. Jeremy and Sebastian have truly put the word Universal in ULMFiT  the framework can be applied to almost any NLP task!The best part about ULMFiT and the subsequent frameworks well see soon? You dont need to train models from scratch! These researchers have done the hard bit for you  take their learning and apply it in your own projects. ULMFiT outperformed state-of-the-art methods in six text classification tasks.You can read this excellent tutorial by Prateek Joshi on how to get started with ULMFiT for any text classification problem.Want to take a guess at what ELMo stands for? Its short for Embeddings from Language Models. Pretty creative, eh? Apart from its name resembling the famous Sesame Street character, ELMo grabbed the attention of the ML community as soon as it was released.ELMo uses language models to obtain embeddings for each word while also considering the context in which the word fits into the sentence or paragraph. Context is such a crucial aspect of NLP that most people failed to grasp before. ELMo uses bi-directional LSTMs to create the embeddings. Dont worry if that sounds like a mouthful  check out this article to get a really simple overview of what LSTMs are and how they work.Like ULMFiT, ELMo significantly improves the performance of a wide variety of NLP tasks, like sentiment analysis and question answering. Read more about it here.Quite a few experts have claimed that the release of BERT marks a new era in NLP. Following ULMFiT and ELMo, BERT really blew away the competition with its performance. As the original paper states, BERT is conceptually simple and empirically powerful.BERT obtained state-of-the-art results on 11 (yes, 11!) NLP tasks. Check out their results on the SQuAD benchmark:Interested in getting started? You can use either the PyTorch implementation or Googles own TensorFlow code to try and replicate the results on your own machine.Im fairly certain you are wondering what BERT stands for at this point.  ItsBidirectional Encoder Representations from Transformers. Full marks if you got it right the first time.How could Facebook stay out of the race? They have open-sourced their own deep learning NLP framework called PyText. It was released earlier this week so Im still to experiment with it, but the early reviews are extremely promising. According to research published by FB, PyText has led to a 10% increase in accuracy of conversational models and reduced the training time as well.PyText is actually behind a few of Facebooks own products like the FB Messenger. So working on this adds some real-world value to your own portfolio (apart from the invaluable knowledge youll gain obviously).You can try it out yourself by downloading the code from this GitHub repo.If you havent heard of Google Duplex yet, where have you been?! Sundar Pichai knocked it out of the park with this demo and it has been in the headlines ever since:Since this is a Google product, theres a slim chance of them open sourcing the code behind it. But wow! Thats a pretty awesome audio processing application to showcase. Of course it raises a lot of ethical and privacy questions, but thats a discussion for later in this article. For now, just revel in how far we have come with ML in recent years.Who better than Sebastian Ruder himself to provide a handle on where NLP is headed in 2019? Here are his thoughts:This is easily the most popular field right now in the deep learning space. I feel like we have plucked the low-hanging fruits of computer vision to quite an extent and are already in the refining stage. Whether its image or video, we have seen a plethora of frameworks and libraries that have made computer vision tasks a breeze.We at Analytics Vidhya spent a lot of time this year working on democratizing these concepts. Check out our computer vision specific articles here, covering topics from object detection in videos and images to lists of pretrained models to get your deep learning journey started.Heres my pick of the best developments we saw in CV this year.And if youre curious about this wonderful field (actually going to become one of the hottest jobs in the industry soon), then go ahead and start your journey with our Computer Vision using Deep Learning course.Ian Goodfellow designed GANs in 2014, and the concept has spawned multiple and diverse applications since. Year after year we see the original concept being tweaked to fit a practical use case. But one thing has remained fairly consistent till this year  images generated by machines were fairly easy to spot. There would always be some inconsistency in the frame which made the distinction fairly obvious.But that boundary has started to seep away in recent months. And with the creation of BigGANs, that boundary could be removed permanently. Check out the below images generated using this method:Unless you take a microscope to it, you wont be able to tell if theres anything wrong with that collection. Concerning or exciting? Ill leave that up to you, but theres no doubt GANs are changing the way we perceive digital images (and videos).For the data scientists out there, these models were trained on the ImageNet dataset first and then the JFT-300M data to showcase that these models transfer well from one set to the other. I would also to direct you to the GAN Dissection page  a really cool way to visualize and understand GANs.This was a really cool development. There is a very common belief that you need a ton of data along with heavy computational resources to perform proper deep learning tasks. That includes training a model from scratch on the ImageNet dataset. I understand that perception  most of us thought the same before a few folks at fast.ai found a way to prove all of us wrong.Their model gave an accuracy of 93% in an impressive 18 minutes timeframe. The hardware they used, detailed in theirblog post, contained 16 public AWS cloud instances, each with 8 NVIDIA V100 GPUs. They built the algorithm using the fastai and PyTorch libraries.The total cost of putting the whole thing together came out to be just$40!Jeremy has described their approach, including techniques, in much more detailhere. A win for everyone!Image processing has come leaps and bounds in the last 4-5 years, but what about video? Translating methods from a static frame to a dynamic one has proved to be a little tougher than most imagined. Can you take a video sequence and predict what will happen in the next frame? It had been explored before but the published research had been vague, at best.NVIDIA decided to open source their approach earlier this year, and it was met with widespread praise. The goal of their vid2vid approach is to learn a mapping function from a given input video in order to produce an output video which depicts the contents of the input video with incredible precision.You can try out their PyTorch implementation available on their GitHub here.Like I mentioned earlier, we might see modifications rather than inventions in 2019. It might feel like more of the same  self-driving cars, facial recognition algorithms, virtual reality, etc. Feel free to disagree with me here and add your point of view  I would love to know what else we can expect next year that we havent already seen.Drones, pending political and government approvals, might finally get the green light in the United States (India is far behind there). Personally, I would like to see a lot of the research being implemented in real-world scenarios. Conferences like CVPR and ICML portray the latest in this field but how close are those projects to being used in reality?Visual question answering and visual dialog systems could finally make their long-awaited debut soon. These systems lack the ability to generalize but the expectation is that well see an integrated multi-modal approach soon.Self-supervised learning came to the forefront this year. I can bet on that being used in far more studies next year. Its a really cool line of learning  the labels are directly determined from the data we input, rather than wasting time labelling images manually. Fingers crossed!This section will appeal to all data science professionals. Tools and libraries are the bread and butter of data scientists. I have been part a part of plenty of debates about which tool is the best, which framework supersedes the other, which library is the epitome of economical computations, etc. Im sure quite a lot of you will be able to relate to this as well.But one thing we can all agree on  we need to be on top of the latest tools in the field, or risk being left behind. The pace with which Python has overtaken everything else and planted itself as the industry leader is example enough of this. Of course a lot of this comes down to subjective choices (what tool is your organization using, how feasible is it to switch from the current framework to a new one, etc.), but if you arent even considering the state-of-the-art out there, then I implore you to start NOW.So what made the headlines this year? Lets find out!Whats all the hype about PyTorch? Ive mentioned it multiple times already in this article (and youll see more instances later). Ill leave it to my colleague Faizan Shaikh to acquaint you with the framework.Thats one of my favorite deep learning articles on AV  a must-read! Given how slow TensorFlow can be at times, it opened the door for PyTorch to capture the deep learning market in double-quick time. Most of the code that I see open soruced on GitHub is a PyTorch implemnantation of the concept. Its not a coincidence  PyTorch is super flexible and the latest version (v1.0) already powers many Facebook products and services at scale, including performing 6 billion text translations a day.PyTorchs adoption rate is only going to go up in 2019 so now is as good a time as any to get on board.Automated machine learning (or AutoML) has been gradually making inroads in the last couple of years. Companies like RapidMiner, KNIME, DataRobot and H2O.ai have released excellent products showcasing the immense potential of this service.Can you imagine working on a ML project where you only need to work with a drag-and-drop interface without coding? Its a scenario thats not too far off in the future. But apart from these companies, there was a significant release in the ML/DL space  Auto Keras!Its an open source library for performing AutoML tasks. The idea behind it is to make deep learning accessible to domain experts who perhaps dont have a ML background. Make sure you check it out here. It is primed to make a huge run in the coming years.Weve been building and designing machine learning and deep learning models in our favorite IDEs and notebooks since we got into this line of work. How about taking a step out and trying something different? Yes, Im talking about performing deep learning in your web browser itself!This is now a reality thanks to the release of TensorFlow.js. That link has a few demos as well which demonstrate how cool this open source concept is. There are primarily three advantages/features of TensorFlow.js:I wanted to focus particularly on AutoML in this thread. Why? Because I feel its going to be a real-game changer in the data science space in the next few years. But dont just take my word for it! Heres H2O.ais Marios Michailidis, Kaggle Grandmaster, with his view of what to expect from AutoML in 2019:Machine learning continues its march into being one of the most important trends of the future  of where the world is going towards to. This expansion has increased the demand for skilled applications in this space. Given its growth , it is imperative that automation is the key into utilising the data science resources as best as possible. The applications are limitless: Credit, insurance, fraud, computer vision, acoustics,sensors, recommenders, forecasting, NLP  you name it. It is a privilege to be working in this space . The trends that will continue being important can be defined as:If I had to pick one field where I want to see more penetration, it would be reinforcement learning. Apart from the occasional headlines we see at irregular intervals, there hasnt yet been a game-changing breakthrough. The general perception I have seen in the community is that its too math-heavy and there are no real industry applications to work on.While this is true to a certain extent, I would love to see more practical use cases coming out of RL next year. In my monthly GitHub and Reddit series, I tend to keep at least one repository or discussion on RL to at least foster a discussion around the topic. This might well be the next big thing to come out of all that research.OpenAI have released a really helpful toolkit to get beginners started with the field, which I have mentioned below. You can also check out this beginner-friendly introduction on the topic (it has been super helpful for me).If theres anything I have missed, would love to hear your thoughts on it.If research in RL has been slow, the educational material around it has been minimal (at best). But true to their word, OpenAI have open sourced some awesome material on the subject. They are calling this project Spinning Up in Deep RL and you can read all about it here.Its actually quite a comprehensive list of resources on RL and they have attempted to keep the code and explanations as simple as possible. There is quite a lot of material which includes things like RL terminologies, how to grow into an RL research role, a list of important papers, a supremely well-documented code repository, and even a few exercised to get you started.No more procrastinating now  if you were planning to get started with RL, your time has come!To accelerate research and get the community more involved in reinforcement learning, the Google AI team has open sourced Dopamine, a TensorFlow framework that aims to create research by making it more flexible and reproducible.You can find the entire training data along with the TensorFlow code (just 15 Python notebooks!) on this GitHub repository. Heres the perfect platform for performing easy experiments in a controlled and flexible environment. Sounds like a dream for any data scientist.Xander Steenbrugge, speaker at DataHack Summit 2018 and founder of the ArxivInsights channel, is quite the expert in reinforcement learning. Here are his thoughts on the current state of RL and what to expect in 2019:BONUS: Check out Xanders video about overcoming sparse rewards in Deep RL (the first challenge highlighted above).Imagine a world ruled by algorithms that dictate every action humans take. Not exactly a rosy scenario, is it? Ethics in AI is a topic we at Analytics Vidhya have always been keen to talk about. It becomes bogged down amid all the technical discussions when it should be considered along with those topics.Quite a few organizations were left with egg on their face this year with Facebooks Cambridge Analytica scandal and Googles internal rife about designing weapons headlining the list of scandals. But all of this led to the big tech companies penning down charters and guidelines they intend to follow.There isnt one out-of-the-box solution or one size fits all solution to handling the ethical aspect of AI. It requires a nuanced approach combined with a structured path put forward by the leadership. Lets see a couple of major moves that shook the landscape earlier this year.It was heartening to see the big corporations putting emphasis on this side of AI (even though the road that led to this point wasnt pretty). I want to direct your attention to the guidelines and principles released by a couple of these companies:These all essentially talk about fairness in AI and when and where to draw the line. Always a good idea to reference them when youre starting a new AI based project.GDPR, or the General Data Protection Regulation, has definitely had an impact on the way data is collected for building AI applications. GDPR came into play to ensure users have more control over their data (what information is collected and shared about them).So how does that affect AI? Well, if the data scientist does not have data (or enough of it), building any model becomes a non-starter. This has certainly put a spanner in the works of how social platforms and other sites used to work. GDPR will make for a fascinating case study down the line but for now, it has limited the usefulness of AI for a lot of platforms.This is a bit of a grey field. Like I mentioned, theres no one solution to it. We have to come together as a community to integrate ethics within AI projects. How can we make that happen? As Analytics Vidhyas Founder and CEO Kunal Jain highlighted in his talk at DataHack Summit 2018, we will need to pen down a framework which others can follow.I expect to see new roles being added in organizations that primarily deal with ethical AI. Corporate best practices will need to be re-structured and governance approaches re-drawn as AI becomes central to the companys vision. I also expect the Government to play a more active role in this regard with new or modified policies coming into play. 2019 will be a very interesting year, indeed.Impactful  the only word that succinctly describes the amazing developments in 2018. Ive become an avid user of ULMFiT this year and Im looking forward to exploring BERT soon. Exciting times, indeed.I would love to hear from you as well! What developments did you find the most useful? Are you working on any project using the frameworks/tools/concepts we saw in this article? And what are your predictions for the coming year? I look forward to hearing your thoughts and ideas in the comments section below.",https://www.analyticsvidhya.com/blog/2018/12/key-breakthroughs-ai-ml-2018-trends-2019/
A Comprehensive Guide to Digital Marketing and Analytics Every Data Science Professional Must Read,"Learn everything about Analytics|Introduction|Table of Contents|What makes Digital Media better than Traditional Media?|As a Digital Analytics Professional, why should I even care about Digital Media?|Sub-channels of Digital Media|Consider a Real-World Scenario|Google Search Network vs. Google Display Network|How does Google Choose Ads for Every Search and their Placements?|Adwords (now Google Ads) vs. Adsense|DoubleClick for Publishers (DFP) vs. DoubleClick for Advertisers (DFA)|DoubleClick Search vs. DoubleClick Bid Management System|Cookies, Pixels and Tag Containers|DSP vs. SSP vs. Ad Exchange|Data Management Platform|Identity Resolution|Digital Analytics Tools|End Notes","Share this:|Like this:|Related Articles|A Technical Overview of AI & ML (NLP, Computer Vision, Reinforcement Learning) in 2018 & Trends for 2019|WNS Hackathon Solutions by Top Finishers|
Tavish Srivastava
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the biggest challenges of breaking into the field of digital analytics is that the landscape of digital marketing is extremely complex. Its a hard task finding professionals who know the best of both worlds  digital marketing and data science. There is a serious shortage in the supply of adequate talent, while spending on digital marketing continues to rise unabated.This spending is quite prevalent in the developed economies. But heres the good news  developing countries are starting to catch up, and are not far behind the curve. Check out the below chart and see for yourself the growth in the digital marketing spend share over time in India:Figure source : eMarketerI have seen positions open for years in the digital analytics space because of the shortage of such niche talent coupled with the immense growth of this field. Over the last few months, I have spent some time trying to understand the digital marketing landscape and how it integrates with our field of data science. In this article, I have shared my compiled notes that should simplify this complex world of digital marketing and help you understand how you can use your data science tools in this terrain.Excited yet? Good. Lets start with the most basic question and take it from there!Marketing is all about getting 4 things right  reaching out to the right customer with the right product at the right time through the right channel. Marketing also needs a lot of testing to see which combination works for these 4 factors, so we need a channel that takes minimum time to hit the market. Traditional media channels like cable TV, flyers, radio, physical banners, etc. present a lot of challenges getting all of these 4 things right. To name a few:1. Once printed or aired, an advertisement cannot be changed.2. High time to market. This is actually a big challenge for any dynamic industry. A lot of offers require a quick response time (like Amazon Prime sales) and advertisers dont wish to announce the sale beforehand to make sure customers regularly come to their website to check out offers.3. No way to accurately measure viewed ads. All we know is the response from ad campaigns.4. Limited ways to reach out to a specific audience.5. Even if we manage to reach the right customers, we might not really reach out at the right time. For instance, if we advertise during specific TV shows that are viewed by our target audience, we dont really know whether the target customer is looking at the TV at the time when our ad is being played. Similarly, if we send a mail to a specific prospect who is traveling at the time, its a lost opportunity.6. Given a low response from these channels, most of these traditional methods have a high cost per acquisition.Digital marketing is able to overcome all the challenges mentioned above.How will become clearer as we cover other related topics in this article. With internet penetration reaching 60%+ around the globe and rapidly increasing (see chart below), digital media is now an ideal place to catch your audiences attention.Figure Source : Research GateToday, we can reach out to 4 billion+ internet users through digital media. Another survey (below) indicates 2 hr. 28 mins spent per day by adult internet users in India. Noting that average time spent by the total adult population is 1 hour, total internet user average should be somewhere between 1 hr to 2.5 hrs (say, 2 hours). An estimate of the ad opportunity available per day on the internet is 4 Bn*2 hours = 8 Bn hours/day.Even if we assume only 1% of this internet time can be monetized with ads, we are still talking about 80 Million hours/day!Figure source : eMarketerThis exercise would have given you a sense of the magnitude of opportunity in digital marketing. And as an analyst, you would have definitely noticed the incremental curve for both internet users and the average time spent on the internet. Given the scale and precision of this channel, digital media presents multiple methods for advertising. Lets broadly classify these sub-channels of digital media.A data scientist can contribute literally in any domain and any industry. For instance, there are data science roles in creating new medicine, intelligent voice assistants like Google Home, targeting strategy, etc. For each of these roles, a data scientist might need minimal knowledge of the domain, but can still make strong contributions with the tools we have. For instance, to contribute towards building a new medicine, you need not know all the biology behind it. All you need to know is an objective function and the parameters you need to optimize. In these scenarios, you can easily break down the work into what the business person needs to tell and what you need to know already.However, digital analytics is slightly different. Given the fast changing dynamics in the industry, we are getting better data everyday. As an analyst, you need to decide:The barrier between a business person and an analyst becomes very blurry in this space. Here is how companies describe this new skill set they need for digital analytics:The T-shaped talent suffices for most data scientist roles in different domains. However, digital analytics professionals need the Pi-shaped talent to be a rockstar. If you already understand the world of data science, then this article will help you grasp the second aspect of the Pi-shaped talent requirement, the digital ecosystem, and open up a land of opportunities for you. In no way does this article capture everything under the digital marketing umbrella, but this is as good a place as any to get your journey started.We generally talk about 3 types of digital media  owned media, paid media, and earned media. Heres a description of each:Data science has a strong application in each of these 3 types of media. Before we get into the actual applications of data science, lets first broadly break down the journey of a prospect into stages.The Different Stages of Customer AcquisitionThere are three journeys involved in customer acquisition.Our end objective of a successful campaign is to show ads to only those prospect that have a strong propensity of being a profitable customer.Looks simple, right? But in the real world, it really isnt! Heres why:As you might have guessed, it becomes very difficult to collaborate between all the three stages to reach ensure a successful campaign. Lets look at a a real-life scenario that will make this challenge a lot more clearer.Company X sells life insurance and acquires 50% of customers through paid digital media. A typical life insurance starts low on profit because of all the overhead costs involved, and gradually increases to maximum profit within 2 to 4 years. Year 5 onward, good customers start leaving the insurance to pick up better deals in the market, and hence the profit starts to decline. See this illustrative table that shows year wise profit:Just relying on the average can so often be deceiving. It seems like every acquisition is worth (lifetime value) $1,130 to your company, but all customers are different and as analysts, we try to find the most profitable segment. Now suppose you found that there is one attribute, total number of insurance policies held by the customer till date, that can strongly separate highly profitable customers from others. Here is how simple this rule can be:Now, here is the challenge. To stay compliant with the regulatory authority, you cannot deny a policy just because of profitability reasons. Hence, the only way you can craft your acquisition portfolio is if you can control who is looking at your ads. For a simplistic view, lets say 50% of your acquisition comes from your website directly and the remaining comes through Googles Paid Search. Acquisition coming directly to your site is hard to control as these prospects are already down the conversion funnel.So all you can do is control who looks at your paid search ads. Controlling paid search targeting looks straight forward  just tell Google to show ads to those people with #insurance policy <= 2. But there are a few challenges in doing this:So you see the challenge  the variable that is available for real-time targeting by ad server companies is not available with you to link profitability. Similarly, companies cannot make their profitability linked information available to Google to do specific targeting. Even though such direct targeting is difficult with paid media, we still have a lot of ways to get around this challenge.We will talk more about these targeting audience methods later in this article. Before we move on to targeting audience methods on digital media, lets get some jargon out of the way. The best way to understand these jargon terms is to put them in contrast against each other.When a common person thinks about Google, we think about the Search Engine (well, at least I do!). When we search for a keyword, we see a list of links. Advertisers have to pay for sponsored ads that appear at the top of the rankings. The list of links that come in organic search are free of cost and the rank is determined by Googles proprietary Page Rank algorithm. In the picture below, Adobe has paid for the sponsored ad, whereas the Google Analytics link below it is an organic search.The Google Search network is far bigger than just the search engine. It currently includes:What about Google Display Network (GDN)?In addition to the search network, Google also partners with 2 million sites that can publish ad banners on their site. GDN has a huge coverage of about 90% internet users. The biggest challenge with GDN is the low response rate because these visitors are not really looking out for your product. These visitors are simply looking at news, or the weather or are watching a video, etc.So, we classify such a visitor as a top-of-the-funnel prospect as he/she has a long way to go before making a purchase. Another challenge with this display network is attribution. Because a customer might view your ad today and make a purchase significantly later, attribution of these purchases becomes very subjective. GDN is very successful for brand awareness and multi-touch acquisitions (which requires a customer to see your products ad multiple times before making the final purchase). You might see GDN ads on your favorite sites if they partner with Google. Heres an example:You should now have an understanding of Googles core business of advertising. This is the majority revenue generator for Google. Every company wants to partner with Google to advertise their product/service. But how does Google choose which ads have to be displayed and when? Lets review the Google Ad auction briefly before we jump to the digital marketing jargon.Youll be amazed to know how Google does this. Lets try to understand the logic through an illustrative example.I searched for the term insurance liberty mutual on Google Search. At this point, Google hosts a live auction for this one instance ad inventory. This auction will be conducted in mili/microseconds and the winners will get the top positions on the sponsored ad slots. Is a high bid all you need to get the top spots? Of course not. Because my search was specifically for liberty mutual insurance, no other competitor should ideally get the top spot. Otherwise Googles customer will have a hard time getting to the relevant links (the core skill Google offers). Following is the result for an actual Google search of the term:As you can see, insurance-quote-instantly also participated in this auction but did not win against liberty mutual. So how does Google bring in this dimension of relevancy to remove monetary bias? Google has a concept of quality score for each rank which is kind of a page rank score on organic search. This quality score is then multiplied by the bid to calculate the ad score. This ad score is finally used to rank order the ads for a search instance.Here is a simplistic view of what goes on behind the scenes and how advertisers are charged for their ads:Couple of things you should notice in this Google Auction model:CPC vs CPM bidding  Quick note of the types of bidding we do on both search and display networks. CPC bidding is when the advertiser wants to bid on how much they are willing to pay for a click. CPM bidding is based on per 1000 impressions. It really depends on the type of objective an advertiser is looking to achieve. If the objective of the advertiser is branding, all that matters is impressions. However, if the advertiser wants customers to progress in the conversion funnel, they will generally bid on CPC.One exception here are multi-view purchases, where a customer generally takes a decision after looking at an ad many times. In such cases, the advertiser might bid on CPM even when the objective is conversion.Simply put  Adwords is used by advertisers to post ads on Googles display and search networks. Adsense is used by publishers to monetize their content. Adwords manages the demand side of ad inventory whereas Adsense manages the supply side. The below picture illustrates this concept well:Even though Google does the heavy lifting for both the publisher and advertiser in terms of ad serving, it still provides many tools to match the right ad to the right placement. This is where analytics plays such a crucial role. Google delivers Ads to Ad Spaces with publishers through 3 methods:What levers do Publishers have to subset in the pool bidding for their ad space?What levers do Advertisers have to subset in the pool bidding for their ad space?We will cover this topic in our section on Google Analytics later. For now, just note that you have levers on Demographic and Behavioral attributes you can use to target ad inventory. Further, you can choose keywords to make a precise selection of the auction you wish to participate in.DoubleClick is an ad-serving company that was bought by Google for $3.1 billion in 2008. In simple words, DoubleClick for Publishers makes it easier for publishers to monitize their content. It also provides effective tracking of how your content is performing in context of advertisements.DoubleClick for Advertisers, on the other hand, helps advertisers optimize their Search and Display campaigns. Google recently rebranded DFP as Google Ad Manager. DFP comes out much stronger than AdSense when you wish to manage your ads beyond Google Display Network  including Affiliated Brands, Real-time Ad exchanges, etc. Lets see a few examples of how we analysts use the ad tracking done by DFP.Suppose you own a travel blog and are using DFP to manage your ad inventory. There are 4 primary sections in your blog  Food, Destinations, People and Latest Deals. On each blog page, you have 3 banners  Leaderboard, Skyscraper and Square. The below picture will help you visualize each of these positions on your blog page:The below table covers the key metrics that DoubleClick publishes by default:If you use DFA with Google Analytics 360, you will get endless dimensions. However, Google Analytics 360 requires you to part with a significant amount, not something everyone can afford. So lets stick to some of the basic dimensions and what we can do with them. Here are a few dimensions which you can leverage to analyze your sites performance with respect to ad revenue:These dimensions are just examples of dimensions available to you for slicing and dicing information. Lets take a look at a dimension metric view to learn more.Starting with a basic view to see how each category is performing:Clearly, the food section brings in the majority of the revenue (50%+) even though the destination category gets most of the impressions. The destination category has both click through rate (CTR) and revenue per 1000 impressions at the lowest value, indicating our ads are not being optimized well for this category. Seems like quite a huge opportunity, right? Breaking down the destination category further by the traffic sources gives the following results:The above table shows that FB is the major source of traffic for our destination category. However, this source performs sub-optimal on both CTR and Revenue. This narrows down our search further. Are we presenting different information than what customers coming from Facebook are expecting? We can look out for this information by checking the bounce rate for this audience. We will hold that discussion for later. Let us also review how each banner type is performing across pages:Leaderboard (LB) has the maximum number of impressions, which makes sense as it comes on the top and should appear during most visits. Skyscraper has a low CTR and revenue per 1000 impressions, indicating these banners might not be completely visible (definitely scope of improvement there). We can further deep dive into this analysis with DoubleClick for effective targeting, but well keep that for a future article.DoubleClick is a suite of product provided by Google. The below diagram will help explain the types of services DoubleClick provides in the world of digital media:DoubleClick Search is primarily used by advertisers to manage their ads on multiple Search Engines, including Google Search Network. DoubleClick Bid Manager is used to manage Display Ads across the Google Display Network and Real-Time Bidding platforms. DoubleClick Ad Exchange is like the New York Stock Exchange where ad inventory is bought and sold. DoubleClick also provides campaign management services like DoubleClick Studio.Cookies are small text files that are placed on a visitors machine through websites they browse. A cookie contains some key information that can be used when the visitor returns. For instance, a lot of sites use cookies to save ID and passwords. Others use them to refill the checkout cart when the visitor returns. These are primarily first-party cookies.Services like DoubleClick, LiveRamp, etc. place a 3rd party cookie that they use to track user activity across the web space. An important thing to note here is that you can only read those cookies that you have placed, because all cookies have unique properties. For example, Amazon cannot read a cookie that Wells Fargo has placed in a visitors browser.Pixels (tags) are typically an invisible single pixel. These pixels fire, or a JAVA code executes (both mean the same thing), when the webpage is loaded and capture important information about the visitor. Pixels can also place a new cookie in a visitors browser or check if there is an existing cookie already there. Websites need to embed this Java script in the site source code, which looks something like this:Tag containers can contain multiple pixels or tags. One tag can trigger another set of tags and so on. Hence, containers are used to make conditional decisions that can determine if a set of pixels should fire or not.Google Tag Manager (GTM):By now, you would have realized that the digital world is all about maintaining these tags. Why? So you can measure campaigns and create new audiences for prospecting.Here is a simple example  Tom runs a food blog. He actively markets on paid search using Adwords. He also re-markets his customer to return to his blog if the customer has not come back for some time. He additionally tracks his online traffic using Google Analytics. He even leverages DoubleClick to target display ads beyond Google Network. Imagine the number of tags Tom might have to put on his site  one for Adwords, one for re-marketing, one for DoubleClick floodlight and some custom DMP/DSP tags for specific re-marketing campaigns.Handling so many tags within the source code of the site is extremely risky for Toms blog. Google Tag Manager is a solution to this problem. Google Tag Manager provides a single Java script which Tom needs to put in his source code which will allow him to manage all the tags to his site straight from the GTM interface. Additionally, Tom does not need to inform his IT team when making small changes to the tags because tags and the site source code are two separate entities superficially linked by Google Tag Manager. Below is a list of tags that Google Tag Manager can manage for you:Data layer is another key concept you should know. This component is what makes Google Tag Manager such a powerful tool. Simply put, you can think of it as a bridge for data between your site and GTM. GTM can do a two-way communication with the data layer. GTM will then make this data available for all the tags sitting in it.Think of the additional capability this adds to the dynamics! Now your marketing tags and analytics tag can directly pull segments of visitors you have created with on-site and DMP data. This can help you achieve the same level of targetability on off-site as that of on-site. The below schematic will make the process clear:Quick note on Floodlight tags:DoubleClick uses floodlight tags to note visitor activities and sales. They provide two types of tags  FL Counter and FL Sales. FL Counter is primarily used to link some conversion to ad exposure. Hence, it is used to track subscriptions after an ad click from a visitor. FL Sales is primarily to store transactions with a dollar value. It can help you optimize your campaign on total ad spends instead of maximizing conversions.I must confess this is the most confusing and difficult aspect of digital marketing. Feel free to skip this section if it becomes too complex. I will try my best to make these concepts as simple as possible. All these terms are related to technologies used primarily in display media. So before getting into these technologies, lets first try to understand the display media ecosystem.Search media is always bought through programmatic buying, however, display media can be bought directly or programmatically. Programmatic buying is basically technology assisted buying of media. Consider the following scenario:Victor runs a very popular travel blog. He gets about a million visits a month. He now wants to monetize his blog by putting an ad banner at the bottom of the article in the square placement. Out of the million visits, 500k visits will be on blogs that are related to hotel stays and the remaining 500k visits on blogs related to other topics. Victor wants the hotel blogs to specifically have Hotel related ads. He leverages various ad selling options to sell all his ad spaces.The above list is in the priority order of how ad inventory is distributed. The below diagram provides more clarity on the hierarchy:To make this entire process possible, we use technologies like DSP, SSP and Ads Exchange. Here is a schematic of how it generally works:As you can clearly see from the diagram, Direct Buy is done without any trading involved. Programmatic Guaranteed Selling is through DSP to the Publisher. If the inventory is unsold in these two options, it will be made available as Preferred Ads buyer, before it goes into the pool of Ads Exchange. Finally, we go through the private auction and RTB stages respectively.Google DoubleClick Bid Manager is a Demand Side platform. However, Adwords is not truly a Demand Side platform because of multiple reasons  it restricts the ad inventory to only the Google Display Network. There is no option for programmatic Direct Buy or private auctions.Data Management Platform is at the heart of analytics for Display Marketing. Lets try to understand it with an example. First, a quick concept  Remarketing is a way to win back a prospect who has visited your website but has not made an immediate purchase or enquiry. You might have witnessed Amazon ads following you on the web  thats remarketing. XYZ co. is an e-commerce company that wants to create a niche audience for display marketing. They only want to use the display channel for customers:The remarketing ad will be an offer of 5% off on this Laptop A. XYZ wants to publish these ads by placing bids on Ad Exchanges. How can XYZ execute such complex targeting?Short Answer  through a Data Management Platform (DMP).If you want to know the technical details of how this can be done, continue reading this section. Otherwise, all you need to know is that DMP can make specific targeting possible. DMP also provides third-party data, like preferences, interests, etc. You can skip the rest of this section if you want to ignore the technical details.Here is what actually goes on behind the scenes:The above 11 steps happen in less than a mini-second! DMP is used for many different use cases, such as:This is the concept that fascinated and scared me the most. We already know that the entire web world is tracking visitors through cookies. Cookies have been around for a long time. Using cookie DoubleClick can really stitch a persona  for instance, a visitor that reads AV blogs clicks on an online analytics course, but does not convert. The same visitor comes back to another university analytics course and as he/she was tagged as an analytics enthusiast, this new university gave him/her a discount of 20% on the course through on-site optimization. The visitor finally converts and complets the course in 60 days. And so on.So, essentially we know everything about the visitor. Whats left? A very key information, which is, This visitor is John Bell.What is so important about this new information? This is a Personal Identification Information (PII) and will never change over time. Cookies get deleted all the time. But if we can link these cookies to PII, we not only have a persona that can describe a small subset of the population, but we directly have an individual that only describes one person on the planet.This is exactly what an Identity Resolution does. Lets try to get a broad idea of how Identity Resolution works in the industry currently:Given all the background information, lets now talk about the various tools you have at your disposal that can process huge amounts of data coming from a number of channels. These help you analyze the data and implement your strategy in real-time. Even though there are many solutions in the market, most of the companies (big or small) are using one of the two  Google Analytics or Adobe Analytics. Lets review them briefly by comparing a few key attributes:Both the tools have their pros and cons. If you are a small scale company, choosing Google Analytics is a no-brainer. Even if you are a large corporate, the choice is tricky because Adobe on one hand gives active support, but Google integrates seamlessly with well-managed ad inventory. Note that Adobe can also integrate third-party tools for ad targeting, but not all of them are well-managed and might be prone to bot attacks, thus wasting your ad spends.You might have realized that each concept we covered in this guide is closely linked with each other. A comprehensive view is very important to appreciate the entire digital ecosystem. Trust me, it was impossible to find all this information in one place. As a data scientist, we do the hard part of learning all these concepts on the job. So I decided to put everything in one place with the aim of helping any future analyst get up to speed really quickly and keep up with the pace of this dynamically evolving industry.With the knowledge provided in this article, you will not only understand how to build a successful digital analytics driven strategy, but will also start appreciating how your strategy fits into the broader world of digital marketing. This combination of knowledge is extremely rare in the industry because most professionals focus on only one of these aspects. What it takes to create a successful marketing campaign on digital media lies at the intersection of the digital ecosystem, marketing and analytics.",https://www.analyticsvidhya.com/blog/2018/12/guide-digital-marketing-analytics/
WNS Hackathon Solutions by Top Finishers,Learn everything about Analytics|Introduction|Table of Contents|About the Competition  WNS Analytics Wizard 2018|Problem Statement|Winners Solutions|End Notes,"Rank 1  Siddharth Srivastava|Rank 2  Team Cheburek|Rank 3  Team kamakals|Share this:|Like this:|Related Articles|A Comprehensive Guide to Digital Marketing and Analytics Every Data Science Professional Must Read|Building a Face Detection Model from Video using Deep Learning (Python Implementation)|
Aishwarya Singh
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How do you prefer learning a machine learning technique? First get to know how it works on paper and then apply it? Or get your hands dirty straight away by learning the practical side? I prefer the latter  theres nothing like ingraining a concept by right away applying it and watching it in action.Participating in online hackathons, preparing and tuning our models, and competing against fellow top participants can help us evaluate our performance and understand the area where need to improve.There is always something new to learn, and someones unique approach to learn from!At the end of each hackathon, we eagerly wait for the final rankings and look forward to the winning solutions so that we can learn, improve and prepare ourselves for the next hackathon (and perhaps the next project?). We recently conducted the WNS Analytics Wizard 2018, which received an overwhelming response  3800+ registrations, 364 teams, and more than 1,200 submissions!Here is a glimpse of the solutions provided by the folks who finished in the top echelons for the WNS online hackathon conducted on 14  16 September, 2018.The WNS Analytics wizard is a one-of-its-kind online analytics hackathon conducted by WNS, a leading global business process management company. WNS offers business value to 350+ global clients by combining operational excellence with deep domain expertise in key industry verticals. This hackathon was aimed at giving budding data wizards the exciting opportunity to get a sneak peek into real-life business scenarios.We received more than 3800 registrations and a total of 1215 submissions during this 2-day online hackathon. The top finishers for WNS Hackathon are-Lets have a look at the problem statement for WNS HackathonThe problem statement for WNS Analytics Wizard was based on a real-life business use case. As a part of the WNS hackathon, participants were required to classify whether an employee should be promoted or not, by looking at employees past and current performance. Train consisted of 54808 rows and test had 23490 rows. Raw dataset contained 12 features.An employee is first nominated for promotion (based on his/her previous performance) and then goes through a training process. The task is to predict whether a potential promotee in the test set will be promoted or not after the evaluation process. The multiple attributes provided to the participants include the employees past performance, education level, years of experience, previous years rating , number of trainings taken in the last year, training score etc.Now that we have an understanding about the problem statement, lets have a look at the approaches shared by the winners.Siddharth followed a well structured approach that helped him secure the first position in the competition. He has divided the approach into three broad categories  model selection, feature engineering and hyperparameter tuning. Siddharth has listed down the steps he followed Model SelectionFeature Engineering and Missing value imputation
Hyperparameter tuning:Here is the complete code for the above mentioned approach : Rank1 SolutionThe second position was grabbed by Nikita Churkin & Dmitrii Simakov. According to the team, their approach is based on a good validation scheme, careful feature selection and strong regularization. Below is a detailed explanation of the same.ValidationFeature Generation and SelectionModel BuildingThe code for rank 2 solution is shared here.harshsarda29 & maverick_kamakal participated in WNS hackathon as a team and secured the third position. They used a 5 fold stratified scheme for validation and prepared three models XGBoost, LightGBM, CatBoost. Here is a complete step by step approach followed by the team:Missing value imputationCategorical Encodings:Hyper-parameter Finding:Feature Engineering:Model Building:The final model was an ensemble of 29 models consisting of:Here is the link to the code for Rank 3The solutions shared above is a proof that the winners have put in great efforts and truly deserve the rewards for the same. They came up with some innovative solutions and had a well structured approach.I hope you find these solutions useful and have learnt some key takeaways which you can implement in the upcoming hackathons! Register yourself in the upcoming hackathons at DataHack Platform.",https://www.analyticsvidhya.com/blog/2018/12/wns-hackathon-solutions-by-top-finishers/
Building a Face Detection Model from Video using Deep Learning (Python Implementation),Learn everything about Analytics|Introduction|Table of Contents|Promising Applications of Face Detection|Setting up the System  Hardware/Software Requirements|Lets Dive into the Implementation|Projects||Conclusion,"Step 1: Hardware Setup|Step 2: Software Setup|Step 2.1: Install Python|Step 2.2: Install OpenCV|Step 2.3: Install face_recognition API;|Simple Walkthrough|Face Detection Use Case|Share this:|Like this:|Related Articles|WNS Hackathon Solutions by Top Finishers|A Practical Guide to Object Detection using the Popular YOLO Framework  Part III (with Python codes)|
Faizan Shaikh
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computer vision and machine learning have really started to take off, but for most people, the whole idea of what a computer is seeing when its looking at an image is relatively obscure.  Mike KreigerThe wonderful field of Computer Vision has soared into a league of its own in recent years. There are an impressive number of applications already in wide use around the world  and we are just getting started!One of my favorite things in this field is the idea of our community embracing the concept of open source. Even the big tech giants are willing to share new breakthroughs and innovations with everyone so that the techniques do not remain a thing of the rich.One such technology is face detection, which offers a plethora of potential applications in real-world use cases (if used correctly and ethically). In this article, I will show you how to build a capable face detection algorithm using open source tools. Here is a demo to get you excited and set the stage for what will follow:So, are you ready? Read on then!Note: If you want to understand the intricacies of computer vision, this course  Computer Vision using Deep Learning  is the perfect place to start.Let me pull up some awesome examples of applications where face detection techniques are being popularly used. Im sure you must have come across these use cases at some point and not realized what technique was being used behind the scenes!For instance, Facebook replaced manual image tagging with automatically generated tag suggestions for each picture that was uploaded to the platform. Facebook uses a simple face detection algorithm to analyze the pixels of faces in the image and compare it with relevant users. Well learn how to build a face detection model ourselves, but before we get into the technical details of that, lets discuss some other use cases.We are becoming used to unlocking our phones with the latest face unlock feature. This is a very small example of how a face detection technique is being used to maintain the security of personal data. The same can be implemented on a larger scale, enabling cameras to capture images and detect faces.There are a few other lesser known applications of face detection in advertising, healthcare, banking, etc. Most of the companies, or even in many conferences, you are supposed to carry an ID card in order to get entry. But what if we could figure out a way so that you dont need to carry any ID card to get access? Face Detection helps in making this process smooth and easy. The person just looks at the camera and it will automatically detect whether he/she should be allowed to enter or not.Another interesting application of face detection could be to count the number of people attending an event (like a conference or concert). Instead of manually counting the attendees, we install a camera which can capture the images of the attendees and give us the total head count. This can help to automate the process and save a ton of manual effort. Pretty useful, isnt it?You can come up with many more applications like these  feel free to share them in the comments section below.In this article, I will focus upon the practical application of face detection, and just gloss over upon how the algorithms in it actually work. If you want to know more about them, you go through this article.Now that you know the potential applications you can build with face detection techniques, lets see how we can implement this using the open source tools available to us. Thats the advantage we have with our community  the willingness to share and open source code is unparalleled across any industry.For this article specifically, heres what I have used and recommend using:Lets explore these points in a bit more detail to ensure everything is set up properly before we build our face detection model.The first thing you have to do is check if the webcam is setup correctly. A simple trick in Ubuntu  see if the device has been registered by the OS. You can follow the steps given below:The code in this article is built using Python version 3.5. Although there are multiple ways to install Python, I would recommend using Anaconda  the most popular Python distribution for data science. Here is a link to install Anaconda in your system.OpenCV (Open Source Computer Vision) is a library aimed at building computer vision applications. It has numerous pre-written functions for image processing tasks. To install OpenCV, do a pip install of the library:Now that you have setup your system, its finally time to dive in to the actual implementation. First, we will quickly build our program, then break it down to understand what we did.First, create a file face_detector.py and then copy the code given below:Then, run this Python file by typing:If everything works correctly, a new window will pop up with real-time face detection running.To summarize, this is what our above code did:Simple, isnt it? If you want to go into more granular details, I have included the comments in each code section. You can always go back and review what we have done.The fun doesnt stop there! Another cool thing we can do  build a complete use case around the above code. And you dont need to start from scratch. We can make just a few small changes to the code and were good to go.Suppose, for example, you want to build an automated camera-based system to track where the speaker is in real-time. According to his position, the system rotates the camera so that the speaker is always in the middle of the video.How do we go about this? The first step is to build a system which identifies the person(s) in the video, and focuses on the location of the speaker.Lets see how we can implement this. For this article, I have taken a video from Youtube which shows a speaker talking during the DataHack Summit 2017 conference.First, we import the necessary libraries:Then, read the video and get the length:After that, we create an output file with the required resolution and frame rate which is similar to the input file.Load a sample image of the speaker to identify him in the video:All this completed, now we run a loop that will do the following:Lets see the code for this:The code would then give you an output like this:What a terrific thing face detection truly is. Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:Congratulations! You now know how to build a face detection system for a number of potential use cases. Deep learning is such a fascinating field and Im so excited to see where we go next.In this article, we learned how you can leverage open source tools to build real-time face detection systems that have real-world usefulness. I encourage you to build plenty of such applications and try this on your own. Trust me, theres a lot to learn and its just so much fun!As always, feel free to reach out if you have any queries/suggestions in the comment section below!",https://www.analyticsvidhya.com/blog/2018/12/introduction-face-detection-video-deep-learning-python/
A Practical Guide to Object Detection using the Popular YOLO Framework  Part III (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|What is YOLO and Why is it Useful?|How does the YOLO Framework Function?|How to Encode Bounding Boxes?|Intersection over Union and Non-Max Suppression|Anchor Boxes|Combining the Ideas|Implementing YOLO in Python|End Notes,"Share this:|Like this:|Related Articles|Building a Face Detection Model from Video using Deep Learning (Python Implementation)|5 Best Machine Learning GitHub Repositories & Reddit Discussions (November 2018)|
Pulkit Sharma
|34 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How easy would our life be if we simply took an already designed framework, executed it, and got the desired result? Minimum effort, maximum reward. Isnt that what we strive for in any profession?I feel incredibly lucky to be part of our machine learning community where even the top tech behemoths embrace open source technology. Of course its important to understand and grasp concepts before implementing them, but its always helpful when the ground work has been laid for you by top industry data scientists and researchers.This is especially true for deep learning domains like computer vision. Not everyone has the computational resources to build a DL model from scratch. Thats where predefined frameworks and pretained models come in handy. And in this article, we will look at one such framework for object detection  YOLO. Its a supremely fast and accurate framework, as well see soon.So far in our series of posts detailing object detection (links below), weve seen the various algorithms that are used, and how we can detect objects in an image and predict bounding boxes using algorithms of the R-CNN family. We have also looked at the implementation of Faster-RCNN in Python.In part 3 here, we will learn what makes YOLO tick, why you should use it over other object detection algorithms, and the different techniques used by YOLO. Once we have understood the concept thoroughly, we will then implement it it in Python. Its the ideal guide to gain invaluable knowledge and then apply it in a practical hands-on manner.I highly recommend going through the first two parts before diving into this guide:The R-CNN family of techniques we saw in Part 1 primarily use regions to localize the objects within the image. The network does not look at the entire image, only at the parts of the images which have a higher chance of containing an object.The YOLO framework (You Only Look Once) on the other hand, deals with object detection in a different way. It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes. The biggest advantage of using YOLO is its superb speed  its incredibly fast and can process 45 frames per second. YOLO also understands generalized object representation.This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms. In the upcoming sections, we will learn about different techniques used in YOLO algorithm. The following explanations are inspired by Andrew NGs course on Object Detection which helped me a lot in understanding the working of YOLO.Now that we have grasp on why YOLO is such a useful framework, lets jump into how it actually works. In this section, I have mentioned the steps followed by YOLO for detecting objects in a given image.Pretty straightforward, isnt it? Lets break down each step to get a more granular understanding of what we just learned.We need to pass the labelled data to the model in order to train it. Suppose we have divided the image into a grid of size 3 X 3 and there are a total of 3 classes which we want the objects to be classified into. Lets say the classes are Pedestrian, Car, and Motorcycle respectively. So, for each grid cell, the label y will be an eight dimensional vector:Here,Lets say we select the first grid from the above example:Since there is no object in this grid, pc will be zero and the y label for this grid will be:Here, ? means that it doesnt matter what bx, by, bh, bw, c1, c2, and c3contain as there is no object in the grid. Lets take another grid in which we have a car (c2 = 1):Before we write the y label for this grid, its important to first understand how YOLO decides whether there actually is an object in the grid. In the above image, there are two objects (two cars), so YOLO will take the mid-point of these two objects and these objects will be assigned to the grid which contains the mid-point of these objects. The y label for the centre left grid with the car will be:Since there is an object in this grid, pcwill be equal to 1. bx, by, bh, bw will be calculated relative to the particular grid cell we are dealing with. Since car is the second class, c2 = 1 and c1 and c3 = 0. So, for each of the 9 grids, we will have an eight dimensional output vector. This output will have a shape of 3 X 3 X 8.So now we have an input image and its corresponding target vector. Using the above example (input image  100 X 100 X 3, output  3 X 3 X 8), our model will be trained as follows:We will run both forward and backward propagation to train our model. During the testing phase, we pass an image to the model and run forward propagation until we get an output y. In order to keep things simple, I have explained this using a 3 X 3 grid here, but generally in real-world scenarios we take larger grids (perhaps 19 X 19).Even if an object spans out to more than one grid, it will only be assigned to a single grid in which its mid-point is located. We can reduce the chances of multiple objects appearing in the same grid cell by increasing the more number of grids (19 X 19, for example).As I mentioned earlier, bx, by, bh, and bw are calculated relative to the grid cell we are dealing with. Lets understand this concept with an example. Consider the center-right grid which contains a car:So, bx, by, bh, and bw will be calculated relative to this grid only. The y label for this grid will be:pc = 1 since there is an object in this grid and since it is a car, c2 = 1. Now, lets see how to decide bx, by, bh, and bw. In YOLO, the coordinates assigned to all the grids are:bx, by are the x and y coordinates of the midpoint of the object with respect to this grid. In this case, it will be (around) bx = 0.4 and by = 0.3:bhis the ratio of the height of the bounding box (red box in the above example) to the height of the corresponding grid cell, which in our case is around 0.9. So, bh = 0.9. bw is the ratio of the width of the bounding box to the width of the grid cell. So, bw = 0.5 (approximately). The y label for this grid will be:Notice here that bx and by will always range between 0 and 1 as the midpoint will always lie within the grid. Whereas bh and bw can be more than 1 in case the dimensions of the bounding box are more than the dimension of the grid.In the next section, we will look at more ideas that can potentially help us in making this algorithms performance even better.Heres some food for thought  how can we decide whether the predicted bounding box is giving us a good outcome (or a bad one)? This is where Intersection over Union comes into the picture. It calculates the intersection over union of the actual bounding box and the predicted bonding box. Consider the actual and predicted bounding boxes for a car as shown below:Here, the red box is the actual bounding box and the blue box is the predicted one. How can we decide whether it is a good prediction or not? IoU, or Intersection over Union, will calculate the area of the intersection over union of these two boxes. That area will be:IoU = Area of the intersection / Area of the union, i.e.IoU = Area of yellow box / Area of green boxIf IoU is greater than 0.5, we can say that the prediction is good enough. 0.5 is an arbitrary threshold we have taken here, but it can be changed according to your specific problem. Intuitively, the more you increase the threshold, the better the predictions become.There is one more technique that can improve the output of YOLO significantly  Non-Max Suppression.One of the most common problems with object detection algorithms is that rather than detecting an object just once, they might detect it multiple times. Consider the below image:Here, the cars are identified more than once. The Non-Max Suppression technique cleans up this up so that we get only a single detection per object. Lets see how this approach works.1. It first looks at the probabilities associated with each detection and takes the largest one. In the above image, 0.9 is the highest probability, so the box with 0.9 probability will be selected first:2. Now, it looks at all the other boxes in the image. The boxes which have high IoU with the current box are suppressed. So, the boxes with 0.6 and 0.7 probabilities will be suppressed in our example:3. After the boxes have been suppressed, it selects the next box from all the boxes with the highest probability, which is 0.8 in our case:4. Again it will look at the IoU of this box with the remaining boxes and compress the boxes with a high IoU:5. We repeat these steps until all the boxes have either been selected or compressed and we get the final bounding boxes:This is what Non-Max Suppression is all about. We are taking the boxes with maximum probability and suppressing the close-by boxes with non-max probabilities. Lets quickly summarize the points which weve seen in this section about the Non-Max suppression algorithm:There is another method we can use to improve the perform of a YOLO algorithm  lets check it out!We have seen that each grid can only identify one object. But what if there are multiple objects in a single grid? That can so often be the case in reality. And that leads us to the concept of anchor boxes. Consider the following image, divided into a 3 X 3 grid:Remember how we assigned an object to a grid? We took the midpoint of the object and based on its location, assigned the object to the corresponding grid. In the above example, the midpoint of both the objects lies in the same grid. This is how the actual bounding boxes for the objects will be:We will only be getting one of the two boxes, either for the car or for the person. But if we use anchor boxes, we might be able to output both boxes! How do we go about doing this? First, we pre-define two different shapes called anchor boxes or anchor box shapes. Now, for each grid, instead of having one output, we will have two outputs. We can always increase the number of anchor boxes as well. I have taken two here to make the concept easy to understand:This is how the y label for YOLO without anchor boxes looks like:What do you think the y label will be if we have 2 anchor boxes? I want you to take a moment to ponder this before reading further. Got it? The y label will be:The first 8 rows belong to anchor box 1 and the remaining 8 belongs to anchor box 2. The objects are assigned to the anchor boxes based on the similarity of the bounding boxes and the anchor box shape. Since the shape of anchor box 1 is similar to the bounding box for the person, the latter will be assigned to anchor box 1 and the car will be assigned to anchor box 2. The output in this case, instead of 3 X 3 X 8 (using a 3 X 3 grid and 3 classes), will be 3 X 3 X 16 (since we are using 2 anchors).So, for each grid, we can detect two or more objects based on the number of anchors. Lets combine all the ideas we have covered so far and integrate them into the YOLO framework.In this section, we will first see how a YOLO model is trained and then how the predictions can be made for a new and previously unseen image.TrainingThe input for training our model will obviously be images and their corresponding y labels. Lets see an image and make its y label:Consider the scenario where we are using a 3 X 3 grid with two anchors per grid, and there are 3 different object classes. So the corresponding y labels will have a shape of 3 X 3 X 16. Now, suppose if we use 5 anchor boxes per grid and the number of classes has been increased to 5. So the target will be 3 X 3 X 10 X 5 = 3 X 3 X 50. This is how the training process is done  taking an image of a particular shape and mapping it with a 3 X 3 X 16 target (this may change as per the grid size, number of anchor boxes and the number of classes).TestingThe new image will be divided into the same number of grids which we have chosen during the training period. For each grid, the model will predict an output of shape 3 X 3 X 16 (assuming this is the shape of the target during training time). The 16 values in this prediction will be in the same format as that of the training label. The first 8 values will correspond to anchor box 1, where the first value will be the probability of an object in that grid. Values 2-5 will be the bounding box coordinates for that object, and the last three values will tell us which class the object belongs to. The next 8 values will be for anchor box 2 and in the same format, i.e., first the probability, then the bounding box coordinates, and finally the classes.Finally, the Non-Max Suppression technique will be applied on the predicted boxes to obtain a single prediction per object.That brings us to the end of the theoretical aspect of understanding how the YOLO algorithm works, starting from training the model and then generating prediction boxes for the objects. Below are the exact dimensions and steps that the YOLO algorithm follows:Time to fire up our Jupyter notebooks (or your preferred IDE) and finally implement our learning in the form of code! This is what we have been building up to so far, so lets get the ball rolling.The code well see in this section for implementing YOLO has been taken from Andrew NGs GitHub repository on Deep Learning. You will also need to download this zip file which contains the pretrained weights required to run this code.Lets first define the functions that will help us choose the boxes above a certain threshold, find the IoU, and apply Non-Max Suppression on them. Before everything else however, well first import the required libraries:Now, lets create a function for filtering the boxes based on their probabilities and threshold:Next, we will define a function to calculate the IoU between two boxes:Lets define a function for Non-Max Suppression:We now have the functions that will calculate the IoU and perform Non-Max Suppression. We get the output from the CNN of shape (19,19,5,85). So, we will create a random volume of shape (19,19,5,85) and then predict the bounding boxes:Finally, we will define a function which will take the outputs of a CNN as input and return the suppressed boxes:Lets see how we can use the yolo_eval function to make predictions for a random volume which we created above:How does the outlook look?scores represents how likely the object will be present in the volume. boxes returns the (x1, y1, x2, y2) coordinates for the detected objects. classes is the class of the identified object.Now, lets use a pretrained YOLO algorithm on new images and see how it works:After loading the classes and the pretrained model, lets use the functions defined above to get the yolo_outputs.Now, we will define a function to predict the bounding boxes and save the images with these bounding boxes included:Next, we will read an image and make predictions using the predict function:Finally, lets plot the predictions:Not bad! I especially like that the model correctly picked up the person in the mini-van as well.Heres a brief summary of what we covered and implemented in this guide:YOLO is one of my all-time favorite frameworks and Im sure youll see why once you implement the code on your own machine. Its a great way of getting your hands dirty with a popular computer vision algorithm. If you have any questions or feedback regarding this guide, connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/
5 Best Machine Learning GitHub Repositories & Reddit Discussions (November 2018),Learn everything about Analytics|Introduction|GitHub Repositories|Open AIs Deep Reinforcement Learning Resource|NVIDIAs WaveGlow|BERT as a Service|Python Implementation of Googles Quick Draw Game|Visualizing and Understanding GANs|Reddit Discussions|Why Gradient Descent is Even Needed in the First Place|Reverse Engineering a Massive Neural Network|Debate on TensorFlow 2.0 API|Reinforcement Learning with Prediction-Based Rewards|Landing that First Data Scientist Job|End Notes,"Share this:|Like this:|Related Articles|A Practical Guide to Object Detection using the Popular YOLO Framework  Part III (with Python codes)|Building a Random Forest from Scratch & Understanding Real-World Data Products (ML for Programmers  Part 3)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Coding is among one of the best things about being a data scientist. There are often days when I find myself immersed in programming something from scratch. That exhilarating feeling you get when you see your hard work culminate in a successful model? Exhilarating and unparalleled!But as a data scientist (or a programmer), its equally important to create checkpoints of your code at various intervals. Its incredibly helpful to know where you started off from last time so if you have to rollback your code or simply branch out to a different path, theres always a fallback option. And thats why GitHub is such an excellent platform.The previous posts in this monthly series have expounded on why every data scientist should have an active GitHub account. Whether its for collaboration, resume/portfolio, or educational purposes, its simply the best place to enhance your coding skills and knowledge.And now lets get to the core of our article  machine learning code! I have picked out some really interesting repositories which I feel every data scientist should try out on their own.Apart from coding, there are tons of aspects associated with being a data scientist. We need to be aware of all the latest developments in the community, what other machine learning professionals and thought leaders are talking about, what are the moral implications of working on a controversial project, etc. That is what I aim to bring out in the Reddit discussion threads I showcase every month.To make things easier for you, heres the entire collection so far of the top GitHub repositories and Reddit discussions (from April onwards) we have covered each month:Keeping our run going of including reinforcement learning resources in this series, heres one of the best so far  OpenAIs Spinning Up! This is an educational resource open sourced with the aim of making it easier to learn deep RL. Given how complex it can appear to most folks, this is quite a welcome repository.The repo contains a few handy resources:This one is for all the audio/speech processing people out there. WaveGlow is a flow-based generative network for speech synthesis. In other words, its a network (yes, a single network!) that can generate impressive high quality speech frommel-spectrograms.This repo contains the PyTorch implementation of WaveGlow and a pre-trained model to get you started. Its a really cool framework, and you can check out the below links as well if you wish to delve deeper:We covered the PyTorch implmentation of BERT in last months article, and heres a different take on it. For those who are new to BERT, it stands forBidirectionalEncoderRepresentations fromTransformers. Its basically a method for pre-training language representations.BERT has set the NLP world ablaze with its results, and the folks at Google have been kind enough to release quite a few pre-trained models to get you on your way.This repository uses BERT as the sentence encoder and hosts it as a service via ZeroMQ, allowing you to map sentences into fixed-length representations in just two lines of code. Its easy to use, extremely quick, and scales smoothly. Try it out!Quick, Draw is a popular online game developed by Google where a neural network tries to guess what youre drawing. The neural network learns from each drawing, hence increasing its already impressive ability to correctly guess the doodle. The developers have built up a HUGE dataset from the amount of drawings users have made previously. Its an open-source dataset which you can check out here.And now you can build your own Quick, Draw game in Python with this repository. There is a step-by-step explanation of how to do this. Using this code, you can run an app to either draw in front of the computers webcam, or on a canvas.GAN Dissection, pioneered by researchers at MITsComputer Science & Artificial Intelligence Laboratory, is a unique way of visualizing and understanding the neurons of Generative Adversarial Networks (GANs). But it isnt just limited to that  the researchers have also created GANPaintto showcase how GAN Dissection works.This helps you explore what a particular GAN model has learned by inspecting and manipulating its internal neurons. Check out the research paper here and the below video demo, and then head straight to the GitHub repository to dive straight into the code!Has this question ever crossed your mind while learning basic machine learning concepts? This is one of the fundamental algorithms we come across in our initial learning days and has proven to be quite effective in ML competitions as well. But once you start going through this thread, prepare to seriously question what youve studied previously.What started off as a straight forward question turned into a full-blown discussion among the top minds on Reddit. I thoroughly enjoyed browsing through the comments and Im sure anyone with interest in this field (and mathematical rigour) will find it useful.What do you do when the developer of a complex and massive neural network vanishes without leaving behind the documentation needed to understand it? This isnt a fictional plot, but a rather common situation the original poster of the thread found himself in.Its a situation that happens regularly with developers but takes on a whole new level of intrigue when it comes to deep learning. This thread explores the different ways a data scientist can go about examining how a deep neural network model was initially designed. The responses range from practical to absurd, but each adds a layer of perspective which could help you one day if you ever face this predicament.My attention to this thread was drawn by the sheer number of comments (110 at the time of writing)  what in the world could be so controversial about this topic? But when you started scrolling down, the sheer difference in opinions among the debators is mind boggling. Apart from TensorFlow being derided for being not the best framework, theres a lot of love being shown to PyTorch (which isnt all that surprising if youve used PyTorch).It all started when Francois Chollet posted his thoughts on GitHub and lit a (metaphorical) fire under the machine learning community.Another OpenAI entry in this post  and yet another huge breakthrough by them. The title might not leap out of the page as anything special but its important to understand what the OpenAI team have conjured up here. As one of the Redditors pointed out, this takes us one step closer to machines mimicking human behavior.It took around a year of total experience to beat the Montezumas Revenge game at a super human level  pretty impressive!This one is for all the aspiring data scientists reading the article. The author of the thread expounds on how he landed the coveted job, his background, where he studied data science from, etc. After answering these standard questions, he has actually written a very nice post on what others in a similar position can do to further their ambitions.There are some helpful comments as well if you scroll down a little bit. And of course, you can post your own question(s) to the author there.Quite a collection this month. I found the GAN Dissection repository quite absorbing. Im currently in the process of trying to replicate it on my own machine  should be quite the ride. Im also keeping an eye on the Reverse Engineering a Massive Neural Network thread as the ideas spawning there could be really helpful in case I ever find myself in that situation.Which GitHub repository and Reddit thread stood out for you? Which one will you tackle first? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/12/best-machine-learning-github-repositories-reddit-threads-november-2018/
Building a Random Forest from Scratch & Understanding Real-World Data Products (ML for Programmers  Part 3),Learn everything about Analytics|Introduction|Table of Contents|Introduction to Machine Learning: Lesson 6|Introduction to Machine Learning: Lesson 7|End Notes,"Machine learning Applications in Business|Random Forest Interpretation Techniques|Share this:|Like this:|Related Articles|5 Best Machine Learning GitHub Repositories & Reddit Discussions (November 2018)|Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python|
Aishwarya Singh
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Horizontal Market|Vertical Market,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As data scientists and machine learning practitioners, we come across and learn a plethora of algorithms. Have you ever wondered where each algorithms true usefulness lies? Are most machine learning techniques learned with the primary aim of scaling a hackathons leaderboard?Not necessarily. Its important to examine and understand where and how machine learning is used in real-world industry scenarios. Thats where most of us are working (or will eventually work). And thats what I aim to show in this part 3 of our popular series covering the fast.ai introduction to machine learning course!We covered a fairly comprehensive introduction to random forests in part 1 using the fastai library, and followed that up with a very interesting look at how to interpret a random forest model. The latter part is especially quite relevant and important to grasp in todays world.In this article, we will first take a step back and analyze machine learning from a business standpoint. Then well jump straight back to where we left off part 2  building a random forest model from scratch. I encourage you to hop back to the previous posts in case you need to refresh any concept, and carry that learning with you as we move forward.Having learned the basic underlying concept of a random forest model and the techniques used to interpret the results, the obvious follow-up question to ask is  where are these models and interpretation techniques used in real life? Its all well and good knowing the technique, but if we dont know where and when to apply it, it feels like a wasted effort.Jeremy Howard answers this question in Lesson #6, where he explains how a random forest model can be used to interpret and understand the data. This lesson is also a walkthrough covering all the techniques we have learned in the previous two articles.In this section, we will look at various sectors where machine learning has already made its presence felt and is being implemented successfully.The business market, as explained by Jeremy, can broadly be divided into two groups  horizontal and vertical. We will look at these individually, but first, lets understand the most important steps involved in designing a machine learning model.There are broadly four steps we follow for doing this. These collectively form the drivetrain approach, as explained by Jeremy in his paper:Designing Great Data Products.Step 1: Define the ObjectiveBefore diving into the challenge and building a machine learning model, one must have a clear, well-defined objective or an end goal in mind. This may vary depending on what the organization is trying to achieve. A couple of examples are given below:Step 2: LeversLevers are the inputs that can be controlled, or some changes the organization can make, to drive the objective defined in step 1. For instance, to ensure that the customers are satisfied:A machine learning model cannot be a lever, but it can help the organization identify the levers. Its important to understand this distinction clearly.Step 3: Data The next step is to find out what data can be helpful in identifying and setting the lever that the organization may have (or can collect). This can be different from the data already provided or collected by the organization earlier.Step 4: Predictive modelsOnce we have the required data that can be helpful in achieving the above defined goal, the last step is to build a simulation model on this data. Note that a simulation model can have multiple predictive models. For example, building one model identifying what items should be recommended to a user, and another model predicting the probability that a user buys a particular product on a recommendation. The idea is to create an optimization model, rather than a predictive model.You can read the paper I linked above to understand these steps in more detail. Well move on to understand the applications of machine learning from the industry and business point-of-view.As we alluded to earlier, we can divide the business market broadly into two groups  horizontal and vertical. I will elaborate on each in this section to give you an industry-level perspective on things.Horizontal markets are usually defined by demography (can be common across different kinds of business), which is broadly everything involving marketing. Here is a group of marketing applications where machine learning can (and is) be used.Taking the example of Churn, the goal is to determine who is going to leave or attrite. Suppose an organization has a churn model that predicts which employee is going to leave and what can be changed so that the number of employees leaving is reduced.Once the end goal is defined, we can make a list of things that can be changed in order to decrease the number of people who are leaving the organization and collect whatever data we need to build a model.Then we can create a random forest model and use the interpretation techniques we learned previously. For instance, the feature importance aspect from the random forest model can help us understand which features matter the most. Or the pdp plot visualization can be useful in determining how a particular change will affect the target variable (aka probability of an employee attriting).Vertical market refers to a group of businesses sharing the same industry, such as education, healthcare, aerospace, finance, etc. Below are a couple of examples where machine learning is being used in such cases:Its a good exercise to discuss the applications of machine learning in various domains and answer the following questions for each:Our discussion so far would have given you a fair idea about the plethora of machine learning applications in the industry. Well do a quick review of the random forest interpretation techniques and then continue building the random forest model from scratch after that.Well quickly recap these techniques since we have covered them in part 2. For a more detailed explanation, you can take a look at this article:Standard deviationWe calculated the standard deviation of the predictions (for each level in Enclosure and ProductSize) to figure out which categories are being wrongly predicted by the model and why. We found that for categories with a low value count, the model is giving a high standard deviation. So there are higher chances that the predictions for categories with a larger value count are more accurate (since the model is trained well for these categories). Makes sense, right?Feature ImportanceFeature importance is basically determining how important a feature is in predicting the target variable. The top 30 variables for the random forest model are the following:As it is evident from the above plot, YearMade is the most important variable. This makes sense because the older the vehicle, the lesser the saleprice. The model performance improved when the less important features were removed from the training set. This, as you can imagine, can be really helpful in understanding the data and variables. Additionally, we can use one-hot encoding to create columns for each level and then calculate the feature importance:Partial Dependence Plot (PDP)Partial dependence is used to understand the dependence of features on the target variable. This is done by predicting the target for each row, keeping a variable constant. For instance, predicting saleprice for each row when YearMade is 1960, then for YearMade 1961, and so on. The result would be a plot like this:Tree InterpreterThe tree interpreter is used to evaluate the predictions for each row using all the trees in the random forest model. This also helps us understand how much each variable contributed to the final prediction.Before we understand how the contribution for multiple trees is calculated, lets take a look at a single tree:The value of Coupler_System<=5 is 10.189, for Enclosure<=2 is 2.0, and model_id comes out to be 9.955 (considering only the top most path for now).The value for Enclosure <=2 is not only because of the feature Enclosure, but a combination of Coupler_System and Enclosure. In other words, we can say that Coupler_System interacted with Enclosure with a contribution of 0.156. Similarly, we can determine the interaction importance between features.Now we can use the average of all the trees in order to calculate the overall contribution by each feature. For the first row in the validation set, below are the contributions by each variable:Just a reminder that the calculation behind the values generated has been covered in the previous post.ExtrapolationFor this particular topic, Jeremy performs live coding during the lecture by creating a synthetic dataset using linespace. We have set the start and end points as 0 and 1.The next step is to create a dependent variable. For simplicity, we assume a linear relationship between x and y. We can use the following code to generate our target variable and plot the same:Well convert our 1D array into a 2D array which will be used as an input to the random forest model.Out of the 50 data points, well take 40 for training our random forest model and keep the remaining 10 to be used as the validation set.We can now fit a random forest model and compare the predictions against actual values.The results are pretty good, but do you think well get similar results on the validation set? We have trained our model on the first 40 data points, the scale of which is actually very different from that of the validation set. So any new point that the random forest model tries to predict, it inevitably identifies that these points are closer to the highest of the given 40 points.Lets have a look at the plot:This confirms our hunch that random forest cannot extrapolate to a type of data that it has never seen before. Itll basically give you the average of the data it has previously seen. So how should one deal with this type of data? We can potentially use neural nets which have proved to work better with such cases. Another obvious solution is to use time series techniques (which i have personnaly worked on and can confirm that they show far better results).So to conclude lesson #6, we covered the necessary steps involved in building a machine learning model and briefly looked at all the interpretation techniques we learned in the previous article. If you have any questions on this section, please let me know in the comments below the article.We started learning how to build a random forest model from scratch in the previous article. Well take it up from where we left off in this section (lesson #7). By the end of this lesson, youll be able to build an end-to-end random forest model from the ground up on your own. Sounds pretty exciting so lets continue!We have discussed the random forest algorithm in detail  from understanding how it works to how the split points are selected, and how the predictions are calculated. We are now going to put our understanding into code form, one step at a time, i.e., creating a model that works with few features, smaller number of trees, and a subset of the data.Note: Steps 1 to 6 have been covered in the previous article.Step 1:Importing the basic libraries.Step 2:Read the data and split into train and validation sets.Step 3:Take a subset of data to start with.As I previously mentioned, well take smaller steps, so here we pick only two features and work with them. If that works well, we can complete the model by taking all the features.Step 4:Define the set of inputs:Step 5:Define a function that uses a sample of data (with replacement) and creates a decision tree over the same.Step 6:Create a predict function. The mean of the predicted value from each tree for a particular row is returned as the final prediction.Combining all the above functions, we can create a class TreeEnsemble.Step 7:Create a class DecisionTree. We call DecisionTree in the function create_tree, so lets define it here. A decision tree would have a set of independent variables, a target variable, and the index values. For now, we have created only one decision tree (we can make it recursive later).Step 8:Determine the best split point. For every column, we use the function find_better_splitto identify a split point and then return the column name, value, and score for the split.Step 9:Build our first model with 10 trees, a sample size of 1,000 and minimum leaf as 3.For the first tree, the results are:Lets now fill the block that we left above in Step 8  find_better_split. This is by far the most complicated part of the code to understand, but Jeremy has explained using a simple example in excel. I will explain it in an intuitive manner here.For each variable, we split the points to the left and right node, and check the score for every value. The idea is to find a split point where we are able to separate more similar points together.Consider the following example: we have two columns  an independent variable which we try to split on, and a binary target variable.We will split at each value of the first column and calculate a standard deviation to identify how well we were able to classify the target. Lets suppose the first split point is >=3, and then calculate the standard deviation.We can take a weighted average of this value. Similarly, we can calculate for a split at 4, 6, 1 and so on. Lets put this into code:If we try to print the results of the function for both the columns individually, we get the below result:Looks like YearMade 1974 is a better split point.Step 10:Compare with the scikit-learn random forest. But keep in mind that theres a tricky aspect here. While comparing the two models, both of them should have the same input. So lets store the input that we have used in the random forest we just built.And now we build a model on this subset:We see that the split here is on the column YearMade at year 1974.5, very similar to the results of our model. Not bad!Step 11:Theres a problem with the code we have seen so far  can you recognize what it is? We need to optimise it!in the current format, we are checking the score for the split at each row, aka we are checking for a single value multiple times. Have a look at the example we used earlier:The function will check the split points 4 and 1 twice because it actually works row-wise. Its a good idea to optimise our code so as to reduce the computation time (not everyone has a top machine!). The idea is to sort column-wise and then check the score after the split for unique values only.Also, to calculate the standard deviation, we define the following function:We will need to keep a track of the count of data points on each side of the split along with the sum of square of the values. So we initialize the variables rhs_cnt, lhs_cnt, rjs_sum2 and lhs_sum2. Adding all this up, the code looks like this:Ideally, this function should give the same results. Lets check:Note that we created a new function (slightly changed the name from find_better_split to find_better_split_foo), and we need to use this in our DecisionTree class. The following command does it for us:Step 12:Build our tree with more than one split.In step 10, we compared the first level of our model with the scikit-learn random forest model. We will now create a full tree (which splits on both our features) and compare it again. Right now our find_varsplit function looks like this:where we have defined find_better_split separately. We will update this function, so that it automatically checks for the leaf node and stores a list of indices in lhs and rhs after the split.We will again compare both the models. Previously, the max_depth was restricted to 1, and we will make it 2 here (we have only two features for now):And now look at our results:According to the image above, lhs should have 159 samples and a value of 9.66, while rhs should have 841 samples and a value of 10.15.Everything looks perfect so far! Going one level deeper into the tree, the left side of lhs should consist of 150 samples:Great! We are able to build our own tree. Lets create a function to calculate the predictions and then well compare the r-square values.Step 13:Calculate the final predictions. We have called a predict function in TreeEnsemblethat returns the prediction for each row:With this, we have completed building our own random forest model. Lets plot the predictions on the validation set:Checking the performance and r-square against the scikit-learn model:Step 14:Putting it all together!And there you go! That was quite a learning experience and we have now officially built a machine learning technique right from scratch. Something to be truly proud of!Lets quickly recap what we covered in part 3. We started with Lesson 6 which broadly covers the applications of machine learning in various business domains and a revision of the interpretation techniques we saw in part 2.The second half of this article coveredLesson 7 and was a bit code heavy. We built a complete random forest model and compared its performance against the scikit-learns model. It is a good practice to understand how the model actually works, instead of simply implementing the model.With this, we come to the end of understanding, interpreting and building a random forest model. In the next part, we will shift our focus to neural networks. Well be working on the very popular MNIST dataset so that should be quite fun!",https://www.analyticsvidhya.com/blog/2018/12/building-a-random-forest-from-scratch-understanding-real-world-data-products-ml-for-programmers-part-3/
Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python,Learn everything about Analytics|Introduction|Table of Contents|The Advantage of Transfer Learning|Pre-trained Models in NLP|Overview of ULMFiT|Problem Statement|System Setup: Google Colab|Implementation in Python|Whats Next?|End Notes,"Import Required Libraries|Data Preprocessing|Fine-Tuning the Pre-Trained Model and Making Predictions|Share this:|Like this:|Related Articles|Building a Random Forest from Scratch & Understanding Real-World Data Products (ML for Programmers  Part 3)|Highlights from DataHack Summit 2018  a Truly Overwhelming and Resounding Success!|
Prateek Joshi
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Some major benefits of Colab:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing (NLP) needs no introduction in todays world. Its one of the most important fields of study and research, and has seen a phenomenal rise in interest in the last decade. The basics of NLP are widely known and easy to grasp. But things start to get tricky when the text data becomes huge and unstructured.Thats where deep learning becomes so pivotal. Yes, Im talking about deep learning for NLP tasks  a still relatively less trodden path. DL has proven its usefulness in computer vision tasks like image detection, classification and segmentation, but NLP applications like text generation and classification have long been considered fit for traditional ML techniques.Source: TryolabsAnd deep learning has certainly made a very positive impact in NLP, as youll see in this article. We will focus on the concept of transfer learning and how we can leverage it in NLP to build incredibly accurate models using the popular fastai library. I will introduce you to the ULMFiT framework as well in the process.Note- This article assumes basic familiarity with neural networks, deep learning and transfer learning. If you are new to deep learning, I would strongly recommend reading the following articles first:If you are a beginner in NLP, check out this video coursewith 3 real life projects.I praised deep learning in the introduction, and deservedly so. However, everything comes at a price, and deep learning is no different. The biggest challenge in deep learning is the massive data requirements for training the models. It is difficult to find datasets of such huge sizes, and it is way too costly to prepare such datasets. Its simply not possible for most organizations to come up with them.Another obstacle is the high cost of GPUs needed to run advanced deep learning algorithms.Thankfully, we can use pre-trained state-of-the-art deep learning models and tweak them to work for us. This is known as transfer learning. It is not as resource intensive as training a deep learning model from scratch and produces decent results even on small amounts of training data. This concept will be expanded upon later in the article when we implement our learning on quite a small dataset.Pre-trained models help data scientists start off on a new problem by providing an existing framework they can leverage. You dont always have to build a model from scratch, especially when someone else has already put in their hard work and effort! And these pre-trained models have proven to be truly effective and useful in the field of computer vision (check out this article to see our pick of the top 10 pre-trained models in CV).Their success is popularly attributed to the Imagenet dataset. It has over 14 million labeled images with over 1 million images also accompanying bounding boxes. This dataset was first published in 2009 and has since become one of the most sought-after image datasets ever. It led to several breakthroughs in deep learning research for computer vision, with transfer learning being one of them.However, in NLP, transfer learning has not been as successful (as compared to computer vision, anyway). Of course we have pre-trained word embeddings like word2vec, GloVe, and fastText, but they are primarily used to initialize only the first layer of a neural network. The rest of the model still needs to be trained from scratch and it requires a huge number of examples to produce a good performance.What do we really need in this case? Like the aforementioned computer vision models, we require a pre-trained model for NLP which can be fine-tuned and used on different text datasets. One of the contenders for pre-trained natural language models is the Universal Language Model Fine-tuning for Text Classification, or ULMFiT (Imagenet dataset [cs.CL]).How does it work? How widespread are its applications? How can we make it work in Python? In the rest of this article, we will put ULMFiT to the test by solving a text classification problem and check how well it performs.Proposed by fast.ais Jeremy Howard and NUI Galway Insight Centers Sebastian Ruder, ULMFiT is essentially a method to enable transfer learning for any NLP task and achieve great results. All this, without having to train models from scratch. That got your attention, didnt it?ULMFiT achieves state-of-the-art result using novel techniques like:This method involves fine-tuning a pre-trained language model (LM), trained on theWikitext 103 dataset, to a new dataset in such a manner that it does not forget what it previously learned.Language modeling can be considered a counterpart of Imagenet for NLP. It captures general properties of a language and provides an enormous amount of data which can be fed to other downstream NLP tasks. That is why Language modeling has been chosen as the source task for ULMFiT.I highly encourage you to go through the original ULMFiTpaperto understand more about how it works, the way Jeremy and Sebastian went about deriving it, and parse through other interesting details.Alright, enough theoretical concepts  lets get our hands dirty by implementing ULMFiT on a dataset and see what the hype is all about.Our objective here is to fine-tune a pre-trained model and use it for text classification on a new dataset. We will implement ULMFiT in this process. The interesting thing here is that this new data is quite small in size (<1000 labeled instances). A neural network model trained from scratch would overfit on such a small dataset. Hence, I would like to see whether ULMFiT does a great job at this task as promised in the paper.Dataset: We will use the 20 Newsgroup dataset available in sklearn.datasets. As the name suggests, it includes text documents from 20 different newsgroups.We will perform the python implementation on GoogleColab instead of our local machines. If you have never worked on colab before, then consider this a bonus! Colab, or Google Colaboratory, is a free cloud service for running Python. One of the best things about it is that it provides GPUs and TPUs for free and hence, it is pretty handy for training deep learning models.So, it doesnt matter even if you have a system with pretty ordinary hardware specs  as long as you have a steady internet connection, you are good to go. The only other requirement is that you must have a Google account. Lets get started!First, sign in to your Google account. Then select NEW PYTHON 3 NOTEBOOK. This notebook is similar to your typical Jupyter Notebook, so you wont have much trouble working on it if you are familiar with the Jupyter environment. A Colab notebook looks something like the screenshot below:Then go to Runtime, select Change runtime type, then select GPU as the hardware accelerator to utilise GPU for free.Most of the popular libraries like pandas, numpy, matplotlib, nltk, andkeras, come preinstalled with Colab. However, 2 libraries, PyTorch and fastai v1 (which we need in this exercise), will need to be installed manually. So, lets load them into our Colab environment:Import the dataset which we downloaded earlier.Lets create a dataframe consisting of the text documents and their corresponding labels (newsgroup names).(11314, 2)Well convert this into a binary classification problem by selecting only 2 out of the 20 labels present in the dataset. We will select labels 1 and 10 which correspond to comp.graphics and rec.sport.hockey, respectively.Lets have a quick look at the target distribution.The distribution looks pretty even. Accuracy would be a good evaluation metric to use in this case.Its always a good practice to feed clean data to your models, especially when the data comes in the form of unstructured text. Lets clean our text by retaining only alphabets and removing everything else.Now, we will get rid of the stopwords from our text data. If you have never used stopwords before, then you will have to download them from the nltk package as Ive shown below:Now lets split our cleaned dataset into training and validation sets in a 60:40 ratio.Perfect!Before proceeding further, well need to prepare our data for the language model and for the classification model separately. The good news? This can be done quite easily using the fastai library:We can use the data_lm object we created earlier to fine-tune a pre-trained language model. We can create a learner object, learn, that will directly create a model, download the pre-trained weights, and be ready for fine-tuning:The one cycle and cyclic momentum allows the model to be trained on higher learning rates and converge faster. The one cycle policy provides some form of regularisation. We wont go into the depth of how this works as this article is about learning the implementation. However, if you wish to know more about one cycle policy, then feel free to refer to this excellent paper by Leslie Smith  A disciplined approach to neural network hyper-parameters: Part 1  learning rate, batch size, momentum, and weight decay.Total time: 00:09We will save this encoder to use it for classification later.Lets now use the data_clas object we created earlier to build a classifier with our fine-tuned encoder.We will again try to fit our model.Total time: 00:32Wow! We got a whopping increase in the accuracy and even the validation loss is far less than the training loss. It is a pretty outstanding performance on a small dataset. You can even get the predictions for the validation set out of the learner object by using the below code:With the emergence of methods like ULMFiT, we are moving towards more generalizable NLP systems. These models would be able to perform multiple tasks at once. Moreover, these models would not be limited just to the English language, but to several other languages spoken across the globe.We also have upcoming techniques like ELMo, a new word embedding technique, and BERT, a new language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. These techniques have already achieved state-of-the-art results on many NLP tasks. Hence, the golden period for NLP has just arrived and it is here to stay.I hope you found this article helpful. However, there are still a lot more things to explore in ULMFiT using the fastai library which I encourage you guys to go after. If you have any recommendations/suggestions, then feel free to let me know in the comments section below. Also, try to use ULMFiT on different problems and domains of your choice and see how the results pan out.Code: You can find the complete code here.Thanks for reading and happy learning!",https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/
Highlights from DataHack Summit 2018  a Truly Overwhelming and Resounding Success!,Learn everything about Analytics|And DataHack Summit 2018 delivered BIG!|Highlights from DataHack Summit 2018,"Share this:|Like this:|Related Articles|Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python|4 Secrets for a Future Ready Career in Data Science|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What do you do when you have to improve upon the best? When you need to deliver what has never been done before? And you need to deliver it at a scale which differentiates itself because of the scale.These are some questions we had when we started thinking about DataHack Summit 2018.We promised you an experience like never before  where we would bring together people, machines, and their collaborative experience. A chance to see artificial intelligence in a way no other conference in India has even shown.With more than 1,200 attendees from various diverse industries and domains (more than 400 organizations), DataHack Summit 2018 was an unqualified success. This years conferencewas even bigger than last year, from the speaker line up and the enriching content, to the massive sprawling venue.There were more than 50+ power talks and hack sessions from the best industry leaders, thought leaders, practitioners, data scientists, and folks from all sorts of data related roles. The venue, NIMHANS Convention Centre, was bigger and better than last year, with three massive auditoriums, all jam packed with data science professionals eager to learn from the best in the business.We would like to thank all our sponsors for making DataHack Summit an unparalleled success.And this was just about the first 2 days!9 stimulating workshops (yes, 9!) were conducted on topics ranging from Applied Machine Learning to Computer Vision using PyTorch and we received an overwhelmingly positive response on them. Our aim of curating and delivering only the best data science knowledge to our community all culminated and reflected in our content at DataHack Summit 2018.But enough talk  here are a few awesome highlights from the blockbuster conference!Kunal Jain, the man behind Analytics Vidhya, kicked things off on Day 1 as he set the tone for entire conference with a superbly eloquent opening speech. He spoke about the significance of DataHack Summit and what lay in store for all the attendees. Kunal also spoke about the importance of ethics in AI later in the day, a very relevant and timely talk on a sensitive subject.Ronald van Loon, Director at Adversitement and a well-respected thought leader, was the keynote speaker on day 1 of DataHack Summit 2018. He spoke about the future of data in the digital enterprise and kept the audience enraptured throughout his 60-minute talk.Wondering how to get your data science career started? Then this panel discussion was the place to be! Featuring Ronald van Loon, Dr. Sarabjot Singh Anand, Rohit Pandharkar, Charanpreet Singh, and moderated by Kunal Jain, a range of topics were pondered upon, including how to make a career switch from an entirely different domain.Throughout the conference, auditorium 3 saw the most love from the community. EVERY SINGLE talk and hack session was jam-packed as the audience flocked to audi 3  as you can see in the image above! We have such a wonderful community that is so eager to learn new things, and thats what keeps us working so hard to make DataHack Summit a fulfilling experience for everyone.Recognize this person? Of course you do  its none other than Tarry Singh! He was one of the most sought-after speakers at the conference and he brought his relentless work ethic and supreme energy to the stage. Tarry was the keynote speaker on day 2, as he spoke about the different nuances of deep learning. He was also part of the panel discussion on GANs and conducted a very successful workshop for CxOs on how to make the leap from a business executive to an AI leader.Reinforcement Learning was a prevalent topic throughout the summit with eminent personalities like Professor Balaraman Ravindran and Xander Steenbrugge lending their voice to this crucial subject. In fact, Xander even took a live hack session on RL, showing how you can build your own intelligent agent that can play ATARI games!The workshops we hosted just added to the uniqueness of DataHack Summit 2018. 9 interactive and fully sold out workshops were held on the following topics:Business Executive to AI leader  A CXOs Invite-Only Roundtable Hands-On WorkshopGiving our community the chance to network with fellow professionals is something DataHack Summit excels at  and this year was no different. There were plenty of chances to connect with thought leaders, industry veterans and even intermediate level folks  whether it was during lunch, tea, or in-between sessions. We are proud to offer our community the chance to enhance their careers.There were also a number of interactive booths at the venue, set up by Intel, IBM, H2O.ai, Praxis, and Great Learning. Attendees had a chance to interact with them and find out more about their offerings. Check out this interaction between a young data scientist and a robot  truly creating a place WHERE HUMANS MEET ARTIFICIAL INTELLIGENCEThe Startup Showcase track, introduced for the first time, was a massive success with Rice Inc., Empower Energy, and woroxogo brandishing their awesome machine learning-powered services.And finally, a HUGE thanks to our team for putting this all together. This was truly a team effort spanning months of hard work that culminated in a successful conference. Kudos, and we cant wait to see you all again next year!",https://www.analyticsvidhya.com/blog/2018/11/highlights-from-datahack-summit-2018-a-truly-overwhelming-and-resounding-success/
4 Secrets for a Future Ready Career in Data Science,Learn everything about Analytics|Introduction|Scenario 1  Manual Roles|Scenario 2  The Preference of Customers|What did we Learn from these Scenarios?|Is Data Scientist a Robot-Safe Profession?|What can we do to make sure we stay productive and irreplaceable in the long run within the field of data science?|How can you best use DataHack Summit 2018 to upgrade your knowledge on all the 4 key pointers?|End Notes,"Share this:|Like this:|Related Articles|Highlights from DataHack Summit 2018  a Truly Overwhelming and Resounding Success!|Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit|
Tavish Srivastava
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Automation has impacted, and will continue impacting, jobs in many domains. Every single job on this planet is subject to a risk of job replacement by bots  just the intensity might differ. Automation makes running a business more efficient on one hand, and on the other, it keeps on changing the skill set required to stay relevant in the industry.This inevitably leads to unemployment due to mismatches in the skill set. Let me take you through a few scenarios to illustrate my thoughts.You are an HR professional in the year 2000, when most of the company employee documents were on paper. You are very efficient in sorting documents and retrieving them when needed and have been a star performer for more than 5 years because of these skills.Given that the HR processes did not change much over time, you did not pick up computer skills over the next 18 years. However, the way industries work have changed a lot from 2000 to 2018, and now all the employee documentations are on the cloud or a private server.So, your most sell-able skills are now suddenly not that important. You might face difficulties finding a job unless you upgrade yourself for todays evolved industry. Note that your skill set mismatch was not because of the evolution of HR specific processes, but the dynamically changing business processes that you support.You work as a news reader on the radio in the era when there was no television. You are very well informed about current affairs and hence you were a strong performer. But after television became mainstream, radios almost went out of business. Your radio employer had to let you go because they were sustaining heavy losses.Now, given your skill set, you can still try to get a job as a TV news reader but you need to work on your body language and the crippling fear of facing the camera. The good news? You have been hanging out with people who work in the TV news industry and hence you know your opportunity areas and have been actively working on them.Note that this time neither your profession evolved, nor your industry. Its just that the customer started preferring an alternate product/service to the business you support, making your skill a mismatch (or obsolete) in the industry.In the scenarios above, we witnessed that the changes around us are making businesses easy to run but at the same time are creating job skill mismatches, leading to unemployment in specific domains. Below are the three main reasons of job skill shifts in the industry:It is no surprise that automation and changing business domains have disrupted many jobs. An important questions now is:Will some jobs be impacted more than others?Even though no one really knows what jobs will be more/less impacted by automation, here is a framework that helps understand the broad idea.Machines are not good at learning from too few examples and machines are not good at being creative. So if your job has these two attributes, you should be just fine. For instance, driving a car is a very repetitive process and does not involve a lot of creativity. Hence, cab drivers are at a high risk of their job facing automation.In this ever-evolving AI-led world, we (data scientists) are definitely on the better side of the deal. Where does the role of a data scientist fall in the graph shown above? As a data scientist, we do a varied set of jobs to help businesses grow. Each of these jobs fall at a different place in the graph above. The below image shows my thoughts on the different sub-jobs we do as data scientists (proportion might vary with individual roles):As you can see,Not all the parts of a data scientists job come with a 10 year warranty. Depending on your specific role and proportion of work that is difficult to automate, you can estimate your risk of automation.Consider a data scientist in 2010. Key skill sets were knowing logistic and linear regression, and conversant with base SAS and MS Excel. Now, if we bring this data scientist of 2018 without any significant upgrades on tools and technique, he/she can face hard time finding data scientist job. With good certainty it can be said that even though the data science stream will stay up and running for long term, the roles and responsibilities of these jobs are up for big changes. People who have challenges upgrading to these new roles and responsibilities will face strong setbacks in progressing in career.Given the young workforce in data science field, skill set match is not a concern over short term as most of the people working in this field have recently picked up knowledge in latest tools and technique. However, as the field gets old so does the workforce and skill set mismatch within data science domain is definitely possible if this workforce is not able to upgrade their skill set while managing their daily jobs.Four things I would recommend for data scientists in any kind of role to build a future proof profile:With a high focus on data-driven strategies across domains, data scientists are kept busy with their job at hand. Not staying updated on each of the 4 pointers mentioned above canbe dangerous in the long run.To fill this gap in the industry, Analytics Vidhya has handcrafted a four day conference DataHack Summit 2018. After the success of the Summit last year, we have further optimized the schedule to pack it with everything you need to know to come up to speed in terms of tools, technologies, and business domains.Sounds too good an opportunity to pass up? Good! Tickets are almost sold on, so grab yourshereTODAY!Here are a few helpful links for DataHack Summit 2018:We were overwhelmed by the response we got from the participants at DataHack Summit 2017. Lets make DataHack Summit 2018 an even bigger success!",https://www.analyticsvidhya.com/blog/2018/11/4-secrets-for-a-future-ready-career-in-data-science/
Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit,Learn everything about Analytics|Introduction||Table of Contents||Model-Based vs Model-Free Learning|Monte Carlo Methods  An Example|Monte Carlo Reinforcement Learning|Monte Carlo Policy Evaluation|Monte Carlo Control|Monte Carlo Implementation in Python|||End Notes,"Incremental Mean|Frozen Lake Environment|Share this:|Like this:|Related Articles|4 Secrets for a Future Ready Career in Data Science|DataHack Summit 2018 is Almost Here  WHERE HUMANS MEET ARTIFICIAL INTELLIGENCE|
Ankit Choudhary
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats the first thing that comes to your mind when you hear the words reinforcement learning? The most common thought is  too complex with way too much math. But Im here to assure you that this is quite a fascinating field of study  and I aim to break down these techniques in my articles into easy-to-understand concepts.Im sure you must have heard of OpenAI and DeepMind. These are two leading AI organizations who have made significant progress in this field. A team of OpenAI bots was able to defeat a team of amateur gamers in Dota 2, a phenomenally popular and complex battle arena game.Do you think its feasible to build a bot using dynamic programming for something as complex as Dota 2?Its unfortunately a no-go. There are just too many states (millions and millions), and collecting all the specifics of DOTA 2 is an impossible task. This is where we enter the realm of reinforcement learning or more specifically model-free learning.In this article, we will try to understand the basics of Monte Carlo learning. Its used when there is no prior information of the environment and all the information is essentially collected by experience. Well use the OpenAI Gym toolkit in Python to implement this method as well.Lets get the ball rolling!If youre a beginner in this field or need a quick refresher of some basic reinforcement learning terminologies, I highly recommend going through the below articles to truly maximize your learning from this post:We know that dynamic programming is used to solve problems where the underlying model of the environment is known beforehand (or more precisely, model-based learning). Reinforcement Learning is all about learning from experience in playing games. And yet, in none of the dynamic programming algorithms, did we actually play the game/experience the environment. We had a full model of the environment, which included all the state transition probabilities.However, in most real life situations as we saw in the introduction, the transition probabilities from one state to another (or the so called model of the environment) are not known beforehand. It is not even necessary that the task follows a Markov property.Lets say we want to train a bot to learn how to play chess. Consider converting the chess environment into an MDP.Now, depending on the positioning of pieces, this environment will have many states (more than 1050), as well as a large number of possible actions. The model of this environment is almost impossible to design!One potential solution could be to repeatedly play a complete game of chess and receive a positive reward for winning, and a negative reward for losing, at the end of each game. This is called learning from experience.Any method which solves a problem by generating suitable random numbers, and observing that fraction of numbers obeying some property or properties, can be classified as a Monte Carlo method.Lets do a fun exercise where we will try to find out the value of pi using pen and paper. Lets draw a square of unit length and draw a quarter circle with unit length radius. Now, we have a helper bot C3PO with us. It is tasked with putting as many dots as possible on the square randomly 3,000 times, resulting in the following figure:C3PO needs to count each time it puts a dot inside a circle. So, the value of pi will be given by:where N is the number of times a dot was put inside the circle. As you can see, we did not do anything except count the random dots that fall inside the circle and then took a ratio to approximate the value of pi.The Monte Carlo method for reinforcement learning learns directly from episodes of experience without any prior knowledge of MDP transitions. Here, the random component is the return or reward.One caveat is that it can only be applied to episodic MDPs. Its fair to ask why, at this point. The reason is that the episode has to terminate before we can calculate any returns. Here, we dont do an update after every action, but rather after every episode. It uses the simplest idea  the value is the mean return of all sample trajectories for each state.Recalling the idea from multi-armed bandits discussed in this article, every state is a separate multi-armed bandit problem and the idea is to behave optimally for all multi-armed bandits at once.Similar to dynamic programming, there is a policy evaluation (finding the value function for a given random policy) and policy improvement step (finding the optimum policy). We will cover both these steps in the next two sections.The goal here, again, is to learn the value function vpi(s) from episodes of experience under a policy pi. Recall that the return is the total discounted reward:S1, A1, R2, .Sk ~ piAlso recall that the value function is the expected return:We know that we can estimate any expected value simply by adding up samples and dividing by the total number of samples:The question is how do we get these sample returns? For that, we need to play a bunch of episodes and generate them.For every episode we play, well have a sequence of states and rewards. And from these rewards, we can calculate the return by definition, which is just the sum of all future rewards.First Visit Monte Carlo:Average returns only for first time s is visited in an episode.Heres a step-by-step view of how the algorithm works:Every visit Monte Carlo:Average returns for every time s is visited in an episode.For this algorithm, we just change step #3.1 to Add to a listthe return received after every occurrence of this state.Lets consider a simple example to further understand this concept. Suppose theres an environment where we have 2 states  A and B. Lets say we observed 2 sample episodes:A+3 => A indicates a transition from state A to state A, with a reward +3. Lets find out the value function using both methods:It is convenient to convert the mean return into an incremental update so that the mean can be updated with each episode and we can understand the progress made with each episode. We alreadylearnt this when solving the multi-armed bandit problem.We update v(s) incrementally after episodes. For each state St, with return Gt:In non-stationary problems, it can be useful to track a running mean, i.e., forget old episodes:V(St)  V(St) +  (Gt  V(St))Similar to dynamic programming, once we have the value function for a random policy, the important task that still remains is that of finding the optimal policy using Monte Carlo.Recall that the formula for policy improvement in DP required the model of the environment as shown in the following equation:This equation finds out the optimal policy by finding actions that maximize the sum of rewards. However, a major caveat here is that it uses transition probabilities, which is not known in the case of model-free learning.Since we do not know the state transition probabilities p(s,r/s,a), we cant do a look-ahead search like DP.Hence, all the information is obtained via experience of playing the game or exploring the environment.Policy improvement is done by making the policy greedy with respect to the current value function.In this case, we have an action-value function, and therefore no model is needed to construct the greedypolicy.A greedy policy (like the above mentioned one) will always favor a certain action if most actions are not explored properly. There are two solutions for this:Monte Carlo with exploring startsAll the state action pairs have non-zero probability of being the starting pair, in this algorithm. This will ensure each episode which is played will take the agent to new states and hence, there is more exploration of the environment.Monte Carlo with epsilon-SoftWhat if there is a single start point for an environment (for example, a game of chess)? Exploring starts is not the right option in such cases. Recall here that in a multi-armed bandit problem, we discussed theepsilon-greedy approach.Simplest idea for ensuring continual exploration all actions are tried with non-zero probability 1  epsilon choose the action which maximises the action value function and with probability epsilon choose an action at random.Now that we understand the basics of Monte Carlo Control and Prediction, lets implement the algorithm in Python. We will import the frozen lake environment from the popular OpenAI Gym toolkit.The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.The surface is described using a grid like the following:(S: starting point, safe),(F: frozen surface, safe),(H: hole, fall to your doom),(G: goal)The idea is to reach the goal from the starting point by walking only on a frozen surface and avoiding all the holes. Installation details and documentation for the OpenAI Gym are available at thislink. Lets begin!First, we will define a few helper functions to set up the Monte Carlo algorithm.Create EnvironmentFunction for Random PolicyDictionary for storing the state action valueFunction to play episodeFunction to test policy and print win percentageFirst Visit Monte Carlo Prediction and ControlNow, it is time to run this algorithm to solve an 88 frozen lake environment and check the reward:On running this for 50,000 episodes, we get a score of 0.9. And with more episodes, it eventually reaches the optimal policy.The story of Monte Carlo learning does not end here. There is another set of algorithms under this which are called off policy Monte Carlo methods. Off policy methods try to learn an optimal policy using returns generated from another policy.The methods discussed in this article are on policy methods which is basically like learning while doing the job. Whereas off policy methods are akin to learning while watching other people doing the job. I will cover off policy methods in a subsequent article.If you have any questions or suggestions regarding this article, feel free to connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/
DataHack Summit 2018 is Almost Here  WHERE HUMANS MEET ARTIFICIAL INTELLIGENCE,Learn everything about Analytics|Are you ready to experience Artificial Intelligence in action like never before? A world where Humans meet Artificial Intelligence?|Whats in it for you?|Power-Packed Content at DataHack Summit 2018|Keynote Speakers|Power Talks by Industry Thought Leaders and Practitioners|Hack Sessions|9 Exciting End-to-End Workshops|Startup Showcase|So what are you waiting for?,"Book your seat NOW!|Ronald van Loon  Director at Adversitement & a Renowned Thought Leader|Tarry Singh  Founder & CEO of deepkapha.ai|Share this:|Like this:|Related Articles|Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit|Improving Neural Networks  Hyperparameter Tuning, Regularization, and More (deeplearning.ai Course #2)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Well  we are just a week away fromDataHack Summit 2018(22-25 November 2018, Bengaluru)  Indias most advanced conference on AI, Machine Learning, Deep Learning and IoT.If you are a data science professional, some one who dreams about machines and algorithms unleashing a new era in human evolution  this is the place you want to be.Heres a small video outlining the vision behindDataHack Summitfrom the man behind Analytics Vidhya himself, Founder and CEO Kunal Jain:We are bringing together world-class AI practitioners, industry thought leaders, IoT experts, chief data scientists, data officers, machine learning engineers and researchers, technology evangelists, & data hackers from around the globe at this mega conference  from 22  25 November 2018!Tickets are almost sold out so hurry andPower talks, hack sessions, workshops, startup showcase, and a whole lot more  this is just a microcosm of what we have in store for you at DataHack Summit 2018. Come with me  lets take a tour around the conference!DataHack Summit is a win-win situation for everyone  let me show you how.If youre an AI/ML/DL/IoT thought leader or executive looking to understand how things function from a holistic standpoint:And many, many more! Plenty of topics and domains will be covered and trust me, you dont want to miss this.And this isnt all. There are plenty more perks of attending DHS 2018 for leaders  heres a couple of highlights:If youre an experienced data scientist, our hack sessions and talks will feel like youve hit the jackpot:If youre a machine/deep learning researcher, we have:If youre looking to carve a career in data science and machine learning, we have plenty in store for you as well!There really is something at DataHack Summit 2018 for everyone! Lets drill deeper into the content and hold on to your seats, because this is going to be a thrilling ride.Analytics Vidhya is known for the world-class content we publish. This is not something we take lightly  and that is reflected in the articles we publish and the content you will see at our flagship conference.This year, we have more sessions than ever before! These sessions will be covering a plethora of diverse topics that come under the AI umbrella. You can check out the tentative schedule (subject to changes in the coming days).Lets check out some highlights.We are delighted to announce two great industry leaders and influencers as the keynote speakers at DHS 2018 Ronald van Loon and Tarry Singh! Both of these eminent personalities bring a wealth of industry experience and leadership expertise to the Summit.Ronald van Loon is a recognized expert and a popular and well respected thought leader.He has a demonstrated history of helping data driven companies generate business value with best of breed solutions and a hands-on approach.He is a guest author on leading Big Data sites, speaker/chairman/panel member on (inter)national events, and runs a successful series of webinars on Digital Transformation.Tarry Singh is the CEO, Founder and AI Neuroscience Researcher of an AI startup deepkapha.ai. In his 17 years of work experience, he has guided CxOs of numerous global organizations in setting up data-driven organizations from scratch.Tarry speaks regularly at global AI leadership summits worldwide and conducts workshops on a regular basis with his TAs who are currently Ph.Ds in various disciplines such as NLP, Computer Vision and Robotics disciplines.Leading industry experts? Check. Machine Learning practitioners? Check.We have 25+ power talks by data science practitioners and thought leaders from all over the globe. These power talks will encompass various tools, techniques and applications of Artificial Intelligence, Machine Learning and Deep Learning in the industry.Below are some of the prominent speakers who will be presenting at DHS 2018:Check out the full speaker line-uphere.Whats more exciting than coding machine learning concepts from scratch? Hack sessions are one of the biggest features than elevate DataHack Summit to a whole different level.These sessions are a one-hour code walk-through where the speaker presents a live code demonstration in an interactive manner. We received an overwhelmingly positive response on them last year, and are delighted to showcase 15 of them this year!Check out Kunals video where he expands on what you can look forward to:A wide variety of domains will be covered  Machine Learning, Deep Learning, Recommendation Engine, Reinforcement Learning, Natural Language Processing, Time Series, Graph Embeddings, and much, much more.Below are a few popular hack sessions thatll be covered at DHS 2018:One of the most anticipated aspects of DataHack Summit 2018 are the workshops. And we are thrilled to be hosting 9 of them this year! These workshops aim to deliver practical knowledge in an exciting and easy-to-grasp manner, and enable the participants to build their own concepts under the supervision of experienced instructors.Excited, yet? Heres Kunal with an overview of what you can expect from these workshops:Awesome! These 8-hour workshops cover a range of topics in Machine Learning and Deep Learning  and to add the icing on the cake, leading industry practitioners will be your guide! Check out the complete workshop list below and enroll yourself TODAY:Want to see the real power of machine learning in society?A huge addition to DataHack Summit 2018 is the Startup Showcase. Leading startups from across the globe will be showcasing some of the most exciting Artificial Intelligence and Machine Learning products from a variety of domains.Heres a glimpse of the startups who will be presenting at the conference:And there are more startups coming this week!Join us at DataHack Summit next week in Bengaluru, and prepare to meet AI like youve never seen before!",https://www.analyticsvidhya.com/blog/2018/11/datahack-summit-2018-build-india-nextgen-data-science-ecosystem/
"Improving Neural Networks  Hyperparameter Tuning, Regularization, and More (deeplearning.ai Course #2)","Learn everything about Analytics|Introduction|Table of Contents|1. Course Structure|Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization|Module 1: Practical Aspects of Deep Learning|Part I: Setting up your Machine Learning Application|Part II: Regularizing your Neural Network|Part III: Setting up your Optimization Problem|Module 2: Optimization Algorithms|Module 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks|Part I: Hyperparameter tuning|Part II: Batch Normalization|Part III: Multi-Class Classification|End Notes","Train / Dev / Test sets|Bias / Variance|Basic Recipe for Machine Learning|Regularization|How does regularization reduce overfitting?|Dropout Regularization|Other Regularization Methods|Normalizing Inputs|Vanishing / Exploding gradients|Weight Initialization for Deep Networks|Gradient Checking|Mini-Batch Gradient Descent|Understanding Mini-Batch Gradient Descent|Exponentially weighted averages|Bias Correction in Exponentially Weighted Averages|Gradient Descent with Momentum|RMSprop|Adam optimization algorithm|Learning Rate Decay|Tuning process|Using an Appropriate Scale to Pick Hyperparameters|Normalizing Activations in a Network|Fitting Batch Norm into a Neural Network|How does Batch Norm work?|Batch Norm at Test Time|Softmax Regression|Share this:|Like this:|Related Articles|DataHack Summit 2018 is Almost Here  WHERE HUMANS MEET ARTIFICIAL INTELLIGENCE|Want to Become a Data Engineer? Heres a Comprehensive List of Resources to get Started|
Pulkit Sharma
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Building that first model  isnt that what we strive for in the deep learning field? That feeling of euphoria when we see our model running successfully is unparalleled. But the buck doesnt stop there.How can we improve the accuracy of the model? Is there any way to speed up the training process? These are critical questions to ask, whether youre in a hackathon setting or working on a client project. And these aspects become even more prominent when youve built a deep neural network.Features like hyperparameter tuning, regularization, batch normalization, etc. come to the fore during this process. This is part 2 of the deeplearning.ai course (deep learning specialization) taught by the great Andrew Ng. We saw the basics of neural networks and how to implement them in part 1, and I recommend going through that if you need a quick refresher.In this article, we will explore the inner workings of these neural networks, including looking at how we can improve their performance and reduce the overall training time. These techniques have helped data scientists climb machine learning competition leaderboards (among other things) and earn top accolades. Yes, these concepts are invaluable!Part 1 of this series covered concepts like how both shallow and deep neural networks work, how to implement forward and backpropagation on single as well as multiple training examples, among other things. Now comes the question of how to tweak these neural networks in order to extract the maximum accuracy out of them.Course 2, which we will see in this article, spans three modules:Now that we know what all well be covering in this comprehensive article, lets get going!The below pointers summarize what we can expect from this module:This module is fairly comprehensive, and is thus further divided into three parts:Lets walk through each part in detail.While training a deep neural network, we are required to make a lot of decisions regarding the following hyperparameters:There is no specified or pre-defined way of choosing these hyperparameters. The below is what we generally follow:Now how do we identify whether the idea is working? This is where the train / dev / test sets come into play. Suppose we have an entire dataset:We can divide this dataset into three different sets like:There is still one question left after this what should be the length of these training, dev and test sets?Its actually a pretty critical aspect of any machine learning project, and will end up playing a big part in deciding how well the model performs. Lets look at some traditional guidelines that experts follow to decide the length of each set:This is certainly one way of deciding the length of these different sets. This works fine most of the time, but indulge me and consider the following scenario:Suppose we have scraped multiple images of cats from different sites, and also clicked a few images using our own camera. The distribution of both these types of images will be different, right? Now, we split the data in such a way that the training set contains all the scraped images, while the dev and test sets have all the camera images. In this case, the distribution of the training set will be different from the dev and test sets and hence, theres a good chance we might not get good results.In cases like these (different distributions), we can follow the following guidelines:We can also use these sets to look at the bias and variance of the model, These help us decide how well the model is fitting and performing.Consider a dataset which gives us the below plot:What will happen if we fit a straight line to classify the points into different classes? The model will under-fit and have a high bias. On the other hand, if we fit the data perfectly, i.e., all the points are classified into their respective class, we will have high variance (and overfitting). The right model fit is usually found between these two extremes:We want our model to be just right, which means having low bias and low variance. We can decide if the model should have high bias or high variance by checking the train set and dev set error. Generally, we can define it as:I have a very simple method of dealing with certain problems I face in machine learning. Ask a set of questions and then figure out the answers one-by-one. It has proven to be extremely helpful for me in my journey and more often than not has led to improvements in the models performance. These questions are listed below:Question 1: Does the model have high bias?Solution: We can figure out whether the model has high bias by looking at the training set error. High training error results in high bias. In such cases, we can try bigger networks, train models for a longer period of time, or try different neural network architectures.Question 2: Does the model have high variance?Solution : If the dev set error is high, we can say that the model has high variance. To reduce the variance, we can get more data, use regularization, or try different neural network architectures.One of the most popular techniques to reduce variance is called regularization. Lets look at this concept and how it applies to neural networks in part II.We can reduce the variance by increasing the amount of data. But is that really a feasible option every time? Perhaps there is no other data available, and if there is, it might be too expensive for your project to source. This is quite a common problem. And thats why the concept of regularization plays an important role in preventing overfitting.Lets take the example of logistic regression. We try to minimize the loss function:Now, if we add regularization to this cost function, it will look like:This is called L2 regularization.  is the regularization parameter which we can tune while training the model.Now, lets see how to use regularization for a neural network. The cost function for a neural network can be written as:We can add a regularization term to this cost function (just like we did in our logistic regression equation):Finally, lets see how regularization works for a gradient descent algorithm:As you can surmise from the above equations, the reduction in weights will be more in case of regularization (since we are adding a higher quantity from the weights). This is the reason L2 regularization is also known as weight decay.You must be wondering at this pointhow in the world does regularization prevent overfitting in the model? Lets try to understand it in the next section.The primary reason overfitting happens is because the model learns even the tiniest details present in the data. So after learning all the possible patterns it can find, the model tends to perform extremely well on the training set but fails to produce good results on the dev and test sets. It falls apart when faced with previously unseen data.One way to prevent overfitting is to reduce the complexity of the model. This is exactly what regularization does! If we set the regularization parameter () to a large value, the decay in the weights during gradient descent update will be more. Hence, the weights of most of the hidden units will be close to zero.Since the weights are negligible, the model will not learn much from these units.This will end up making the network simpler and thus reduce overfitting:Lets understand this concept through one more example. Suppose we use the tanh activation function:Now, if we set  to a large value, the weight of the units w[l] will be less. To calculate the z[l] value, we will use the following formula:z[l] = w[l] a[l-1] + b[l]Hence, the z-value will be less. If we use the tanh activation function, these low values of z[l] will lie near the origin:Here we are using only the linear region of the tanh function. This will make every layer in the network roughly linear, i.e., we will get linear boundaries that separate the data, thus preventing overfitting.There is one more technique we can use to perform regularization. Consider you are building a neural network as shown below:This neural network is overfitting on the training data.Suppose we add a dropout of 0.5 to all these images. The model will randomly remove 50% of the units from each layer and we finally end up with a much simpler network:This has proven to be a very effective regularization technique. How can we implement it ourselves? Lets check it out!We will be working on this very example where we have three hidden layers. For now, we will consider the third layer, l=3. The dropout vector d for the third hidden layer can be written as:Here, keep_prob is the probability of keeping a unit. Now, we will calculate the activations for the selected units:This a3 value will be reduced by a factor of (1-keep_probs). So to get the expected value of a3, we divide the value:Lets understand the concept of dropout using an example:So, 20% of the total units (i.e. 10) will be randomly shut off.Different sets of hidden layers are dropped randomly in each training iteration. Note that dropout is only done at the time of training the model (not during the test phase). The reason for doing this is because:Apart from L2 regularization and dropout, there are a few other techniques that can be used to reduce overfitting.And that is a wrap as far as regularization techniques are concerned!In this module, we will discuss the different techniques that can be used to speed up the training process.Suppose we have 2 input features and their scatter plot looks like this:This is how we can represent the input as a vector:Well follow the below steps to normalize the input:A key point to note is that we use the same mean and variance to normalize the test set as well. We should do this because we want the same transformation to happen on both the train and test data.Butwhy does normalizing the data make the algorithm faster?In the case of unnormalized data, the scale of features will vary, and hence there will be a variation in the parameters learnt for each feature. This will make the cost function asymmetric:Whereas, in the case of normalized data, the scale will be the same and the cost function will also be symmetric:Normalizing the inputs makes the cost function symmetric. This makes it is easier for the gradient descent algorithm to find the global minima more quickly. And this, in turn, makes the algorithm run much faster.While training deep neural networks, sometimes the derivatives (slopes) can become either very big or very small. It can make the training phase quite difficult. This is the problem of vanishing / exploding gradients. Suppose we are using a neural network with l layers with two input features and we initialized the large weights:The final output at the lth layer will be (consider we are using a linear activation function):For deeper networks, L will be large, making the gradients very large and the learning process slow. Similarly, using small weights will make the gradients very small, and as a result, learning will be slow. We must deal with this problem in order to reduce the training time. Sohow should the weights be initialized?One potential solution to this problem can be random initialization. Consider a single neuron as shown below:For this example, we can initialize the weights as:The primary reason behind initializing the weights randomly is to break symmetry. We want to make sure that different hidden units learn different patterns. There is one more technique that can help ensure that our implementations are correct and will run quickly.Gradient checking is used to find bugs (if any) in the implementation of backpropagation. Consider the following graph:The derivative of the function w.r.to  can be best expressed as:Where  is the small step which we take towards the left and right of . Make sure that the derivative calculated above is nearly equal to the actual derivative of the function. Below are the steps we follow for gradient checking:For each i, we calculate:We use Euclidean distance () to measure whether both these terms are equal:We want this value to be as small as possible. So, if  is 10-7, we say it is a great approximation, If  is 10-5, it is acceptable, and if  is in the range 10-3, we have to change the approximations and recalculate the weights.And thats a wrap for module 1!The objectives behind this module are:We saw in course 1 how vectorization can help us to effectively work with m training examples. We can get rid of explicit for loops and make the training phase faster. So, we take the training examples as:Where X is the vectorized input and Y is their corresponding outputs. But what will happen if we have a large training set, say m = 5,000,000? If we process through all of these training examples in every training iteration, the gradient descent update will take a lot of time. Instead, we can use a mini batch of the training examples and update the weights based on them.Suppose we make a mini-batch containing 1000 examples each. This means we have 5000 batches and the training set will look like this:Here, X{t}, Y{t} represents the tth mini-batch input and output. Now, lets look at how to implement a mini-batch gradient descent:This is equivalent to 1 epoch (1 epoch = single pass through the training set). Note that the cost function for mini batch is given as:Where 1000 is the number of mini batches we saw in our above example. Lets dive deeper and understand mini-batch gradient descent in detail.In batch gradient descent, our cost function should decrease on every single iteration:In the case of mini-batch gradient descent, we only use a specified set of training examples. As a result, the cost function can decrease for some iterations:How can we choose a mini-batch size? Lets see various cases:Below are a few general guidelines to keep in mind while deciding the mini-batch size:Below is a sample of hypothetical temperature data collected for an entire year:The below plot neatly summarizes this temperature data for us:Exponentially weighted average, or exponentially weighted moving average, computes the trends. We will first initialize a term as 0:V0 = 0Now, all the further terms will be calculated as the weighted sum of V0 and the temperature of that day:V1 = 0.9 * V0 + 0.1 * 1V2 = 0.9 * V1 + 0.1 * 2And so on. A more generalized form of exponentially weighted average can be written as:Vt =  * V(t-1) + (1  ) * tUsing this equation for trend, the data will be generalised as:The  value in this example is 0.9, which means Vt is an approximation of average over 1/(1-) days, i.e., 1/(1-0.9) = 10 days temperature. Increasing the value of  will result in approximating over more days, i.e., taking the average temperature of more days. If the  value is small, i.e., we use only 1 days data for approximation, the predictions become much more noisy:Here, the green line is the approximation when  = 0.98 (using 50 days) and the yellow line is when  = 0.5 (using 2 days). It can be seen that using small  results in noisy predictions.The equation of exponentially weighted averages is given by:Vt =  * V(t-1) + (1  ) * tLets look at how we can implement this:This step takes a lot less memory as we are overwriting the previous values. Hence, it is a computational, as well as memory efficient, process.We initialize V0 = 0, so while calculating the V1 value it will only be equal to (1  ) * 1. It will not generalize well to the actual values. We need to use bias correction to overcome this challenge.Instead of using the previous equation, i.e.,Vt =  * V(t-1) + (1  ) * twe include a bias correction term:Vt = [ * V(t-1) + (1  ) * t] / (1  t)When t is small, t will be large, resulting in a smaller value of (1-t). This will make the Vt value larger ensuring that our predictions are accurate.The underlying idea of gradient descent with momentum is to calculate the exponential weighted average of gradients and use them to update weights. Suppose we have a cost function whose contours look like this:The red dot is the global minima, and we want to reach that point. Using gradient descent, the updates will look like:One more way could be to use a larger learning rate. But that could result in large upgrade steps, and we might not reach global minima. Additionally, too small a learning rate makes the gradient descent slower. We want a slower learning in the vertical direction and a faster learning in the horizontal direction which will help us to reach the global minima much faster.Lets see how we can achieve it using momentum:Here, we have two hyperparameters, i.e.,  and . The role of dW and db in the above equation is to provide momentum, VdW and Vdb provides velocity, and  acts as friction and prevents speeding over the limit. Consider a ball rolling down  VdW and Vdb provide velocity to that ball and make it move faster. We do not want our ball to speed up so much that it misses the global minima, and hence  acts as friction.Let me introduce you to a few more optimization algorithms.Consider the example of a simple gradient descent:Suppose we have two parameters w and b as shown below:Look at the contour shown above and the parameters graph. We want to slow down the learning in b direction, i.e., the vertical direction, and speed up the learning in w direction, i.e., the horizontal direction. The steps followed in RMSprop can be summarised as:The slope in the vertical direction (db in our case) is steeper, resulting in a large value of Sdb. As we want slow learning in the vertical direction, dividing db with Sdbin update step will result in a smaller change in b. Hence, learning in the vertical direction will be less. Similarly, a small value of SdW will result in faster learning in the horizontal direction, thus making the algorithm faster.Adam is essentially a combination of momentum and RMSprop. Lets see how we can implement it:There are a range of hyperparameters used in Adam and some of the common ones are:Adam helps to train a neural network model much more quickly than the techniques we have seen earlier.If we slowly reduce the learning rate over time, we might speed up the learning process. This process is called learning rate decay.Initially, when the learning rate is not very small, training will be faster. If we slowly reduce the learning rate, there is a higher chance of coming close to the global minima.Learning rate decay can be given as: = [1 / (1 + decay_rate * epoch_number)] * 0Lets understand it with an example. Consider:This is how, after each epoch, there is a decay in the learning rate which helps us reach the global minima much more quickly. There are a few more learning rate decay methods:Here, t is the mini-batch number.This was all about optimization algorithms and module 2! Take a deep breath, we are about to enter the final module of this article.The primary objectives of module 3 are:Much like the first module, this is further divided into three sections:Hyperparameters. We see this term popularly being bandied about in data science competitions and hackathons. But how important is it in the overall scheme of things?Tuning these hyperparameters effectively can lead to a massive improvement in your position on the leaderboard. Following are a few common hyperparameters we frequently work with in a deep neural network:Learning rate usually proves to be the most important among the above. This is followed by the number of hidden units, momentum, mini-batch size, the number of hidden layers, and then the learning rate decay.Now, suppose we have two hyperparameters. We sample the points in a grid and then systematically explore these values. Consider a five-by-five grid:We check all 25 values and pick whichever hyperparameter works best. Instead of using these grids, we can try random values as well. Why? Because we do not know which hyperparameter value might turn out to be important, and in a grid we only define particular values.The major takeaway from this sub-section is to use random sampling and adequate search.To understand this, consider the number of hidden units hyperparameter. The range we are interested in is from 50 to 100. We can use a grid which contains values between 50 and 100 and use that to find the best value:Now consider the learning rate with a range between 0.0001 and 1. If we draw a number line with these extreme values and sample the values uniformly at random, around 90% of the values will fall between 0.1 to 1. In other words, we are using 90% resources to search between 0.1 to 1, and only 10% to search between 0.0001 to 0.1. This does not look correct! Instead, we can use a log scale to choose the values:Next, we will learn a technique which makes our neural network much more robust to the choice of hyperparameters and also makes the training phase even more faster.Lets recall how a logistic regression looks like:We have seen how normalizing the input in this case can speed up the learning process. In case of deep neural networks, we have a lot of hidden layers and this results in a lot of activations:Wouldnt it be great if we can normalize the mean and variance of these activations (a[2]) in order to make the training of w[3], b[3] more effective? This is how batch normalization works. We normalize the activations of the hidden layer(s) so that the weights of the next layer can be updated faster. Technically, we normalize the values of z[2] and then use an activation function of the normalized values. Here is how we can implement batch normalization:Given some intermediate values in NN Z(1),.,Z(m):Here,  and  are learnable parameters.Consider the neural network shown below:Each unit of the neural network computes two things. It first computes Z, and then applies the activation function on it to compute A. If we apply batch norm at each layer, the computation will look like:After calculating the Z-value, we apply batch norm and then the activation function on that. Parameters in this case are:Finally, lets see how we can apply gradient descent using batch norm:Note that this also works with momentum, RMSprop and Adam.In the case of logistic regression, we now know how normalizing the inputs helps to speed up the learning. Batch norm works in much the same way. Lets take one more use case to understand it better. Consider the training set for a binary classification problem:But when we try to generalize it to a dataset having different distribution, say:The decision boundary in both the cases might be same:But the model would not be able to discover this green decision boundary. So, as the distribution of the input changes, we might have to train the model again. Consider a deep neural network:And lets only consider the learning of layer 3. It will have activations from layer two as its input:The aim of the third hidden layer is to take these activations and map them with the output. These activations change every time as the parameters of the previous layers change. Hence, we see a lot of shift in the activation values. Batch norm reduces the amount that the distribution of these hidden unit values shift around.Additionally, it turns out that batch norm has a regularization effect as well:One thing to note is that while making predictions, there is a slight difference in the way we use batch normalization.We need to process the examples one at a time when making predictions on the test data. In the training period, the steps of batch norm can be written as:We first calculate the mean and variance of that mini-batch, and use that to normalize the z-value. We will be using the entire mini-batch to calculate the mean and standard deviation. We process each image separately, so taking the mean and standard deviation of a single image does not make sense.We use exponentially weighted average to calculate the mean and variance across different mini-batches. Finally, we use these values to scale the test data.Binary classification means dealing with two classes. But when we have more than two classes in a problem, that is called multi-class classification. Suppose we have to recognize cats, dogs and tigers in a set of images. How many types of classes are there? 4  cat, dog, tiger and none of them. If you said three there then think again!For solving such problems, we use softmax regression. At the output layer, instead of having a single unit, we have units equal to the total number of classes (4 in our case). Each unit tells us the probability of the image falling in different classes. Since it tells the probability, the sum of values from each unit is always equal to 1.This is how a neural network for a multi-class classification looks like. So, for layer L, the output will be:Z[L] = W[L]*a[L-1] + b[L]
The activation function will be:Lets understand this with an example. Consider the output from the last hidden layer:We then calculate t using the formula given above:Finally, we calculate the activations:This is how we can solve a multi-class classification problem using the softmax activation function. And that brings us to the end of course 2!Congratulations! We have completed the second course of the deep learning specialization. It was quite an intense exercise writing this article and it really helped solidify my own concepts in the process. To summarize what we covered here:If you have any feedback on this article or have any doubts/questions, kindly share them in the comments section below.",https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/
Want to Become a Data Engineer? Heres a Comprehensive List of Resources to get Started,"Learn everything about Analytics|Introduction|Table of Contents|So, what is a Data Engineer?|The Difference between a Data Scientist and a Data Engineer|The Different Roles in Data Engineering|Core Data Engineering Skills and Resources to Learn Them|Data Engineering Certifications|End Notes","Introduction to Data Engineering|Basic Language Requirement: Python|Operating Systems|In-Depth Database Knowledge|Data Warehousing/Big Data Tools|Basic Machine Learning Familiarity|Share this:|Like this:|Related Articles|Improving Neural Networks  Hyperparameter Tuning, Regularization, and More (deeplearning.ai Course #2)|A Practical Implementation of the Faster R-CNN Algorithm for Object Detection (Part 2  with Python codes)|
Pranav Dar
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","SQL Databases|NoSQL Databases|Hadoop and MapReduce|Apache Spark|Courses covering Hadoop, Spark, HIVE and Spark SQL|Kafka",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Before a model is built, before the data is cleaned and made ready for exploration, even before the role of a data scientist begins  this is where data engineers come into the picture. Every data-driven business needs to have a framework in place for the data science pipeline, otherwise its a setup for failure.Most people enter the data science world with the aim of becoming a data scientist, without ever realizing what a data engineer is, or what that role entails. These data engineers are vital parts of any data science project and their demand in the industry is growing exponentially in the current data-rich environment.There is currently no coherent or formal path available for data engineers. Most folks in this role got there by learning on the job, rather than following a detailed route. My aim for writing this article was to help anyone who wants to become a data engineer but doesnt know where to start and where to find study resources.In this article, I have put together a list of things every aspiring data engineer needs to know. Initially well see what a data engineer is and how the role differs from a data scientist. Then, well move on to the core skills you should have in your skillset before being considered a good fit for the role. I have also mentioned some industry recognized certifications you should consider.Right, lets dive right into it.A data engineer is responsible for building and maintaining the data architecture of a data science project. These engineers have to ensure that there is uninterrupted flow of data between servers and applications. Some of the responsibilities of a data engineer include improving data foundational procedures, integrating new data management technologies and softwares into the existing system, building data collection pipelines, among various other things.One of the most sought-after skills in data engineering is the ability to design and build data warehouses. This is where all the raw data is collected, stored and retrieved from. Without data warehouses, all the tasks that a data scientist does will become either too expensive or too large to scale.ETL (Extract, Transform, and Load) are the steps which a data engineer follows to build the data pipelines. ETL is essentially a blueprint for how the collected raw data is processed and transformed into data ready for analysis.Data engineers usually come from engineering backgrounds. Unlike data scientists, there is not much academic or scientific understanding required for this role. Developers or engineers who are interested in building large scale structures and architectures are ideally suited to thrive in this role.It is important to know the distinction between these 2 roles. Broadly speaking, a data scientist builds models using a combination of statistics, mathematics, machine learning and domain based knowledge. He/she has to code and build these models using the same tools/languages and framework that the organization supports.A data engineer on the other hand has to build and maintain data structures and architectures for data ingestion, processing, and deployment for large-scale data-intensive applications. To build a pipeline for data collection and storage, to funnel the data to the data scientists, to put the model into production  these are just some of the tasks a data engineer has to perform.For any large scale data science project to succeed, data scientists and data engineers need to work hand-in-hand. Otherwise things can go wrong very quickly!To learn more about the difference between these 2 roles, head over to our detailed infographic here.Its essential to first understand what data engineering actually is, before diving into the different facets of the role. What are the different functions a data engineer performs day-to-day? What do the top technology companies look for in a data engineer? Are you expected to know just about everything under the sun or just enough to be a good fit for a specific role? My aim is to provide you an answer to these questions (and more) in the resources below.A Beginners Guide to Data Engineering (Part 1): A very popular post on data engineering from a data scientist at Airbnb. The author first explains why data engineering is such a critical aspect of any machine learning project, and then deep dives into the various component of this subject. I consider this a compulsory read for all aspiring data engineers AND data scientists.A Beginners Guide to Data Engineering (Part 2): Continuing on from the above post, part 2 looks at data modeling, data partitioning, Airflow, and best practices for ETL.A Beginners Guide to Data Engineering (Part 3): The final part of this amazing series looks at the concept of a data engineering framework. Throughout the series, the author keeps relating the theory to practical concepts at Airbnb, and that trend continues here. A truly exquisitely written series of articles.OReillys Suite of Free Data Engineering E-Books:OReilly is known for their excellent books, and this collection is no exception to that. Except, these books are free! Scroll down to the Big Data Architecture section and check out the books there. Some of these require a bit of knowledge regarding Big Data infrastructure, but these books will help you get acquainted with the intricacies of data engineering tasks.While there are other data engineering-specific programming languages out there (like Java and Scala), well be focusing on Python in this article. We have seen a clear shift in the industry towards Python and is seeing a rapid adoption rate. Its become an essential part of a data engineers (and a data scientists) skillset.There are tons of resources online to learn Python. I have mentioned a few of them below.A complete tutorial to learn Data Science with Python from Scratch: This article by Kunal Jain covers a list of resources you can use to begin and advance your Python journey. A must-read resource.Introduction to Data Science using Python: This is Analytics Vidhyas most popular course that covers the basics of Python. We additionally cover core statistics concepts and predictive modeling methods to solidify your grasp on Python and basic data science.Codeacademys Learn Python course: This course assumes no prior knowledge of programming. It starts from the absolute basics of Python and is a good starting point.If you prefer learning through books, below are a couple of free ebooks to get you started:Think Python by Allen Downey:A comprehensive go-through of the Python language. Perfect for newcomers and even non-programmers.Non-Programmers Tutorial for Python 3:As the name suggests, its a perfect starting point for folks coming from a non-IT background or a non-technical background. There are plenty of examples in each chapter to test your knowledge.A key cog in the entire data science machine, operating systems are what make the pipelines tick. A data engineer is expected to know the ins and outs of infrastructure components, such as virtual machines, networks, applications services, etc. How well versed are you with server management? Do you know Linux well enough to navigate around different configurations? How familiar are you with access control methods? These are just some of the questions youll face as a data engineer.Linux Server Management and Security: This Coursera offering is designed for folks looking to understand how Linux works in the enterprise. The course is divided into 4 weeks (and a project at the end) and covers the basics well enough.CS401: Operating Systems: As comprehensive a course as any around operating systems. This contains nine sections dedicated to different aspects of an operating system. The primary focus is on UNIX-based systems, though Windows is covered as well.Raspberry Pi Platform and Python Programming for the Raspberry Pi: A niche topic, for sure, but the demand for this one is off the charts these days. This course aims to make you familiar with the Raspberry Pi environment and get you started with basic Python code on the Raspberry Pi.In order to become a data engineer, you need to have a very strong grasp on database languages and tools. This is another very basic requirement. You need to be able to collect, store and query information from these databases in real-time. There are tons of databases available today but I have listed down resources for the ones that are currently widely used in the industry today. These are divided into SQL and NoSQL databases.Source: MacWorld UKLearn SQL for Free: Another codeacademy entry, you can learn the absolute basics of SQL here. Topics like manipulation, queries, aggregate functions and multiple tables are covered from the ground up. If youre completely new to this field, not many places better than this to kick things off.Quick SQL Cheatsheet: An ultra helpful GitHub repository with regularly updated SQL queries and examples. Ensure you star/bookmark this repository as a reference point anytime you quickly need to check a command.MySQL Tutorial: MySQL was created over two decades ago, and still remains a popular choice in the industry. This resource is a text-based tutorial, presented in an easy-to-follow manner. The cool thing about this site is that practical examples with SQL scripts (and screenshots) accompany each topic.Learn Microsoft SQL Server: This text tutorial explores SQL Server concepts starting from the basics to more advanced topics. Concepts have been explained using codes and detailed screenshots.PostgreSQL Tutorial: An incredible detailed guide to get you started and well acquainted with PostgreSQL. The tutorial has been divided into 16 sections so you can imagine how well this subject has been covered.Oracle Live SQL: Who better to learn Oracles SQL database than the creators themselves? The platform is really well designed and makes for a great end user experience. You can view scripts and tutorials to get your feet wet, and then start coding on the same platform. Sounds awesome!Source: EventilMongoDB from MongoDB: This is currently the most popular NoSQL Database out there. And as with the Oracle training mentioned above, MongoDB is best learned from the masters themselves. I have linked their entire course catalogue here, so you can pick and choose which trainings you want to take.Introduction to MongoDB:This course will get you up and running with MongoDB quickly, and teach you how to leverage its power for data analytics. Its a short three weeks course but has plenty of exercises to make you feel like an expert by the time youre finished!Learn Cassandra: If youre looking for an excellent text-based and beginner-friendly introduction to Cassandra, this is the perfect resource. Topics like Cassandras architecture, installation, key operations, etc. are covered here. The tutorial also has dedicated chapters to explain the data types and collections available in CQL and how to make use of user-defined data types.Redis Enterprise: There are not many resources out there to learn about Redis Databases, but this one site is enough. There are multiple courses and beautifully designed videos to make the learning experience engaging and interactive. And its free!Google Bigtable: Being Googles offering, there are surprisingly sparse resources available to learn how Bigtable works. I have linked a Coursera course that includes plenty of Google Cloud topics but you can scroll down and select Bigtable (or BigQuery). I would, however, recommend going through the full course as it provides valuable insights into how Googles entire Cloud offerings work.Couchbase: Multiple trainings are available here (scroll down to see the free trainings), and they range from beginner to advanced. If Couchbase is your organizations database of choice, this is where youll learn everything about it.Distributed file systems like Hadoop (HDFS) can be found in any data engineer job description these days. Its a common role requirement and one you should be familiar with intimately. Apart from that, you need to gain an understanding of platforms and frameworks like Apache Spark, Hive, PIG, Kafka, etc. I have listed the resources for all these topics in this section.Hadoop Fundamentals: This is essentially a learning path for Hadoop. It includes 5 courses that will give you a solid understanding of what Hadoop is, the architecture and components that define it, how to use it, its applications and a whole lot more.Hadoop Starter Kit: This is a really good and comprehensive free course for anyone looking to get started with Hadoop. It includes topics like HDFS, MapReduce, Pig and HIVE with free access to clusters for practising what youve learned.Hortonworks Tutorials: As the creators of Hadoop, Hortonworks have a well respected set of courses for learning various things related to Hadoop. From beginners to advanced, this page has a very comprehensive list of tutorials. Ensure you check this out.Introduction to MapReduce: Before reading this article, you need to have some basic knowledge of how Hadoop works. Once done, come back and take a deep dive into the world of MapReduce.Hadoop Beyond Traditional MapReduce  Simplified: This article covers an overview of the Hadoop ecosystem that goes beyond simply MapReduce.Prefer books? No worries, I have you covered! Below are a few free ebooks that cover Hadoop and its components.Hadoop Explained: A basic introduction to the complicated world of Hadoop. It gives a high-level overview of how Hadoop works, its advantages, applications in real-life scenarios, among other things.Hadoop: What you Need to Know: This one is on similar lines to the above book. As the description says, the books covers just about enough to ensure you can make informed and intelligent decisions about Hadoop.Data-Intensive Text Processing with MapReduce:This free ebook covers the basics of MapReduce, its algorithm design, and then deep dives into examples and applications you should know about. Its recommended that you take the above courses first before reading this book.You should also join the Hadoop LinkedIn group to keep yourself up-to-date and to ask any queries you might have.Comprehensive Guide to Apache Spark, RDDs and Dataframes (using PySpark): This is the ultimate article to get you stared with Apache Spark. It covers the history of Apache Spark, how to install it using Python, RDD/Dataframes/Datasets and then rounds-up by solving a machine learning problem. A must-read guide.Step by Step Guide for Beginners to Learn SparkR: In case you are a R user, this one is for you! You can of course use Spark with R and this article will be your guide.Spark Fundamentals: This course covers the basics of Spark, its components, how to work with them, interactive examples of using Spark, introduction to various Spark libraries and finally understanding the Spark cluster. What more could you ask for from one course?Introduction to Apache Spark and AWS: This is a practical and practice focused course. You will work with the Gutenberg Project data, the worlds largest open collection of ebooks. You will need knowledge of Python and the Unix command line to extract the most out of this course.Big Data Essentials: HDFS, MapReduce and Spark RDD: This course takes real-life datasets to teach you basic Big Data technologies  HDFS, MapReduce and Spark. Its a typical Coursera course  detailed, filled with examples and useful datasets, and taught by excellent instructors.Big Data Analysis: Hive, Spark SQL, DataFrames and GraphFrames: MapReduce and Spark tackle the issue of working with Big Data partially. Learn high-level tools with this intuitive course where youll master your knowledge of Hive and Spark SQL, among other things.Big Data Applications: Real-Time Streaming:One of the challenges of working with enourmous amounts of data is not just the computational power to process it, but to do so as quickly as possible. Applications like recommendation engines require real-time data processing and to store and query this amount of data requires knowledge of systems like Kafka, Cassandra and Redis, which this course provides.But to take this course, you need a working knowledge of Hadoop, Hive, Python, Spark and Spark SQL.Simplifying Data Pipelines with Apache Kafka:Get the low down on what Apache Kafka is, its architecture and how to use it. You need a basic understanding of Hadoop, Spark and Python to truly gain the most from this course.Kafkas Official Documentation: This is an excellent intuitive introduction to how Kafka works and the various components that go toward making it work. This page also includes a nice explanation of what a distributed streaming platform is.Putting the Power of Kafka into the Hands of Data Scientists:Not quite a learning resource per se, but a very interesting and detailed article on how data engineers at Stitch Fix built a platform tailored to the requirements of their data scientists.While machine learning is primarily considered the domain of a data scientist, a data engineer needs to be well versed with certain techniques as well. Why, you ask? Getting models into production and making pipelines for data collection or generation need to be streamlined, and these require at least a basic understanding of machine learning algorithms.Machine Learning Basics for a Newbie: A superb introduction to the world of machine learning by Kunal Jain. The aim of the article is to do away with all the jargon youve heard or read about. The guide cuts straight to heart of the matter, and you end up appreciating that style of writing.Essentials of Machine Learning Algorithms: This is an excellent article that provides a high-level understanding of various machine learning algorithms. It includes an implementation of these techniques in R and Python as well  a perfect place to start your journey.Must-Read Books for Beginners on Machine Learning and Artificial Intelligence:If books are more to your taste, then check out this article! This is a collection of the best of the best, so even if you read only a few of these books, youll have gone a long way towards your dream career.24 Ultimate Data Science Projects to Boost your Knowledge and Skills: Once youve acquired a certain amount of knowledge and skill, its always highly recommended to put your theoretical knowledge into practice. Check out these datasets, ranked in order of their difficulty, and get your hands dirty.Googles Certified ProfessionalSource: Fourcast.ioThis is one of the premier data engineering certifications available today. To earn this certification, you need to successfully clear a challenging 2 hour multiple choice exam. You can find the general outline of what to expect on this link. Also available are links to get hands-on practice with Google Cloud technologies. Ensure you check this out!IBM Certified Data EngineerTo attain this certification, you need to pass one exam  this one. The exam contains 54 questions out of which you have to answer 44 correctly. I recommend going through what IBM expects you to know before you sit for the exam. The exam link also contains further links to study materials you can refer to for preparing yourself.Clouderas CCP Data EngineerThis is another globally recognized certification, and a pretty challenging one for a newcomer. Your concepts need to be up-to-date and in-depth, you should have some hands-on experience with data engineering tools like Hadoop, Oozie, AWS Sandbox, etc. But if you clear this exam, you are looking at a very promising start to this field of work!Cloudera has mentioned that it would help if you took their training for Apache Spark and Hadoop since the exam is heavily based on these two tools.Becoming a data engineer is no easy feat, as youll have gathered from all the above resources. It requires a deep understanding of tools, techniques and a solid work ethic to become one. This role is in huge demand in the industry thanks to the recent data boom and will continue to be a rewarding career option for anyone willing to take it.Once you go through this path, you will be gunning for the data engineer role! Let me know your feedback and suggestions about this set of resources in the comments section below.",https://www.analyticsvidhya.com/blog/2018/11/data-engineer-comprehensive-list-resources-get-started/
A Practical Implementation of the Faster R-CNN Algorithm for Object Detection (Part 2  with Python codes),Learn everything about Analytics|Introduction|Table of Contents|A BriefOverview of the Different R-CNN Algorithms for Object Detection|Understanding the Problem Statement|Setting up the System|Data Exploration|Implementing Faster R-CNN|End Notes,"Like this:|Related Articles|Want to Become a Data Engineer? Heres a Comprehensive List of Resources to get Started|Top 5 Machine Learning GitHub Repositories & Reddit Discussions (October 2018)|
Pulkit Sharma
|139 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Which algorithm do you use for object detection tasks? I have tried out quite a few of them in my quest to build the most precise model in the least amount of time. And this journey, spanning multiple hackathons and real-world datasets, has usually always led me to the R-CNN family of algorithms.It has been an incredible useful framework for me, and thats why I decided to pen down my learnings in the form of a series of articles. The aim behind this series is to showcase how useful the different types of R-CNN algorithms are. The first part received an overwhelmingly positive response from our community, and Im thrilled to present part two!In this article, we will first briefly summarize what we learned in part 1, and then deep dive into the implementation of the fastest member of the R-CNN family  Faster R-CNN. I highly recommend going through this article if you need to refresh your object detection concepts first:A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1).Part 3 of this series is published now and you can check it out here: A Practical Guide to Object Detection using the Popular YOLO Framework  Part III (with Python codes)We will work on a very interesting dataset here, so lets dive right in!Lets quickly summarize the different algorithms in the R-CNN family (R-CNN, Fast R-CNN, and Faster R-CNN) that we saw in the first article. This will help lay the ground for our implementation part later when we will predict the bounding boxes present in previously unseen images (new data).R-CNN extracts a bunch of regions from the given image using selective search, and then checks if any of these boxes contains an object. We first extract these regions, and for each region, CNN is used to extract specific features. Finally, these features are then used to detect objects. Unfortunately, R-CNN becomes rather slow due to these multiple steps involved in the process.R-CNNFast R-CNN, on the other hand, passes the entire image to ConvNet which generates regions of interest (instead of passing the extracted regions from the image). Also, instead of using three different models (as we saw in R-CNN), it uses a single model which extracts features from the regions, classifies them into different classes, and returns the bounding boxes.All these steps are done simultaneously, thus making it execute faster as compared to R-CNN. Fast R-CNN is, however, not fast enough when applied on a large dataset as it also uses selective search for extracting the regions.Fast R-CNNFaster R-CNN fixes the problem of selective search by replacing it with Region Proposal Network (RPN). We first extract feature maps from the input image using ConvNet and then pass those maps through a RPN which returns object proposals. Finally, these maps are classified and the bounding boxes are predicted.Faster R-CNNI have summarized below the steps followed by a Faster R-CNN algorithm to detect objects in an image:What better way to compare these different algorithms than in a tabular format? So here you go!Now that we have a grasp on this topic, its time to jump from the theory into the practical part of our article. Lets implement Faster R-CNN using a really cool (and rather useful) dataset with potential real-life applications!We will be working on a healthcare related dataset and the aim here is to solve a Blood Cell Detection problem. Our task is to detect all the Red Blood Cells (RBCs), White Blood Cells (WBCs), and Platelets in each image taken via microscopic image readings. Below is a sample of what our final predictions should look like:The reason for choosing this dataset is that the density of RBCs, WBCs and Platelets in our blood stream provides a lot of information about the immune system and hemoglobin. This can help us potentially identify whether a person is healthy or not, and if any discrepancy is found in their blood, actions can be taken quickly to diagnose that.Manually looking at the sample via a microscope is a tedious process. And this is where Deep Learning models play such a vital role. They can classify and detect the blood cells from microscopic images with impressive precision.The full blood cell detection dataset for our challenge can be downloaded from here. I have modified the data a tiny bit for the scope of this article:Note that we will be using the popular Keras framework with a TensorFlow backend in Python to train and build our model.Before we actually get into the model building phase, we need to ensure that the right libraries and frameworks have been installed. The below libraries are required to run this project:Most of the above mentioned libraries will already be present on your machine if you have Anaconda and Jupyter Notebooks installed. Additionally, I recommenddownloading the requirement.txt file from this link and use that to install the remaining libraries. Type the following command in the terminal to do this:Alright, our system is now set and we can move on to working with the data!Its always a good idea (and frankly, a mandatory step) to first explore the data we have. This helps us not only unearth hidden patterns, but gain a valuable overall insight into what we are working with. The three files I have created out of the entire dataset are:Lets read the .csv file (you can create your own .csv file from the original dataset if you feel like experimenting) and print out the first few rows. Well need to first import the below libraries for this:There are 6 columns in the train file. Lets understand what each column represents:Lets now print an image to visualize what were working with:This is what a blood cell image looks like. Here, the blue part represents the WBCs, and the slightly red parts represent the RBCs. Lets look at how many images, and the different type of classes, there are in our training set.So, we have 254 training images.We have three different classes of cells, i.e., RBC, WBC and Platelets. Finally, lets look at how an image with detected objects will look like:This is what a training example looks like. We have the different classes and their corresponding bounding boxes. Lets now train our model on these images. We will be using the keras_frcnn library to train our model as well as to get predictions on the test images.For implementing the Faster R-CNN algorithm, we will be following the steps mentioned inthis Github repository. So as the first step, make sure you clone this repository. Open a new terminal window and type the following to do this:Move the train_images and test_images folder, as well as the train.csv file, to the cloned repository. In order to train the model on a new dataset, the format of the input should be:where,We need to convert the .csv format into a .txt file which will have the same format as described above. Make a new dataframe, fill all the values as per the format into that dataframe, and then save it as a .txt file.Whats next?Train our model! We will be using the train_frcnn.py file to train the model.It will take a while to train the model due to the size of the data. If possible, you can use a GPU to make the training phase faster. You can also try to reduce the number of epochs as an alternate option. To change the number of epochs, go to the train_frcnn.py file in the cloned repository and change the num_epochsparameter accordingly.Every time the model sees an improvement, the weights of that particular epoch will be saved in the same directory as model_frcnn.hdf5. These weights will be used when we make predictions on the test set.It might take a lot of time to train the model and get the weights, depending on the configuration of your machine. I suggest using the weights Ive got after training the model for around 500 epochs. You can download these weightsfrom here. Ensure you save these weights in the cloned repository.So our model has been trained and the weights are set. Its prediction time! Keras_frcnn makes the predictions for the new images and saves them in a new folder.We just have to make two changes in the test_frcnn.py file to save the images:Lets make the predictions for the new images:Finally, the images with the detected objects will be saved in the results_imgs folder. Below are a few examples of the predictions I got after implementing Faster R-CNN:Result 1Result 2Result 3Result 4R-CNN algorithms have truly been a game-changer for object detection tasks. There has suddenly been a spike in recent years in the amount of computer vision applications being created, and R-CNN is at the heart of most of them.Keras_frcnn proved to be an excellent library for object detection, and in the next article of this series, we will focus on more advanced techniques like YOLO, SSD, etc.If you have any query or suggestions regarding what we covered here, feel free to post them in the comments section below and I will be happy to connect with you!",https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-detection/
Top 5 Machine Learning GitHub Repositories & Reddit Discussions (October 2018),Learn everything about Analytics|Introduction|GitHub Repositories|Faster R-CNN and Mask R-CNN in PyTorch 1.0|Tencent ML Images (Largest Open-Source Multi-Label Image Database)|PyTorch Implementation of Google AIs BERT (NLP)|Extracting Latest Arxiv Research Papers and their Abstracts|DeepMimic|Bonus: AdaNet by Google AI|Reddit Discussions|What Developments can we Expect in Machine Learning in the Next 5 Years?|Advice for a Non-ML Engineer who Manages Machine Learning Researchers|Topic Ideas for Machine Learning Projects|Why do Machine Learning Papers have Such Terrible Math?|The Disadvantages of the Hype Around Machine Learning|End Notes,"Share this:|Like this:|Related Articles|A Practical Implementation of the Faster R-CNN Algorithm for Object Detection (Part 2  with Python codes)|An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Should I use GitHub for my projects?  Im often asked this question by aspiring data scientists. Theres only one answer to this  Absolutely!.GitHub is an invaluable platform for data scientists looking to stand out from the crowd. Its an online resume for displaying your code to recruiters and other fellow professionals. The fact that GitHub hosts open-source projects from the top tech behemoths like Google, Facebook, IBM, NVIDIA, etc. is what adds to the gloss of an already shining offering.If youre a beginner in data science, or even an established professional, you should have a GitHub account. And to save you the time of looking for the most interesting repositories out there (and there are plenty), I am delighted to scour the platform and bring them straight to you in this monthly series.This months collection comes from a variety of use cases  computer vision (object detection and segmentation), PyTorch implementation of Google AIs record-breaking BERT framework for NLP, extracting the latest research papers with their summaries, among others. Scroll down to start learning!Why do we include Reddit discussions in this series? I have personally found Reddit an incredibly rewarding platform for a number of reasons  rich content, top machine learning/deep learning experts taking the time to propound their thoughts, a stunning variety of topics, open-source resources, etc. I could go on all day, but suffice to say I highly recommend going through these threads I have shortlisted  they are unique and valuable in their own way.You can check out the top GitHub repositories and Reddit discussions (from April onwards) we have covered each month below:Computer vision has become so incredibly popular these days that organizations are rushing to implement and integrate the latest algorithms in their products. Sounds like a pretty compelling reason to jump on the bandwagon, right?Of course, object detection is easily the most sought-after skill to learn in this domain. So heres a really cool project from Facebook that aims to provide the building blocks for creating segmentation and detection models using the their popular PyTorch 1.0 framework. Facebook claims that this is upto two times faster than its Detectron framework, and comes with pre-trained models. Enough resources and details to get started!I encourage you to check out a step-by-step introduction to the basic object detection algorithms if you need a quick refresher. And if youre looking to get familiar with the basics of PyTorch, check out this awesome beginner-friendly tutorial.This repository is a goldmine for all deep learning enthusiasts. Intrigued by the heading? Just wait till you check out some numbers about this dataset:17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories. Incredible!This project also include a pre-trained Resnet-101 model, which has so far achieved a 80.73% accuracy on ImageNet via transfer learning. The repository contains exhaustive details and code on how and where to get started. This is a significant step towards making high quality data available to the community.Oh, and did I mentioned that these images are annotated? What are you waiting for, go ahead and download it NOW!Wait, another PyTorch entry? Just goes to show how popular this framework has become. For those who havent heard of BERT, its a language representation model that stands forBidirectional Encoder Representations from Transformers. It sounds like a mouthful, but it has been making waves in the machine learning community.BERT has set all sorts of new benchmarks in 11 natural language processing (NLP) tasks. A pre-trained language model being used on a wide range of NLP tasks might sound outlandish to some, but the BERT framework has transformed it into reality. It even emphatically outperformed humans on the popular SQuAD question answering test.This repository contains the PyTorch code for implementing BERT on your own machine. As Google Brains Research Scientist Thang Luong tweeted, this could well by the beginning of a new era in NLP.In case youre interested in reading the research paper, thats also available here. And in case youre eager (like me) to see the official Google code, bookmark (or star)this repository.How can we stay on top of the latest research in machine learning? It seems we see breakthroughs on an almost weekly basis and keeping up with them is a daunting, if not altogether impossible, challenge. Most top researchers post their full papers on arxiv.org so is there any way of sorting through the latest ones?Yes, there is! This repository uses Python (v3.x) to return the latest results by scraping arxiv papers and summarizing their abstracts. This is an really useful tool, as it will help us stay in touch with the latest papers and let us pick the one(s) we want to read. As mentioned in the repository, you can run the below command to search for a keyword:The script returns five results by default if you fail to specify how many instances you want.I always try to include at least one reinforcement learning repository in these lists  primarily because I feel everyone in this field should be aware of the latest advancements in this space. And this months entry is a fascinating one  motion imitation with deep reinforcement learning.This repository in an implementation of the DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills paper presented at SIGGRAPH 2018. Quoting from the repository, The framework uses reinforcement learning to train a simulated humanoid to imitate a variety of motion skills. Check out the above project link which includes videos and code on how to implement this framework on your own.I just couldnt leave out this incredibly useful repository. AdaNet is a lightweight and scalable TensorFlow-based framework for automatically learning high-quality models. The best part about it is that you dont need to intervene too much  the framework is smart and flexible enough for building better models.You can read more about AdaNet here. Google, as usual, does a great job of explaining complex concepts.Ah, the question on everybodys mind. Will autoML be ruling the roost? How will the hardware have advanced? Will there finally be official rules and policies around ethics? Will machine learning have integrated itself into the very fabric of society? Will reinforcement learning finally have found a place in the industry?These are just some of the many thoughts propounded in this discussion. Individuals have their own predictions about what they expect and what they want to see, and this discussion does an excellent job of combining the two. The conversation varies between technical and non-technical topics so you have the luxury of choosing which ones you prefer reading about.Interesting topic. Weve seen this trend before  a non-ML person is assigned to lead a team of ML experts and it usually ends in frustration for both parties. Due to various reasons (time constraints being top of that list), it often feels like things are at an impasse.I implore all project managers, leaders, CxOs, etc. to take the time and go through this discussion thread. There are some really useful ideas that you can implement in your own projects as soon as possible. Getting all the technical and non-technical folks on the same page is a crucial cog in the overall projects success so its important that the leader(s) sets the right example.Looking for a new project to experiment with? Or need ideas for your thesis? Youve landed at the right place. This is a collection of ideas graduate students are working on to hone and fine tune their machine learning skills. Some of the ones that stood out for me are:This is where Reddit becomes so useful  you can pitch your idea in this discussion and youll receive feedback from the community on how you can approach the challenge.This one is a fully technical discussion as you might have gathered from the heading. This is an entirely subjective question and answers vary depending on the level of experience the reader has and how well the researcher has put across his/her thoughts. I like this discussion because there very specific examples of linked research papers so you can explore them and form your own opinion.Its a well known (and accepted) fact that quite a lot of papers have math and findings all cobbled together  not everyone has the patience, willingness or even the ability to present their study in a lucid manner. Its always a good idea to work on your presentation skills while you can.How do established professionals feel when their field starts getting tons of attention from newbies? Its an interesting question that potentially spans domains, but this thread focuses on machine learning.This is not a technical discussion per se, but its interesting to note how top data scientists and applied machine learning professionals feel about the recent spike in interest in this field. The discussion, with over 120+ comments, is rich in thought and suggestions. Things get especially interesting when the topic of how to deal with non-technical leaders and team members comes up. There are tons of ideas to steal from here!This year really has seen some amazing research being open-sourced. Regardless of what happens after Microsofts official takeover of GitHub, it remains the primary platform for collaboration among programmers, developers and data scientists. I implore everyone reading this to start using GitHub more regularly, even if its just for browsing the latest repositories.Which GitHub repository and/or Reddit discussion stood out for you? Are there any other libraries or frameworks you feel I should have included in this article? Let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/11/best-machine-learning-github-repositories-reddit-threads-october-2018/
An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation),Learn everything about Analytics|Introduction|Table of Contents|Text Summarization Approaches|Understanding the TextRank Algorithm|Understanding the Problem Statement|Implementation of the TextRank Algorithm,"PageRank Algorithm||TextRank Algorithm|Whats Next?|End Notes|Share this:|Like this:|Related Articles|Top 5 Machine Learning GitHub Repositories & Reddit Discussions (October 2018)|DataHack Radio #13: Data Science and AI in the Oil & Gas Industry with Yogendra Pandey, Ph.D.|
Prateek Joshi
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",You can download the dataset well be using from here.|Import Required Libraries|Read the Data|Inspect the Data|Split Text into Sentences|Download GloVe Word Embeddings|Text Preprocessing|Vector Representation of Sentences|Similarity Matrix Preparation|Applying PageRank Algorithm|Summary Extraction|Please find the code in this GitHub Repo.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Text Summarization is one of those applications of Natural Language Processing (NLP) which is bound to have a huge impact on our lives. With growing digital media and ever growing publishing  who has the time to go through entire articles / documents / books to decide whether they are useful or not? Thankfully  this technology is already here.Have you come across the mobile app inshorts? Its an innovative news app that converts news articles into a 60-word summary. And that is exactly what we are going to learn in this articleAutomatic Text Summarization.Automatic Text Summarization is one of the most challenging and interesting problems in the field of Natural Language Processing (NLP). It is a process of generating a concise and meaningful summary of text from multiple text resources such as books, news articles, blog posts, research papers, emails, and tweets.The demand for automatic text summarization systems is spiking these days thanks to the availability of large amounts of textual data. Through this article, we will explore the realms of text summarization. We will understand how the TextRank algorithm works, and will also implement it in Python. Strap in, this is going to be a fun ride!Automatic Text Summarization gained attention as early as the 1950s. A research paper, published by Hans Peter Luhn in the late 1950s, titled The automatic creation of literature abstracts, used features such as word frequency and phrase frequency to extract important sentences from the text for summarization purposes.Another important research, done by Harold P Edmundson in the late 1960s, used methods like the presence of cue words, words used in the title appearing in the text, and the location of sentences, to extract significant sentences for text summarization. Since then, many important and exciting studies have been published to address the challenge of automatic text summarization.Text summarization can broadly be divided into two categories  Extractive Summarization and Abstractive Summarization.In this article, we will be focusing on the extractive summarization technique.Before getting started with the TextRank algorithm, theres another algorithm which we should become familiar with  the PageRank algorithm. In fact, this actually inspired TextRank! PageRank is used primarily for ranking web pages in online search results. Lets quickly understand the basics of this algorithm with the help of an example.Source: http://www.scottbot.net/HIAL/Suppose we have 4 web pages  w1, w2, w3, and w4. These pages contain links pointing to one another. Some pages might have no link  these are called dangling pages.In order to rank these pages, we would have to compute a score called the PageRank score. This score is the probability of a user visiting that page. To capture the probabilities of users navigating from one page to another, we will create a square matrix M, having n rows and n columns, where n is the number of web pages.Each element of this matrix denotes the probability of a user transitioning from one web page to another. For example, the highlighted cell below contains the probability of transition from w1 to w2.The initialization of the probabilities is explained in the steps below:Hence, in our case, the matrix M will be initialized as follows:Finally, the values in this matrix will be updated in an iterative fashion to arrive at the web page rankings.Lets understand the TextRank algorithm, now that we have a grasp on PageRank. I have listed the similarities between these two algorithms below:TextRank is an extractive and unsupervised text summarization technique. Lets take a look at the flow of the TextRank algorithm that we will be following:So, without further ado, lets fire up our Jupyter Notebooks and start coding!Note: If you want to learn more about Graph Theory, then Id recommend checking out this article.Being a major tennis buff, I always try to keep myself updated with whats happening in the sport by religiously going through as many online tennis updates as possible. However, this has proven to be a rather difficult job! There are way too many resources and time is a constraint.Therefore, I decided to design a system that could prepare a bullet-point summary for me by scanning through multiple articles. How to go about doing this? Thats what Ill show you in this tutorial. We will apply the TextRank algorithm on a dataset of scraped articles with the aim of creating a nice and concise summary.Please note that this is essentially a single-domain-multiple-documents summarization task, i.e., we will take multiple articles as input and generate a single bullet-pointsummary. Multi-domain text summarization is not covered in this article, but feel free to try that out at your end.So, without any further ado, fire up your Jupyter Notebooks and lets implement what weve learned so far.First, import the libraries well be leveraging for this challenge.Now lets read our dataset. I have provided the link to download the data in the previous section (in case you missed it).Lets take a quick glance at the data.
We have 3 columns in our dataset  article_id, article_text, and source. We are most interested in the article_text column as it contains the text of the articles.Lets print some of the values of the variable just to see what they look like.Output:Now we have 2 options  we can either summarize each article individually, or we can generate a single summary for all the articles. For our purpose, we will go ahead with the latter.Now the next step is to break the text into individual sentences. We will use thesent_tokenize( )function of thenltk library to do this.Lets print a few elements of the list sentences.Output:GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large).We will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available here. Heads up  the size of these word embeddings is 822 MB.Lets extract the words embeddings or word vectors.We now have word vectors for 400,000 different terms stored in the dictionary  word_embeddings.It is always a good practice to make your textual data noise-free as much as possible. So, lets do some basic text cleaning.Get rid of thestopwords (commonly used words of a language  is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code:Now we can import the stopwords.Lets define a function to remove these stopwords from our dataset.We will use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors.Now, lets create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence.Note: For more text preprocessing best practices, you may check our video course,Natural Language Processing (NLP) using Python.The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Lets create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.Lets first define a zero matrix of dimensions (n * n). We will initialize this matrix with cosine similarity scores of the sentences. Here,n is the number of sentences.We will use Cosine Similarity to compute the similarity between a pair of sentences.And initialize the matrix with cosine similarity scores.Before proceeding further, lets convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings.Finally, its time to extract the top N sentences based on their rankings for summary generation.And there we go! An awesome, neat, concise, and useful summary for our articles.Automatic Text Summarization is ahot topic of research, and in this article, we have covered just the tip of the iceberg. Going forward, we will explore the abstractive text summarization technique where deep learning plays a big role. In addition, we can also look into the following summarization tasks:Problem-specificAlgorithm-specificI hope this post helped you in understanding the concept of automatic text summarization. It has a variety of use cases and has spawned extremely successful applications. Whether its for leveraging in your business, or just for your own knowledge, text summarization is an approach all NLP enthusiasts should be familiar with.I will try to cover theabstractive text summarization technique using advanced techniques in a future article. Meanwhile, feel free to use the comments section below to let me know your thoughts or ask any questions you might have on this article.",https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/
"DataHack Radio #13: Data Science and AI in the Oil & Gas Industry with Yogendra Pandey, Ph.D.","Learn everything about Analytics|Introduction|Yogendra Pandeys Background|Applications of AI and ML in the Oil and Gas Industry|The Oil and Gas Pipeline, and the role of AI and ML in this Industry|A Couple of Resources for Keeping up with the Latest in Oil and Gas|End Notes","Share this:|Related Articles|An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)|An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine Learning for Programmers  Part 2)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you know that the oil and gas industry is currently only using close to 1% of the data it generates? A mind-boggling figure, and not one we usually think about when talking about artificial intelligence and machine learning applications.In episode #13 of the DataHack Radio podcast, we are joined by Yogendra Narayan Pandey, Ph.D, as he takes us on a knowledge-rich journey in the world of oil and gas.This is not a field that grabs a lot of headlines in the AI and ML community, but as youll find out in this podcast, the potential applications are vast. The amount of data collected in a typical oil and gas exploration and production process is staggeringly high, and that in turn spawns multiple use cases where machine learning techniques like regression, clustering and neural networks can be applied.Yogendra has done a phenomenal job of condensing the end-to-end oil and gas life-cycle into byte-sized knowledge for you and me to capture. Its well worth spending your time listening to this podcast and broadening your horizons. Happy listening!Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms:Yogendra is the Founder and Managing Consultant at PRABUDDHA, an organization that provides AI solutions for the oil and energy industry. Heis a chemical engineer from IIT-Varanasi and successfully completed his Ph.D. from the University of Houston in the same field (his dissertation topic was A Simulation Approach to Thermodynamics in Interfacial Phenomena).In his professional career, Yogendra has worked for organizations like Halliburton, Innowatts, and W.D. Von Gonten Laboratories. His role in all these organizations has been in the capacity of a data scientist. His passion for the oil and gas industry has driven him to pursue and make a mark in this field.In the initial part of the podcast, Yogendra has described his work in this fascinating space following his Ph.D. Anyone with an interest in data science and the energy field will really appreciate this episode!Oil and gas is a high-risk industry, so this makes the validation phase longer than usual. Decision makers have to be far more cautious, and this is one of the primary reasons why AI and ML have seen a slow adoption rate in this domain. But as Yogendra mentioned, this scenario is starting to change as technological advancements gather pace.One of the most important use cases of AI in oil and gas are predictive maintenance and equipment failure analytics. Another application is around autonomous drilling rigs, which means designing an end-to-end fully automatic drilling system. This system is smart enough to understand where to drill (optimal well path), how to drill, and the optimal duration required to finish the job. Like most AI applications these days, these autonomous rigs aim to augment the manual effort workers put in, rather than replace their jobs.To give you a very high-level overview, we can broadly divide the end-to-end oil and gas life-cycle (starting from a drop of oil found thousands of feet beneath the surface) into three major segments:For drilling operations, a large setup offshore can generate up to 1-2 terabytes of data everyday. The same goes for a large downstream refinery  it can generate up to 1 terabyte of data per day. So if you were wondering where and how much data this industry can come up, this is a pretty good place to start!Each segment mentioned above has been explained eloquently and in detail with multiple examples by Yogendra in the podcast and trust me, the entire process is incredibly enthralling. My favorite part was about how a model can tell you whether a certain region has oil in it or not with a remarkably high accuracy rate (a probabilistic model). This helps the organization(s) decide whether its worth drilling in that region. Unsupervised learning techniques like clustering are heavily leveraged in this process.Other algorithms used by data scientists in this domain for forecasting include regression, Hidden Markov Models for time series, Recurrent Neural Networks, Gated Recurrent Units (GRUs), and Long Short Term Memory (LSTMs), among others.Before hearing this podcast, I honestly had a very vague idea of the oil and gas industry, and how AI and ML are being used to transform traditional processes and drive revenue. It took just 50-odd minutes for me to get a good idea of the entire oil and gas pipeline, from identifying the drilling regions and starting the drilling process to getting the oil to petrol pumps, etc.I was fully immersed in this podcast and Im sure you will be as well. Make sure you listen to this and let us know your thoughts on how else AI and ML can drive change in the oil and gas industry.",https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-oil-gas-ai/
An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine Learning for Programmers  Part 2),Learn everything about Analytics|Introduction|Table of contents|Overview of Part 1 (Lessons 1 and 2)|Introduction to Machine Learning: Lesson 3|Introduction to Machine Learning : Lesson 4|Introduction to Machine Learning : Lesson 5|Additional Topics|End Notes,"|Confidence based on Tree Variance|Feature Importance|One-Hot Encoding|Removing redundant features|Partial Dependence|Tree Interpreter|Extrapolation|Random Forest from Scratch|Share this:|Like this:|Related Articles|DataHack Radio #13: Data Science and AI in the Oil & Gas Industry with Yogendra Pandey, Ph.D.|MADRaS: A Multi-Agent DRiving Simulator for Autonomous Driving Research|
Aishwarya Singh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine Learning is a fast evolving field  but a few things would remain as they were years ago. One such thing is ability to interpret and explain your machine learning models. If you build a model and can not explain it to your business users  it is very unlikely that it will see the light of the day.Can you imagine integrating a model into your product without understanding how it works? Or which features are impacting your final result?In addition to backing from stakeholders, we as data scientists benefit from interpreting our work and improving upon it. Its a win-win situation all around!The first article of this fast.ai machine learning course saw an incredible response from our community. Im delighted to share part 2 of this series, which primarily deals with how you can interpret a random forest model. We will understand the theory and also implement it in Python to solidify our grasp on this critical concept.As always, I encourage you to replicate the code on your own machine while you go through the article. Experiment with the code and see how different your results are from what I have covered in this article. This will help you understand the different facets of both the random forest algorithm and the importance of interpretability.Before we dive into the next lessons of this course, lets quickly recap what we covered in the first two lessons. This will give you some context as to what to expect moving forward.We will continue working on the same dataset in this article. We will have a look at what are the different variables in the dataset and how can we build a random forest model to make valuable interpretations.Alright, its time to fire up our Jupyter notebooks and dive right in to lesson#3!You can access the notebook for this lesson here. This notebook will be used for all the three lessons covered in this video. You can watch the entire lesson in the below video (or just scroll down and start implementing things right away):NOTE: Jeremy Howard regularly provides various tips that can be used for solving a certain problem more efficiently, as we saw in the previous article as well. A part of this video is about how to deal with very large datasets. I have included this in the last section of the article so we can focus on the topic at hand first. Lets continue from where we left off at the end of lesson 2. We had created new features using the date column and dealt with the categorical columns as well. We will load the processed dataset which includes our newly engineered features and thelog of thesalepricevariable (since the evaluation metric is RMSLE):We will define the necessary functions which well be frequentlyusing throughout our implementation.The next step will be to implement a random forest model and interpret the results to understand our dataset better. We have so far learned that random forest is a group of many trees, each trained on a different subset of data points and features. Each individual tree is as different as possible, capturing unique relations from the dataset. We make predictions by running each row through each tree and taking the average of the values at the leaf node. This average is taken as the final prediction for the row.While interpreting the results, it is necessary that the process is interactive and takes lesser time to run. To make this happen, we will make two changes in the code (as compared to what we implemented in the previous article):Were only using a sample as working with the entire data will take a long time to run. An important thing to note here is that the sample should not be very small. This might end up giving a different result and thatll be detrimental to our entire project. A sample size of 50,000 works well.Previously, we made predictions for each row using every single tree and then we calculated the mean of the results and the standard deviation.You might have noticed that this works in a sequential manner. Instead, we can call the predict function on multiple trees in parallel! This can be achieved using theparallel_trees function in the fastai library.The time taken here is less and the results are exactly the same! We will now create a copy of the data so that any changes we make do not affect the original dataset.Once we have the predictions, we can calculate the RMSLE to determine how well the model is performing. But the overall value does not help us identify how close the predicted values are for a particular row or how confident we are that the predictions are correct. We will look at the standard deviation for the rows in this case.If a row is different from those present in the train set, each tree will give different values as predictions. This consequently means means that the standard deviation will be high. On the other hand, the trees would make almost similar predictions for a row that is quite similar to the ones present in the train set, t, i.e., the standard deviation will be low. So, based on the value of the standard deviations we can decide how confident we are about the predictions.Lets save these predictions and standard deviations:Now, lets take up a variable from the dataset and visualization its distribution and understand what it actually represents. Well begin with theEnclosure variable.The actual sale price and the prediction values are almost similar in three categories  EROPS, EROPS w AC, OROPS (the remaining have null values). Since these null value columns do not add any extra information, we will drop them and visualize the plots for salesprice and prediction:Note that the small black bars represent standard deviation. In the same way, lets look at another variable  ProductSize.We will take a ratio of the standard deviation values and the sum of predictions in order to compare which category has a higher deviation.The standard deviation is higher for the Large and Compact categories. Why do you that is? Take a moment to ponder the answer before reading on.Have a look at the bar plot of values for each category in ProductSize.Found the reason? We have a lesser number of rows for these two categories. Thus, the model is giving a relatively poor prediction accuracy for these variables.Using this information, we can say that we are more confident about the predictions for the mini, medium and medium/large product size, and less confident about the small, compact and large ones.Feature importance is one of the key aspects of a machine learning model. Understanding which variable is contributing the most to a model is critical to interpreting the results. This is what data scientists strive for when building models that need to be explained to non-technical stakeholders.Our dataset has multiple features and it is often difficult to understand which feature is dominant. This is where the feature importance function of random forest is so helpful. Lets look at the top 10 most important features for our current model (including visualizing them by their importance):Thats a pretty intuitive plot. Heres a bar plot visualization of the top 30 features:ClearlyYearMade is the most important feature, followed by Coupler_System.The majority of the features seems to have little importance in the final model. Lets verify this statement by removing these features and checking whether this affects the models performance.So, we will build a random forest model using only the features that have a feature importance greater than 0.005:When you think about it, removing redundant columns should not decrease the model score, right? And in this case, the model performance has slightly improved. Some of the features we dropped earlier might have been highly collinear with others, so removing them did not affect the model adversely. Lets check feature importance again to verify our hypothesis:The difference between the feature importance of theYearMade and Coupler_Systemvariables is more significant. From the list of features removed, some features were highly collinear to YearMade, resulting in distribution of feature importance between them.On removing these features, we can see that the difference between the importance of YearMade and CouplerSystem has increased from the previous plot. Here is a detailed explanation of how feature importance is actually calculated:And that wraps up the implementation of lesson #3! I encourage you to try out these codes and experiment with them on your own machine to truly understand how each aspect of a random forest model works.In this lesson, Jeremy Howard gives a quick overview of lesson 3 initially before introducing a few important concepts like One Hot Encoding, Dendrogram, and Partial Dependence. Below is the YouTube video of the lecture (or you can jump straight to the implementation below):In the first article of the series, we learned that a lot of machine learning models cannot deal with categorical variables. Using proc_df, we converted the categorical variables into numeric columns. For example, we have a variable UsageBand,which has three levels -High, Low, and Medium. We replaced these categories with numbers (0, 1, 2) to make things easier for ourselves.Surely there must be another way of handling this that takes a significantly less effort on our end? There is!Instead of converting these categories into numbers, we can create separate columns for each category. The column UsageBand can be replaced with three columns:Each of these has 1s and 0s as the values. This is called one-hot encoding.What happens when there are far more than 3 categories? What if we have more than 10? Lets take an example to understand this.Assume we have a column zip_code in the dataset which has a unique value for every row. Using one-hot encoding here will not be beneficial for the model, and will end up increasing the run time (a lose-lose scenario).Using proc_df in fastai, we can perform one-hot encoding by passing a parameter max_n_cat. Here, we have set the max_n_cat=7, which means that variables having levels more than 7 (such as zip code) will not be encoded, while all the other variables will be one-hot encoded.This can be helpful in determining if a particular level in a particular column is important or not. Since we have separated each level for the categorical variables, plotting feature importance will show us comparisons between them as well:Earlier,YearMade was the most important feature in the dataset, but EROPS w AC has a higher feature importance in the above chart. Curious what this variable is? Dont worry, we will discuss what EROPS w AC actually represents in the following section.So far, weve understood that having a high number of features can affect the performance of the model and also make it difficult to interpret the results. In this section, we will see how we can identify redundant features and remove them from the data.We will use cluster analysis, more specifically hierarchical clustering, to identify similar variables. In this technique, we look at every object and identify which of them are the closest in terms of features. These variables are then replaced by their midpoint. To understand this better, let us have a look at the cluster plot for our dataset:From the above dendrogram plot, we can see that the variables SaleYear and SaleElapsed are very similar to each other and tend to represent the same thing.Similarly, Grouser_Tracks, Hydraulics_Flow, and Coupler_Systemare highly correlated. The same happens with ProductGroup & ProductGroupDesc and fiBaseModel & fiModelDesc. We will remove each of these features one by one and see how it affects the model performance.First, we define a function to calculate the Out of Bag (OOB) score (to avoid repeating the same lines of code):For the sake of comparison, below is the original OOB score before dropping any feature:We will now drop one variable at a time and calculate the score:This hasnt heavily affected the OOB score. Let us now remove one variable from each pair and check the overall score:The score has changed from 0.8901 to 0.8885. We will use these selected features on the complete dataset and see how our model performs:Once these variables are removed from the original dataframe, the models score turns out to be 0.907 on the validation set.Ill introduce another technique here that has the potential to help us understand the data better. This technique is called Partial Dependence and its used to find out how features are related to the target variable.Let us compare YearMade and SalePrice. If you create a scatter plot for YearMade and SaleElapsed, youd notice that some vehicles were created in the year 1000, which is not practically possible.These could be the values which were initially missing and have been replaced with 1,000. To keep things practical, we will focus on values that are greater than 1930 for the YearMade variableand create a plot using the popular ggplot package.This plot shows that the sale price is higher for more recently made vehicles, except for one drop between 1991 and 1997. There could be various reasons for this drop  recession, customers preferred vehicles of lower price, or some other external factor. To understand this, we will create a plot that shows the relationship between YearMade and SalePrice, given that all other feature values are the same.This plot is obtained by fixing the YearMade for each row to 1960, then 1961, and so on. In simple words, we take a set of rows and calculate SalePrice for each row when YearMade is 1960. Then we take the whole set again and calculate SalePrice by setting YearMade to 1962. We repeat this multiple times, which results in the multiple blue lines we see in the above plot. The dark black line represents the average. This confirms our hypothesis that the sale price increases for more recently manufactured vehicles.Similarly, you can check for other features like SaleElapsed, or YearMade and SaleElpased together. Performing the same step for the categories under Enclosure (since Enclosure_EROPS w AC proved to be one of the most important features), the resulting plot looks like this:Enclosure_EROPS w AC seems to have a higher sale price as compared to the other two variables (which have almost equal values). So what in the world is EROPS? Its an enclosed rollover protective structure which can be with or without an AC. And obviously, EROPS with an AC will have a higher sale price.Tree interpreter in another interesting technique that analyzes each individual row in the dataset. We have seen so far how to interpret a model, and how each feature (and the levels in each categorical feature) affect the model predictions. So we will now use this tree interpreter concept and visualize the predictions for a particular row.Lets import the tree interpreter library and evaluate the results for the first row in the validation set.These are the original values for first row (and its every column) in the validation set. Using tree interpreter, we will make predictions for the same using a random forest model. Tree interpreter gives three results  prediction, bias and contribution.The value of Coupler_System < 0.5 increased the value from 10.189 to 10.345 and enclosure less than 0.2 reduced the value from 10.345 to 9.955, and so on. So the contributions will represent this change in the predicted values. To understand this in a better way, take a look at the table below:In this table, we have stored the value against each feature and the split point (verify from the image above). The change is the difference between the value before and after the split. These are plotted using a waterfall chart in Excel. The change seen here is for an individual tree. An average of change across all the trees in the random forest is given by contribution in the tree interpreter.Printing the prediction and bias for the first row in our validation set:The value of contribution of each feature in the dataset for this first row:Note: If you are watching the video simultaneously with this article, the values may differ. This is because initially the values were sorted based on index which presented incorrect information. This was corrected in the later video and also in the notebook we have been following throughout the lesson.You should have a pretty good understanding of the random forest algorithm at this stage. In lesson #5, we will focus on how to identify whether model is generalizing well or not. Jeremy Howard also talks about tree interpreters, contribution, and understanding the same using a waterfall chart (which we have already covered in the previous lesson, so will not elaborate on this further). The primary focus of the video is on Extrapolation and understanding how we can build a random forest algorithm from scratch.A model might not perform well if its built on data spanning four years and then used to predict the values for the next one year. In other words, the model does not extrapolate. We have previously seen that there is a significant difference between the training score and validation score, which might be because our validation set consists of a set of recent data points (and the model is using time dependent variables for making predictions).Also, the validation score is worse than the OOB scorewhich should not be the case, right? A detailed explanation of the OOB score has been given in part 1 of the series. One way of fixing this problem is by attacking it directly  deal with the time dependent variables.To figure out which variables are time dependent, we will create a random forest model that tries to predict if a particular row is in the validation set or not. Then we will check which variable has the highest contribution in making a successful prediction.Defining the target variable:The model is able to separate the train and validation sets with a r-square value 0.99998, and the most important features are SaleID, SaleElapsed, MachineID. It is evident from the tables above that the mean value of these three variables is significantly different. We will drop these variables, fit the random forest again and check the feature importance:Although these variables are obviously time dependent, they can also be important for making the predictions. Before we drop these variables, we need to check how they affect the OOB score. The initial OOB score in a sample is calculated for comparison:Dropping each feature one by one:Looking at the results, age, MachineID and SaleDayofYear actually improved the score while others did not. So, we will remove the remaining variables and fit the random forest on the complete dataset.After removing the time dependent variables, the validation score (0.915) is now better than the OOB score (0.909). We can now play around with other parameters like n_estimator on max_features. To create the final model, Jeremy increased the number of trees to 160 and here are the results:The validation score is 0.92 while the RMSE drops to 0.21. A great improvement indeed!We have learned about how a random forest model actually works, how the features are selected and how predictions are eventually made. In this section, we will create our own random forest model from absolute scratch. Here is the notebook for this section : Random Forest from scratch.Well start with importing the basic libraries:Well just use two variables to start with. Once we are confident that the model works well with these selected variables, we can use the complete set of features.We have loaded the dataset, split it into train and validation sets, and selected two features YearMade and MachineHoursCurrentMeter.The first thing to think about while building any model from scratch is  what information do we need? So, for a random forest, we need:Lets define a class with the inputs as mentioned above and set the random seed to 42.We have created a function create_trees that will be called as many times as the number assigned to n_trees. The function create_trees generatesa randomly shuffled set of rows (of size = sample_sz) and returns DecisionTree. Well see DecisionTree in a while, but first lets figure out how predictions are created and saved.We learned earlier that in a random forest model, each single tree makes a prediction for each row and the final prediction is calculated by taking the average of all the predictions. So we will create a predict function, where .predict is used on every tree to create a list of predictions and the mean of this list is calculated as our final value.The final step is to create the DecisionTree. We first select a feature and split point that gives the least error. At present, this code is only for a single decision. We can make this recursive if the code runs successfully.self.n defines the number of rows used in each tree and self.c is the number of columns. Self.val calculates the mean of predictions for each index. This code is still incomplete and will be continued in the next lesson. Yes, part 3 is coming soon!I consider this one of the most important articles in this ongoing series. I cannot stress enough on how important model interpretability is. In real-life industry scenarios, you will quite often face the situation of having to explain the models results to the stakeholder (who is usually a non-technical person).Your chances of getting the model approved will lie in how well you are able to explain how and why the model is behaving the way it is. Plus its always a good idea to always explain any models performance to yourself in a way that a layman will understand  this is always a good practice!Use the comments section below to let me know your thoughts or ask any questions you might have on this article. And as I mentioned, part 3 is coming soon so stay tuned!",https://www.analyticsvidhya.com/blog/2018/10/interpret-random-forest-model-machine-learning-programmers/
MADRaS: A Multi-Agent DRiving Simulator for Autonomous Driving Research,Learn everything about Analytics|Introduction|More Details about this System|Multi-Agent Systems|But there are a few things to consider first..|Recalling the Why of our Problem|End Notes|Authors,"Platooning|Pooling knowledge|Leveraging intent|Share this:|Like this:|Related Articles|An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine Learning for Programmers  Part 2)|A Computer Vision Approach to Hand Gesture Recognition|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In this article, we present MADRaS: Multi-Agent DRiving Simulator. It is a multi-agent version ofTORCS, a racing simulator popularly used for autonomous driving research by the reinforcement learning and imitation learning communities. You can read more about TORCS in the below resources:MADRaS is a multi-agent extension ofGym-TORCSand is open source, lightweight, easy to install, and has the OpenAI Gym API, which makes it ideal for beginners in autonomous driving research. It enables independent control of tens of agents within the same environment, opening up a prolific direction of research in multi-agent reinforcement learning and imitation learning research aimed at acquiring human-like negotiation skills in complicated traffic situationsa major challenge in autonomous driving that all major players are racing to solve.Most open-source autonomous driving simulators (likeCARLA*,DeepDrive,AirSim, andUdacity* SDC) innately support only egocentric control; that is, single agent behavior, and have preprogrammed behaviors for the other agents. The difficulty in introducing agents with custom behaviors in these simulators restricts the diversity of real-world scenarios that can be simulated.To address this issue, we developed MADRaS, wherein each car on the racing track can be independently controlled, enabling the creation of rich, custom-made traffic scenarios, and learning the policy of control of multiple agents simultaneously.The task of negotiation in traffic can be posed as that of finding the winning strategy in a multi-agent game, wherein multiple entities (cars, buses, two-wheelers, and pedestrians) are trying to achieve their objectives of getting from one place to another fast, yet safely and reliably. Imitation learning algorithms like Behavioral Cloning, Active Learning, and Apprenticeship Learning (Inverse Reinforcement Learning followed by Reinforcement Learning) have proved to be effective for learning such sophisticated behaviors, under a multitude of simplifying assumptions and constraining conditions.A major portion of the contemporary literature makes the single-agent assumption; that is, the agent acts in an environment with a plethora of other agentssimilar or differentbut does notinteractwith any of them, robbing it of data and information that could potentially be extremely useful in decision making, at both the egocentric and collaborative levels.Driving, however, is inherently multi-agent, and the following is a partial list of things that become possible once we get rid of the single-agent assumption.Source: eDrivingOne of the earliest instances of multi-agent systems being deployed in vehicles (startingway back in 1993!) was in the use of platooning, wherein vehicles travel at highway speeds with small inter-vehicle spacing to reduce congestion and still achieve high throughput without compromising safety. Now it seems obvious that autonomous cars in the near future will communicate, cooperate, and form platoons over intersecting lengths of their commutes.Source: phys.orgApart from transferring information about pile-ups and possible diversions ahead to all the vehicles in the geographical vicinity, this power of reliable communication can be used to pool together theknowledgeof multiple learning agents. An intuitive motivation could be to consider a large gridworld. With a single learning agent, one couldsolvethe gridworld innhours of training. With multiple learning agents pooling their experiences, we could cut down the training time significantly, possibly even linearly!Theres a host of untapped literatureon communication among multiple agents in various environments (not autonomous driving yet.) See:Now this raises important questions about the reliability of the communication between vehicles. With the imminent advent of 5G,1fast and reliable communication between vehicles can help lead to the training and deployment of completely hands-free autonomous cars.Drivers on the road constantly anticipate the potential actions of fellow drivers. As an example, for close maneuvering in car parks and intersections, eye contact is made to ensure a shared understanding. Defense Advanced Research Projects Agency (DARPA) stated that traffic vehicle drivers, unnerved by being unable to make eye contact with the robots, had resorted to watching the front wheels of the robots for an indication of their intent.Source: The StarMulti-agent learning comes with its own share of complications:But remember why we started solving fully autonomous driving (FAD) in the first place. Writing forTechnology Review, Will Knightoutlines the possibilities of our driverless car future:The list goes on..So, today were excited to release MADRaS for the community to kickstart research into making FAD a reality. With the ability of introducing multiple learning agents in the environment at the same time, this simulator, built on top ofTORCS, can be used to benchmark and try out existing and new multi-agent learning algorithms for self-driving cars such as: Multi-Agent Deep Deterministic Policy Gradient (MADDPG),PSMADDPG, and the lot. And since this extends TORCS, it supports the deployment of all the single-agent learning algorithms as well. Scripts for training a DDPG agent are provided as a sample.Check out the following video for an overview of the features and the general interface.This project was developed byAbhishek NaikandAnirban Santara(an Intel Student Ambassador for AI) during their internship at the Parallel Computing Lab, Intel Labs, Bangalore, India. This project was driven by Intels urge to address the absence of an open source multi-agent autonomous driving simulator that can be utilized by machine learning (particularly, reinforcement learning) scientists to rapidly prototype and evaluate their ideas. Although the system was developed and optimized entirely on the IntelCore i7 processor and IntelXeonprocessors, we believe that it would run smoothly on otherx86 platforms, too. Currently, we are working on integrating MADRaS with theIntelNervanaplatform Reinforcement Learning Coachand we invite the community to participate in its development.Please feel free to report any incompatibility or bug bycreating an issuein the GitHub repository. We hope MADRaS enables new and veteran researchers in academia and the industry to make this FAD a reality!",https://www.analyticsvidhya.com/blog/2018/10/madras-multi-agent-driving-simulator/
A Computer Vision Approach to Hand Gesture Recognition,Learn everything about Analytics|Introduction|Table of Contents|Constructing the System|Algorithm for Static Gesture Recognition|Algorithm for Dynamic Gesture Classification|Footnotes,"Share this:|Like this:|Related Articles|MADRaS: A Multi-Agent DRiving Simulator for Autonomous Driving Research|Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes)|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Soldiers communicate with each other through gestures. But sometimes those gestures are not visible due to obstructions or poor lighting. For that purpose, an instrument is required to record the gesture and send it to the fellow soldiers. The two options for gesture recognition are through Computer Vision and through some sensors attached to the hands.The first option is not viable in this case as proper lighting is required for recognition through Computer Vision. Hence the second option of using sensors for recognitions has been used. We present a system which recognizes the gestures given in thislink.The given gestures include motions of fingers, wrist, and elbow. To detect any changes in them we have used flex sensors which detect the amount by which it has been bent at each of these joints. To take into account for the dynamic gestures an Inertial Measurement Unit (IMU-MPU-9250) was used. The parameters used from the IMU are acceleration, gyroscopic acceleration, and angles in all three axes. An Arduino* Mega was used to receive the signals from the sensors and send it to the processor.A flex sensor is a strip which has a resistance proportional to the amount of strain in the sensor. Thus it gives out a variable voltage value according to the strain. An IMU (MPU-6050) gives out linear acceleration and gyroscopic acceleration in all three axes (x, y, z).The gestures can be classified into two sub-classes:The number of features primarily used for both the sub classes differFirst of all the angles have to be calculated from the acceleration values using these formulae.The angle values have some noise in them and thus have to be filtered out in order to get smooth values out of it. Thus we have used a Kalman filter for filtering the values. Then both the flex sensor values and angles are fed into a pre-trained Support Vector Machine (SVM) with Radial Basis Function (Gaussian) Kernel. And thus the output is obtained.Figure 1: Principal Component Analysis of the dataset using all the features. Each of the colored cluster represents a particular gesture. As accelerations are also included the clusters are quite elongated.Figure 2: Principal Component Analysis of the dataset using just flex sensor values and angles. Here each colored cluster represents a particular gesture. Also these clusters are classifiable.The angles, liner accelerations, and gyroscopic accelerations are filtered using a Kalman Filter. The values are stores in a temporary file with each line representing one time point. Then every value is normalized column-wise. Then 50 time points are sampled out of them. After that they are linearized into one single vector of 800 dimensions.Then it is fed into a SVM with Radial Basis Function kernel (Gaussian). Because some gestures like Column Formation, Vehicle, Ammunition, and Rally-Point are similar to each other we have grouped such similar features as one class. If the first SVM classifies into one of these groups then they are fed into another SVM which is trained just to classify the gestures in that group.Figure 3: Two samples of graph of x-axis acceleration the gesture door.Salient Features of the system:*Since we were told to show the output on a screen we have not used a Raspberry Pi Zero (microprocessor) for processing purposes. But it can be used for that and we have checked the feasibility of the algorithms speed in that processor also.** We generated our own data for training and testing.***For detailed documentation and code visit myGitHub.",https://www.analyticsvidhya.com/blog/2018/10/computer-vision-approach-hand-gesture-recognition/
Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes),Learn everything about Analytics|Introduction|Project to Practice Time Series Forecasting|Problem Statement|Table of Contents|Understanding the Problem Statement|Moving Average|Linear Regression|k-Nearest Neighbours|Auto ARIMA||Prophet|Long Short Term Memory (LSTM)|End Notes,"Introduction|Implementation|Inference|Introduction||Implementation||Results||Inference|Introduction|Implementation||Results||Inference|Introduction|Implementation|Results|Inference|Introduction||Implementation||Results||Inference|Introduction|Implementation||Results|Inference|Share this:|Related Articles|A Computer Vision Approach to Hand Gesture Recognition|An Introductory Guide to Deep Learning and Neural Networks (Notes from deeplearning.ai Course #1)|
Aishwarya Singh
|90 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Predicting how the stock market will perform is one of the most difficult things to do. There are so many factors involved in the prediction  physical factors vs. physhological, rational and irrational behaviour, etc. All these aspects combine to make share prices volatile and very difficult to predict with a high degree of accuracy.Can we use machine learning as a game changer in this domain? Using features like the latest announcements about an organization, their quarterly revenue results, etc., machine learning techniques have the potential to unearth patterns and insights we didnt see before, and these can be used to make unerringly accurate predictions.In this article, we will work with historical data about the stock prices of a publicly listed company. We will implement a mix of machine learning algorithms to predict the future stock price of this company,starting with simple algorithms like averaging and linear regression, and then move on to advanced techniques like Auto ARIMA and LSTM.The core idea behind this article is to showcase how these algorithms are implemented. I will briefly describe the technique and provide relevant links to brush up on the concepts as and when necessary. In case youre a newcomer to the world of time series,I suggest going through the following articles first:Time Series forecasting & modeling plays an important role in data analysis. Time series analysis is a specialized branch of statistics used extensively in fields such as Econometrics & Operation Research.Time Series is being widely used in analytics & data science. This is specifically designed time series problem for you and the challenge is to forecast traffic.Practice NowWell dive into the implementation part of this article soon, but first its important to establish what were aiming to solve. Broadly, stock market analysis is divided into two parts  Fundamental Analysis and Technical Analysis.As you might have guessed, our focus will be on the technical analysis part. Well be using a dataset fromQuandl(you can find historical data for various stocks here) and for this particular project, I have used the data for Tata Global Beverages. Time to dive in!Note: Here is the dataset I used for the code: DownloadWe will first load the dataset and define the target variable for the problem:There are multiple variables in the dataset  date, open, high, low, last, close, total_trade_quantity, and turnover.Another important thing to note is that the market is closed on weekends and public holidays.Notice the above table again, some date values are missing  2/10/2018, 6/10/2018, 7/10/2018. Of these dates, 2nd is a national holiday while 6th and 7th fall on a weekend.The profit or loss calculation is usually determined by the closing price of a stock for the day, hence we will consider the closing price as the target variable. Lets plot the target variable to understand how its shaping up in our data:In the upcoming sections, we will explore these variables and use different techniques to predict the daily closing price of the stock.Average is easily one of the most common things we use in our day-to-day lives. For instance, calculating the average marks to determine overall performance, or finding the average temperature of the past few days to get an idea about todays temperature  these all are routine tasks we do on a regular basis. So this is a good starting point to use on our dataset for making predictions.The predicted closing price for each day will be the average of a set of previously observed values. Instead of using the simple average, we will be using the moving average technique which uses the latest set of values for each prediction. In other words, for each subsequent step, the predicted values are taken into consideration while removing the oldest observed value from the set. Here is a simple figure that will help you understand this with more clarity.We will implement this technique on our dataset. The first step is to create a dataframe that contains only the Date and Close price columns, then split it into train and validation sets to verify our predictions.Just checking the RMSE does not help us in understanding how the model performed. Lets visualize this to get a more intuitive understanding. So here is a plot of the predicted values along with the actual values.The RMSE value is close to 105 but the results are not very promising (as you can gather from the plot). The predicted values are of the same range as the observed values in the train set (there is an increasing trend initially and then a slow decrease).In the next section, we will look at two commonly used machine learning techniques  Linear Regression and kNN, and see how they perform on our stock market data.The most basic machine learning algorithm that can be implemented on this data is linear regression. The linear regression model returns an equation that determines the relationship between the independent variables and the dependent variable.The equation for linear regression can be written as:Here, x1, x2,.xn represent the independent variables while the coefficients 1, 2, . n represent the weights. You can refer to the following article to study linear regression in more detail:For our problem statement, we do not have a set of independent variables. We have only the dates instead. Let us use the date column to extract features like  day, month, year, mon/fri etc. and then fit a linear regression model.We will first sort the dataset in ascending order and then create a separate dataset so that any new feature created does not affect the original data.This creates features such as:Year, Month, Week, Day, Dayofweek, Dayofyear, Is_month_end, Is_month_start, Is_quarter_end, Is_quarter_start, Is_year_end, and Is_year_start.Note: I have used add_datepart from fastai library. If you do not have it installed, you can simply use the command pip install fastai. Otherwise, you can create these feature using simple for loops in python. I have shown an example below.Apart from this, we can add our own set of features that we believe would be relevant for the predictions. For instance, my hypothesis is that the first and last days of the week could potentially affect the closing price of the stock far more than the other days. So I have created a feature that identifies whether a given day is Monday/Friday or Tuesday/Wednesday/Thursday. This can be done using the following lines of code:If the day of week is equal to 0 or 4, the column value will be 1, otherwise 0. Similarly, you can create multiple features. If you have some ideas for features that can be helpful in predicting stock price, please share in the comment section.We will now split the data into train and validation sets to check the performance of the model.The RMSE value is higher than the previous technique, which clearly shows that linear regression has performed poorly. Lets look at the plot and understand why linear regression has not done well:Linear regression is a simple technique and quite easy to interpret, but there are a few obvious disadvantages. One problem in using regression algorithms is that the model overfits to the date and month column. Instead of taking into account the previous values from the point of prediction, the model will consider the value from the same date a month ago, or the same date/month a year ago.As seen from the plot above, for January 2016 and January 2017, there was a drop in the stock price. The model has predicted the same for January 2018. A linear regression technique can perform well for problems such as Big Mart sales where the independent features are useful for determining the target value.Another interesting ML algorithm that one can use here is kNN (k nearest neighbours). Based on the independent variables, kNN finds the similarity between new data points and old data points. Let me explain this with a simple example.Consider the height and age for 11 people. On the basis of given features (Age and Height), the table can be represented in a graphical format as shown below:To determine the weight for ID #11, kNN considers the weight of the nearest neighbors of this ID. The weight of ID #11 is predicted to be the average of its neighbors. If we consider three neighbours (k=3) for now, the weight for ID#11 would be = (77+72+60)/3 = 69.66 kg.For a detailed understanding of kNN, you can refer to the following articles:Introduction to k-Nearest Neighbors: SimplifiedA Practical Introduction to K-Nearest Neighbors Algorithm for RegressionUsing the same train and validation set from the last section:There is not a huge difference in the RMSE value, but a plot for the predicted and actual values should provide a more clear understanding.The RMSE value is almost similar to the linear regression model and the plot shows the same pattern. Like linear regression, kNN also identified a drop in January 2018 since that has been the pattern for the past years. We can safely say that regression algorithms have not performed well on this dataset.Lets go ahead and look at some time series forecasting techniques to find out how they perform when faced with this stock prices prediction challenge.ARIMA is a very popular statistical method for time series forecasting. ARIMA models take into account the past values to predict the future values. There are three important parameters in ARIMA:Parameter tuning for ARIMA consumes a lot of time. So we will use auto ARIMA which automatically selects the best combination of (p,q,d) that provides the least error. To read more about how auto ARIMA works, refer to this article:As we saw earlier, an auto ARIMA model uses past data to understand the pattern in the time series. Using these values, the model captured an increasing trend in the series. Although the predictions using this technique are far better than that of the previously implemented machine learning models, these predictions are still not close to the real values.As its evident from the plot, the model has captured a trend in the series, but does not focus on the seasonal part. In the next section, we will implement a time series model that takes both trend and seasonality of a series into account.There are a number of time series techniques that can be implemented on the stock prediction dataset, but most of these techniques require a lot of data preprocessing before fitting the model. Prophet, designed and pioneered by Facebook, is a time series forecasting library that requires no data preprocessing and is extremely simple to implement. The input for Prophet is a dataframe with two columns: date and target (ds and y).Prophet tries to capture the seasonality in the past data and works well when the dataset is large. Here is an interesting article that explains Prophet in a simple and intuitive manner:Prophet (like most time series forecasting techniques) tries to capture the trend and seasonality from past data. This model usually performs well on time series datasets, but fails to live up to its reputation in this case.As it turns out, stock prices do not have a particular trend or seasonality. It highly depends on what is currently going on in the market and thus the prices rise and fall. Hence forecasting techniques like ARIMA, SARIMA and Prophet would not show good results for this particular problem.Let us go ahead and try another advanced technique  Long Short Term Memory (LSTM).LSTMs are widely used for sequence prediction problems and have proven to be extremely effective. The reason they work so well is because LSTM is able to store past information that is important, and forget the information that is not. LSTM has three gates:For a more detailed understanding of LSTM and its architecture, you can go through the below article:For now, let us implement LSTM as a black box and check its performance on our particular data.Wow! The LSTM model can be tuned for various parameters such as changing the number of LSTM layers, adding dropout value or increasing the number of epochs. But are the predictions from LSTM enough to identify whether the stock price will increase or decrease? Certainly not!As I mentioned at the start of the article, stock price is affected by the news about the company and other factors like demonetization or merger/demerger of the companies. There are certain intangible factors as well which can often be impossible to predict beforehand.Time series forecasting is a very intriguing field to work with, as I have realized during my time writing these articles. There is a perception in the community that its a complex field, and while there is a grain of truth in there, its not so difficult once you get the hang of the basic techniques.I am interested in finding out how LSTM works on a different kind of time series problem and encourage you to try it out on your own as well. If you have any questions, feel free to connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
An Introductory Guide to Deep Learning and Neural Networks (Notes from deeplearning.ai Course #1),Learn everything about Analytics|Introduction|Table of Contents|1. Understanding the Course Structure|2. Course 1 : Neural Networks and Deep Learning|2.1 Module 1: Introduction to Deep Learning|2.2 Module 2: Introduction to Deep Learning|Part I: Logistic Regression as a Neural Network|Part II  Python and Vectorization|2.3 Module 3: Shallow Neural Networks|2.4 Module 4: Deep Neural Networks|End Notes,"What is a Neural Network?|Supervised Learning with Neural Networks|Why is Deep Learning Taking off?|Binary Classification|Logistic Regression|Logistic Regression Cost Function|Gradient Descent|Derivatives|More Derivative Examples|Computation Graph|Derivatives with a Computation Graph|Logistic Regression Gradient Descent|Gradient Descent on m Examples|Vectorization|Vectorizing Logistic Regression|Broadcasting in Python|A note on Python/Numpy Vectors|Neural Networks Overview|Neural Network Representation|Computing a Neural Networks Output|Vectorizing across multiple examples|Activation Function|Why do we need non-linear activation functions?|Gradient Descent for Neural Networks|Random Initialization|Deep L-Layer Neural Network|Forward Propagation in a Deep Neural Network|Getting your matrix dimensions right|Why Deep Representations?|Building Blocks of Deep Neural Networks|Forward and Backward Propagation|Parameters vs Hyperparameters|Share this:|Like this:|Related Articles|Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes)|Deep Learning in the Trenches: Understanding Inception Network from Scratch|
Pulkit Sharma
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Having a solid grasp on deep learning techniques feels like acquiring a super power these days. From classifying images and translating languages to building a self-driving car, all these tasks are being driven by computers rather than manual human effort. Deep learning has penetrated into multiple and diverse industries, and it continues to break new ground on an almost weekly basis.Understandably, a ton of folks are suddenly interested in getting into this field. But where should you start? What should you learn? What are the core concepts that actually make up this complex yet intriguing field?Im excited to pen down a series of articles where I will break down the basic components that every deep learning enthusiast should know thoroughly. My inspiration comes from deeplearning.ai, who released an awesome deep learning specialization course which I have found immensely helpful in my learning journey.In this article, I will be writing about Course 1 of the specialization, where the great Andrew Ng explains the basics of Neural Networks and how to implement them. Lets get started!Note: We will follow a bottom-up approach throughout this series  we will first understand the concept from the ground-up, and only then follow its implementation. This approach has proven to be very helpful for me.This deep learning specialization is made up of 5 courses in total. Course #1, our focus in this article, is further divided into 4 sub-modules:Ready to dive in? Then read on!Alright, now that we have a sense of the structure of this article, its time to start from scratch. Put on your learning hats because this is going to be a fun experience.The objectives behind this first module are below:Lets begin with the crux of the matter and a very critical question. What is a neural network?Consider an example where we have to predict the price of a house. The variables we are given are the size of the house in square feet (or square meters) and the price of the house. Now assume we have 6 houses. So first lets pull up a plot to visualize what were looking at:On the x-axis, we have the size of the house and on the y-axis we have its corresponding price. A linear regression model will try to draw a straight line to fit the data:So, the input(x) here is the size of the house and output(y) is the price. Now lets look at how we can solve this using a simple neural network:Here, a neuron will take an input, apply some activation function to it, and generate an output. One of the most commonly used activation function is ReLU (Rectified Linear Unit):ReLU takes a real number as input and returns the maximum of 0 or that number. So, if we pass 10, the output will be 10, and if the input is -10, the output will be 0. We will discuss activation functions in detail later in this article.For now lets stick to our example. If we use the ReLU activation function to predict the price of a house based on its size, this is how the predictions may look:So far, we have seen a neural network with a single neuron, i.e., we only had one feature (size of the house) to predict the house price. But in reality, well have to consider multiple features like number of bedrooms, postal code, etc.? House price can also depend on the family size, neighbourhood location or school quality. How can we define a neural network in such cases?It gets a bit complicated here. Refer to the above image as you read  we pass 4 features as input to the neural network as x, it automatically identifies some hidden features from the input, and finally generates the output y. This is how a neural network with 4 inputs and an output with single hidden layer will look like:Now that we have an intuition of what neural networks are, lets see how we can use them for supervised learning problems.Supervised learning refers to a task where we need to find a function that can map input to corresponding outputs (given a set of input-output pairs). We have a defined output for each given input and we train the model on these examples. Below is a pretty handy table that looks at the different applications of supervised learning and the different types of neural networks that can be used to solve those problems:Below is a visual representation of the most common Neural Network types:In this article, we will be focusing on the standard neural networks. Dont worry, we will cover the other types in upcoming articles.As you might be aware, supervised learning can be used on both structured and unstructured data.In our house price prediction example, the given data tells us the size and the number of bedrooms. This is structured data, meaning that each feature, such as the size of the house, the number of bedrooms, etc. has a very well defined meaning.In contrast, unstructured data refers to things like audio, raw audio, or images where you might want to recognize whats in the image or text (like object detection). Here, the features might be the pixel values in an image, or the individual words in a piece of text. Its not really clear what each pixel of the image represents and therefore this falls under the unstructured data umbrella.Simple machine learning algorithms work well with structured data. But when it comes to unstructured data, their performance tends to take quite a dip. This is where neural networks have proven to be so effective and useful. They perform exceptionally well on unstructured data. Most of the ground-breaking research these days has neural networks at its core.To understand this, take a look at the below graph:As the amount of data increases, the performance of traditional learning algorithms, like SVM and logistic regression, does not improve by a whole lot. In fact, it tends to plateau after a certain point. In the case of neural networks, the performance of the model increases with an increase in the data you feed to the model.There are basically three scales that drive a typical deep learning process:To improve the computation time of the model, activation function plays an important role. If we use a sigmoid activation function, this is what we end up with:The slope, or the gradient of this function, at the extreme ends is close to zero. Therefore, the parameters are updated very slowly, resulting in very slow learning. Hence, switching from a sigmoid activation function to ReLU (Rectified Linear Unit) is one of the biggest breakthroughs we have seen in neural networks. ReLU updates the parameters much faster as the slope is 1 when x>0. This is the primary reason for faster computation of the models.The objectives behind module 2 are to:This module is further divided into two parts:Lets walk through each part in detail.In a binary classification problem, we have an input x, say an image, and we have to classify it as having a cat or not. If it is a cat, we will assign it a 1, else 0. So here, we have only two outputs  either the image contains a cat or it does not. This is an example of a binary classification problem.We can of course use the most popular classification technique, logistic regression, in this case.We have an input X (image) and we want to know the probability that the image belongs to class 1 (i.e. a cat). For a given X vector, the output will be:y = w(transpose)X + bHere w and b are the parameters. Since our output y is probability, it should range between 0 and 1. But in the above equation, it can take any real value, which doesnt make sense for getting the probability. So logistic regression also uses a sigmoid function to output probabilities:For any value as input, it will only return values in the 0 to 1 range. The formula for a sigmoid function is:So, if z is very large, exp(-z) will be close to 0, and therefore the output of the sigmoid will be 1. Similarly, if z is very small, exp(-z) will be infinity and hence the output of the sigmoid will be 0.Note that the parameter w is nx dimensional vector, and b is a real number. Now lets look at the cost function for logistic regression.To train the parameters w and b of logistic regression, we need a cost function. We want to find parameters w and b such that at least on the training set, the outputs you have (y-hat) are close to the actual values (y).We can use a loss function defined below:The problem with this function is that the optimization problem becomes non-convex, resulting in multiple local optima. Hence, gradient descent will not work well with this loss function. So, for logistic regression, we define a different loss function that plays a similar role as that of the above loss function and also solves the optimization problem by giving a convex function:Loss function is defined for a single training example which tells us how well we are doing on that particular example. On the other hand, a cost function is for the entire training set. Cost function for logistic regression is:We want our cost function to be as small as possible. For that, we want our parameters w and b to be optimized.This is a technique that helps to learn the parameters w and b in such a way that the cost function is minimized. The cost function for logistic regression is convex in nature (i.e. only one global minima) and that is the reason for choosing this function instead of the squared error (can have multiple local minima).Lets look at the steps for gradient descent:The updated equation for gradient descent becomes:Here,  is the learning rate that controls how big a step we should take after each iteration.If we are on the right side of the graph shown above, the slope will be positive. Using the updated equation, we will move to the left (i.e. downward direction) until the global minima is reached. Whereas if we are on the left side, the slope will be negative and hence we will take a step towards the right (downward direction) until the global minima is reached.Pretty intuitive, right?The updated equations for the parameters of logistic regression are:Consider a function, f(a) = 3a, as shown below:The derivative of this function at any point will give the slope at that point. So,f(a=2) = 3*2 = 6f(a=2.001) = 3*2.001 = 6.003Slope/derivative of the function at a = 2 is:Slope = height/widthSlope = 0.003 / 0.001 = 3This is how we calculate the derivative/slope of a function. Lets look at a few more examples of derivatives.Consider the 3 functions below and their corresponding derivatives:f(a) = a2 , d(f(a))/d(a) = 2af(a) = a3, d(f(a))/d(a) = 3a2Finally, f(a) = log(a), d(f(a))/d(a) = 1/aIn all the above examples, the derivative is a function of a, which means that the slope of a function is different at different points.These graphs organize the computation of a specific function. Consider the below example:J(a,b,c) = 3(a+bc)We have to calculate J given a, b, and c. We can divide this into three steps:Lets visualize these steps for a = 5, b = 3 and c = 2:This is the forward propagation step where we have calculated the output, i.e., J. We can also use computation graphs for backward propagation where we update the parameters, a,b and c in the above example.Now lets see how we can calculate derivatives with the help of a computation graph. Suppose we have to calculate dJ/da. The steps will be:Similarly, we can calculate dJ/db and dJ/dc:Now we will take the concept of computation graphs and gradient descent together and see how the parameters of logistic regression can be updated.Just to quickly recap, the equations of logistic regression are:where L is the loss function. Now, for two features (x1 and x2), the computation graph for calculating the loss will be:Here, w1, w2, and b are the parameters that need to be updated. Below are the steps to do this (for w1):Similarly, we can calculate dw2 and db. Finally, the weights will be updated using the following equations:Keep in mind that this is for a single training example. We will have multiple examples in real-world scenarios. So, lets look at how gradient descent can be calculated for m training examples.We can define the predictions and cost function for m training examples as:The derivative of a loss function w.r.t the parameters can be written as:Lets now see how we can apply logistic regression for m examples:These for loops end up making the computation very slow. There is a way by which these loops can be replaced in order to make the code more efficient. We will look at these tricks in the coming sections.Up to this point, we have seen how to use gradient descent for updating the parameters for logistic regression. In the above example, we saw that if we have m training examples, we have to run the loop m number of times to get the output, which makes the computation very slow.Instead of these for loops, we can use vectorization which is an effective and time efficient approach.Vectorization is basically a way of getting rid of for loops in our code. It performs all the operations together for m training examples instead of computing them individually. Lets look at non-vectorized and vectorized representation of logistic regression:Non-vectorized form:Now, lets look at the vectorized form. We can represent the w and x in a vector form:Now we can calculate Z for all the training examples using:Z = np.dot(W,X)+b (numpy is imported as np)The dot function of NumPy library uses vectorization by default. This is how we can vectorize the multiplications. Lets now see how we can vectorize an entire logistic regression algorithm.Keeping with the m training examples, the first step will be to calculate Z for all of these examples:Z = np.dot(W.T, X) + bHere, X contains the features for all the training examples while W is the coefficient matrix for these examples. The next step is to calculate the output(A) which is the sigmoid of Z:A = 1 / 1 + np.exp(-Z)Now, calculate the loss and then use backpropagation to minimize the loss:dz = A  YFinally, we will calculate the derivative of the parameters and update them:dw = np.dot(X, dz.T) / mdb = dz.sum() / mW = W  dwb = b  dbBroadcasting makes certain parts of the code much more efficient. But dont just take my word for it! Lets look at some examples:If we add 100 to a (41) matrix, it will copy 100 to a (41) matrix. Similarly, in the example below, (13) matrix will be copied to form a (23) matrix:The general principle will be:If we add, subtract, multiply or divide an (m,n) matrix with a (1,n) matrix, this will copy it m times into an (m,n) matrix. This is called broadcasting and it makes the computations much faster. Try it out yourself!If you form an array using:a = np.random.randn(5)It will create an array of shape (5,) which is a rank 1 array. Using this array will cause problems while taking the transpose of the array. Instead, we can use the following code to form a vector instead of a rank 1 array:a = np.random.randn(5,1) # shape (5,1) column vectora = np.random.randn(1,5) # shape (1,5) row vectorTo convert a (1,5) row vector to a (5,1) column vector, one can use:a = a.reshape((5,1))Thats it for module 2. In the next section, we will dive deeper into the details of a Shallow Neural Network.The objectives behind module 3 are to:In logistic regression, to calculate the output (y = a), we used the below computation graph:In case of a neural network with a single hidden layer, the structure will look like:And the computation graph to calculate the output will be:Consider the following representation of a neural network:Can you identify the number of layers in the above neural network? Remember that while counting the number of layers in a NN, we do not count the input layer. So, there are 2 layers in the NN shown above, i.e., one hidden layer and one output layer.The first layer is referred as a[0], second layer as a[1], and the final layer as a[2]. Here a stands for activations, which are the values that different layers of a neural network passes on to the next layer. The corresponding parameters are w[1], b[1] and w[1], b[2]:This is how a neural network is represented. Next we will look at how to compute the output from a neural network.Lets look in detail at how each neuron of a neural network works. Each neuron takes an input, performs some operation on them (calculates z = w[T] + b), and then applies the sigmoid function:This step is performed by each neuron. The equations for the first hidden layer with four neurons will be:So, for given input X, the outputs for each neuron will be:z[1] = W[1]x + b[1]a[1] = (z[1])z[2] = W[2]x + b[2]a[2] = (z[2])To compute these outputs, we need to run a for loop which will calculate these values individually for each neuron. But recall that using a for loop will make the computations very slow, and hence we should optimize the code to get rid of this for loop and run it faster.The non-vectorized form of computing the output from a neural network is:Using this for loop, we are calculating z and a value for each training example separately. Now we will look at how it can be vectorized. All the training examples will be merged in a single matrix X:Here, nx is the number of features and m is the number of training examples. The vectorized form for calculating the output will be:Z[1] = W[1]X + b[1]A[1] = (Z[1])Z[2] = W[2]X + b[2]A[2] = (Z[2])This will reduce the computation time (significantly in most cases).While calculating the output, an activation function is applied. The choice of an activation function highly affects the performance of the model. So far, we have used the sigmoid activation function:However, this might not the best option in some cases. Why? Because at the extreme ends of the graph, the derivative will be close to zero and hence the gradient descent will update the parameters very slowly.There are other functions which can replace this activation function:We can choose different activation functions depending on the problem were trying to solve.If we use linear activation functions on the output of the layers, it will compute the output as a linear function of input features. We first calculate the Z value as:Z = WX + bIn case of linear activation functions, the output will be equal to Z (instead of calculating any non-linear activation):A = ZUsing linear activation is essentially pointless. The composition of two linear functions is itself a linear function, and unless we use some non-linear activations, we are not computing more interesting functions. Thats why most experts stick to using non-linear activation functions.There is only one scenario where we tend to use a linear activation function. Suppose we want to predict the price of a house (which can be any positive real number). If we use a sigmoid or tanh function, the output will range from (0,1) and (-1,1) respectively. But the price will be more than 1 as well. In this case, we will use a linear activation function at the output layer.Once we have the outputs, whats the next step? We want to perform backpropagation in order to update the parameters using gradient descent.The parameters which we have to update in a two-layer neural network are: w[1], b[1], w[2] and b[2], and the cost function which we will be minimizing is:The gradient descent steps can be summarized as:Lets quickly look at the forward and backpropagation steps for a two-layer neural networks.Forward propagation:Backpropagation:These are the complete steps a neural network performs to generate outputs. Note that we have to initialize the weights (W) in the beginning which are then updated in the backpropagation step. So lets look at how these weights should be initialized.We have previously seen that the weights are initialized to 0 in case of a logistic regression algorithm. But should we initialize the weights of a neural network to 0? Its a pertinent question. Lets consider the example shown below:If the weights are initialized to 0, the W matrix will be:Using these weights: And finally at the backpropagation step: No matter how many units we use in a layer, we are always getting the same output which is similar to that of using a single unit. So, instead of initializing the weights to 0, we randomly initialize them using the following code:We multiply the weights with 0.01 to initialize small weights. If we initialize large weights, the activation will be large, resulting in zero slope (in case of sigmoid and tanh activation function). Hence, learning will be slow. So we generally initialize small weights randomly.Its finally time to learn about deep neural networks! These have become todays buzzword in the industry and the research field. No matter which research paper I pick up these days, there is inevitably a mention of how a deep neural network was used to power the thought process behind the study.The objectives of our final module are to:In this section, we will look at how the concepts of forward and backpropogation can be applied to deep neural networks. But you might be wondering at this point what in the world deep neural networks actually are?Shallow vs depth is a matter of degree. A logistic regression is a very shallow model as it has only one layer (remember we dont count the input as a layer):A deeper neural network has more number of hidden layers:Lets look at some of the notations related to deep neural networks:These are some of the notations which we will be using in the upcoming sections. Keep them in mind as we proceed, or just quickly hop back here in case you miss something.For a single training example, the forward propagation steps can be written as:We can vectorize these steps for m training examples as shown below:These outputs from one layer act as an input for the next layer. We cant compute the forward propagation for all the layers of a neural network without a for loop, so its fine to have a for loop here. Before moving further, lets look at the dimensions of various matrices that will help us understand these steps in a better way.Analyzing the dimensions of a matrix is one of the best debugging tools to check how correct our code is. We will discuss what should be the correct dimension for each matrix in this section. Consider the following example:Can you figure out the number of layers (L) in this neural network? You are correct if you guessed 5. There are 4 hidden layers and 1 output layer. The units in each layer are:n[0] = 2, n[1] = 3, n[2] = 5, n[3] = 4, n[4] = 2, and n[5] = 1The generalized form of dimensions of W, b and their derivatives is:where m is the number of training examples. These are some of the generalized matrix dimensions which will help you to run your code smoothly.We have seen some of the basics of deep neural networks up to this point. But why do we need deep representations in the first place? Why make things complex when easier solutions exist? Lets find out!In deep neural networks, we have a large number of hidden layers. What are these hidden layers actually doing? To understand this, consider the below image:Deep neural networks find relations with the data (simpler to complex relations). What the first hidden layer might be doing, is trying to find simple functions like identifying the edges in the above image. And as we go deeper into the network, these simple functions combine together to form more complex functions like identifying the face. Some of the common examples of leveraging a deep neural network are:Consider any layer in a deep neural network. The input to this layer will be the activations from the previous layer (l-1), and the output of this layer will be its own activations.This layer first calculates the z[l] on which the activations are applied. This z[l] is saved as cache. For the backward propagation step, it will first calculate da[l], i.e., derivative of the activation at layer l, derivative of weights dw[l], db[l], dz[l], and finally da[l-1]. Lets visualize these steps to reduce the complexity:This is how each block (layer) of a deep neural network works. Next, we will see how to implement all of these blocks.The input in a forward propagation step is a[l-1] and the outputs are a[l] and cache z[l], which is a function of w[l] and b[l]. So, the vectorized form to calculate Z[l] and A[l] is:Z[l] = W[l] * A[l-1] + b[l]A[l] = g[l](Z[l])We will calculate Z and A for each layer of the network. After calculating the activations, the next step is backward propagation, where we update the weights using the derivatives. The input for backward propagation is da[l] and the outputs are da[l-1], dW[l] and db[l]. Lets look at the vectorized equations for backward propagation:This is how we implement deep neural networks.Deep Neural Networks perform surprisingly well (maybe not so surprising if youve used them before!). Running only a few lines of code gives us satisfactory results. This is because we are feeding a large amount of data to the network and it is learning from that data using the hidden layers.Choosing the right hyperparameters helps us to make our model more efficient. We will cover the details of hyperparameter tuning in the next article of this series.This is an oft-asked question by deep learning newcomers. The major difference between parameters and hyperparameters is that parameters are learned by the model during the training time, while hyperparameters can be changed before training the model.Parameters of a deep neural network are W and b, which the model updates during the backpropagation step. On the other hand, there are a lot of hyperparameters for a deep NN, including:This was a brief overview of the difference between these two aspects. I am happy to answer any questions you might have about this, in the comments section below.Congrats for completing the first course in this specialization! We now know how to implement forward and backward propagation and gradient descent for deep neural networks. We have also seen how vectorization helps us to get rid of explicit for loops, making our code efficient in the process.In the next article (which will cover course #2), we will see how we can improve deep neural networks by hyperparameter tuning, regularization and optimization. Its one of the more tricky and fascinating aspects of deep learning.If you have any feedback or have any doubts/questions, please feel free to share them in the comments section below. I look forward to hearing your thoughts!",https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/
Deep Learning in the Trenches: Understanding Inception Network from Scratch,Learn everything about Analytics|Introduction|Table of Contents|Summary of the Going Deeper with Convolutions Paper|Implementation of GoogLeNet in Keras|End Notes,"Objective of the Paper|Proposed Architectural Details|Training Methodology|Share this:|Like this:|Related Articles|An Introductory Guide to Deep Learning and Neural Networks (Notes from deeplearning.ai Course #1)|Must Read Booksfor Beginners on Machine Learning and Artificial Intelligence|
Faizan Shaikh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep learning is rapidly gaining steam as more and more research papers emerge from around the world. These papers undoubtedly contain a ton of information, but they can often be difficult to parse through. And to understand them, you might have to go through that paper multiple number of times (and perhaps even other dependent papers!).This is truly a daunting task for non-academicians like us.Personally, I find the task of going through a research paper, interpreting the crux behind it, and implementing the code as an important skill every deep learning enthusiast and practitioner should possess. Practically implementing research ideas brings out the thought process of the author and also helps transform those ideas into real-world industry applications.So in this article (and the subsequent series of articles) my motive of writing is two-fold:This article assumes that you have a good grasp on the basics of deep learning. In case you dont, or simply need a refresher, check out the below articles first and then head back here pronto:This article focuses on the paper Going deeper with convolutions from which the hallmark idea of inception network came out. Inception network was once considered a state-of-the-art deep learning architecture (or model) for solving image recognition and detection problems.It put forward a breakthrough performance on the ImageNet Visual Recognition Challenge (in 2014), which is a reputed platform for benchmarking image recognition and detection algorithms. Along with this, it set off a ton of research in the creation of new deep learning architectures with innovative and impactful ideas.We will go through the main ideas and suggestions propounded in the aforementioned paper and try to grasp the techniques within. In the words of the author:In this paper, we will focus on an efficient deep neural network architecture for computer vision, code named Inception, which derives its name from () the famous we need to go deeper internet meme.That does sound intriguing, doesnt it? Well, read on then!Theres a simple but powerful way of creating better deep learning models. You can just make a bigger model, either in terms of deepness, i.e., number of layers, or the number of neurons in each layer. But as you can imagine, this can often create complications:A solution for this, as the paper suggests, is to move on to sparsely connected network architectures which will replace fully connected network architectures, especially inside convolutional layers. This idea can be conceptualized in the below images:Densely connected architectureSparsely connected architectureThis paper proposes a new idea of creating deep architectures. This approach lets you maintain the computational budget, while increasing the depth and width of the network. Sounds too good to be true! This is how the conceptualized idea looks:Let us look at the proposed architecture in a bit more detail.The paper proposes a new type of architecture  GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. Below is the model summary:Notice in the above image that there is a layer called inception layer. This is actually the main idea behind the papers approach. The inception layer is the core concept of a sparsely connected architecture.Idea of an Inception moduleLet me explain in a bit more detail what an inception layer is all about. Taking an excerpt from the paper:(Inception Layer) is a combination of all those layers (namely, 11 Convolutional layer, 33 Convolutional layer, 55 Convolutional layer) with their output filter banks concatenated into a single output vector forming the input of the next stage.Along with the above-mentioned layers, there are two major add-ons in the original inception layer:Inception LayerTo understand the importance of the inception layers structure, the author calls on the Hebbian principle from human learning. This says that neurons that fire together, wire together. The author suggests that when creating a subsequent layer in a deep learning model, one should pay attention to the learnings of the previous layer.Suppose, for example, a layer in our deep learning model has learned to focus on individual parts of a face. The next layer of the network would probably focus on the overall face in the image to identify the different objects present there. Now to actually do this, the layer should have the appropriate filter sizes to detect different objects.This is where the inception layer comes to the fore. It allows the internal layers to pick and choose which filter size will be relevant to learn the required information. So even if the size of the face in the image is different (as seen in the images below), the layer works accordingly to recognize the face. For the first image, it would probably take a higher filter size, while itll take a lower one for the second image.The overall architecture, with all the specifications, looks like this:Note that this architecture came about largely due to the authors participating in an image recognition and detection challenge. Hence there are quite a few bells and whistles which they have explained in the paper. These include:Among this, the auxiliary training done by the authors is quite interesting and novel in nature. So we will focus on that for now. The details for the rest of the techniques can be taken from the paper itself, or in the implementation which we will see below.To prevent the middle part of the network from dying out, the authors introduced two auxiliary classifiers (the purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. The weight value used in the paper was 0.3 for each auxiliary loss.Now that you have understood the architecture of GoogLeNet and the intuition behind it, its time to power up Python and implement our learnings using Keras! We will use the CIFAR-10 dataset for this purpose.CIFAR-10 is a popular image classification dataset. It consists of 60,000 images of 10 classes (each class is represented as a row in the above image). The dataset is divided into 50,000 training images and 10,000 test images.Note that you must have the required libraries installed to implement the code we will see in this section. This includes Keras and TensorFlow (as a back-end for Keras). You can refer to the official installation guidein case you dont have Keras already installed on your machine.Now that we have dealt with the prerequisites, we can finally start coding the theory we covered in the earlier sections. The first thing we need to do is import all the necessary libraries and modules which we will use throughout the code.Our model gave an impressive 80%+ accuracy on the validation set, which proves that this model architecture is truly worth checking out!This was a really enjoyable article to write and I hope you found it equally useful. Inception v1 was the focal point on this article, wherein I explained the nitty gritty of what this framework is about and demonstrated how to implement it from scratch in Keras.In the next couple of articles, I will focus on the advancements in Inception architectures. These advancements were detailed in later papers, namely Inception v2, Inception v3, etc. And yes, they are as intriguing as the name suggests, so stay tuned!If you have any suggestions/feedback related to the article, do post them in the comments section below.",https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/
Must Read Booksfor Beginners on Machine Learning and Artificial Intelligence,Learn everything about Analytics|Overview|Introduction|Machine Learning|Artificial Intelligence|End Notes,"Machine Learning Yearning|The Hundred-Page Machine Learning Book|Programming Collective Intelligence|Machine Learning for Hackers|Machine Learning by Tom M Mitchell|The Elements of Statistical Learning|Learning from Data|Pattern Recognition and Machine Learning|Natural Language Processing with Python|Artificial Intelligence: A Modern Approach|Artificial Intelligence for Humans|Paradigm of Artificial Intelligence Programming|Artificial Intelligence: A New Synthesis|Superintelligence|The Singularity is Near|Life 3.0  Being Human in the Age of Artificial Intelligence|The Master Algorithm|Share this:|Like this:|Related Articles|Deep Learning in the Trenches: Understanding Inception Network from Scratch|An NLP Approach to Mining Online Reviews using Topic Modeling (with Python codes)|
Analytics Vidhya Content Team
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published on October 25, 2015, and updated on October 17, 2018Machine Learning has granted incredible power to humans. The power to run tasks in an automated manner, the power to make our lives comfortable,the power to improve things continuously by studying decisions at a large scale, and the power to create species who think better than humans. This list can go on and on.Still sceptical about AI and ML? Read what Googles CEO Mr. Sundar Pichai had to say all the way back in 2015:Machine learning is a core, transformative way by which were rethinking everything were doing. Were thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. Were in the early days, but youll see us in a systematic way think about how we can apply machine learning to all these areas. Sundar Pichai, CEO, GoogleMust Read Booksfor Beginners on Machine Learning and Artificial IntelligenceThose who know of these advancements are keen to master thisconcept, including us at Analytics Vidhya. When we started with this mission, we found various forms of digitized study material. They seemed promising and comprehensive, yet lacked a perspective. Our curiosity didnt let us rest for long and we resorted to books.When Elon Musk, one of the busiest men on the planet, was asked about his secret of success, he replied, I used to read books. A LOT. Later, Kimbal Musk, Elons brother, said, He would even complete two books in a day.In this article, weve listed some of the must-read books on Machine Learning and Artificial Intelligence. These books are in no particular rank or order. The motive of this article is not to promote any particular book, but to make you aware of a world whichexists beyond video tutorials, blogs and podcasts.I encourage you to check out these 10 Free E-books on Machine Learning as well which are a great starting point (or a refresher) for anyone in this field.We had to start with a book by the great Andrew Ng. It is still a work in progress, but several chapters have been released and can be downloaded FOR FREE today.This book will help the reader get up to speed with building AI systems. It will effectivelyteach you how to make the various decisions required with organizing a machine learning project. Theres no better person to start off this list, in our opinion.You can sign up on the site to receive updates as soon as a new chapter is released.Author: Andriy BurkovI love this book. Having read a ton of books trying to teach machine learning from various angles and perspectives, I struggled to find one that could succinctly summarize difficult topics and equations. Until Andriy Burkov managed to do it in some 100-odd pages.It is beautifully written, is easy to understand and has been endorsed by thought leaders like Peter Norvig. Need I say more? Beginner or established, every data scientist should get their hands on this book.\Programming Collective Intelligence, PCI as it is popularly known, is one of the best books to start learning machine learning.If there is one book to choose on machine learning  it is this one. I havent met a data scientist yet who has read this book and does not recommend to keep it on your bookshelf. A lot of them have re-read this book multiple times.The book was written long before data science and machine learning acquired the cult status they have today  but the topics and chapters are entirely relevant even today! Some of the topics covered in the book are collaborative filtering techniques, search engine features, Bayesian filtering and Support vector machines.If you dont have a copy of this book  order it as soon as you finish reading this article!The book uses Python to deliver machine learning in a fascinating manner.This book is written by Drew Conway and John Myles White. It is majorly based on data analysis in R. This book is best suited for beginners having a basic knowledge and grasp of R. It covers the use of advanced R in data wrangling. It has interesting case studies which will help you to understand the importance of using machine learning algorithms.After youve read the above books, you are good to dive into the world of machine learning. And this is a great introductory book to start your journey. Itprovides a nice overview of ML theoremswith pseudocode summaries of their algorithms. Apart from case studies,Tom has used basic examples to help you understand these algorithms easily.Most of the experts you ask in this field never fail to mention this book which helped them at the start of their careers. Its such a well-written and explained book that we feel it should be made mandatory in every machine learning course!This is quite a popular book. It was written by Trevor Hastie, Robert Tibshirani and Jerome Friedman. This book aptly explains various machine learning algorithms mathematically from a statistical perspective. It provides a powerful world created by statistics and machine learning. This books lays emphasis on mathematical derivations to define the underlying logic behind an algorithm. Keep in mind that you need to have a rudimentary understanding of linear algebra before picking this up.Theres a beginner friendly version of these concepts in a book by some of the same authors, called Introduction to Statistical Learning. Make sure you check that out if this one is too complex for you right now.Free PDF Link: DownloadThis book is written byYaser Abu Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. It provides a perfect introduction to machine learning.This book prepares you to understand complex areas of machine learning. Yaser, a very popular and brilliant professor, has provided to the point explanations instead of lengthy and go-around explanations. If you choose this book, Id suggest you to refer to online tutorials of Yaser Abu Mostafa as well. Theyre awesome.Free PDF Link: DownloadThis book is written by Christopher M Bishop. This book serves as a excellent referencefor students keen to understand the use of statistical techniques in machine learning and pattern recognition. This books assumes the knowledge of linear algebra and multivariate calculus. It provides a comprehensiveintroduction to statistical pattern recognition techniques using practice exercises.Free PDF Link: DownloadFolks interested in getting into Natural Language Processing (NLP) should read this book. Its written in a lucid and clear manner with extremely well-presented codes in Python. Readers are given access to well-annotated datasets to analyse and deal with unstructured data, linguistic structure in text, among other NLP things.The authors of this book are Steven Bird, Ewan Klein, and Edward Loper. A ML superteam!Who better to learn AI from than the great Peter Norvig? You have to take a course from Norvig to understand his style of teaching. But once you do, you will remember it for a long, long time.This book is written by Stuart Russell and Peter Norvig. It is best suitedfor people new to A.I. More than just providing an overview of artificial intelligence,this bookthoroughly covers subjects from search algorithms and reducing problems to search problems, working with logic, planning, and more advanced topics in AI, such as reasoning with partial observability, machine learning and language processing. Make it the first book on A.I in your book shelf.Free PDF Link: DownloadThis book is written by Jeff Heaton. It teaches basic artificial intelligence algorithms such asdimensionality, distance metrics, clustering, error calculation, hill climbing, Nelder Mead, and linear regression. It explains these algorithms using interesting examples and cases. Needless to say, this book requires good commands over mathematics. Otherwise, youll have tough time deciphering the equations.Another one by Peter Norvig!This book teaches advanced common lisp techniques to build major A.I systems. It delves deep into the practical aspects of A.I and teaches its readers the method to build and debug robust practical programs. It also demonstrates superior programming style and essential AI concepts. Id recommend reading this book, if you are serious about a career in A.I specially.This book is written by Nils J Nilsson. After reading the above 3 books, youd like something which could challenge your mind. Heres what you are looking for. This books covers topics such asNeural networks, genetic programming, computer vision, heuristic search, knowledge representation and reasoning, Bayes networks and explains them with great ease. I wouldnt recommend this book for a beginner. However, its a must read for advanced level user.Nick Bostrom has authored (or co-authored) over 200 publications, including this book called Superintelligence. Most of the world is enthralled and captivated by what AI can do and its potential to change the world.But how many of us stop to think about how AI will affect our society? Are we considering the human aspect at all when building AI products and services? If not, we really should. In this thought-provoking book, Nick Bostrom lays down a future scenario where machines reach the superintelligent stage and deliberately or accidentally lead to the extinction of humans.This might sound like a sci-fi movie plot, but the way Mr. Bostrom has laid down his arguments and the thinking behind them will definitely sway you and make you take him seriously. We consider this a must-read for everyone working in the AI space.Similar to the above idea propounded by Nick Bostrom, Ray Kurzweils Singularity is Near delves into the thick depths of superintelligent machines. It is a slightly long read, but well worth it in the end. The way Mr. Ray has described the Singularity is breathtaking and will make you stop in your tracks.Singularity, as Ray Kurzweil has described it, is the point where humans and the intelligence of machines will merge. Once this happens, machines will be far more intelligent than all of the human species combined. Its NOT science fiction but a truly poignant description of what might happen in the future if we arent careful with what and how we work with AI.When Stephan Hawking endorses a book, one sits up and listens. This book by Max Tegmark is an international bestseller and deals with the topic of superintelligence.Some of the basic questions this book asks (and answers) are (taken from Amazons summary): How can we grow our prosperity through automation, without leaving people lacking income or purpose? How can we ensure that future AI systems do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will AI help life flourish as never before, or will machines eventually outsmart us at all tasks, and even, perhaps, replace us altogether?This is one of our favorite books in this list. Can there be just one algorithm that deals with all the aspects of technology? Instead of building AI products for specific functions, can we build one single algorithm for all functions? This thought is quite similar to what Albert Einstein spent the latter years of his life trying to discover.Pedro Domingos is a masterful writer, and he deals with the intricacies of his subject extremely well. Make sure you add this to your reading list!Disclosure:The Amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission.This is just the tip of the iceberg. Books are a wonderful source of knowledge for anyone willing to learn from them. This collection spans various aspects of AI and ML  from the mathematics and statistics side to the intangible factors like ethics and impact of society. All of these should be considered together when working on an AI and ML project.Having said that, there is truly no substitute for experience. Once youve devoured all these books can provide, always apply your learning to real-world problems and challenges. And as always, if you have any questions or suggestions for us on this article, feel free to share them in the comments section below. We look forward to connecting with you!",https://www.analyticsvidhya.com/blog/2018/10/read-books-for-beginners-machine-learning-artificial-intelligence/
An NLP Approach to Mining Online Reviews using Topic Modeling (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Importance of Online Reviews|Setting the Problem Statement|Why Should you use Topic Modeling for this task?|Python Implementation||Other Methods to Leverage Online Reviews|Whats Next?|End Notes,"Data Preprocessing|Building an LDA model|Topics Visualization|Full code is availablehere.|Share this:|Like this:|Related Articles|Must Read Booksfor Beginners on Machine Learning and Artificial Intelligence|DataHack Radio #12: Exploring the Nuts and Bolts of Natural Language Processing with Sebastian Ruder|
Prateek Joshi
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"E-commerce has revolutionized the way we shop. That phone youve been saving up to buy for months? Its just a search and a few clicks away.Items are delivered within a matter of days (sometimes even the next day!).For online retailers, there are no constraints related to inventory management or space management They can sell as many different products as they want. Brick and mortar stores can keep only a limited number of products due to the finite space they have available.I remember when I used to place orders for books at my local bookstore, and it used to take over a week for the book to arrive. It seems like a story from the ancient times now!Source: http://www.yeebaplay.com.brBut online shopping comes with its own caveats. One of the biggest challenges is verifying the authenticity of a product. Is it as good as advertised on the e-commerce site? Will the product last more than a year? Are the reviews given by other customers really true or are they false advertising? These are important questions customers need to ask before splurging their money.This is a great place to experiment and apply Natural Language Processing (NLP) techniques.This article will help you understand the significance of harnessing online product reviews with the help of Topic Modeling.Please go through the below articles in case you need a quick refresher on Topic Modeling:A few days back, I took the e-commerce plunge and purchased a smartphone online. It was well within my budget, and it had an above decent rating of 4.5 out of 5.Unfortunately, it turned out to be a bad decision as the battery backup was well below par. I didnt go through the reviews of the product and made a hasty decision to buy it based on its ratings alone. And I know Im not the only one out there who made this mistake!Ratings alone do not give a complete picture of the products we wish to purchase, as I found to my detriment. So, as a precautionary measure, I always advise people to read a products reviews before deciding whether to buy it or not.But then an interesting problem comes up. What if the number of reviews is in the hundreds or thousands? Its just not feasible to go through all those reviews, right? And this is where natural language processing comes up trumps.A problem statement is the seed from which your analysis blooms. Therefore, it is really important to have a solid, clear and well-defined problem statement.How we can analyze a large number of online reviews using Natural Language Processing (NLP)? Lets define this problem.Online product reviews are a great source of information for consumers. From the sellers point of view, online reviews can be used to gauge the consumers feedback on the products or services they are selling. However, since these online reviews are quite often overwhelming in terms of numbers and information, an intelligent system, capable of finding key insights (topics) from these reviews, will be of great help for both the consumers and the sellers. This system will serve two purposes:To solve this task, we will use the concept of Topic Modeling (LDA) on Amazon Automotive Review data. You can download it from thislink. Similar datasets for other categories of products can be found here.As the name suggests, Topic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including:A good topic model, when trained on sometext about the stock market, should result in topics like bid, trading, dividend, exchange, etc. The below image illustrates how a typical topic model works:In our case, instead of text documents, we have thousands of online product reviews for the items listed under the Automotive category. Our aim here is to extract a certain number of groups of important words from the reviews. These groups of words are basically the topics which would help in ascertaining what the consumers are actually talking about in the reviews.In this section, well power up our Jupyter notebooks (or any other IDE you use for Python!). Here well work on the problem statement defined above to extract useful topics from our online reviews dataset using the concept of Latent Dirichlet Allocation (LDA).Note: As I mentioned in the introduction, I highly recommend going through this article to understand what LDA is and how it works.Lets first load all the necessary libraries:To import the data, first extract the data to your working directory and then use theread_json( ) function of pandas to read it into a pandas dataframe.As you can see, the data contains the following columns:For the scope of our analysis and this article, we will be using only the reviews column, i.e.,reviewText.Data preprocessing and cleaningis an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, lets define a function that would plot a bar graph of n most frequent words in the data.Lets try this function and find out which are the most common words in our reviews dataset.Most common words are the, and, to, so on and so forth. These words are not so important for our task and they do not tell any story. We have to get rid of these kinds of words. Before that lets remove the punctuations and numbers from our text data.Lets try to remove the stopwordsand short words (<2 letters) from the reviews.Lets again plot the most frequent words and see if the more significant words have come out.We can see some improvement here. Terms like battery, price, product, oil have come up which are quite relevant for the Automotive category. However, we still have neutral terms like the, this, much, they which are not that relevant.To further remove noise from the text we can use lemmatization from thespaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word.Lets tokenize the reviews and then lemmatize the tokenized reviews.As you can see, we have not just lemmatized the words but also filtered only nouns and adjectives. Lets de-tokenize the lemmatized reviews and plot the most common words.It seems that now most frequent terms in our data are relevant. We can now go ahead and start building our topic model.We will start by creating the term dictionary of our corpus, where every unique term is assigned an indexThen we will convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above.The code above will take a while. Please note that I have specified the number of topics as 7 for this model using the num_topics parameter. You can specify any number of topics using the same parameter.Lets print out the topics that our LDA model has learned.The fourth topic Topic 3has terms like towel, clean, wax, water, indicating that the topic is very much related to car-wash. Similarly, Topic 6seems to be about the overall value of the product as it hasterms like price, quality, and worth.To visualize our topics in a 2-dimensional space we will use the pyLDAvis library. This visualization is interactive in nature and displays topics along with the most relevant words.Apart from topic modeling, there are many other NLP methods as well which are used for analyzing and understanding online reviews. Some of them are listed below:Information retrieval saves us from the labor of going through product reviews one by one. It gives us a fair idea of what other consumers are talking about the product.However, it does not tell us whether the reviews are positive, neutral, or negative. This becomes an extension of the problem of information retrieval where we dont just have to extract the topics, but also determine the sentiment. This is an interesting task which we will cover in the next article.Topic modeling is one of the most popular NLP techniques with several real-world applications such as dimensionality reduction, text summarization, recommendation engine, etc.. The purpose of this article was to demonstrate the application of LDA on a raw, crowd-generated text data. I encourage you to implement the code on other datasets and share your findings.If you have anysuggestion, doubt, or anything else that you wish to share regarding topic modeling, then please feel free to use the comments section below.If you are looking to get into the field of Natural Language Processing, then we have a video course designed for you covering Text Preprocessing, Topic Modeling, Named Entity Regognition, Deep Learning for NLP and many more topics.
",https://www.analyticsvidhya.com/blog/2018/10/mining-online-reviews-topic-modeling-lda/
DataHack Radio #12: Exploring the Nuts and Bolts of Natural Language Processing with Sebastian Ruder,Learn everything about Analytics|Introduction|Sebastian Ruders Background and Passion for Linguistics|Applying NLP Techniques on Different Languages|Collecting/Generating Text Data for Projects|ULMFiT  Research Co-Authored with Jeremy Howard|Sebastians Thoughts on Major Challenges in NLP and How to Overcome Them|End Notes,"Share this:|Related Articles|An NLP Approach to Mining Online Reviews using Topic Modeling (with Python codes)|A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Theres text everywhere around us, from digital sources like social media to physical objects like books and print media. The amount of text data being generated every day is mind boggling and yet were not even close to harnessing the full power of natural language processing.I see a ton of aspiring data scientists interested in this field, but they often turn away daunted by the challenges NLP presents. Its such a niche line of work, and we at Analytics Vidhya would love to see more of our community actively participate in ground-breaking work in this field.So we thought what better way to do this than get an NLP expert on our DataHack Radio podcast? Yes, weve got none other than Sebastian Ruder in Episode 12! This podcast is a knowledge goldmine for NLP enthusiasts, so make sure you tune in.Its catapulted to the top of my machine learning recommended podcasts. I strongly feel every aspiring and established data science professional should take the time to hear Sebastian talk about the diverse and often complex NLP domain.If youre looking for a place to get started with NLP, youve landed at the right place! Check out Analytics Vidhyas Natural Language Processing using Python course and enrol yourself TODAY!Subscribe to DataHack Radio today and listen to this, as well as all previous episodes, on any of the below platforms:Sebastians background is in computational linguistics, which is essentially a combination of computer science and linguistics. His interest in mathematics and languages piqued in high school and he carved a career out of that.He completed his graduation in Germany in this field as well, and has been immersed in the research side of things ever since. This is a very niche field so there are a lot of things Sebastian had to pick up from scratch and learn on his own. Quite an inspiring story for folks looking to transition into machine learning  a healthy dose of passion added to tons of dedication and an inquisitive mind.He is currently working at Aylien as a research scientist and also pursuing a Ph.D. from the National University of Ireland in, you guessed it, Natural Language Processing.Relationship extraction, named entity recognition and sentiment analysis are some of the areas where Sebastian has worked during his initial Ph.D. years.Theres a certain bias in the machine learning world when it comes to NLP. When someone mentions text, the first language that pops into our mind is English. How difficult is it to transfer a NLP model between languages? Next to impossible, as it turns out. Sebastian explained this with an example between his native tongue German and English.German is more syntactically richer than English, while in the latter you can go a long way with techniques like tokenization and building models on the word-level. In German, you need to be more careful of how words are composed. This is important in computational linguistics as hierarchy on words matters.And of course, a common difference is how the words as used. Because of this, there are different rules in place for individual languages which is what makes working with text so challenging.For sentiment analysis, Sebastian mentioned that the primary example is looking at different categories of product reviews. These are usually already well-defined and getting training data is comparatively easier. Apart from this, social media data (especially Twitter) is popularly used to mine text and extract sentiments.Other sources that are referred to include print media, like digital newspaper articles, magazines, blogs, among others. If youre applying deep learning techniques, then scanned images of text can also be used for training your model.Sebastian recently co-authored a fascinating research paper with the great Jeremy Howard called Universal Language Model Language Fine-tuning (ULMFiT). The paper made waves in the NLP community as soon as it was released, and the techniques are available in the fastai library.ULMFiT is an effective transfer learning method that can be applied to almost any NLP task. At the time of the release, it outperformed six state-of-the-art text classification tasks. Sebastian and Jeremy have done all the hard work for you  they have released pretrained models that you can plug into your existing project and generate incredibly accurate text classification results.You can read the paper in full here.The most prominent challenge in most machine learning applications is first getting the data you need to train the model, and then find the right amount of computational resources to actually do the training. This has often proved to be a step too far for a lot of project.So Sebastian introduced us to the idea of increased sample efficiency wherein you can train models without having to collect millions of text data points. In addition to this, the trained model should not overfit on this relatively smaller sample, and should generalize well.Another challenge, which we touched on earlier, is the lack of datasets in non-English languages. The majority of data, and subsequently algorithms, are from English origins. We should seriously think about democratizing data from other languages to reduce this gap and eliminate the current state of bias.Ive always had a deep fascination with the field of NLP, given my interest in literature. So it was a pleasure to hear Sebastian deep dive into the nuts and bolts of how different text challenges work. Like I mentioned at the start of the article, this is definitely one of my favorite podcasts weve hosted on DataHack Radio and I hope you find it as useful as I did.",https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-podcast-nlp-research-sebastian-ruder/
A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1),Learn everything about Analytics|Introduction|Table of Contents|1. A Simple Way of Solving an Object Detection Task (using Deep Learning)|2. Understanding Region-Based Convolutional Neural Network|3. Understanding Fast RCNN|4. Understanding Faster RCNN|5. Summary of the Algorithms covered|End Notes,"2.1 Intuition of RCNN|2.2 Problems with RCNN|3.1 Intuition of Fast RCNN|3.2 Problems with Fast RCNN|4.1. Intuition of Faster RCNN||4.2 Problems with Faster RCNN|Share this:|Like this:|Related Articles|DataHack Radio #12: Exploring the Nuts and Bolts of Natural Language Processing with Sebastian Ruder|An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers  Part 1)|
Pulkit Sharma
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How much time have you spent looking for lost room keys in an untidy and messy house? It happens to the best of us and till date remains an incredibly frustrating experience. But what if a simple computer algorithm could locate your keys in a matter of milliseconds?That is the power of object detection algorithms. While this was a simple example, the applications of object detection span multiple and diverse industries, from round-the-clock surveillance to real-time vehicle detection in smart cities. In short, these are powerful deep learning algorithms.In this article specifically, we will dive deeper and look at various algorithms that can be used for object detection. We will start with the algorithms belonging to RCNN family, i.e. RCNN, Fast RCNN and Faster RCNN. In the upcoming article of this series, we will cover more advanced algorithms like YOLO, SSD, etc.I encourage you to go through thisprevious article on object detection, where we cover the basics of this wonderful technique and show you an implementation in Python using the ImageAI library.Part 2 and Part 3 of this series are also published now. You can access them here:Lets get started!The below image is a popular example of illustrating how an object detection algorithm works. Each object in the image, from a person to a kite, have been located and identified with a certain level of precision.Lets start with the simplest deep learning approach, and a widely used one, for detecting objects in images  Convolutional Neural Networks or CNNs. If your understanding of CNNs is a little rusty, I recommend going throughthis articlefirst.But Ill briefly summarize the inner workings of a CNN for you. Take a look at the below image:We pass an image to the network, and it is then sent through various convolutions and pooling layers. Finally, we get the output in the form of the objects class. Fairly straightforward, isnt it?For each input image, we get a corresponding class as an output. Can we use this technique to detect various objects in an image? Yes, we can! Lets look at how we can solve a general object detection problem using a CNN.1. First, we take an image as input:2. Then we divide the image into various regions:3. We will then consider each region as a separate image.
4. Pass all these regions (images) to the CNN and classify them into various classes.
5. Once we have divided each region into its corresponding class, we can combine all these regions to get the original image with the detected objects:The problem with using this approach is that the objects in the image can have different aspect ratios and spatial locations. For instance, in some cases the object might be covering most of the image, while in others the object might only be covering a small percentage of the image. The shapes of the objects might also be different (happens a lot in real-life use cases).As a result of these factors, we would require a very large number of regions resulting in a huge amount of computational time. So to solve this problem and reduce the number of regions, we can use region-based CNN, which selects the regions using a proposal method. Lets understand what this region-based CNN can do for us.Instead of working on a massive number of regions, the RCNNalgorithm proposes a bunch of boxes in the image and checks if any of these boxes contain any object. RCNN uses selective search to extract these boxes from an image (these boxes are called regions).Lets first understand what selective search is and how it identifies the different regions. There are basically four regions that form an object: varying scales, colors, textures, and enclosure. Selective search identifies these patterns in the image and based on that, proposes various regions. Here is a brief overview of how selective search works:Below is a succint summary of the steps followed in RCNN to detect objects:You might get a better idea of the above steps with a visual example (Images for the example shown below are taken from this paper) . So lets take one!And this, in a nutshell, is how an RCNN helps us to detect objects.So far, weve seen how RCNN can be helpfulfor object detection. But this technique comes with its own limitations. Training an RCNN model is expensive and slow thanks to the below steps:All these processes combine to make RCNN very slow. It takes around 40-50 seconds to make predictions for each new image, which essentially makes the model cumbersome and practically impossible to build when faced with a gigantic dataset.Heres the good news  we have another object detection technique which fixes most of the limitations we saw in RCNN.What else can we do to reduce the computation time a RCNN algorithm typically takes? Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object).Ross Girshick, the author ofRCNN, came up with this idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use a RoI pooling layer to reshape all the proposed regions into a fixed size, so that it can be fed into a fully connected network.Lets break this down into steps to simplify the concept:So, instead of using three different models (like inRCNN), Fast RCNN uses a single model which extracts features from the regions, divides them into different classes, and returns the boundary boxes for the identified classes simultaneously.To break this down even further, Ill visualize each step to add a practical angle to the explanation.This is how Fast RCNN resolves two major issues of RCNN, i.e., passing one instead of 2,000 regions per image to the ConvNet, and using one instead of three different models for extracting features, classification and generating bounding boxes.But even Fast RCNNhas certain problem areas. It also uses selective search as a proposal method to find the Regions of Interest, which is a slow and time consumingprocess. It takes around 2 seconds per image to detect objects, which is much better compared to RCNN. But when we consider large real-life datasets, then even a Fast RCNN doesnt look so fast anymore.But theres yet another object detection algorithm that trump Fast RCNN. And something tells me you wont be surprised by its name.Faster RCNN is the modified version of Fast RCNN. The major difference between them is that Fast RCNN uses selective search for generating Regions of Interest, while Faster RCNN uses Region Proposal Network, aka RPN. RPN takes image feature maps as an input and generates a set of object proposals, each with an objectness score as output.The below steps are typically followed in a Faster RCNNapproach:Let me briefly explain how this Region Proposal Network (RPN) actually works.To begin with, Faster RCNN takes the feature maps from CNN and passes them on to the Region Proposal Network. RPN uses a sliding window over these feature maps, and at each window, it generates k Anchor boxes of different shapes and sizes:Anchor boxes are fixed sized boundary boxes that are placed throughout the image and have different shapes and sizes. For each anchor, RPN predicts two things:We now have bounding boxes of different shapes and sizes which are passed on to the RoI pooling layer. Now it might be possible that after the RPN step, there are proposals with no classes assigned to them. We can take each proposal and crop it so that each proposal contains an object. This is what the RoI pooling layer does. It extracts fixed sized feature maps for each anchor:Then these feature maps are passed to a fully connected layer which has a softmax and a linear regression layer. It finally classifies the object and predicts the bounding boxes for the identified objects.All of the object detection algorithms we have discussed so far use regions to identify the objects. The network does not look at the complete image in one go, but focuses on parts of the image sequentially. This creates two complications:The below table is a nice summary of all the algorithms we have covered in this article. I suggest keeping this handy next time youre working on an object detection challenge!Object detection is a fascinating field, and is rightly seeing a ton of traction in commercial, as well as research applications. Thanks to advances in modern hardware and computational resources, breakthroughs in this space have been quick and ground-breaking.This article is just the beginning of our object detection journey. In the next article (Part 2 and Part 3) of this series, we will encounter modern object detection algorithms such as YOLO and RetinaNet. So stay tuned!I always appreciate any feedback or suggestions on my articles, so please feel free to connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/
An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers  Part 1),Learn everything about Analytics|Introduction|Table of contents|Course Structure and Materials|Introduction to Machine Learning: Lesson 1||Introduction to Machine Learning: Lesson 2|Additional Topics|End Notes,"Importing necessary libraries|Downloading the Dataset||Reading the files|Introduction to Random Forest|Data Preprocessing|Missing Value Treatment|Model Building|Creating a Validation set|Building a single tree|Introduction to Bagging|Out-of-Bag (OOB) Score|Subsampling|Other Hyperparameters to Experiment with and Tune|Min sample leaf|Max feature|Tips and tricks in Jupyter Notebooks|Curse of dimensionality|Continuous, categorical, ordinal variables|Overfitting and Underfitting|Root mean squared log error|R-square|Extremely Randomized Tree|Share this:|Like this:|Related Articles|A Step-by-Step Introduction to the Basic Object Detection Algorithms (Part 1)|Simplifying Data Preparation and Machine Learning Tasks using RapidMiner|
Aishwarya Singh
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Programming is a crucial prerequisite for anyone wanting to learn machine learning. Sure quite a few autoML tools are out there, but most are still at a very nascent stage and well beyond an individuals budget. The sweet spot for a data scientist lies in combining programming with machine learning algorithms.Fast.ai is led by the amazing partnership of Jeremy Howard and Rachel Thomas. So when they recently released their machine learning course, I couldnt wait to get started.What I personally liked about this course is the top-down approach to teaching. You first learn how to code an algorithm in Python, and then move to the theory aspect. While not a unique approach, it certainly has its advantages.While going these videos, I decided to curate my learning in the form of a series of articles for our awesome community! So in this first post, I have provided a comprehensive summary (including the code) of the first two videos where Jeremy Howard teaches us how to build a random forest model using the fastai library, and how tuning the different hyperparameters can significantly alter our models accuracy.You need to have a bit of experience in Python to follow along with the code.So if youre a beginner in machine learning and have not used Python and Jupyter Notebooks before, I recommend checking out the below two resources first:The video lectures are available on YouTube and the course has been divided into twelve lectures as per the below structure:This course assumes that you have Jupyter Notebook installed on your machine. In case you dont (and dont prefer installing it either), you can choose any of the following (these have a nominal fee attached to them):All the Notebooks associated with each lecture are available on fast.ais GitHub repository. You can clone or download the entire repository in one go. You can locate the full installation steps under the to-install section.Ready to get started? Then check out theJupyter Notebook and the below video for the first lesson.In this lecture, we will learn how to build a random forest model in Python. Since a top-down approach is followed in this course, we will go ahead and code first while simultaneously understanding how the code work. Well then look into the inner workings of the random forest algorithm.Lets deep dive into what this lecture covers.The above two commands will automatically modify the notebook when the source code is updated. Thus, using ext_autoreloadwill automatically and dynamically make the changes in your notebook.Using %matplotlib inline, we can visualize the plots inside the notebook.Using import* will import everything in the fastai library. Other necessary libraries have also been imported for reading the dataframe summary, creating random forest models and metrics for calculating the RMSE (evaluation metric).The dataset well be using is the Blue Book for Bulldozers. The problem statement for this challenge is described below:The goal is to predict the sale price of a particular piece of heavy equipment at an auction, based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations. Fast Iron is creating a blue book for bulldozers, for customers to value what their heavy equipment fleet is worth at an auction.The evaluation metric is RMSLE (root mean squared log error). Dont worry if you havent heard of it before, well understand and deal with it during the code walk-through. Assuming you have successfully downloaded the dataset, lets move on to coding!This command is used to set the location of our dataset. We currently have the downloaded dataset stored in a folder named bulldozers within the data folder. To check what are the files inside the PATH, you can type:Or,The dataset provided is in a .csv format. This is a structured dataset, with columns representing a range of things, such as ID, Date, state, product group, etc. For dealing with structured data, pandas is the most important library. We already imported pandas as pd when we used the import* command earlier. We will now use the read_csv function of pandas to read the data :Let us look at the first few rows of the data:Since the dataset is large, this command does not show us the complete column-wise data. Instead, we will see some dots for the data that isnt being displayed (as shown in the screenshot):To fix this, we will define the following function, where we set max.rows and max.columns to 1000.We can now print the head of the dataset using this newly minted function. We have taken the transpose to make it visually appealing (we see column names as the index).Remember the evaluation metric is RMSLE  which is basically the RMSE between the log values of the result. So we will transform the target variable by taking its log values. This is where the popular librarynumpy comes to the rescue.The concept of how a Random Forest model works from scratch will be discussed in detail in the later sections of the course, but here is a brief introduction in Jeremy Howards words:Sounds like a smashing technique, right?RandomForestRegressor and RandomForestClassifierfunctions are used in Python for regression and classification problems respectively. Since were dealing with a regression challenge, we will stick to the RandomForestRegressor.The m.fit function takes two inputs:The target variable here isdf_raw.SalePrice. The independent variables are all the variables except SalePrice. Here, we are usingdf_raw.drop to drop the SalePrice column (axis = 1 represents column). This would throw up an error like the one below:This suggests that the model could not deal with the value Conventional. Most machine learning models (including random forest) cannot directly use categorical columns. We need to convert these columns into numbers first. So naturally the next step is to convert all the categorical columns into continuous variables.Lets take each categorical column individually. First, consider the saledate column which is of datetime format. From the date column, we can extract numerical values such as  year, month, day of month, day of the week, holiday or not, weekend or weekday, was it raining?, etc.Well leverage theadd_datepartfunction from the fastai library to create these features for us. The function creates the following features:Lets run the function and check the columns:The next step is to convert the categorical variables into numbers. We can use the train_cats function from fastai for this:While converting categorical to numeric columns, we have to take the following two issues into consideration:Although this wont make much of a difference in our current case since random forest works on splitting the dataset (we will understand how random forest works in detail in the shortly), its still good to know this for other algorithms.The next step is to look at the number of missing values in the dataset and understand how to deal with them. This is a pretty widespread challenge in both machine learning competitions and real-life industry problems.We use .isnull().sum() to get the total number of missing values. This is divided by the length of the dataset to determine the ratio of missing values.The dataset is now ready to be used for creating a model. Data cleaning is always a tedious and time consuming process. Hence, ensure to save the transformed dataset so that the next time we load the data, we will not have to perform the above tasks again.We will save it in a feather format, as this lets us access the data efficiently:We have to impute the missing values and store the data as dependent and independent part. This is done by using the fastai function proc_df. The function performs the following tasks:We have dealt with the categorical columns and the date values. We have also taken care of the missing values. Now we can finally power up and build the random forest model we have been inching towards.The n_jobs is set to -1 to use all the available cores on the machine. This gives us a score (r^2) of 0.98, which is excellent. The caveat here is that we have trained the model on the training set, and checked the result on the same. Theres a high chance that this model might not perform as well on unseen data (test set, in our case).The only way to find out is to create a validation set and check the performance of the model on it. So lets create a validation set that contains 12,000 data points (and the train set will contain the rest).Here, we will train the model on our new set (which is a sample of the original set) and check the performance across both  train and validation sets.In order to compare the score against the train and test sets, the below function returns the RMSE value and score for both datasets.The result of the above code is shown below. The train set has a score of 0.98, while the validation set has a score of 0.88. A bit of a drop-off, but the model still performed well overall.Now that you know how to code a random forest model in Python, its equally important to understand how it actually works underneath all that code. Random forest is often cited as a black box model, and its time to put that misconception to bed.We observed in the first lesson that the model performs extremely well on the training data (the points it has seen before) but dips when tested on the validation set (the data points model was not trained on). Let us first understand how we created the validation set and why its so crucial.Creating a good validation set that closely resembles the test set is one of the most important tasks in machine learning. The validation score is representative of how our model performs on real-world data, or on the test data.Keep in mind that if theres atime component involved, then the most recent rows should be included in the validation set. So, our validation set will be of the same size as the test set (last 12,000 rows from the training data).The data points from 0 to (length  12000) are stored as the train set (x_train, y_train). A model is built using the train set and its performance is measured on both the train and validation sets as before.From the above code, we get the results:Its clear that the model is overfitting on the training set. Also, it takes a smidge over 1 minute to train. Can we reduce the training time? Yes, we can! To do this, we will further take a subset of the original dataset:A subset of 30,000 samples has been created from which we take 20,000 for training the Random Forest model.Random Forest is a group of trees which are called estimators. The number of trees in a random forest model is defined by the parameter n_estimator. We will first look at a single tree (set n_estimator = 1) with a maximum depth of 3.Plotting the tree:The tree is a set of binary decisions. Looking at the first box, the first split is on coupler-system value: less than/equal to 0.5 or greater than 0.5. After the split, we get 3,185 rows with coupler_system>0.5 and remaining 16,815 with <0.5. Similarly, next split is on enclosure and Year_made.For the first box, a model is created using only the average value (10.189). This means that all the rows have a predicted value of 10.189 and the MSE (Mean Squared Error) for these predictions is 0.459. Instead, if we make a split and separate the rows based on coupler_system <0.5, the MSE is reduced to 0.414 for samples satisfying the condition (true) and 0.109 for the remaining samples.So how do we decide which variable to split on? The idea is to split the data into two groups which are as different from each other as possible. This can be done by checking each possible split point for each variable, and then figuring out which one gives the lower MSE. To do this, we can take a weighted average of the two MSE values after the split. The splitting stops when it either reaches the pre-specifiedmax_depthvalue or when each leaf node has only one value.We have a basic model  a single tree, but this is not a very good model. We need something a bit more complex that builds upon this structure. For creating a forest, we will use a statistical technique called bagging.In the bagging technique, we create multiple models, each giving predictions which are not correlated to the other. Then we average the predictions from these models. Random Forest is a bagging technique.If all the trees created are similar to each other and give similar predictions, then averaging these predictions will not improve the model performance. Instead, we can create multiple trees on a different subset of data, so that even if these trees overfit, they will do so on a different set of points. These samples are taken with replacement.In simple words, we create multiple poor performing models and average them to create one good model. The individual models must be as predictive as possible, but together should be uncorrelated. We will now increase the number of estimators in our random forest and see the results.If we do not give a value to the n_estimator parameter, it is taken as 10 by default. We will get predictions from each of the 10 trees. Further, np.stack will be used to concatenate the predictions one over the other.The dimensions of the predictions is (10, 12000) . This means we have 10 predictions for each row in the validation set.Now for comparing our models results against the validation set, here is the row of predictions, the mean of the predictions and the actual value from validation set.The actual value is 9.17 but none of our predictions comes close to this value. On taking the average of all our predictions we get 9.07, which is a better prediction than any of the individual trees.Its always a good idea to visualize your model as much as possible. Here is a plot that shows the variation in r^2 value as the number of trees increases.As expected, the r^2 becomes better as the number of trees increases. You can experiment with the n_estimator value and see how the r^2 value changes with each iteration. Youll notice that after a certain number of trees, the r^2 value plateaus.Creating a separate validation set for a small dataset can potentially be a problem since it will result in an even smaller training set. In such cases, we can use the data points (or samples) which the tree was not trained on.For this, we set the parameter oob_score =True.The oob_score is 0.84 which is close to that of the validation set. Let us look at some other interesting techniques by which we can improve our model.Earlier, we created a subset of 30,000 rows and the train set was randomly chosen from this subset. As an alternative, we can create a different subset each time so that the model is trained on a larger part of the data.We use set_rf_sample to specify the sample size. Let us check if the performance of the model has improved or not.We get a validation score of 0.876. So far, we have worked on a subset of one sample. We can fit this model on the entire dataset (but it will take a long time to run depending on how good your computational resources are!).This can be treated as the stopping criteria for the tree. The tree stops growing (or splitting) when the number of samples in the leaf node is less than specified.Here we have specified the min_sample_leafas 3. This means that the minimum number of samples in the node should be 3 for each split. We see that the r^2 has improved for the validation set and reduced on the test set, concluding that the model does not overfit on the training data.Another important parameter in random forest is max_features. We have discussed previously that the individual trees must be as uncorrelated as possible. For the same, random forest uses a subset of rows to train each tree. Additionally, we can also use a subset of columns (features) instead of using all the features. This is achieved by tweaking themax_featuresparameter.Setting max_features has slightly improved the validation score. Here the max_features is set to 0.5 which means using 50% of the features for each split. Keep in mind that this parameter can also take values like log2 or sqrt.Jeremy Howard mentioned a few tips and tricks for navigating Jupyter Notebooks which newcomers will find quite useful. Below are some of the highlights:The curse of dimensionality is the idea that the more dimensions we have, the more points sit on the edge of that space. So if the number of columns is more, it creates more and more empty space. What that means, in theory, is that the distance between points is much less meaningful. This should not be true because the points still are different distances away from each other. Even though they are on the edges, we can still determine how far away they are from each other.The evaluation metric of our dataset is RMSLE. The formula for this isWe first take the mean of squared differences of log values. We take a square root of the result obtained. This is equivalent to calculating the root mean squared error (rmse) of log of the values.Here is the mathematical formula for R-square:The value of R-square can be anything less than 1. If the r square is negative, it means that your model is worse than predicting mean.In scikit-learn, we have another algorithm ExtraTreeClassifier which is extremely randomized tree model. Unlike Random forest, instead of trying each split point for every variable, it randomly tries a few split points for a few variables.This article was a pretty comprehensive summary of the first two videos from fast.ais machine learning course. During the first lesson we learnt to code a simple random forest model on the bulldozer dataset. Random forest (and most ml algorithms) do not work with categorical variables. We faced a similar problem during the random forest implementation and we saw how can we use the date column and other categorical columns in the dataset for creating a model.In the second video, the concept of creating a validation set was introduced. We then used this validation set to check the performance of the model and tuned some basic hyper-parameters to improve the model. My favorite part from this video was plotting and visualizing the tree we built. I am sure you would have learnt a lot through these videos. I will shortly post another article covering the next two videos from the course.Update: Here is part two of the series (Covers Lesson 3, 4 and 5)An Intuitive Guide to Interpret a Random Forest Model using fastai library (Machine learning for ProgrammersPart 2)",https://www.analyticsvidhya.com/blog/2018/10/comprehensive-overview-machine-learning-part-1/
Simplifying Data Preparation and Machine Learning Tasks using RapidMiner,Learn everything about Analytics|Introduction|Table of Contents|Loading and Inspecting the Data|Transforming Data|Viewing the Process|Predicting Delays using Automated Machine Learning|Data Preparation and Machine Learning Simplified,"Pivot Tables|Mergingdata|Joins|Generating columns|Cleansing data|History|RapidMiner Studio process|About RapidMiner|Share this:|Like this:|Related Articles|An Introduction to Random Forest using the fastai Library (Machine Learning for Programmers  Part 1)|5 Amazing Machine Learning GitHub Repositories & Reddit Threads from September 2018|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Its a well-known fact that we spend too much time on data preparation and not as much time as we want on building cool machine learning models. In fact, a Harvard Business Review publication confirmed what we always knew:analytics teams spend 80%of their time preparing data. And they are typically slowed down by clunky data preparation tools coupled with a scarcity of data science experts.But not for much longer, folks! RapidMiner recently released a really nice functionality for data preparation, RapidMiner Turbo Prep. You will soon know why we picked this name , but the basic idea is that Turbo Prep provides a new data preparation experience that is fast and fun to use with a drag and drop interface.Lets walk through some of the possibilities of this feature, as well as demonstrate how it integrates withRapidMiner Auto Model, our automated machine learning product. These two features truly make data prep and machine learning fast, fun, and simple. If you would like to follow along, make sure you haveRapidMiner Studio 9.0 downloaded.All free users have access to Auto Model and Turbo Prep for 30 days.First, were going to start by loading some data. Data can be added from all repository-based sources or be imported from your local machine.RapidMiner Turbo Prep start screenIn this example, were using a dataset of all the domestic flights leaving from New England in 2007, roughly 230,000 rows. You can find this dataset inside Studios pre-installed repository. Click Load Data / Community Samples / Community Data Sets / Transportation.Loading sample data setsOnce you load the data it can be seen immediately in a data-centric view, along with some data quality indicators. At the top of the columns, the distributions and quality measurements of the data are displayed. These indicate whether the columns will be helpful for machine learning and modeling. Say, for example, the majority of the data in a column is missing, this could confuse a machine learning model, so it is often better to remove it all together. If the column acts as an ID, that means practically all of the values only occur once in the data set, so this not useful for identifying patterns, and also should be removed.Data centric view of RapidMiner Turbo PrepAs a first step, in order to look at the data in aggregate, we are going to create a pivot table. To generate this pivot table, first, we will look at the airport codes, indicated by Origin, with the airport name OriginName, and calculate the average delay at these locations. We can see the result immediately by dragging DepDelay into the Aggregates area, which calculates the average. In this case, the biggest delays are happening at the Nantucket airport, which is a very small airport; there is a high average delay of more than 51 minutes. In order to take the number of flights into account, we will also add in Origin count and sort to show the largest airport by flight. In this case, Boston Logan Airport is the largest with almost 130,000 flights.Pivot table in RapidMiner Turbo PrepThis pivot table helped us quickly determine that we should focus on Boston Logan, so we will exit out of this view and go back to the original data we started with. Now, to only show BOS flight data: select the Origin column, right click, and select Transformations, then Filter. Immediately, there will be a preview of the data, so you know whether the results are correct. All the statistics and quality measurements are updated as well.Applying a filterNext, were going to bring in some additional data about the weather in New England for the same year. This data set can be found in the same Transportation folder as the flight data. We know from personal experience, that weather can create delays, so we want to add this data in to see if the model picks up on it. In a longer scenario, we might take a look at the flight data alone at first and discover that the model is 60% accurate. Then add in the weather information and see how the accuracy of the model improves. But for this demonstration, we will go straight to adding in the weather. In this data, there is a single Date column but in our flight data there were two columns, one for the day and one for the month, so well need to transform the weather data to match.Single Date Column in weather dataStart the transformation by copying the Date column so there are two duplicate columns next to each other. Then rename the columns to W_Day and W_Month for consistency.Copied and renamed Date columns in weather dataThen we need to split the data from these columns. To do so, click on the W_Day column and select Change Type which will display Change to number with the option to Extract. In this case we need to extract the day relative to the month and click Apply. In the case of the W_Month column, we need to follow the same steps, except we need to extract the month relative to the year and click Apply. Once the results look good, we commit the transformation.Extracting the day from the monthExtracting the month from the yearNow,we need to merge the two data sets together.Turbo Prep usessmart algorithmsto intelligently identify data matches.Two data sets are a good match if they have two columns that match with each other.And two columns match well with each other if they contain similar values.In this example, we see a pretty high match of 94%.% match of the two data setsNowif we would like to join on the airport code, we select merge type Inner Join and AirportCode from the dropdown,and it ranks the best matching columns. The best match is theOrigincode column in the other data set, which is a goodsign. Next, we pick the month and the month pops up to the top showing its the best match.Last, we select day and DayofMonth,whichis at the top of the list as the best match.This is helpful to make sure that the joins and merges deliver the correct results.Clicking Update preview will show usthe threejoined keysin purple, all of the weather informationin blue, and all of the original flight information for Boston Loganin green.Merged data viewNext, wewillgenerate a new columnbasedonexisting informationin our data sets. The data in the DepDelay columnindicates the number of minutes the flight was delayed.If a flight is a minute late, we would not (for this purpose)actuallyconsider that to be delayed so this column, as is,isnt all that important to us.What wewantto do isuse this column todefinewhat a delay is.For this example, we will consider any flights more than 15 minuteslateas a delay.Togeneratethisnew column,wewill click Generate andwill start bycreating anew column called Delay Class. Next, we caneitherdrag inor type out,the formula of if(). The drag in, or type out DepDelay wherea delaygreaterthan fifteen minutesistrue,andthe restisfalse. Ultimately, the formula will read, if([DepDelay]>15,TRUE,FALSE). Then,we want to update the preview to see the amount of false versustrue. In our case, the formula seems to work, so we commit the Generate and the new column is added.Generating a Delay Class columnThe last step before Modeling,here,is cleansing our data. When we first saw our data, we could see a couple of data quality issues indicated, for example,in the Cancellation column, so we know that needs to be addressed. We could go through the data column by column,or we could use the Auto Cleansing feature.Clicking on that feature will pop up a dialogue box, prompting us to identify what we would like to predict.RapidMiner Turbo Prep auto cleansing optionDefining a target in auto cleanseBased on thatselection, RapidMiner suggests and selects the columns that should be removed. These are suggested because there is too much data missing or because the data is too stable, for example.By simply clicking Next those columns are removed. There are more ways to improve the data quality, but that step is the only one we will use for this example,leavingthe rest to the default settings. Then,we click Commit Cleanse.Removing columns with quality issues in auto cleanseWe made quite a few changes and we can review them all by clicking on History, which shows all of the individual steps we took.If we want to, we canclick on one of the steps and decide to roll back the changes before that step or create a copy of the rollback step.History viewPossibly the most exciting aspect of Turbo Prep is that wecansee the full data preparation process by clicking Process, leading to a fully annotated view inRapidMinerStudio.There are no black boxes in RapidMiner.Wecan see every step and can make edits and changes as necessary.Whenever we see a model that we like, we can click on it and can open the process. The process is generated with annotations, so we get a full explanation of what happens.We can save this, we can apply it on new data sets, say the flight data from 2008,andwe can share this process with our colleagues, or we can deploy it and run itfrequently.Process view in RapidMiner StudioThe results can also be exported into the RapidMiner repositories,intovarious file formats, orit can be handed over to RapidMiner Auto Model for immediate model creation.In this case, we are going to explore buildingsome quickmodelsusingRapidMiner Auto Modelbysimplyclicking on theAuto Modeltab.RapidMiner Auto ModelFrom here, weareable tobuild some clustering, segmentation, or outlier predictions. In this case, we want to predictthe Delay Classcolumn.To do that, we just click on the Delay Class andPredict is already selected so we continue on and click Next.Predicting the Delay ClassIn the Prepare Target view, we can choose to map values or change the class of highestinterest,but we are most interested in predicting delays, so we will keep thedefaultsettings here.Prepare target view in RapidMiner Auto ModelOn the nextscreen, we seethose quality measurementsare visible again, andwe see thatthere are no red columns in this overview.Thatsbecause we did the auto cleansing already in Turbo Prep.But we do still have a couple of suspicious columns marked in yellow. It isimportantthat Auto Model is pointing out the DepDelay as a suspicious column because this is the column that we used to create our predictions. If you recall,when theDepDelay is greater than 15 minuteslatethen this isa delay, otherwise, it is not.If we keptthis in,all ofthe models would focus on that one column and that is not what we want to base our predictions on, so we have to remove the column.In this case, we are also going to remove the other two suspicious columnsby clicking Deselect Yellowbut those could stay in. Thisis an important feature of Turbo Prep and Auto Model,while we automate as much as we can, we still give the option to overwrite the recommendations.Removing suspicious columns in yellowWith all three suspicious columns deselected, we click Next and move on to the Model Types view.In this view, we see a couple of models selected already(suggested by Auto Model), Nave Bayes and GLMandwe can choose to see Logistic Regression as well here.Selecting the model typesIn a few seconds, we see theNaveBayes model and can start inspecting itby clicking on Model underneath Nave Bayes in the Results window.Here we have a visual way to inspect the model, so, for example, theActualLapsedTime attributeisnt super helpful, butwe can dropdownand select Min Humidity instead and start to see that the two classes differ a bit.Actual Lapsed TimeMin HumidityTheres another way to see this informationas well,throughAuto Model, by clicking on Simulator underneath Model in the Results window. Herewe can experiment with the model a bit.Right off the bat, we see that for the average inputs for our model, its more likelythat the flight will be delayed. Andthenwe can make some changes. Visibility seems to be pretty important, indicated by the length of the gray bar beneath the class name,soletschange the visibility a little bitbyreducingit, whichmakes it even more likely that the flight is delayed.Nave Bayes simulator with average inputsNave Bayes simulator with decreased visibilityIn Overview we can see how well the different models performed, here we see that GLM and Logistic Regression performed better than Nave Bayes.We could alsolook at the ROC Comparison, or the individual Model Performance and Lift Chart.Auto Model results overviewFinally, you can see the data itself, under General and the most important influence factorsby clicking onWeights.Here the most influential factor is if the incoming aircraft is delayed, which makes sense. We may wantto consider taking that out because it might not be something that we can influence but we will keep it in for now.Important influence factorsAnd justlike Turbo Prep, Auto Model processescan be openedin RapidMiner Studio,showingthefullprocesswith annotations.With Auto Model, every step is explained with its importance and why certain predictions are made during model creation. We can see exactly how the full model was created; there are no black boxes!Auto Model processThrough this demonstration,weve shownthatTurbo Prepis an incredibly exciting and useful new capability, radically simplifyingand acceleratingthe time-consuming data preparation task.We demonstrated that it makes it easy toquickly extract, join, filter, group, pivot, transform and cleanse data.You can also connect toa variety of sources like relational databases, spreadsheets, applications, social media, and more.You can also create repeatable data prep steps, making it faster to reuse processes. Data can also be saved as Excel or CSV or sent to data visualization products like Qlik.We alsodemonstrated that once wereready to build predictive models withthenewlytransformed dataits simple tojump intoAuto Model withjustone click.RapidMinerAuto Model, unlike any othertoolavailable in the market,automates machine learningbyleveraging a decade of data sciencewisdomso youcan focus onquicklyunlocking valuable insights from your data.And best of all,there are no black boxes, we can always see exactly what happened in the background and we can replicate it.If you havent triedthese two features yet,were offering a 30-day trial of Studio Large to all free userssodownload it now.RapidMiner brings artificial intelligence to the enterprise through an open and extensible data science platform. Built for analytics teams, RapidMiner unifies the entire data science lifecycle from data prep to machine learning to predictive model deployment. 400,000 analytics professionals use RapidMiner products to drive revenue, reduce costs, and avoid risks. For more information visit www.rapidminer.com.This sponsored post has been written by RapidMiner and all opinions expressed in this post are entirely those of RapidMiner.",https://www.analyticsvidhya.com/blog/2018/10/rapidminer-data-preparation-machine-learning/
5 Amazing Machine Learning GitHub Repositories & Reddit Threads from September 2018,Learn everything about Analytics|Introduction|GitHub Repositories|Papers with Code|Object Detection using Deep Learning|Train Imagenet Models in 18 Minutes|Pypeline Creating Concurrent Data Pipelines|Everybody Dance Now  Pose Estimation|Reddit Discussions|Beginner Friendly AI Papers you can Implement|What Happens when an Already Accepted Research Paper is Found to have Flaws?|Having Trouble Understanding a Research Paper? This Thread has All the Answers|How can you Prepare for a Research Oriented Role?|Researchers who Claim they will Release Code Mentioned in a Paper but Never Do|End Notes,"Share this:|Like this:|Related Articles|Simplifying Data Preparation and Machine Learning Tasks using RapidMiner|DataHack Radio #11: Decision Intelligence with Google Clouds Chief Decision Scientist, Cassie Kozyrkov|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Welcome to the September edition of our popular GitHub repositories and Reddit discussions series! GitHub repositories continue to change the way teams code and collaborate on projects. Theyre a great source of knowledge for anyone willing to tap into their infinite potential.As more and more professionals are vying to break into the machine learning field, everyone needs to keep themselves updated with the latest breakthroughs and frameworks. GitHub serves as a gateway to learn from the best in the business. And as always, Analytics Vidhya is at the forefront of bringing the best of the bunch straight to you.This months GitHub collection is awe-inspiring. Ever wanted to convert a research paper into code? We have you covered. How about implementing the top object detection algorithms using a framework of your choosing? Sure, we have that as well. And the fun doesnt stop there! Scroll down to check out this, and other top repositories, launched in September.On the Reddit front, I have included the most thought-provoking discussions in this field. My suggestion is to not only read through these threads but also actively participate in them to enhance and supplement your existing knowledge.You can check out the top GitHub repositories and top Reddit discussions (from April onwards) we have covered each month below:How many times have you come across research papers and wondered how to implement them on your own? I have personally struggled on multiple occasions to convert a paper into code. Well, the painstaking process of scouring the internet for specific pieces of code is over!Hundreds of machine learning and deep learning research papers and their respective codes are included here. This repository is truly stunning in its scope and is a treasure trove of knowledge for a data scientist. New links are added weekly and the NIPS 2018 conference papers have been added as well!If theres one GitHub repository you bookmark, make sure its this one.Object detection is quickly becoming commonplace in the deep learning universe. And why wouldnt it? Its a fascinating concept with tons of real-life applications, ranging from games to surveillance. So how about a one-stop shop where you can find all the top object detection algorithms designed since 2014?Yes, you landed in the right place. This repository, much like the one above, contains links to the full research papers and the accompanying object detection code to implement the approach mentioned in them. And the best part? The code is available for multiple frameworks! So whether youre a TensorFlow, Keras, PyTorch, or Caffe user, this repository has something for everyone.At the time of publishing this article, 43 different papers were listed here.Yes, you really can train a model on the ImageNet dataset in under 18 minutes. The great Jeremy Howard and his team of students designed an algorithm that outperformed even Google, according to the popular DAWNBench benchmark.This benchmark measures the training time, cost and other aspects of deep learning models.And now you can reproduce their results on your own machine! You need to have Python 3.6 (or higher) to get started. Go ahead and dive right in.Source: North ConceptsData engineering is a critical function in any machine learning project. Most aspiring data scientists these days tend to skip over this part, preferring to focus on the model building side of things. Not a great idea! You need to be aware (and even familiar) with how data pipelines work, what role Hadoop, Spark and Dask have to play, etc.Sounds daunting? Then check out this repository. Pypeline is a simple yet very effective Python library for creating concurrent data pipelines. The aim of this library is to solve low to medium data tasks (that involve concurrency or parallelism) where the use of Spark might feel unnecessary.This repository contains codes, benchmarks, documentation and other resources to help you become a data pipeline expert!This one is a personal favourite. I covered the release of the research paper back in August and have continued to be in awe of this technique. It is capable of transferring motion between human objects in different videos. I high recommend checking out the video available in the above link, it will blow your mind!This repository contains a PyTorch implementation of this approach. The sheer amount of details this algorithm can pick up and replicate are staggering. I cant wait to try this on my own machine!This thread continues our theme of implementing research papers. Its an ideal spot for beginners in AI looking for a new challenge. Therea two fold advantage of checking out this thread:Dont you love the open source community?A keen-eyed Redditor recently found a flaw in one of the CVPR (Computer Vision and Pattern Recognition) 2018 research papers. This is quite a big thing since the paper had already been accepted by the conference committee and successfully presented to the community.The original author of the paper took time out to respond to this mistake. It led to a very civil and thought-provoking discussion between the top ML folks on what should be done when a mistake like this is unearthed. Should the paper be retracted or amended with the corrections? There are over 100 comments in this thread and render this a must-read for everyone.We all get stuck at some point while going through a research paper. The math can often be difficult to understand, and the approach used can bamboozle the best of us. So why not reach out to the community and ask for help?Thats exactly what this thread aims to do. Make sure you follow the format mentioned in the original post and your queries will be answered. There are plenty of Q&As already there so you can browse through them to get a feel for how the process works.A pertinent question. A lot of people I speak to are interested in getting into the research side of ML, without having a clue of what to expect. Is a background in mathematics and statistics enough? Or should you be capable enough of cracking open all research papers and making sense of them on your own?The answer lies more towards the latter. Research is a field where the experts can guide you, but no one really knows the right answer until someone figures it out. Theres no single book or course that can prepare you for such a role. Needless to say, this thread is an enlightening one with different takes on what the prerequisites are.This is a controversial topic, but one I feel everyone should be aware of. Researchers release the paper and mention that the code will follow soon in order to make the reviewers happy. But sometimes this just doesnt happen. The paper gets accepted to the conference, but the code is never released to the community.Its a question of ethics than anything else. Why not mention that the data is private and can only be shared with a select few? If your code cannot be validated, whats the point of presenting it to the audience? This is a microcosm of the questions asked in this thread.Curating this list and writing about each repository and discussion thread was quite a thrill. It filled with me a sense of wonder and purpose  there is so much knowledge out there and most of it is open-source. It would be highly negligent of us to not learn from it and put it to good use.If there are any other links you feel the community should know about, feel free to let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/10/best-machine-learning-github-repositories-reddit-threads-september-2018/
"DataHack Radio #11: Decision Intelligence with Google Clouds Chief Decision Scientist, Cassie Kozyrkov",Learn everything about Analytics|Introduction|Journey from MATLAB to R|Initial Challenges at Google|Cassies Thoughts on her Current Role & Decision Intelligence|Democratizing Artificial Intelligence & Machine Learning (and their future)|End Notes,"Share this:|Related Articles|5 Amazing Machine Learning GitHub Repositories & Reddit Threads from September 2018|Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What is decision intelligence? How does it tie into the world of data science? And what does Google have to do with it all? Click on the above SoundCloud link and find out!Welcome to episode #11 of DataHack Radio, where we were joined by Google Clouds Chief Decision Scientist, Cassie Kozyrkov! Cassie is a well-known speaker in the data science sphere, and often pens down her thoughts in articulate fashion in this field. She takes us on a journey into her life at Google and how she went from being a Statistician at Google to her current role.This is a short summary of Cassie in conversation with Kunal. Give the podcast a listen and find out how a top Google scientist thinks, works, and structures her thoughts! As a bonus, there are some brilliant quotable quotes in this episode, which you will find yourself chuckling and nodding to.Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms:The story behind Cassies shift from using MATLAB to R is a fascinating one. While working with MATLAB, she wanted to make a particular kind of chart. After spending hours trying to figure it out, she decided to try her hand at R.It took her half an hour to design the plot she wanted! And the rest, as they say, is history. Shes a big R fan, and turns to Python if absolutely necessary.Cassie joined Google in 2014 as a Statistician and one of her early projects was getting rid of duplicate entries in Google Maps. It was a far more challenging task than one might think. How do you actually define duplicate entries? You need to define good processes for measuring and verifying each duplicate entry. On a global scale, this is not a straightforward task.There were a lot of statistical techniques involved in this process, like hypothesis testing. But she wasnt the only statistician on board this project, which meant coordinating and collaborating with others. Just getting people to agree on one definition of a duplicate entry was a long winding process (anyone who has worked on a project staffed with over 50 employees will be able to relate to this!).Its quite risky for data scientists to join teams that dont quite know what theyre doing.Decision Intelligence (DI) augments data science with theory from social science, decision theory, and managerial science, among other applications. DI provides a framework for best practices in organizational decision-making and processes for applying machine learning and AI at scale.Cassie finds herself these days working on multiple projects, especially in the initial stages. This way she can assign the task of particular things to the correct people, instead of letting projects get bogged down due to teams not being aware of what the next step should be.If you just rely on data science in a project, the whole thing just might flop. You need to add some extra muscle, which decision intelligence supplies. This is quite a fascinating concept and worth listening to in the podcast.She also aims to help the outside world (outside Google, that is) do some of this stuff, in a more organised and better manner. Check out some of her talks in various global forums to get an idea of what she means by that.When it comes to AI, I think the whole world is making a mistake of talking about it as some form of holy water, when its just water.What Cassie means by the above quote is that when you start thinking about it as holy water, it instantly means its accessible only to a select few. This is absolutely not true for AI and theres no special magic attached to it.Think of it as a different way of communicating your wishes to a computer. You can use it to power your business and improve your results, without needing to rely on intuition and good old luck. Cassie encourages everyone to explore programming and machine learning, at least at a basic level. Its such a wonderful gateway to a whole new world where you have the power to change results, so why shouldnt you leverage that?Coming to the future of this field, Cassie is most excited about the applied side of machine learning and AI. She uses her popular analogy of a microwave and other kitchen appliances to explain this  a truly innovative way of thinking about this domain!I personally had only vaguely heard about decision intelligence before listening to this podcast, so it was quite an eye-opener. Its a intriguing discipline and one I feel anyone in the data science field should explore.This is one those podcasts you just dont want to end, it has so much knowledge packed into an hour! I hope you enjoy listening to it as much as I did.",https://www.analyticsvidhya.com/blog/2018/10/datahack-radio-decision-intelligence-google-cloud-cassie-kozyrkov/
Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python),Learn everything about Analytics|Introduction|Table of Contents||What is a topic model?|When is Topic Modeling used?|Overview of Latent Semantic Analysis (LSA)|Implementation of LSA in Python|Pros and Cons of LSA||Other Techniques for Topic Modeling|End Notes,"Steps involved in the implementation of LSA|Data reading and inspection|Data Preprocessing|Document-Term Matrix|Topic Modeling|Topics Visualization|Share this:|Like this:|Related Articles|DataHack Radio #11: Decision Intelligence with Google Clouds Chief Decision Scientist, Cassie Kozyrkov|Building DataHack Summit 2018  Indias Most Advanced AI Conference. Are you Ready?|
Prateek Joshi
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever been inside a well-maintained library? Im always incredibly impressed with the way the librarians keep everything organized, by name, content, and other topics. But if you gave these librarians thousands of books and asked them to arrange each book on the basis of their genre, they will struggle to accomplish this task in a day, let alone an hour!However, this wont happen to you if these books came in a digital format, right? All the arrangement seems to happen in a matter of seconds, without requiring any manual effort. All hail Natural Language Processing (NLP).Source: confessionsofabookgeek.comHave a look at the below text snippet:As you might gather from the highlighted text, there are three topics (or concepts)  Topic 1, Topic 2, and Topic 3. A good topic model will identify similar words and put them under one group or topic. The most dominant topic in the above example is Topic 2, which indicates that this piece of text is primarily about fake videos.Intrigued, yet? Good! In this article, we will learn about a text mining approach called Topic Modeling. It is an extremely useful technique for extracting topics, and one you will work with a lot when faced with NLP challenges.Note: I highly recommend going through this articleto understand terms like SVD and UMAP. They are leveraged in this article so having a basic understanding of them will help solidify these concepts. We have a paid NLP course as well with a dedicated module for Topic Modeling.
A Topic Model can be defined as an unsupervised technique to discover topics across various text documents. These topics are abstract in nature, i.e., words which are related to each other form a topic. Similarly, there can be multiple topics in an individual document. For the time being, lets understand a topic model as a black box, as illustrated in the below figure:This black box (topic model) forms clusters of similar and related words which are called topics. These topics have a certain distribution in a document, and every topic is defined by the proportion of different words it contains.Recall the example we saw earlier of arranging similar books together. Now suppose you have to perform a similar task with a few digital text documents. You would be able to manually accomplish this, as long as the number of documents is manageable (aka not too many of them). But what happens when theres an impossible number of these digital text documents?Thats where NLP techniques come to the fore. And for this particular task, topic modeling is the technique we will turn to.Source: topix.io/tutorial/tutorial.htmlTopic modeling helps in exploring large amounts of text data, finding clusters of words, similarity between documents, and discovering abstract topics. As if these reasons werent compelling enough, topic modeling is also used in search engines wherein the search string is matched with the results. Getting interesting, isnt it? Well, read on then!All languages have their own intricacies and nuances which are quite difficult for a machine to capture (sometimes theyre even misunderstood by us humans!). This can include different words that mean the same thing, and also the words which have the same spelling but different meanings.For example, consider the following two sentences:In the first sentence, the word novel refers to a book, and in the second sentence it means new or fresh.We can easily distinguish between these words because we are able to understand the context behind these words. However, a machine would not be able to capture this concept as it cannot understand the context in which the words have been used. This is where Latent Semantic Analysis (LSA) comes into play as it attempts to leverage the context around the words to capture the hidden concepts, also known as topics. So, simply mapping words to documents wont really help. What we really need is to figure out the hidden concepts or topics behind the words. LSA is one such technique that can find these hidden topics. Lets now deep dive into the inner workings of LSA.Lets say we have m number of text documents with n number of total unique terms (words). We wish to extract k topics from all the text data in the documents. The number of topics, k, has to be specified by the user.Its time to power up Python and understand how to implement LSA in a topic modeling problem. Once your Python environment is open, follow the steps I have mentioned below.Lets load the required libraries before proceeding with anything else.In this article, we will use the 20 Newsgroup dataset from sklearn. You can download the dataset here, and follow along with the code.Output: 11,314The dataset has 11,314 text documents distributed across 20 different newsgroups.To start with, we will try to clean our text data as much as possible. The idea is to remove the punctuations, numbers, and special characters all in one step using the regex replace([^a-zA-Z#],  ), which will replace everything, except alphabets with space. Then we will remove shorter words because they usually dont contain useful information. Finally, we will make all the text lowercase to nullify case sensitivity.Its good practice to remove the stop-words from the text data as they are mostly clutter and hardly carry any information. Stop-words are terms like it, they, am, been, about, because, while, etc.To remove stop-words from the documents, we will have to tokenize the text, i.e., split the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words.This is the first step towards topic modeling. We will use sklearns TfidfVectorizer to create a document-term matrix with 1,000 terms.We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms.The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearns TruncatedSVD to perform the task of matrix decomposition.Since the data comes from 20 different newsgroups, lets try to have 20 topics for our text data. The number of topics can be specified by using then_componentsparameter.The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, lets print a few most important words in each of the 20 topics and see how our model has done.To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like PCA and t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection).As you can see above, the result is quite beautiful. Each dot represents a document and the colours represent the 20 newsgroups. Our LSA model seems to have done a good job. Feel free to play around with the parameters of UMAP to see how the plot changes its shape.The entire code for this article can be found in this GitHub repository.Latent Semantic Analysis can be very useful as we saw above, but it does have its limitations. Its important to understand both the sides of LSA so you have an idea of when to leverage it and when to try something else.Pros:Cons:Apart from LSA, there are other advanced and efficient topic modeling techniques such asLatent Dirichlet Allocation (LDA) andlda2Vec. We have a wonderful article on LDA which you can check out here. lda2vec is a much more advanced topic modeling which is based on word2vec word embeddings. If you want to find out more about it, let me know in the comments section below and Ill be happy to answer your questions/.This article is an attempt to share my learnings with all of you. Topic modeling is quite an interesting topic and equips you with the skills and techniques to work with many text datasets. So, I urge you all to use the code given in this article and apply it to a different dataset. Let me know if you have any questions or feedback related to this article. Happy text mining!",https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/
Building DataHack Summit 2018  Indias Most Advanced AI Conference. Are you Ready?,Learn everything about Analytics|Content at DataHack Summit 2018|Startup Showcase & Research Track|DataHack Summit 2018 Venue|So what are you waiting for?,"DataHack Summit 2018 is going to be an even bigger, better, brimming with knowledge and the most exciting event ever in Artificial Intelligence!|Share this:|Like this:|Related Articles|Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)|A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Usain Bolt created a World record by running 200m sprint in19.30 seconds in 2008. What do you think he thought while he was preparing for 2009?He had his mind set to beat his own personal best and he did!Why am I talking about Bolt here? Well, I find myself in a similar situation.DataHack Summit 2017 was an unprecedented success. We created Indias largest conference with unilateral focus on data science practitioners. The community loved the focus, the content and the knowledge sharing at the event. If you havent seen already  check out the highlights below.What are we thinking now as we are building Indias most advanced data science conference? If you cut through my mind and get a peek inside  this is what you will find The venue is bigger than last year but tickets are selling like hot cakes so make sure you grab yours before theyre sold out. Prices go up on September 30th so avail the discount today! Head over here to book your seatfor Indias most advanced conference on AI, Machine Learning, Deep Learning, and IoT!Lets take a quick tour around DHS 2018 to see how its shaping up and what we have in store for you.If there is one place we bet our reputation on  it is the content we create and we curate. DataHack Summit 2018 will be an epitome of this. To be honest, we are having a tough time saying no to very exciting talk proposals. Here are a feweminent speakers in AI and ML who will be speaking at DataHack Summit 2018:The most exciting thing which people would see are theHack sessions.They saw a tremendous response from the audience last year, and have been expanded to reflect the latest breakthrough developments. Below are a few topics to whet your appetite (click on each session to read more about what will be covered):And here are a few awesome hackers, who will be performing live hack sessions:Check out the full speaker line-uphere.We will top up the sessions and Hack Sessions with an exclusive Startup Showcase and Research Track. We will showcase some of the most exciting AI and ML startups across the globe to showcase their offerings. Prepare to have your mind blown by some of the most amazing uses of AI and ML in a variety of domains.In addition to this, there is an entire track dedicated to cutting-edge research! We are giving individuals the opportunity to come and present their work in front of our community.This years venue is none other than the NIMHANS Convention Center in Bengaluru. There are three auditoriums (yes, three!)  so you are going to see 3 parallel tracks. So you can look forward to more sessions, more industry leaders, and more engagement!And all this space means an opportunity for even more events. There will be more hack sessions this year, and each session will have an even bigger audience than before.DataHack Summit 2018 will have bigger and swankier LED screens as well! So regardless of where youre sitting, the presentation and code will be visible from all corners of the room.Reserve your seat TODAY! There is an incredible deal on offer and prices will go up on September 30th. So act now and become a part of Indias most advanced AI and ML conference.See you at DataHack Summit 2018!",https://www.analyticsvidhya.com/blog/2018/09/building-datahack-summit-2018-indias-most-advanced-ai-conference-are-you-ready/
A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes),Learn everything about Analytics|Introduction|Table of contents|1. Univariate versus Multivariate Time Series|2. Dealing with a Multivariate Time Series  VAR|3. Why Do We Need VAR?|4. Stationarity of a Multivariate Time Series|5. Train-Validation Split|6. Python implementation|End Notes,"|1.1 Univariate Time Series|1.2 Multivariate Time Series (MTS)|Share this:|Like this:|Related Articles|Building DataHack Summit 2018  Indias Most Advanced AI Conference. Are you Ready?|The Winning Approaches from codeFest 2018  NLP, Computer Vision and Machine Learning!|
Aishwarya Singh
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the most critical factor that decides whether a business will rise or fall. Thats why we see sales in stores and e-commerce platforms aligning with festivals. These businesses analyze years of spending data to understand the best time to throw open the gates and see an increase in consumer spending.But how can you, as a data scientist, perform this analysis? Dont worry, you dont need to build a time machine! Time Series modeling is a powerful technique that acts as a gateway to understanding and forecasting trends and patterns.But even a time series model has different facets. Most of the examples we see on the web deal with univariate time series. Unfortunately, real-world use cases dont work like that. There are multiple variables at play, and handling all of them at the same time is where a data scientist will earn his worth.In this article, we will understand what a multivariate time series is, and how to deal with it. We will also take a case study and implement it in Python to give you a practical understanding of the subject.This article assumes some familiarity with univariate time series, its properties and various techniques used for forecasting. Since this article will be focused on multivariate time series, I would suggest you go through the following articles which serve as a good introduction to univariate time series:But Ill give you a quick refresher of what a univariate time series is, before going into the details of a multivariate time series. Lets look at them one by one to understand the difference.A univariate time series, as the name suggests, is a series with a single time-dependent variable.For example, have a look at the sample dataset below that consists of the temperature values (each hour), for the past 2 years. Here, temperature is the dependent variable (dependent on Time).If we are asked to predict the temperature for the next few days, we will look at the past values and try to gauge and extract a pattern. We would notice that the temperature is lower in the morning and at night, while peaking in the afternoon. Also if you have data for thepast few years, you would observe that it is colder during the months of November to January, while being comparatively hotter in April to June.Such observations will help us in predicting future values. Did you notice that we used only one variable (the temperature of the past 2 years,)? Therefore, this is called Univariate Time Series Analysis/Forecasting.A Multivariate time series has more than one time-dependent variable. Each variable depends not only on its past values but also has some dependency on other variables. This dependency is used for forecasting future values. Sounds complicated? Let me explain.Consider the above example. Now suppose our dataset includes perspiration percent, dew point, wind speed, cloud cover percentage, etc. along with the temperature value for the past two years. In this case, there are multiple variables to be considered to optimally predict temperature. A series like this would fall under the category of multivariate time series. Below is an illustration of this:Now that we understand what a multivariate time series looks like, let us understand how can we use it to build a forecast.In this section, I will introduce you to one of the most commonly used methods for multivariate time series forecasting  Vector Auto Regression (VAR).In a VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. To explain this in a better manner, Im going to use a simple visual example:We have two variables, y1 and y2. We need to forecast the value of these two variables at time t, from the given data for past n values. For simplicity, I have considered the lag value to be 1. For calculating y1(t), we will use the past value of y1 and y2. Similarly, to calculate y2(t), past values of both y1 and y2 will be used. Below is a simple mathematical way of representing this relation:Here,These equations are similar to the equation of anAR process. Since the AR process is used for univariate time series data, the future values are linear combinations of their own past values only. Consider the AR(1) process:y(t) = a + w*y(t-1) +eIn this case, we have only one variable  y, a constant term  a, an error term  e, and a coefficient  w. In order to accommodate the multiple variable terms in each equation for VAR, we will use vectors. We can write the equations (1) and (2) in the following form :The two variables are y1 and y2, followed by a constant, a coefficient metric, lag value, and an error metric. This is the vector equation for a VAR(1) process. For a VAR(2) process, another vector term for time (t-2) will be added to the equation to generalize for p lags:The above equation represents a VAR(p) process with variables y1, y2 yk. The same can be written as:The term t in the equation represents multivariate vector white noise. For a multivariate time series,t should be a continuous random vector that satisfies the following conditions:Recall the temperate forecasting example we saw earlier. An argument can be made for it to be treated as a multiple univariate series. We can solve it using simple univariate forecasting methods like AR. Since the aim is to predict the temperature, we can simply remove the other variables (except temperature) and fit a model on the remaining univariate series.Another simple idea is to forecast values for each series individually using the techniques we already know. This would make the work extremely straightforward! Then why should you learn another forecasting technique? Isnt this topic complicated enough already?From the above equations (1) and (2), it is clear that each variable is using the past values of every variable to make the predictions. Unlike AR, VAR is able to understand and use the relationship between several variables. This is useful for describing the dynamic behavior of the data and also provides better forecasting results. Additionally, implementing VAR is as simple as using any other univariate technique (which you will see in the last section).We know from studying the univariate concept that a stationary time series will more often than not give us a better set of predictions. If you are not familiar with the concept of stationarity, please go through this article first: A Gentle Introduction to handling non-stationary Time Series.To summarize, for a given univariate time series:y(t) = c*y(t-1) +  tThe series is said to be stationary if the value of |c| < 1. Now, recall the equation of our VAR process:Note: I is the identity matrix.Representing the equation in terms of Lag operators, we have:Taking all the y(t) terms on the left-hand side:The coefficient of y(t) is called the lag polynomial. Let us represent this as (L):For a series to be stationary, the eigenvalues of |(L)-1| should be less than 1 in modulus. This might seem complicated given the number of variables in the derivation. This idea has been explained using a simple numerical example in the following video. I highly encourage watching it to solidify your understanding:Similar to the Augmented Dickey-Fuller test for univariate series, we have Johansens test for checking the stationarity of any multivariate time series data. We will see how to perform the test in the last section of this article.If you have worked with univariate time series data before, youll be aware of the train-validation sets. The idea of creating a validation set is to analyze the performance of the model before using it for making predictions.Creating a validation set for time series problems is tricky because we have to take into account the time component. One cannot directly use the train_test_split or k-fold validation since this will disrupt the pattern in the series. The validation set should be created considering the date and time values.Suppose we have to forecast the temperate, dew point, cloud percent, etc. for the next two months using data from the last two years. One possible method is to keep the data for the last two months aside and train the model on the remaining 22 months.Once the model has been trained, we can use it to make predictions on the validation set. Based on these predictions and the actual values, we can check how well the model performed, and the variables for which the model did not do so well. And for making the final prediction, use the complete dataset (combine the train and validation sets).In this section, we will implement the Vector AR model on a toy dataset. I have used the Air Quality dataset for this and you can download it from here.The data type of theDate_Time column is objectand we need to change it to datetime. Also, for preparing the data, we need the index to have datetime. Follow the below commands:The next step is to deal with the missing values. Since the missing values in the data are replaced with a value -200, we will have to impute the missing value with a better number. Consider this  if the present dew point value is missing, we can safely assume that it will be close to the value of the previous hour. Makes sense, right? Here, I will impute -200 with the previous value.You can choose to substitute the value using the average of a few previous values, or the value at the same time on the previous day (you can share your idea(s) of imputing missing values in the comments section below).Below is the result of the test:We can now go ahead and create the validation set to fit the model, and test the performance of the model:The predictions are in the form of an array, where each list represents the predictions of the row. We will transform this into a more presentable format.Output of the above code:After the testing on validation set, lets fit the model on the complete datasetBefore I started this article, the idea of working with a multivariate time series seemed daunting in its scope. It is a complex topic, so take your time in understanding the details. The best way to learn is to practice, and so I hope the above Python implemenattion will be useful for you.I enocurage you to use this approach on a dataset of your choice. This will further cement your understanding of this complex yet highly useful topic. If you have any suggestions or queries, share them in the comments section.",https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/
"The Winning Approaches from codeFest 2018  NLP, Computer Vision and Machine Learning!",Learn everything about Analytics|Introduction|Liguipedia  Natural Language Processing|Vista  Computer Vision|Enigma  Machine Learning|End Notes,"Winners|Problem Statement|Winners Approach|Winner|Problem Statement|Winners Solution|Winner|Problem Statement|Winners Solution|Share this:|Like this:|Related Articles|A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)|Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python|
Aishwarya Singh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Vidhyas hackathons are one of the best ways to evaluate how far youve traveled in your data science journey. And what better way than to put your skills to the test against the top data scientists from around the globe?Participating in these hackathons also helps you understand where you need to improve and what else you can learn to get a better score in the next competition. And a very popular demand after each hackathon is to see how the winning solution was designed and the thought process behind it. Theres a lot to learn from this, including how you can develop your own unique framework for future hackathons.We are all about listening to our community, so we decided to curate the winning approaches from our recently concluded hackathon series, codeFest! This was a series of three hackathons in partnership with IIT-BHU, conducted between 31st August and 2nd September. The competition was intense, with more than 1,900 aspiring data scientists going head-to-head to grab the ultimate prize!Each hackathon had a unique element to it. Interested in finding out more? You can view the details of each competition below:Its time to check out the winners approaches!Abhinav Gupta and Abhishek Sharma.The participants were given a list of tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc. The challenge was to find the tweets which showed a negative sentiment towards such companies or products.The metric used for evaluating the performance of the classification model was weighted F1-Score.Abhinav and Abhishek have summarized their approach in a very intuitive manner, explaining everything from preprocessing and feature engineering to model building.Pre-processing:Feature Extraction:Classifiers used:Theyhypertuned each of the above classifiers and found that LSTM (with attention mechanism) produced the best result.EnsembleDeepak Rawat.The Vista hackathon had a pretty intriguing problem statement. The participants had to build a model that counted the number of people in a given group selfie/photo. The dataset provided had already been split, wherein the training set consisted of images with coordinates of the bounding boxes and headcount for each image.The evaluation metric for this competition was RMSE (root mean squared error) over the headcounts predicted for test images.Check out Deepaks approach in his own words below:Mask R-CNN andResNet101Both stages are connected to the backbone structure.Pre-processing Model BuildingRaj Shukla.As a part of enigma competition, the target was to predict the number of upvotes on a question based on other information provided. For every question  its tag, number of views received, number of answers, username and reputation of the question author, was provided. Using this information, the participant had to predict the upvote count that the question will receive.The evaluation metric for this competition was RMSE (root mean squared error). Below is the data dictionary for your reference:Here is Rajs approach to cracking the Enigma hackathon:Feature Engineering:My focus was on feature engineering, i.e., using the existing features to create new features. Below are some key features I cooked up:Model Building:A big thank you to everyone for participating in codeFest 2018! This competition was all about quick and structured thinking, coding, experimentation, and finding the one approach that got you up the leaderboard. In short, what machine learning is all about!Missed out this time? Dont worry, you can check out all upcoming hackathons on our DataHack platform and register yourself today!",https://www.analyticsvidhya.com/blog/2018/09/the-winning-approaches-from-codefest-2018-nlp-computer-vision-and-machine-learning/
Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python,Learn everything about Analytics|Introduction|Table of Contents|What is the Multi-Armed Bandit Problem (MABP)?|Use Cases|Solution Strategies|Non-Stationary Bandit problems|Python Implementation from scratch for Ad CTR Optimization|End Notes,"Clinical Trials|Network Routing|Online Advertising|Game Designing|Action-Value Function|Regret|No Exploration (Greedy Approach)|Epsilon Greedy Approach|Softmax Exploration|Decayed Epsilon Greedy|Upper Confidence Bound|Share this:|Like this:|Related Articles|The Winning Approaches from codeFest 2018  NLP, Computer Vision and Machine Learning!|10 Mind-Blowing TED Talks on Artificial Intelligence Every Data Scientist & Business Leader Must Watch|
Ankit Choudhary
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you have a favorite coffee place in town? When you think of having a coffee, you might just go to this place as youre almost sure that you will get the best coffee. But this means youre missing out on the coffee served by this places cross-town competitor.And if you try out all the coffee places one by one, the probability of tasting the worse coffee of your life would be pretty high! But then again, theres a chance youll find an even better coffee brewer. But what does all of this have to do with reinforcement learning?Cafe Coffee Day vs StarbucksIm glad you asked.The dilemma in our coffee tasting experiment arises from incomplete information. In other words, we need to gather enough information to formulate the best overall strategy and then explore new actions. This will eventually lead to minimizing the overall bad experiences.A multi-armed bandit is a simplified form of this analogy. It is used to represent similar kinds of problems and finding a good strategy to solve them is already helping a lot of industries.In this article, we will first understand what actually is a multi-armed bandit problem, its various use cases in the real-world, and then explore some strategies on how to solve it. I will then show you how to solve this challenge in Python using a click-through rate optimization dataset.A bandit is defined as someone who steals your money. A one-armed bandit is a simple slot machine wherein you insert a coin into the machine, pull a lever, and get an immediate reward. But why is it called a bandit? It turns out all casinos configure these slot machines in such a way that all gamblers end up losing money!A multi-armed bandit is a complicated slot machine wherein instead of 1, there are several levers which a gambler can pull, with each lever giving a different return. The probability distribution for the reward corresponding to each lever is different and is unknown to the gambler.The task is to identify which lever to pull in order to get maximum reward after a given set of trials. This problem statement is like a single step Markov decision process, which I discussed in this article. Each arm chosen is equivalent to an action, which then leads to an immediate reward.Exploration Exploitation in the context of Bernoulli MABPThe below table shows the sample results for a 5-armed Bernoulli bandit with arms labelled as 1, 2, 3, 4 and 5:This is called Bernoulli, as the reward returned is either 1 or 0. In this example, it looks like the arm number 3 gives the maximum return and hence one idea is to keep playing this arm in order to obtain the maximum reward (pure exploitation).Just based on the knowledge from the given sample, 5 might look like a bad arm to play, but we need to keep in mind that we have played this arm only once and maybe we should play it a few more times (exploration) to be more confident. Only then should we decide which arm to play (exploitation).Bandit algorithms are being used in a lot of research projects in the industry. I have listed some of their use cases in this section.The well being of patients during clinical trials is as important as the actual results of the study. Here, exploration is equivalent to identifying the best treatment, and exploitation is treating patients as effectively as possible during the trial.Clinical TrialsRouting is the process of selecting a path for traffic in a network, such as telephone networks or computer networks (internet). Allocation of channels to the right users, such that the overall throughput is maximised, can be formulated as a MABP.Network RoutingThe goal of an advertising campaign is to maximise revenue from displaying ads. The advertiser makes revenue every time an offer is clicked by a web user. Similar to MABP, there is a trade-off between exploration, where the goal is to collect information on an ads performance using click-through rates, and exploitation, where we stick with the ad that has performed the best so far.Online AdsBuilding a hit game is challenging. MABP can be used to test experimental changes in game play/interface and exploit the changes which show positive experiences for players.Game DesigningIn this section, we will discuss some strategies to solve a multi-armed bandit problem. But before that, lets get familiar with a few terms well be using from here on.The expected payoff or expected reward can also be called an action-value function. It is represented by q(a) and defines the average reward for each action at a time t.Suppose the reward probabilities for a K-armed bandit are given by {P1, P2, P3  Pk}. If the ith arm is selected at time t, then Qt(a) = Pi.The question is, how do we decide whether a given strategy is better than the rest? One direct way is to compare the total or average reward which we get for each strategy after n trials. If we already know the best action for the given bandit problem, then an interesting way to look at this is the concept of regret.Lets say that we are already aware of the best arm to pull for the given bandit problem. If we keep pulling this arm repeatedly, we will get a maximum expected reward which can be represented as a horizontal line (as shown in the figure below):But in a real problem statement, we need to make repeated trials by pulling different arms till we am approximately sure of the arm to pull for maximum average return at a time t. The loss that we incur due to time/rounds spent due to the learning is called regret. In other words, we want to maximise my reward even during the learning phase.Regret is very aptly named, as it quantifies exactly how much you regret not picking the optimal arm.Now, one might be curious as to how does the regret change if we are following an approach that does not do enough exploration and ends exploiting a suboptimal arm. Initially there might be low regret but overall we are far lower than the maximum achievable reward for the given problem as shown by the green curve in the following figure.Based on how exploration is done, there are several ways to solve the MABP. Next, we will discuss some possible solution strategies.A nave approach could be to calculate the q, or action value function, for all arms at each timestep. From that point onwards, select an action which gives the maximum q. The action values for each action will be stored at each timestep by the following function:It then chooses the action at each timestep that maximises the above expression, given by:However, for evaluating this expression at each time t, we will need to do calculations over the whole history of rewards. We can avoid this by doing a running sum.So, at each time t, the q-value for each action can be calculated using the reward:The problem here is this approach only exploits, as it always picks the same action without worrying about exploring other actions that might return a better reward. Some exploration is necessary to actually find an optimal arm, otherwise we might end up pulling a suboptimal arm forever.One potential solution could be to now, and we can then explore new actions so that we ensure we are not missing out on a better choice of arm. With epsilon probability, we will choose a random action (exploration) and choose an action with maximum qt(a) with probability 1-epsilon.With probability 1- epsilon  we choose action with maximum value (argmaxa Qt(a))With probability epsilon  we randomly choose an action from a set of all actions AFor example, if we have a problem with two actions  A and B, the epsilon greedy algorithm works as shown below:This is much better than the greedy approach as we have an element of exploration here. However, if two actions have a very minute difference between their q values, then even this algorithm will choose only that action which has a probability higher than the others.The solution is to make the probability of choosing an action proportional to q. This can be done using the softmax function, where the probability of choosing action a at each step is given by the following expression:The value of epsilon is very important in deciding how well the epsilon greedy works for a given problem. We can avoid setting this value by keeping epsilon dependent on time. For example, epsilon can be kept equal to 1/log(t+0.00001). It will keep reducing as time passes, to the point where we starting exploring less and less as we become more confident of the optimal action or arm.The problem with random selection of actions is that after sufficient timesteps even if we know that some arm is bad, this algorithm will keep choosing that with probability epsilon/n. Essentially, we are exploring a bad action which does not sound very efficient. The approach to get around this could be to favour exploration of arms with a strong potential in order to get an optimal value.Upper Confidence Bound (UCB) is the most widely used solution method for multi-armed bandit problems. This algorithm is based on the principle of optimism in the face of uncertainty.In other words, the more uncertain we are about an arm, the more important it becomes to explore that arm.The intuitive reason this works is that when acting optimistically in this way, one of two things happen:UCB is actually a family of algorithms. Here, we will discuss UCB1.Steps involved in UCB1:We will not go into the mathematical proof for UCB. However, it is important to understand the expression that corresponds to our selected action. Remember, in the random exploration we just had Q(a) to maximise, while here we have two terms. First is the action value function, while the second is the confidence term.Regret ComparisonAmong all the algorithms given in this article, only the UCB algorithm provides a strategy where the regret increases as log(t), while in the other algorithms we get linear regret with different slopes.An important assumption we are making here is that we are working with the same bandit and distributions from which rewards are being sampled at each timestep stays the same. This is called a stationary problem. To explain it with another example, say you get a reward of 1 every time a coin is tossed, and the result is head. Say after 1000 coin tosses due to wear and tear the coin becomes biased then this will become a non-stationary problem.To solve a non-stationary problem, more recent samples will be important and hence we could use a constant discounting factor alpha and we can rewrite the update equation like this:Note that we have replaced Nt(at) here with a constant alpha, which ensures that the recent samples are given higher weights, and increments are decided more by such recent samples. There are other techniques which provide different solutions to bandits with non-stationary rewards. You can read more about them in this paper.As mentioned in the use cases section, MABP has a lot of applications in the online advertising domain.Suppose an advertising company is running 10 different ads targeted towards a similar set of population on a webpage. We have results for which ads were clicked by a user here.Each column index represents a different ad. We have a 1 if the ad was clicked by a user, and 0 if it was not. A sample from the original dataset is shown below:This is a simulated dataset and it has Ad #5 as the one which gives the maximum reward.First, we will try a random selection technique, where we randomly select any ad and show it to the user. If the user clicks the ad, we get paid and if not, there is no profit.Total reward for the random selection algorithm comes out to be 1170. As this algorithm is not learning anything, it will not smartly select any ad which is giving the maximum return. And hence even if we look at the last 1000 trials, it is not able to find the optimal ad.Now, lets try the Upper Confidence Bound algorithm to do the same:The total_reward for UCB comes out to be 2125. Clearly, this is much better than random selection and indeed a smart exploration technique that can significantly improve our strategy to solve a MABP. After just 1500 trials, UCB is already favouring Ad #5 (index 4) which happens to be the optimal ad, and gets the maximum return for the given problem.Being an active area of research MABP will percolate to various other fields in the industry. These algorithms are so simple and powerful that they are being used increasingly by even small tech companies, as the computation resources required for them are often low.Going forward, there are other techniques based on probabilistic models such as Thompson Sampling explained by Professor Balaraman in this amazing video.You can attend a highly anticipated and extremely useful talk on reinforcement learning from him at DataHack Summit 2018 in Bangalore as well! For more details, please visithttps://www.analyticsvidhya.com/datahack-summit-2018/.",https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/
10 Mind-Blowing TED Talks on Artificial Intelligence Every Data Scientist & Business Leader Must Watch,"Learn everything about Analytics|The Incredible Inventions of Intuitive AI  Maurice Conti|How Algorithms Shape our World  Kevin Slavin|What Happens When our Computers Get Smarter than We Are?  Nick Bostrom|Can we Build AI without Losing Control Over it?  Sam Harris|How a Driverless Car Sees the Road  Chris Urmson|How Were Teaching Computers to Understand Pictures  Fei-Fei Li|How Computers Learn to Recognize Objects Instantly  Joseph Redmon|The Jobs Well Lose to Machines  Anthony Goldbloom|How AI can Enhance our Memory, Work and Social Lives  Tom Gruber|How AI can Compose a Personalized Soundtrack to your Life Pierre Barreau|End Notes","Share this:|Like this:|Related Articles|Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python|Lets Think in Graphs: Introduction to Graph Theory and its Applications using Python|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TED talks are simply fascinating. They provide tightly knit stories in short doses with mind-blowing information and experiences. It is amazing how much knowledge has been shared in this world using this simple and powerful medium. With Artificial Intelligence and Machine Learning getting so much attention in the spheres of research and business, I started looking out for TED talks on Artificial Intelligence in particular.I was in for such a treat  information treat to be precise. I gained much more from watching these TED talks than I have from following some of the most popular YouTube channels out there. Hence, I thought of sharing this incredible content with our community. To save your time  I have done all the hard work of watching all the talks on this topic till date and I have shortlisted the best ones for you.Ready for high-quality knowledge enrichment? Dig in!Note: If you are looking to understand what AI is all about and how itll impact your career, theres no better place than our AI and ML for Business Leaders course! Its a comprehensive course that gives you the lowdown on what AI and ML are, the common techniques used in the industry, which functions and roles are getting impacted and how, among other things. Check it out today!We all know how creative AI can be, but some of the things Maurice Conti and his team came up with are mindblowing. Using AI for designing new things, like cars, bridges, drones, entire buildings, etc. is no longer limited to the silver screen. This video paints a vivid picture of how AI and humans can (and hopefully will) work together in the future to accomplish tasks neither could perform by themselves.The digital era we find ourselves in the midst of is run by algorithms. Theyre everywhere  from predicting stock prices to recommending the next movie you should watch. And these algorithms are only going to embed themselves even deeper into our lifestream. This is a thoughtfully curated talk by Kevin Slavin looks at how these algos are shaping our world, and asks a very pressing question  at what point do we admit weve lost control of them?Nick Bostrom, author of the popular book Superintelligence, looks at a future where machines will become smarter than humans. Will machines rule us then? Will humans have any power in that world? These are just some of the questions Nick wants all of us to think about before we heedlessly build AI systems. Its a deep topic, and quite fitting that it comes from a philosopher.Keeping up the trend of superintelligent AI, Sam Harris delivers a riveting presentation on why we should be scared of building smart AI systems. Sam is a neuroscientist and philosopher, and he delves into both these domains to present a concerning perspective on the future. One of my favorite talks in this list.Can you imagine a world where drivers dont exist? Or at least, they arent human? Well, you better buckle up, because that world is transforming into a reality in front of us. Chris Urmson, former head of Googles driverless car program, gives us the lowdown on how autonomousvehicles take decisions in real-time about what to do next.Computer vision is the hottest research field in machine learning right now. It has spawned applications like real-time facial recognition and object detection. But how does it work? The wonderful Fei-Fei Li delivers this thrilling talk on how machines are being trained using computer vision techniques. This talk was delivered three years ago, and the state-of-the-art algorithms have since come quite a long way. Shows how quickly AI is advancing!Continuing our theme of computer vision talks, heres Joseph Redmon demonstrating how object detection works in real-time. I remember watching this a few months back and being left awed by the demo. It still has the same effect! Joseph built his model using the YOLO framework. Pretty cool, right? Driverless cars are the primary beneficiaries of these advances.Anthony Goldbloom is the co-founder and CEO of Kaggle and an all-around nice person. Hes as good a person as any to give a perspective on the jobs machines will automate in the future (in fact some of the things he mentioned are already happening as I type this!). The key takeaway from this talk is that we need to upskill ourselves at every opportunity we get, otherwise the risk of being left behind will always hang like a shadow over us.Tom Gruber is the co-creator of Siri, so he knows what hes talking about. He takes a more positive view on the advances in AI and how it can help us enhance the way we live our day-to-day lives.He shares his vision for a future where AI helps us achieve superhuman performance in perception, creativity, and cognitive function.Music and AI  a perfect combination. Pierre Barreau demonstrates AIVA, an artificial intelligence system that creates musical scores! The system has been trained on over 30,000 music compositions (including from the likes of Mozart). Pierre shows us a glimpse of how AIVA was designed using deep neural networks, and the visualizations are spectacular.This is by no means an exhaustive list. There are plenty more Ted Talks available on both the official platform and YouTube that relate to AI. But the reason Ive chosen these talks is to give you a sense of what industry leaders and experts think about this topic. They are at the forefront of this domain and control a lot about the way we approach things.Which talk in this list was your favorite? And which ones outside of these talks would you recommend we listen to? Let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/09/best-ted-talks-artificial-intelligence-must-watch/
Lets Think in Graphs: Introduction to Graph Theory and its Applications using Python|Introduction to Trees,Learn everything about Analytics|Introduction|Table of Contents|Introduction to Graphs|Why Graphs?|Origin of Graph theory: Seven Bridges of Knigsberg|Fundamentals of Graphs|Basic properties and terminologies related to Graphs|Types of Graphs|Continuing the problem of the Seven Bridges of Knigsberg|Graph Traversal|Implementing Graph Theory in Python to Solve an Airlines Challenge|End Notes,"Share this:|Like this:|Related Articles|10 Mind-Blowing TED Talks on Artificial Intelligence Every Data Scientist & Business Leader Must Watch|IBM Open Sources Comprehensive Python Toolkit for Detecting & Fighting Bias (30 Metrics, 9 Algorithms)|
Pulkit Sharma
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There is a magic in graphs. The prole of a curve reveals in a ash a whole situation  the life history of an era of prosperity. The curve informs the mind, awakens the imagination, convinces. Henry D. HubbardVisualizations are a powerful way to simplify and interpret the underlying patterns in data. The first thing I do, whenever I work on a new dataset is to explore it through visualization. And this approach has worked well for me. Sadly, I dont see many people using visualizations as much. That is why I thought I will share some of my secret sauce with the world!Use of graphs is one such visualization technique. It is incredibly useful and helps businesses make better data-driven decisions. But to understand the concepts of graphs in detail, we must first understand its base  Graph Theory.In this article, we will be learning the concepts of graphs and graph theory. We will also look at the fundamentals and basic properties of graphs, along with different types of graphs.We will then work on a case study to solve a commonly seen problem in the aviation industry by applying the concepts of Graph Theory using Python.Lets get started!Consider the plot shown below:Its a nice visualization of the store-wise sales of a particular item. But this isnt a graph, its a chart. Now you might be wondering why is this a chart and not a graph, right?Well, a chart represents the graph of a function. Let me explain this by expanding on the above example.Out of the total units of a particular item, 15.1% are sold from store A, 15.4% from store B, and so on. We can represent it using a table:Corresponding to each store is their contribution (in %) to the overall sales. In the above chart, we mapped store A with 15.1% contribution, store B with 15.4%, so on and so forth. Finally, we visualized it using a pie chart. But then whats the difference between this chart and a graph?To answer this, consider the visual shown below:The points in the above visual represent the characters of Game of Thrones, while the lines joining these points represent the connection between them. Jon Snow has connections with multiple characters, and the same goes for Tyrion, Cersei, Jamie, etc.And this is what a graph looks like. A single point might have connections with multiple points, or even a single point. Typically, a graph is a combination of vertices (nodes) and edges. In the above GOT visual, all the characters are vertices and the connections between them are edges.We now have an idea of what graphs are, but why do we need graphs in the first place? Well look at this pertinent question in the next section.Suppose you booked an Uber cab. One of the most important things that is critical to Ubers functioning is its ability to match drivers with riders in an efficient way. Consider there are 6 possible rides that you can be matched with. So, how does Uber allocate a ride to you? We can make use of graphs to visualize how the process of allotting a ride might be:As you can interpret, there are 6 possible rides (Ride 1, Ride 2, . Ride 6) which the rider can be matched with. Representing this in graph form makes it easier to visualize and finally fulfill our aim, i.e., to match the closest ride to the user. The numbers in the above graph represent the distance (in kilometers) between the rider and his/her corresponding ride. We (and of course Uber) can clearly visualize that Ride 3 is the closest option.Note: For simplicity, I have taken only the distance metric to decide which ride will be allotted to the rider. Whearas in a real life scenario, there are multiple metrics through which the allotment of a ride is decided, such as rating of the rider and driver, traffic between different routes, time for which the rider is idle, etc.Similarly, online food delivery aggregators like Zomato can select a rider who will pick up our orders from the corresponding restaurant and deliver it to us. This is one of the many use cases of graphs through which we can solve a lot of challenges. Graphs make visualizations easier and more interpretable.To understand the concept of graphs in detail, we must first understand graph theory.Well first discuss the origins of graph theory to get an intuitive understanding of graphs. There is an interesting story behind its origin, and I aim to make it even more intriguing using plots and visualizations.It all started with the Seven Bridges of Knigsberg. The challenge (or just a brain teaser) with Knigsbergs bridges, was to be able to walk through the city by crossing all the seven bridges only once. Let us visualize it first to have a clear understanding of the problem:Give it a try and see if you can walk through the city with this restraint. You have to keep two things in mind while trying to solve the above problem (or should i say riddle?):You can try any number of combinations, but it remains an impossible challenge to crack. There is no way in which one can walk through the city by crossing each bridge only once.Leonhard Euler delved deep into this puzzle to come up with the reason why this is such an impossible task. Lets analyze how he did this:There are four distinct places in the above image: two islands (B and D), and two parts of the mainland (A and C) and a total of seven bridges. Let us first look at each land separately and try to find patterns (if any exist at all) :One inference from the above image is that each land is connected with anodd number of bridges. If you wish to cross each bridge only once, then you can enter and leave a land only if it is connected to an even number of bridges. In other words, we can generalize that if there are even number of bridges, its possible to leave the land, while its impossible to do so with an odd number.Lets try to add one more bridge to the current problem and see whether it can crack open this problem:Now we have 2 lands connected with an even number of bridges, and 2 lands connected with an odd number of bridges. Lets draw a new route after the addition of the new bridge:The addition of a single bridge solved the problem! You might be wondering if the number of bridges played a significant part in solving this problem? Should it be even all the time? Well, thats not always the case. Euler explained that along with the number of bridges, the number of pieces of land with an odd number of connected bridges matters as well. Euler converted this problem from land and bridges to graphs, where he represented the land as vertices and the bridges as edges:Here, the visualization is simple and crystal clear. Before we move further and delve deeper into this problem, let us first understand the fundamentals and basic properties of a graph.There are many key points and key words that we should keep in mind when we are dealing with graphs. In this section, we will discuss all those keywords in detail.These are some of the fundamentals which you must keep in mind when dealing with graphs. Now onto understanding the basic properties of a graph.So far, we have seen what a graph looks like and its different components. Now we will turn our focus to some basic properties and terminologies related to a graph. We will be using the below given graph (referred to as G) and understand each terminology using the same:Take a moment and think about possible solutions to the following questions:I will try to answer all these questions using basic graph terminologies:These are some of the terminologies related to graphs. Next we will discuss the different types of graphs.There are vairous and diverse types of graphs. In this section, we will discuss some of the most commonly used ones.Now that we have an understanding of the different types of graphs, their components, and some of the basic graph-related terminologies, lets get back to the problem which we were trying to solve, i.e. the Seven Bridges of Knigsberg. We shall explore in even more detail how Leonhard Euler approached and explained his reasoning.We saw earlier that Euler transformed this problem using graphs:Here, A, B, C, and D represent the land, and the lines joining them are the bridges. We can calculate the degree of each vertex.deg(B) = 5deg(A) = deg(C) = deg(D) = 3Euler showed that the possibility of walking through a graph (city) using each edge (bridge) only once, strictly depends on the degree of vertices (land). And such a path, which contains each edge of a graph only once, is called Eulers path.Can you figure out Eulers path for our problem? Lets try!And this is how the classic Seven Bridges of Knigsberg challenge can be solved using graphs and Eulers path. And this is basically the origin of Graph Theory. Thanks to Leonhard Euler!Trees are one of the most powerful and effective ways of representing a graph. In this section, we will learn what binary search trees are, how they work, and how they make visualizations more interpretable. But before all that, take a moment to understand what trees actually are in this context.Trees are graphs which do not contain even a single cycle:In the above example, the first graph has no cycle (aka a tree), while the second graph has a cycle (A-B-E-C-A, hence its not a tree).The elements of a tree are called nodes. (A, B, C, D, and E) are the nodes in the above tree. The first node (or the topmost node) of a tree is known as the root node, while the last node (node C, D and E in the above example) is known as the leaf node. All the remaining nodes are known as child nodes (node B in our case).Its time to move on to one of the most important topics in Graph Theory, i.e., Graph Traversal.Suppose we want to identify the location of a particular node in a graph. What might me the possible solution to identify nodes of a graph? How to start? What should be the starting point? Once we know the starting point, how to proceed further? I will try to answer all these questions in this section by explaining the concepts of Graph Traversal.Graph Traversal refers to visiting every vertex and edge of a graph exactly once in a well-defined order. As the aim of traversing is to visit each vertex only once, we keep a track of vertices covered so that we do not cover same vertex twice. There are various methods for graph traversal and we will discuss some of the famous methods:We start from the source node (root node) and traverse the graph, layer wise. Steps for Breadth First Search are:Let me explain it with a visualization:So in Breadth First Search, we start from the Source Node (A in our case) and move down to the first layer, i.e. Layer 1. We cover all the nodes in that layer by moving horizontally (B -> C). Then we go to the next layer, i.e. Layer 2 and repeat the same step (we move from D -> E -> F). We continue this step until all the layers and vertices are covered.Key advantage of this approach is that we will always find the shortest path to the goal. This is appropriate for small graphs and trees but for more complex and larger graphs, its performance is very slow and it also takes a lot of memory. We will look at another traversing approach which takes less memory space as compared to BFS.Let us first look at the steps involved in this approach:The sequence for Depth First Search for the above example will be:A -> B -> D -> E -> C -> FOnce a path has been fully explored it can be removed from memory, so DFS only needs to store the root node, all the children of the root node and where it currently is. Hence, it overcomes the memory problem of BFS.In this approach all the nodes of a tree are arranged in a sorted order. Lets have a look at an example of Binary Search Tree:As mentioned earlier, all the nodes in the above tree are arranged based on a condition. Suppose we want to access the node with value 45. If we would have followed BFS or DFS, we would have required a lot of computational time to reach to it. Now lets look at how a Binary Search Tree will help us to reach to the required node using least number of steps. Steps to reach to the node with value 45 using Binary Search Tree:This approach is very fast and takes very less memory as well. Most of the concepts of Graph Theory have been covered. Next, we will try to implement these concepts to solve a real-life problem using Python.And finally, we get to work with data in Python! In this dataset, we have records of over 7 million flights from the USA. The below variables have been provided:It is a gigantic dataset and I have taken only a sample from it for this article. The idea is to give you an understanding of the concepts using this sample dataset, and you can then apply them to the entire dataset. Download the dataset which we will be using for the case study from here. We will first import the usual libraries, and read the dataset, which is provided in a .csv format:Lets have a look at the first few rows of the dataset using the head() function:Here, CRSDepTime, CRSArrTime, DepTime, and ArrTime represent the scheduled time of departure, the scheduled time of arrival, the actual time of departure, and the actual time of arrival respectively. Origin and Dest are the Origin and Destination of the journey.There can often be multiple paths from one airport to another, and the aim is to find the shortest possible path between all the airports. There are two ways in which we can define a path as the shortest:We can solve such problems using the concepts of graph theory which we have learned so far. Can you recall what we need to do to make a graph?The answer is identifying the vertices and edges! We can convert the problem to a graph by representing all the airports as vertices, and the route between them as edges. We will be using NetworkX for creating and visualizing graphs. NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. You can refer to the documentation of NetworkX here.After installing NetworkX, we will create the edges and vertices for our graph using the dataset:It will store the vertices and edges automatically. Take a quick look at the edges and vertices of the graph which we have created:Let us plot and visualize the graph using thematplotlib and draw_networkx() functions of networkx.The above amazing visualization represents the different flight routes. Suppose a passenger wants to take the shortest route from AMA to PBI. Graph theory comes to the rescue once again!Lets try to calculate the shortest path based on the airtime between the airports AMA and PBI. We will be using Dijkstras shortest path algorithm. This algorithm finds the shortest path from a source vertex to all the vertices of the given graph. Let me give you a brief run through of the steps this algorithm follows:Let us take an example to understand this algorithm in a better way:Here the source vertex is A. The numbers represent the distance between the vertices. Initially, the sptSet is empty so we will assign distances to all the vertices. The distances are:{0, INF, INF, INF, INF, INF}, where INF represents INFINITE.Now, we will pick the vertex with the minimum distance, i.e., A and it will be included in the sptSet. So, the new sptSet is {A}. The next step is to pick a vertex which is not in the sptSet and is closest to the source vertex. This, in our case, is B with a distance value of 2. So this will be added to the sptSet.sptSet = {A,B}Now we will update the distances of vertices adjacent to vertex B:The distance value of the vertex F becomes 6. We will again pick the vertex with the minimum distance value which is not already included in SPT (C with a distance value of 4).sptSet = {A,B,C}We will follow similar steps until all the vertices are included in the sptSet. Lets implement this algorithm and try to calculate the shortest distance between the airports. We will use thedijkstra_path() function of networkx to do so:This is the shortest possible path between the two airports based on the distance between them. We can also calculate the shortest path based on the airtime just by changing the hyperparameter weight=AirTime:This is the shortest path based on the airtime. Intuitive and easy to understand, this was all about graph theory!This is just one of the many applications of Graph Theory. We can apply it to almost any kind of problem and get solutions and visualizations. Some of the application of Graph Theory which I can think of are:These are some of the applications. You can come up with many more. Feel free to share them in the comment section below. I hope you have enjoyed the article. Looking forward to your responses.",https://www.analyticsvidhya.com/blog/2018/09/introduction-graph-theory-applications-python/
"IBM Open Sources Comprehensive Python Toolkit for Detecting & Fighting Bias (30 Metrics, 9 Algorithms)",Learn everything about Analytics|Overview|Introduction|Our take on this,"Share this:|Like this:|Related Articles|Lets Think in Graphs: Introduction to Graph Theory and its Applications using Python|Performing Speech and Object Recognition using just One Model with MITs ML System|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Bias is a serious issue in machine learning models. Quite often we tend to skim through the data in our eagerness to build the model, and then scratch our heads when the model doesnt translate well in real-world situations. Its a pervasive issue, and one that experts have been trying to mitigate for years.Source: YouTubeWith the seriousness of this challenge in mind, IBM has released a toolkit that contains a set of fairness metrics for datasets and models, explanations for these metrics, and algorithms that can deal with any bias that is unearthed. And the best part? Its open source (and in Python)! Check out the below links to get started by yourself:The toolkit, officially labelled the AI Fairness 360 Open Source Toolkit, contains over 30 fairness metrics and 9 algorithms that aim to deal with bias. These algorithms are state-of-the-art, and are mentioned below:The above mentioned official site has multiple tutorials in different industry functions to give you a taste of how to use the toolkit. These include credit scoring, medical expenditure, and gender bias in facial recognition. What are you waiting for? Get started already!We need to remember that data isnt just numbers on a spreadsheet, but is linked to human beings. Bias is an omnipotent issue. I cannot stress enough on how important dealing with it is, especially when were running algorithms that will directly impact lives.Can you imagine running a credit risk model, or a loan default model, and turning away folks who most desperately need the money? They were perfectly eligible for it, but due to some bias in the data, and subsequently the model, we failed to consider that aspect. Unacceptable, right? Lets keep that in mind next time we work on a project and try to use this toolkit, if other known methods are not working.",https://www.analyticsvidhya.com/blog/2018/09/ibm-open-sources-comprehensive-python-toolkit-for-detecting-fighting-bias-30-metrics-9-algorithms/
Performing Speech and Object Recognition using just One Model with MITs ML System,Learn everything about Analytics|Overview|Introduction|Our take on this,"Share this:|Like this:|Related Articles|IBM Open Sources Comprehensive Python Toolkit for Detecting & Fighting Bias (30 Metrics, 9 Algorithms)|Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The state-of-the-art models in deep learning are able to detect objects in images and perform speech recognition, but separately using different models for each. If I told you that theres a way to build one model that can combine these two functions, youd most likely claim that this is what AGI is supposed to be, and were nowhere near that!Well, I have news for you  MIT researchers have designed such a model. Given an image and an audio caption, the model highlights the relevant regions in the image that are spoken about. And it does this in near real-time. Granted the research is still in a very nascent stage and the model can only recognize a few hundred different words and objects, but this will surely get your excitement levels up if youre into machine learning.Source: MIT NewsThe researchers built on a slightly older research in this area. They used the previous study to associate particular words with specific pixel patches. The model was then trained on a classification database on the Mechnical Turk platform using a total of 400,000 image-caption pairs. 1,000 random pairs were held out and used as the test set. It wont surprise seasoned deep learning users to know that a convolutional neural network (CNN) is at the heart of the model.There are two types of CNNs are play here  an image analyzing one, and an audio analyzing one.MITs team have written a very detailed blog post explaining the intricacies of this technology and you can check it out here.Once the researchers perfect (or at least improve) the model, this system has the capability to save hours upon hours of manual effort. And it will do wonders for speech recognition and object detection, wont it? Imagine the possibilities!According to the blog post mentioned above, there are approximately 7,000 spoken languages in the world, and only 100 of them have enough data to be used in a speech recognition model. Once this system is fine tuned and ready for action, this number could rise significantly higher.",https://www.analyticsvidhya.com/blog/2018/09/speech-object-recognition-one-model-mit-ml/
Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming,Learn everything about Analytics|Introduction|Why learn dynamic programming?|Table of Contents|Understanding Agent Environment Interface using tic-tac-toe|Introduction to Markov Decision Process|Dynamic Programming|DP in action: Finding optimal policy for Frozen Lake environment using Python|End Notes,"Sunnys Motorbike Rental company|State Value Function:How good it is to be in a given state?|State-Action Value Function:How good an action is at a particular state?|Bellman Expectation Equation:The value information from successor states is being transferred back to the current state|Bellman Optimality Equation: Find the optimal policy|Policy Evaluation: Find out how good a policy is?|Policy Improvement: Improve an arbitrary policy|Policy Iteration: Policy Evaluation + Policy Improvement|Value Iteration|Frozen Lake Environment|Policy Iteration in python|Value Iteration in python|Share this:|Like this:|Related Articles|Performing Speech and Object Recognition using just One Model with MITs ML System|DataHack Radio #10: The Role of Computer Science in the Data Science World with Dr. Jeannette M. Wing|
Ankit Choudhary
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Reinforcement learning is responsible for the two biggest AI wins over human professionals  Alpha Go and OpenAI Five. Championed by Google and Elon Musk, interest in this field has gradually increased in recent years to the point where its a thriving area of research nowadays.In this article, however, we will not talk about a typical RL setup but explore Dynamic Programming (DP). DP is a collection of algorithms that can solve a problem where we have the perfect model of the environment (i.e. probability distributions of any change happening in the problem setup are known) and where an agent can only take discrete actions.DP essentially solves a planning problem rather than a more general RL problem. The main difference, as mentioned, is that for an RL problem the environment can be very complex and its specifics are not known at all initially.But before we dive into all that, lets understand why you should learn dynamic programming in the first place using an intuitive example.Apart from being a good starting point for grasping reinforcement learning, dynamic programming can help find optimal solutions to planning problems faced in the industry, with an important assumption that the specifics of the environment are known.DP presents a good starting point to understand RL algorithms that can solve more complex problems.Sunny manages a motorbike rental company in Ladakh. Being near the highest motorable road in the world, there is a lot of demand for motorbikes on rent from tourists. Within the town he has 2 locations where tourists can come and get a bike on rent. If he is out of bikes at one location, then he loses business.The problem that Sunny is trying to solve is to find out how many bikes he should move each day from 1 location to another so that he can maximise his earnings.Here, we exactly know the environment (g(n) & h(n)) and this is the kind of problem in which dynamic programming can come in handy.Similarly, if you can properly model the environment of your problem where you can take discrete actions, then DP can help you find the optimal solution.In this article, we will use DP to train an agent using Python to traverse a simple environment, while touching upon key concepts in RL such as policy, reward, value function and more.Most of you must have played the tic-tac-toe game in your childhood. If not, you can grasp the rules of this simple game from its wiki page. Suppose tic-tac-toe is your favourite game, but you have nobody to play it with. So you decide to design a bot that can play this game with you. Some key questions are:Can you define a rule-based framework to design an efficient bot?You sure can, but you will have to hardcode a lot of rules for each of the possible situations that might arise in a game. However, an even more interesting question to answer is:Can you train the bot to learn by playing against you several times? And that too without being explicitly programmed to play tic-tac-toe efficiently?A few considerations for this are:For more clarity on the aforementioned reward, let us consider a match between bots O and X:Consider the following situation encountered in tic-tac-toe:If bot X puts X in the bottom right position for example, it results in the following situation:Bot O would be rejoicing (Yes! They are programmed to show emotions) as it can win the match with just one move. Now, we need to teach X not to do this again. So we give a negative reward or punishment to reinforce the correct behaviour in the next trial. We say that this action in the given state would correspond to a negative reward and should not be considered as an optimal action in this situation.Similarly, a positive reward would be conferred to X if it stops O from winning in the next move:Now that we understand the basic terminology, lets talk about formalising this whole process using a concept called a Markov Decision Process or MDP.A Markov Decision Process (MDP) model contains:Now, let us understand the markov or memoryless property.Any random process in which the probability of being in a given state depends only on the previous state, is a markov process.In other words, in the markov decision process setup, the environments response at time t+1 depends only on the state and action representations at time t, and is independent of whatever happened in the past.The above diagram clearly illustrates the iteration at each time step wherein the agent receives a reward Rt+1 and ends up in state St+1based on its action At at a particular state St. The overall goal for the agent is to maximise the cumulative reward it receives in the long run. Total reward at any time instant t is given by:where T is the final time step of the episode. In the above equation, we see that all future rewards have equal weight which might not be desirable. Thats where an additional concept of discounting comes into the picture. Basically, we define  as a discounting factor and each reward after the immediate reward is discounted by this factor as follows:For discount factor < 1, the rewards further in the future are getting diminished. This can be understood as a tuning parameter which can be changed based on how much one wants to consider the long term ( close to 1) or short term ( close to 0).Can we use the reward function defined at each time step to define how good it is, to be in a given state for a given policy? The value function denoted as v(s) under a policy  represents how good a state is for an agent to be in. In other words, what is the average reward that the agent will get starting from the current state under policy ?E in the above equation represents the expected reward at each state if the agent follows policy  and S represents the set of all possible states.Policy, as discussed earlier, is the mapping of probabilities of taking each possible action at each state ((a/s)). The policy might also be deterministic when it tells you exactly what to do at each state and does not give probabilities.Now, its only intuitive that the optimum policy can be reached if the value function is maximised for each state. This optimal policy is then given by:The above value function only characterizes a state. Can we also know how good an action is at a particular state? A state-action value function, which is also called the q-value, does exactly that. We define the value of action a, in state s, under a policy , as:This is the expected return the agent will get if it takes action At at time t, given state St, and thereafter follows policy .Bellman was an applied mathematician who derived equations that help to solve an Markov Decision Process.Lets go back to the state value function v and state-action value function q. Unroll the value function equation to get:In this equation, we have the value function for a given policy  represented in terms of the value function of the next state.Choose an action a, with probability (a/s) at the state s, which leads to state s with prob p(s/s,a). This gives a reward [r + *v(s)] as given in the square bracket above.This is called the Bellman Expectation Equation. The value information from successor states is being transferred back to the current state, and this can be represented efficiently by something called a backup diagram as shown below.The Bellman expectation equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.We have n (number of states) linear equations with unique solution to solve for each state s.The goal here is to find the optimal policy, which when followed by the agent gets the maximum cumulative reward. In other words, find a policy , such that for no other  can the agent get a better expected return. We want to find a policy which achieves maximum value for each state.Note that we might not get a unique policy, as under any situation there can be 2 or more paths that have the same return and are still optimal.Optimal value function can be obtained by finding the action a which will lead to the maximum of q*. This is called the bellman optimality equation for v*.Intuitively, the Bellman optimality equation says that the value of each state under an optimal policy must be the return the agent gets when it follows the best action as given by the optimal policy. For optimal policy *, the optimal value function is given by:Given a value function q*, we can recover an optimum policy as follows:The value function for optimal policy can be solved through a non-linear system of equations. We can can solve these efficiently using iterative methods that fall under the umbrella of dynamic programming.Dynamic programming algorithms solve a category of problems called planning problems. Herein given the complete model and specifications of the environment (MDP), we can successfully find an optimal policy for the agent to follow. It contains two main steps:To solve a given MDP, the solution must have the components to:Policy evaluation answers the question of how good a policy is. Given an MDP and an arbitrary policy , we will compute the state-value function. This is called policy evaluation in the DP literature. The idea is to turn bellman expectation equation discussed earlier to an update.To produce each successive approximation vk+1 from vk, iterative policy evaluation applies the same operation to each state s. It replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated, until it converges to the true value function of a given policy .Let us understand policy evaluation using the very popular example of Gridworld.A bot is required to traverse a grid of 44 dimensions to reach its goal (1 or 16). Each step is associated with a reward of -1. There are 2 terminal states here: 1 and 16 and 14 non-terminal states given by [2,3,.,15]. Consider a random policy for which, at every state, the probability of every action {up, down, left, right} is equal to 0.25. We will start with initialising v0 for the random policy to all 0s.This is definitely not very useful. Lets calculate v2 for all the states of 6:Similarly, for all non-terminal states, v1(s) = -1.For terminal states p(s/s,a) = 0 and hence vk(1) = vk(16) = 0 for all k. So v1 for the random policy is given by:Now, for v2(s) we are assuming  or the discounting factor to be 1:As you can see, all the states marked in red in the above diagram are identical to 6 for the purpose of calculating the value function. Hence, for all these states, v2(s) = -2.For all the remaining states, i.e., 2, 5, 12 and 15, v2 can be calculated as follows:If we repeat this step several times, we get v:Using policy evaluation we have determined the value function v for an arbitrary policy . We know how good our current policy is. Now for some state s, we want to understand what is the impact of taking an action a that does not pertain to policy . Lets say we select a in s, and after that we follow the original policy . The value of this way of behaving is represented as:If this happens to be greater than the value function v(s), it implies that the new policy  would be better to take. We do this iteratively for all states to find the best policy. Note that in this case, the agent would be following a greedy policy in the sense that it is looking only one step ahead.Lets get back to our example of gridworld. Using v, the value function obtained for random policy , we can improve upon  by following the path of highest value (as shown in the figure below). We start with an arbitrary policy, and for each state one step look-ahead is done to find the action leading to the state with the highest value. This is done successively for each state.As shown below for state 2, the optimal action is left which leads to the terminal state having a value . This is the highest among all the next states (0,-18,-20). This is repeated for all states to find the new policy.Overall, after the policy improvement step using v, we get the new policy :Looking at the new policy, it is clear that its much better than the random policy. However, we should calculate v using the policy evaluation technique we discussed earlier to verify this point and for better understanding.Once the policy has been improved using v to yield a better policy , we can then compute v to improve it further to . Repeated iterations are done to converge approximately to the true value function for a given policy  (policy evaluation). Improving the policy as described in the policy improvement section is called policy iteration.In this way, the new policy is sure to be an improvement over the previous one and given enough iterations, it will return the optimal policy. This sounds amazing but there is a drawback  each iteration in policy iteration itself includes another iteration of policy evaluation that may require multiple sweeps through all the states. Value iteration technique discussed in the next section provides a possible solution to this.We saw in the gridworld example that at around k = 10, we were already in a position to find the optimal policy. So, instead of waiting for the policy evaluation step to converge exactly to the value function v, we could stop earlier.We can also get the optimal policy with just 1 step of policy evaluation followed by updating the value function repeatedly (but this time with the updates derived from bellman optimality equation). Lets see how this is done as a simple backup operation:This is identical to the bellman update in policy evaluation, with the difference being that we are taking the maximum over all actions. Once the updates are small enough, we can take the value function obtained as final and estimate the optimal policy corresponding to that.Some important points related to DP:It is of utmost importance to first have a defined environment in order to test any kind of policy for solving an MDP efficiently. Thankfully, OpenAI, a non profit research organization provides a large number of environments to test and play with various reinforcement learning algorithms. To illustrate dynamic programming here, we will use it to navigate the Frozen Lake environment.The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.The surface is described using a grid like the following:(S: starting point, safe),(F: frozen surface, safe),(H: hole, fall to your doom),(G: goal)The idea is to reach the goal from the starting point by walking only on frozen surface and avoiding all the holes. Installation details and documentation is available at this link.Once gym library is installed, you can just open a jupyter notebook to get started.Now, the env variable contains all the information regarding the frozen lake environment. Before we move on, we need to understand what an episode is. An episode represents a trial by the agent in its pursuit to reach the goal. An episode ends once the agent reaches a terminal state which in this case is either a hole or the goal.Description of parameters for policy iteration functionpolicy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.environment: Initialized OpenAI gym environment objectdiscount_factor: MDP discount factortheta: A threshold of a value function change. Once the update to value function is below this numbermax_iterations: Maximum number of iterations to avoid letting the program run indefinitelyThis function will return a vector of size nS, which represent a value function for each state.Lets start with the policy evaluation step. The objective is to converge to the true value function for a given policy . We will define a function that returns the required value function.Now coming to the policy improvement part of the policy iteration algorithm. We need a helper function that does one step lookahead to calculate the state-value function. This will return an array of length nA containing expected value of each actionNow, the overall policy iteration would be as described below. This will return a tuple (policy,V) which is the optimal policy matrix and value function for each state.The parameters are defined in the same manner for value iteration.The value iteration algorithm can be similarly coded:Finally, lets compare both methods to look at which of them works better in a practical setting. To do this, we will try to learn the optimal policy for the frozen lake environment using both techniques described above. Later, we will check which technique performed better based on the average return after 10,000 episodes.We observe that value iteration has a better average reward and higher number of wins when it is run for 10,000 episodes.In this article, we became familiar with model based planning using dynamic programming, which given all specifications of an environment, can find the best policy to take. I want to particularly mention the brilliant book on RL by Sutton and Barto which is a bible for this technique and encourage people to refer it. More importantly, you have taken the first step towards mastering reinforcement learning. Stay tuned for more articles covering different algorithms within this exciting domain.",https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/
DataHack Radio #10: The Role of Computer Science in the Data Science World with Dr. Jeannette M. Wing,Learn everything about Analytics|Introduction|Professor Jeannette Wings Background|Using Formal Methods Techniques to Improve Machine Learning Algorithms|Research Projects in Academia and Microsoft|Difference between Working in Academia v Industry|Where are Computer Science and Data Science Heading in the Next 5 Years?|End Notes,"|Share this:|Related Articles|Nuts & Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming|Tired of Debugging Code? Facebooks SapFix Tool Automates the Entire Process|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you noticed that the recent surge of data scientists have a background in computer science? Its not a coincidence. These two domains are important in their own right but when merged together, they produce powerful results.We are thrilled to announce the release of episode 10 of our DataHack Radio podcast with none other than Professor Jeannette M. Wing! She has over 4 decades of experience in academia and the industry, and there is no one better to give a perspective on how computer science has evolved, and how it meshes with the data science world.
I have briefly summarized the key takeaways from this episode below. I recommend listening to the podcast to truly get a feel for how computer science and data science are a powerful combination when used together. Enjoy this episode!Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms:Professor Wing has always been fascinated by mathematics and engineering since her childhood. She went to graduation school at MIT and started majoring in electrical engineering there. During her initial days at the university, she was introduced to the world of computer science and that prompted her to change majors. And there was no looking back from that point on.Post her days at MIT (where she also successfully completed her Ph.D in computer science), she worked at the University of Southern California for a couple of years before joining Carnegie Mellon University. She was the computer science department head twice at Carnegie Mellon. In between those two stints, she worked at the National Science Foundation (NSF).During her second time as the department head at Carnegie Mellon, Microsoft approached her and she took up a role there in 2013. Within a year of joining, she was put in charge of all the basic research labs, including in Silicon Valley, New York, Bangalore, and Beijing, among others.And then last year came Columbia University and a chance to work in academia again. At Columbia, she is the Avanessians Director of the Data Science Institute and Professor of Computer Science. She reports directly to the President of the University.Although there has been decades of research done in computer science to formally show how one can prove how a program is correct, this is all with respect to mathematical logic. What data science is now bringing is the complexity for proving how a property is correct with respect to inherently probabilistic and statistical methods.Professor Wing firmly believes that a lot of the new data science methods should be revisited by the formal methods techniques. Its a challenge for the formal methods community to help data science grow using these concepts, something which hasnt yet happened.In case you are not aware, formal methods are mathematics based techniques especially used in computer science. You can read more about them here.Professor Wing, in her current role at Columbia University, is working with the AI community to understand what methods and logic are required to specify the relevant properties that these machine learned models should have. She feels this will help build safe and trustworthy AI systems for the future, a topic Professor Wing is a strong advocate of.At Microsoft, she was overlooking several research projects in multiple locations as I mentioned above. The Bangalore lab, in particular, had a couple of big strengths:Im really just an academic at heart.  Professor WingA very common question from folks new to data science is  whats the difference between working in academia versus getting industry experience? And Professor Wing was kind enough to cover this topic.She echoes the wide-held belief that being a scholar has its own distinct advantages. You have more freedom to explore questions like why something works, rather than just focusing on how it works (which is what happens in most industry roles). The science part of both computer and data science comes from research and academia far more than the industry.It was a privilege hosting Professor Wing on our podcast. Her explanation of formal methods and the important part they are playing in the software industry was a true delight to listen to. Fans of mathematics will surely love this episode.Happy listening!",https://www.analyticsvidhya.com/blog/2018/09/datahack-radio-data-science-podcast-jeanette-wing/
Tired of Debugging Code? Facebooks SapFix Tool Automates the Entire Process,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio #10: The Role of Computer Science in the Data Science World with Dr. Jeannette M. Wing|Heroes of Deep Learning: Top Takeaways for Aspiring Data Scientists from Andrew Ngs Interview Series|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Debugging code is a tedious and cumbersome task. Im yet to meet a data scientist or data engineer who looks forward to minutely combing through his/her code to look for what went wrong (and I suspect I wont be meeting this person anytime soon).There have been a few releases this year focusing on automatically finding errors in your programming script. And now Facebook is the most high profile company to throw its hat into the ring with the announcement of SapFix.Source: FollownewsEven though SapFix is an independent tool, Facebook is currently using it concurrently with Sapienz, another software testing tool used within the company. As of today, Sapienz finds bugs in the programs and SapFix works on fixing them before the code reaches the production environment.The below workflow, taken from Facebooks blog post, describes how SapFix works:SapFix generates multiple potential fixes for each bug. It is then designed to evaluate their quality on the basis of three factors:Once the code has been fully tested, the tool sends them over to a human engineer for approval. Sounds pretty similar to how most testing work goes, right? SapFix is intelligent enough to even give its own recommendations to the engineers, based on the above evaluation criteria.While Sapienz is in almost wide-scale use at Facebook, SapFix is still in a pretty nascent stage. The engineering teams are working to craft the finer details and round the edges before its fully deployed in day-to-day operations within the organization. And once thats done, they plan to open source to the wider ML community.If youre one of those rare people who loves programming so much that debugging doesnt feel like a burden, I tip my hat to you. I find it a very arduous task and one I would love to avoid. Ive been using PixieDebugger lately for Python, and that has certainly helped me get an intuitive feel for where Im going wrong.But to automate the entire process? What a welcome announcement! I cant wait for SapFix to be open sourced.",https://www.analyticsvidhya.com/blog/2018/09/acebooks-sapfix-automates-code-debugging-process/
Heroes of Deep Learning: Top Takeaways for Aspiring Data Scientists from Andrew Ngs Interview Series,Learn everything about Analytics|Introduction|Geoffrey Hinton|Ian Goodfellow|Yoshua Bengio|Pieter Abbeel|Yuanquing Lin|Andrej Karpathy|Ruslan Salakhutdinov|Yann LeCun|End Notes,"Share this:|Like this:|Related Articles|Tired of Debugging Code? Facebooks SapFix Tool Automates the Entire Process|Perform Automated Machine Learning for Free with Microsofts Open Source Python Toolkit|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Key takeaways from the video|Key takeaways from the video|Key takeaways from the video|Key takeaways from the video|Key takeaways from the video|Key takeaways from the video|Key takeaways from the interview|Key takeaways from the video,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Andrew Ng is the most recognizable personality of the modern deep learning world. His machine learning course is cited as the starting point for anyone looking to understand the math behind algorithms. But even the great Andrew Ng looks up to and takes inspiration from other experts.In this amazing and in-depth video series, he has interviewed some of the most eminent personalities in the world of deep learning (eight heroes, to be precise). The interviews span the length and breadth of deep learning, including topics like backpropogation, GANs, transfer learning, etc. Even artificial intelligence crops up in between conversations. But dont worry if these terms sound overwhelming, we have listed down the key takeaways from each interview just for you.Source: ForbesThe heroes Andrew Ng has interviewed are:What a stellar cast of experts! Now its time to dive in and look at the top takeaways from each video.Geoffrey Hinton is best known for his work on artificial neural networks (ANNs). His contributions in the field of deep learning are the main reason behind the success of the field and he is often called the Godfather of Deep Learning (with good reason). His research on the backpropagation algorithm brought about a drastic change in the performance of deep learning models.Ian Goodfellow is a rockstar in the deep learning space and is currently working as a research scientist at Google Brain. He is best known for his invention of generative adversarial networks (GANs). His book on Deep Learning covers a broad range of topics like mathematical and conceptual backgrounds and deep learning techniques used in the industry, which can be a good starting point for any deep learning enthusiast. We strongly recommend reading that book, its free!Yoshua Bengio is a computer scientist, well known for his work on ANN and deep learning. He is the co-founder of Element AI, a Montreal-based business incubator that seeks to transform AI research into real-world business applications.Pieter Abbeel is the Director of the UC Berkeley Robot Learning Lab. His work in reinforcement learning is often cited by scholars as the best in the modern era. He has previously worked in a senior role at OpenAI.Yuanquing Lin is the Director at the Institute of Deep Learning at Baidu. He has a background in mathematics and physics, and holds a Ph.D in machine learning. A word of caution  the English might be a little hard to understand in the video as its not Mr.Yuanquings first language.Andrej Karpathy is the director of artificial intelligence and Autopilot Vision at Tesla. Like Pieter Abbeel, Andrej previously worked at OpenAI, but as a research scientist. He is a widely considered and cited as a leading expert in the field of computer vision, especially image recognition (though of course hes an expert in quite a lot of deep learning areas).This is one the most intriguing videos in the series!Ruslan Salakhutdinov is the director of AI Research at Apple and is known as the developer of Bayesian Program Learning. His areas of specialization are many, but are listed as probabilistic graphical models, large-scale optimization, and of course, deep learning. Heres a fun fact  his doctoral adviser? None other than Geoffrey Hinton!Yann LeCun is the founding father of convolutional nets. He is currently the Chief AI Scientist and VP at Facebook. He is a professor, researcher, and R&D manager with academic and industry experience in AI, machine learning,deep learning, computer vision, intelligent data analysis, data mining, data compression, digital librarysystems, and robotics. And thats just scraping the surface of what this expert is capable of.This is easily the most fascinating interview series on YouTube concerning deep learning. There is SO MUCH to learn from each of these seven heroes. If you havent seen these videos before, were glad you stopped by because this will feel like hitting the jackpot.Andrew Ng is a wonderful interviewer and him conversing with other experts feels like a dream. Grab your pen and notebook because theres a whole host of things for you to learn.",https://www.analyticsvidhya.com/blog/2018/09/heroes-deep-learning-top-takeaways-andrew-ng-interview-series/
Perform Automated Machine Learning for Free with Microsofts Open Source Python Toolkit,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Heroes of Deep Learning: Top Takeaways for Aspiring Data Scientists from Andrew Ngs Interview Series|How Machine Learning Algorithms & Hardware Power Apples Latest Watch and iPhones|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you hear the words automated machine learning, what comes to your mind first? For me, its usually H2O.ais Driverless AI, or Googles Cloud AutoML. Microsoft is probably a bit down in that list (Azure, anyone?).But that list might be about to see some changes. Microsoft has released an open-source automated machine learning toolkit on GitHub that helps a user perform neural architecture search and hyperparameter tuning. Microsoft is calling the toolkit Neural Network Intelligence (NNI).According to Microsoft, the tool dispatches and runs trial jobs that generated are by tuning algorithms to search for the best neural architecture and/or hyper-parameters at different environments (e.g. local, remote servers and cloud). The below diagram illustrates this point well:So who does this NNI toolkit target? And why should you consider using it (or at least giving it a go)? Use the below checklist to find out:You can install NNI through pip by using the below command:Note that youll need to have Python version 3.5 or greater to use this toolkit.Ill be honest  when I took my first steps into the dreamy world of data science, I hadnt imagined autoML picking up so quickly. I used to hear my seniors talk about how all of that was at least 5-7 years away (back in 2016). Just goes to show how quickly technology has advanced, its even taken people in-the-know by surprise.And were already at the stage of seeing autoML going open-source (or at least parts of it)! I covered Auto-Keras last month and that was quite a big deal in the ML community. Im sure Microsofts NNI will help speed up the automated designing of models to quite an extent as well. Its definitely worth exploring and since its free, there are no excuses for not doing so!",https://www.analyticsvidhya.com/blog/2018/09/neural-network-intelligence-microsoft-automated-machine-learning/
How Machine Learning Algorithms & Hardware Power Apples Latest Watch and iPhones,Learn everything about Analytics|Introduction|The A12 Chip|So whats the deal with the neural engine?|The Apple Watch|End Notes,"Share this:|Like this:|Related Articles|Perform Automated Machine Learning for Free with Microsofts Open Source Python Toolkit|A Gentle Introduction to Handling a Non-Stationary Time Series in Python|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This is a great time to be a data scientist  all the top tech giants are integrating machine learning into their flagship products and the demand for such professionals is at an all-time high. And its only going to get better!Apple has been a major advocate of machine learning, and has packed its products with features like FaceID, Augmented Reality, Animoji, Healthcare sensors, etc. While watching Apples keynote event yesterday, I couldnt help but wonder at the new chip technology they have developed that uses the power of machine learning algorithms.In this article, well check out some of the ways Apple has used machine learning to enrich the user experience. And believe me, some of the numbers youll see will blow your mind.And if youre already itching to get started with building your first ML models on an iPhone using Apples CoreML, check out this excellent article.Source: The VergeDesigned in-house by Apples developers, the A12 chip features an even more advanced neural engine than last year (when the neural engine made its official debut inside the A11 chip). The A11 chip powers the iPhone X, 8, and 8 Plus so you can imagine why the A12 has created quite a stir in the machine learning community.The A12 is using features as small as 7 nanometers as compared to 10 in the A11, which explains the acceleration in speed. And did you really think Apple would let the event slide without mentioning battery life? The A12 chip has a smart compute system that automatically recognizes which tasks should run on the primary part of the chip, which ones should be sent to the GPU, and which ones should be delegated to the neural engine.Source: Apple InsiderThe neural engines key functions are two-fold:This years engine has eight cores which is how the chip can perform 5 trillion operations per second. Last years version had two cores and could go up to 600 billion operations per second. Its a nice microcosm of how rapidly technology is evolving in front of our eyes.And the neural engine can do even more..It will help iPhone users take better pictures (how much better can you get every year?!). When you press the shutter button, the neural network identifies the kind of scene in the lens, and makes a clear distinction between any object in the image and the background. So next time you take a photograph, just remember how quick the neural network must be, to do all this in a matter of milliseconds.You can learn all about object detection and computer vision algorithms in our Computer Vision using Deep Learning course! Its a comprehensive offering and an invaluable addition to your machine learning skillset.The Apple Watch Series 4 feels like a health monitoring device more than at any point since its debut four years back. Of course all the excitement is around the watchs design and how its 35% bigger than last years product. But lets step out of that limelight and look at one of the more intriguing features  new health sensors.The Watch comes with an electrocardiogram (ECG) sensor. Why is this important, you ask? Well for starters, its the first smartwatch to pack in this feature. But more importantly, the sensor measure not just your hearts rate, but also its rhythm. This helps monitor any irregular rhythm and the Watch immediately alerts you in case of any impending danger. These sensors have been approved by the FDA and the American Heart Association.Further, these the Series 4 watches are integrated with an improved accelerometer and gyroscope. This will help the sensors in detecting if the wearer has fallen over. Once a person has fallen over and shown no sign of movement for 60 seconds, the device sends out an emergency call to up to five (pre-defined) emergency contacts simultaneously.Im sure you must have guessed by now whats behind all these updates? Yes, its machine learning. Healthcare, as I mentioned in this article, is ripe for taking in machine learning terms. There are billions of data points at play, and combining ML with domain expertise is where the jackpot lies. Im glad to see companies like Apple utilizing it, albeit in their own products.The competition between the likes of Apple, Google, and others is heating up and artificial intelligence and machine learning could be the key to winning the battle. Hardware is critical here  as it gets significant upgrades each year, more and more complex algorithms can be built in.Fascinated by all this and looking for a way to get started with data science? Try out our Introduction to Data Science course today! We will help you take your first steps into this awesome new world.You couldnt have picked a better time to get into data science, honestly. A quick glance at Apples official job postings shows more than 400 openings for machine learning related positions. The question then remains whether there are enough experienced people to fulfill that demand.You can view the entire Apple event here.",https://www.analyticsvidhya.com/blog/2018/09/how-machine-learning-hardware-and-algorithms-power-apples-latest-watch-and-iphones/
A Gentle Introduction to Handling a Non-Stationary Time Series in Python,Learn everything about Analytics|Introduction|Table of contents|1. Introduction to Stationarity|2. Loading the Data|3. Methods to Check Stationarity|3. Types of Stationarity|4. Making a Time Series Stationary|End Notes,"Visual test|Statistical test|Differencing|Seasonal Differencing|Transformation|Share this:|Like this:|Related Articles|How Machine Learning Algorithms & Hardware Power Apples Latest Watch and iPhones|Google has Open Sourced the Amazing What-If Tool to Perform Code-Free Visual ML Experiments|
Aishwarya Singh
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",ADF (Augmented Dickey Fuller) Test|2 . KPSS (Kwiatkowski-Phillips-Schmidt-Shin) Test,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What do these applications have in common: predicting the electricity consumption of a household for the next three months, estimating traffic on roads at certain periods, and predicting the price at which a stock will trade on the New York Stock Exchange?They all fall under the concept of time series data! You cannot accurately predict any of these results without the time component. And as more and more data is generated in the world around us, time series forecasting keeps becoming an ever more critical technique for a data scientist to master.But time series is a complex topic with multiple facets at play simultaneously.For starters, making the time series stationary is critical if we want the forecasting model to work well. Why? Because most of the data you collect will have non-stationary trends. And if the spikes are erratic how can you be sure the model will work properly?The focus of this article is on the methods for checking stationarity in time series data. This article assumes that the reader is familiar with time series, ARIMA, and the concept of stationarity. Below are some references to brush up on the basics:Stationarity is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties  mean, variance and covariance, do not vary with time.Let us understand this using an intuitive example. Consider the three plots shown below:The three examples shown above represent non-stationary time series. Now look at a fourth plot:In this case, the mean, variance and covariance are constant with time. This is what a stationary time series looks like.Think about this for a second  predicting future values using which of the above plots would be easier? The fourth plot, right? Most statistical models require the series to be stationary to make effective and precise predictions.So to summarize, a stationary time series is the one for which the properties (namely mean, variance and covariance) do not depend on time. In the next section we will cover various methods to check if the given series is stationary or not.In this and the next few sections, I will be introducing methods to check the stationarity of time series data and the techniques required to deal with any non-stationary series. I have also provided the python code for applying each technique.You can download the dataset well be using from this link: AirPassengers.Before we go ahead and analyze our dataset, lets load and preprocessthe data first.Looks like we are good to go!The next step is to determine whether a given series is stationary or not and deal with it accordingly. This section looks at some common methods which we can use to perform this check.Consider the plots we used in the previous section. We were able to identify the series in which mean and variance were changing with time, simply by looking at each plot. Similarly, we can plot the data and determine if the properties of the series are changing with time or not.Although its very clear that we have a trend (varying mean) in the above series, this visual approach might not always give accurate results. It is better to confirm the observations using some statistical tests.Instead of going for the visual test, we can use statistical tests like the unit root stationary tests. Unit root indicates that the statistical properties of a given series are not constant with time, which is the condition for stationary time series. Here is the mathematics explanation of the same :Suppose we have a time series :yt = a*yt-1 +  twhere yt is the value at the time instant t and  t is the error term. In order to calculate yt we need the value of yt-1, which is :yt-1 = a*yt-2 +  t-1If we do that for all observations, the value of yt will come out to be:yt = an*yt-n + t-i*aiIf the value of a is 1 (unit) in the above equation, then the predictions will be equal to the yt-n and sum of all errors from t-n to t, which means that the variance will increase with time. This is knows as unit root in a time series. We know that for a stationary time series, the variance must not be a function of time. The unit root tests check the presence of unit root in the series by checking if value of a=1. Below are the two of the most commonly used unit root stationary tests:The Dickey Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test are:Null Hypothesis: The series has a unit root (value of a =1)Alternate Hypothesis: The series has no unit root.If we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary (we will understand more about difference stationary in the next section).Python code:Results of ADF test: The ADF tests gives the following results  test statistic, p value and the critical value at 1%, 5% , and 10% confidence intervals. The results of our test for this particular series are:
Test for stationarity: If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary).In our above example, the test statistic > critical value, which implies that the series is not stationary. This confirms our original observation which we initially saw in the visual test.KPSS is another test for checking the stationarity of a time series (slightly less popular than the Dickey Fuller test). The null and alternate hypothesis for the KPSS test are opposite that of the ADF test, which often creates confusion.The authors of the KPSS test have defined the null hypothesis as the process is trend stationary, to an alternate hypothesis of a unit root series. We will understand the trend stationarity in detail in the next section. For now, lets focus on the implementation and see the results of the KPSS test.Null Hypothesis: The process is trend stationary.Alternate Hypothesis: The series has a unit root (series is not stationary).Python code:Results of KPSS test: Following are the results of the KPSS test  Test statistic, p-value, and the critical value at 1%, 2.5%, 5%, and 10% confidence intervals. For the air passengers dataset, here are the results:Test for stationarity: If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, if fail to reject the null hypothesis (series is stationary). For the air passenger data, the value of the test statistic is greater than the critical value at all confidence intervals, and hence we can say that the series is not stationary.I usually perform both the statistical tests before I prepare a model for my time series data. It once happened that both the tests showed contradictory results. One of the tests showed that the series is stationary while the other showed that the series is not! I got stuck at this part for hours, trying to figure out how is this possible. As it turns out, there are more than one type of stationarity.So in summary, the ADF test has an alternate hypothesis of linear or difference stationary, while the KPSS test identifies trend-stationarity in a series.Let us understand the different types of stationarities and how to interpret the results of the above tests.Its always better to apply both the tests, so that we are sure that the series is truly stationary. Let us look at the possible outcomes of applying these stationary tests.Now that we are familiar with the concept of stationarity and its different types, we can finally move on to actually making our series stationary. Always keep in mind that in order to use time series forecasting models, it is necessary to convert any non-stationary series to a stationary series first.In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. Mathematically, differencing can be written as:yt = yt  y(t-1)where yt is the value at a time tApplying differencing on our series and plotting the results:In seasonal differencing, instead of calculating the difference between consecutive values, we calculate the difference between an observation and a previous observation from the same season. For example, an observation taken on a Monday will be subtracted from an observation taken on the previous Monday. Mathematically it can be written as:yt = yt  y(t-n)Transformations are used to stabilize the non-constant variance of a series. Common transformation methods include power transform, square root, and log transform. Lets do a quick log transform and differencing on our air passenger dataset:As you can see, this plot is a significant improvement over the previous plots. You can use square root or power transformation on the series and see if they come up with better results. Feel free to share your findings in the comments section below!In this article we covered different methods that can be used to check the stationarity of a time series. But the buck doesnt stop here. The next step is to apply a forecasting model on the series we obtained. You can refer to the following article to build such a model: Beginners Guide to Time Series Forecast.You can connect with me in the comments section below if you have any questions or feedback on this article.",https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/
Google has Open Sourced the Amazing What-If Tool to Perform Code-Free Visual ML Experiments,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Gentle Introduction to Handling a Non-Stationary Time Series in Python|Rosetta  How Facebook uses Machine Learning to Process Text in Billions of Images|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Building a machine learning model is not a one-step process. As a data scientist, you need to ask several questions during this process and perform multiple iterations before finalizing your model.How much should you tune a hyperparameter? How is the accuracy going to be affected if you drop a variable? How diverse is the dataset? These are just some of the questions you should be asking during the model building process. But finding answers to these questions is no walk in the park, either. You need to perform experiments for each idea, and that means line upon line of code. Doesnt sound like an efficient method, does it?Googles AI research team has designed a What-If Tool which is part of their open-source Tensorboard web application. The tool lets users analyze their machine learning models without the need of writing code. The UI is exemplary and offers a rich interactive interface for exploring different model results.Source: Google AI blogThe tool has plenty of features packed in, including:The code for What-If is open-source and available on GitHub.A few demo examples using pretrained models are available in the GitHub repository. These include detecting misclassification, analyzing a models performance across multiple subgroups, and assessing bias in binary classification models.Intrigued? We are as well. The below video takes a few examples to explain how you can use the tool on your own machine:I can see this becoming an integral part of most data science operations very soon (its already in full-fledged use at Google). One use case I can think of is ranking your models from worst to best performing as you tweak certain hyperparameters. The visual aspect certainly makes the case for using this tool even more appealing.Where do you plan to use this tool? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/09/google-launches-what-if-tool-perform-code-free-ml-experiments/
Rosetta  How Facebook uses Machine Learning to Process Text in Billions of Images,Learn everything about Analytics|Overview|Introduction||Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google has Open Sourced the Amazing What-If Tool to Perform Code-Free Visual ML Experiments|Artificial Intelligence, Machine Learning and Big Data  A Comprehensive Report|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Facebook processes a ludicrous number of images per day. Even with the recent controversies it has had to face, the number of people using the platform has not diminished a whole lot. And the uploading and sharing of photos continues unabated.Now Facebook faces a challenge every day. Quite a lot of these images have text in them (could be a meme, quote, street sign, menu, business card, etc.). How can the big tech giant make use of this text? How can they extract it and use it to improve the user experience?Given the sheer amount of images Facebook has to process, using a traditional optical character recognition (OCR) software wont cut it. The OCR might be able to recognize the characters, but it definitely wont understand the context.Step up Rosetta, Facebooks own large-scale machine learning system.Rosetta extracts text from more than a billion public Facebook and Instagram images (and even videos) on a daily basis. The text isnt just limited to English, Rosetta is able to recognize multiple languages in real time. This text data is then fed to a text recognition model that has been trained on classifiers with the singular aim of understanding the context of the text in each image.Text extraction is performed in two steps, independent of each other:The below image is a nice illustration of Rosettas architecture:I strongly recommend reading the entire blog post on Facebooks Code site. It is a marvellous explanation of how Rosetta works, and especially how the detection and recognition models were designed from scratch. Alternatively, you can watch the below video from KDD2018 which summarises the inner workings of Rosetta in under two and half minutes:Its always a pleasure to read Facebook and Googles AI research posts. Theres so much knowledge to be gained with each breakthrough or service they write about. Most of us in the data science domain must have wondered how a behemoth like Facebook uses machine learning in real-world cases (except their news feed, of course) and bit by bit, the curtains are drawn back.If youre a NLP enthusiast, the text detection using Faster R-CNN approach sounds pretty intriguing, doesnt it? Rosetta is already being heavily used by Facebook and Instagram.Theres a lot more work to be done since text comes in all forms and structures, and Facebooks research team is just getting started.",https://www.analyticsvidhya.com/blog/2018/09/facebook-rosetta-process-text-billions-images/
"Artificial Intelligence, Machine Learning and Big Data  A Comprehensive Report",Learn everything about Analytics|Introduction|What does the report cover?|Highlights from the Report,"Download it right now here!|Industry-wise Comparison|The Hottest Tools and Languages in the Industry|Whats in store by 2020?|Share this:|Like this:|Related Articles|Rosetta  How Facebook uses Machine Learning to Process Text in Billions of Images|Jupytext lets you use Jupyter Notebooks as Julia, Python and R Scripts or Markdown documents!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence and Machine Learning are the hottest jobs in the industry right now. 2018 has seen an even bigger leap in interest in these fields and it is expected to grow exponentially in the next five years! For instance, did you know that more than 50,000 positions related to Data and Analytics are currently vacant in India?We are excited to release a comprehensive report together with Great Learning on how AI, ML and Big Data are changing and evolving the world around us. Additionally, this report aims to provide an overview of the kind of career opportunities available in these fields right now, and the different roles we might see in the future.The aim behind creating this report is to provide our Data Science community with the context of changes happening at a macro level, and how they can best prepare for these upcoming changes. So, if you are already a Data Science professional or want to get into Data Science, we expect this report to be useful in providing you a context and preparing you for the future.  Kunal Jain, Founder and CEO, Analytics VidhyaWondering whats in the report and if you should download it? Check out what all is included below:There are a whole host of amazing statistics and insights in the report that will blow your mind. For example there are over 10 lakh registered companies in India. A survey by Gartner shows that around 75% of these companies are either already investing or are planning to invest in the field of Big Data.Lets go through some of the most intriguing patterns and insights from this report.An oft-asked question Ive seen is  which industries have the most job opportunities? Heres your answer:Data science isnt confined to one narrow field. Its reach spans across domains and applications. The banking and finance sector is clearly the biggest market for data science professionals. 44% of all jobs were created in this domain in 2017. Yes, 44%! E-commerce and healthcare have also emerged as promising areas for data science professionals.Python or R? Ah, that question again. But now we have a concrete answer! The below graph, using the job postings from Indeed.com, shows a neat analysis of the data science skills in demand these days:The most number of jobs listed contain SQL as a requirement. But Python has truly become a universally popular tool and is eating up that ground with incredible speed. These languages are followed by Java, Hadoop (especially for data engineers, a very necessary role), and R. SAS is a bit further behind Tableau round-up our top 10.This gives you a really good idea of what the industry demand is in todays market. It might be time to buckle up and upskill your existing skillset!Whats in store for all the aspiring data scientists, engineers, and analysts? The below chart depicts the expected number of jobs by 2020 in various industries:Data science jobs in healthcare are expected to soar. Agriculture, transportation and aviation are also expected to integrate a lot of data science tasks soon (the transformation is well under way). Cyber security, lagging a touch behind at the moment, should see significant investment. Its a field ripe for data science and we expect to see professionals moving in that direction in the next couple of years.There is a whole lot more in this report. We have an entire section dedicated to understanding the different roles in data science, their expected skills, etc. So what are you waiting for?Get your hands on the report right now!",https://www.analyticsvidhya.com/blog/2018/09/artificial-intelligence-machine-learning-and-big-data-a-comprehensive-report/
"Jupytext lets you use Jupyter Notebooks as Julia, Python and R Scripts or Markdown documents!",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Artificial Intelligence, Machine Learning and Big Data  A Comprehensive Report|Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Jupyter notebooks are powerful and interactive documents for writing iterative python codes, but quite often their size becomes bloated. I personally find editing and debugging scripts more convenient than going through each separate cell in the notebook.Jupytext has solved this conundrum for me. The tool lets you convert an existing Jupyter notebook to a script or a markdown document which can be used for editing (and sharing) the code file! For instance, if you wish to debug any python code in Jupyter using a script, you can simply save the notebook in the desirable format (.jl, .py or .R) and edit the code.In order to preserve the output while editing the script or markdown document, you can use paired notebooks. With paired notebooks, along with the script or markdown document, the notebook is saved as a .ipynb file. You can edit the text file outside the notebook and on reloading the notebook, inputs are taken from the text file. How cool is that?Check out the below demo presented by the team where they fix the order of plot legends in a python script (on the right window). The changes can be seen in the notebook (on the left side of the GIF).You can install Jupytext right now and get started with it by following the below steps:1. Install jupytext2. Configure Jupyter to use Jupytext:3. Restart jupyter for the changes to take effect:For more details on Jupytext, you can have a look at the officialGithub page.Im sure Im not the only one who finds editing codes in scripts more convenient. Jupytext is going to make proramming work easier and definitely save a ton of time. What I like most about Jupytext is undoubtedly the paired notebook. I cant believe it wasnt there before! It makes for really neat and useful watching as my script edits reflect in the notebook in real-time.Let me know if you try this out or if you have any questions on the tool.",https://www.analyticsvidhya.com/blog/2018/09/jupytext-lets-you-use-jupyter-notebooks-as-julia-python-and-r-scripts-or-markdown-documents/
Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Reading a video and extracting frames |How to handle video files in Python|Calculating the screen time  A simple solution|My learnings  what worked and what did not||Conclusion,"Step  1: Read the video, extract frames from it and save them as images|Step  2: Label a few images for training the model|Step 3: Building the model|Step  4: Make predictions for the remaining images|Step  5 Calculate the screen time of both TOM and JERRY|Share this:|Like this:|Related Articles|Jupytext lets you use Jupyter Notebooks as Julia, Python and R Scripts or Markdown documents!|MIT Open Sources Computer Vision Model that Teaches Itself Object Detection in 45 Minutes (with GitHub codes)|
Pulkit Sharma
|42 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When I started my deep learning journey, one of the first things I learned was image classification. Its such a fascinating part of the computer vision fraternity and I was completely immersed in it! But I have a curious mind and once I had a handle on image classification, I wondered if I could transfer that learning to videos.Was there a way to build a model that automatically identified specific people in a given video at a particular time interval? Turns out, there was and Im excited to share my approach with you!Source: Coastline AutomationNow to give you some context on the problem well be solving, keep in mind that screen time is extremely important for an actor. It is directly related to themoney he/she gets. Just to give you a sense of this commission, did you know that Robert Downey Jr. Downey picked up $10 million for just 15 minutes of screen time in Spider-Man Homecoming? Incredible.How cool would it be if we could take any video and calculate the screen time of any actor present in it?In this article, I will help you understand how to use deep learning on video data. To do this, we will be working with videos from the popular TOM and JERRY cartoon series. The aim is to calculate the screen time of both TOM and JERRY in any given video.Sounds interesting? Read on then!Note: This article assumes you have a prior knowledge of image classification using deep learning. If not, I recommend going through this article which will help you get a grasp of the basics of deep learning and image classification.Ever heard of a flip book? If you havent, youre missing out! Check out the one below:(Source: giphy.com)We have a different image on each page of the book, and as we flip these pages, we get an animation of a shark dancing. You could even call it a kind of video. The visualization gets better the faster we flip the pages. In other words, this visual is a collection of different images arranged in a particular order.Similarly, videos are nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem. There is just one extra step of extracting frames from the video.Remember, our challenge here is to calculate the screen time of both Tom and Jerry from a given video. Let me first summarize the steps we will follow in this article to crack this problem:Believe me, just following these steps will help you in solving many such video related problems in deep learning. Time to get our Python hats on now, and dig into this challenge.Let us start with importing all the necessary libraries. Go ahead and install the below libraries in case you havent already:Now we will load the video and convert it into frames. You can download the video used for this example from this link. We will first capture the video from the given directory using the VideoCapture() function, and then well extract frames from the video and save them as an image using the imwrite() function. Lets code it:Done!Once this process is complete, Done! will be printed on the screen as confirmation that the frames have been created.Let us try to visualize an image (frame). We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function.Getting excited, yet?This is the first frame from the video. We have extracted one frame for each second, from the entire duration of the video. Since the duration of the video is 4:58 minutes (298 seconds), we now have 298 images in total.Our task is to identify which image has TOM, and which image has JERRY. If our extracted images would have been similar to the ones present in the popular Imagenet dataset, this challenge could have been a breeze. How? We could simply have used models pre-trained on that Imagenet data and achieved a high accuracy score! But then wheres the fun in that?We have cartoon images so itll be very difficult (if not impossible) for any pre-trained model to identify TOM and JERRY in a given video.So how do we go about handling this? A possible solution is to manually give labels to a few of the images and train the model on them. Once the model has learned the patterns, we can use it to make predictions on a previously unseen set of images.Keep in mind that there could be frames when neither TOM nor JERRY are present. So, we will treat it as a multi-class classification problem. The classes which I have defined are:Dont worry, I have labelled all the images so you dont have to! Go ahead and download the mapping.csv file which contains each image name and their corresponding class (0 or 1 or 2).The mapping file contains two columns:Our next step is to read the images which we will do based on their names, aka, the Image_ID column.Tada! We now have the images with us. Remember, we need two things to train our model:Since there are three classes, we will one hot encode them using the to_categorical() function of keras.utils.We will be using a VGG16 pretrained model which takes an input image of shape (224 X 224 X 3). Since our images are in a different size, we need to reshape all of them. We will use the resize() function of skimage.transform to do this.All the images have been reshaped to 224 X 224 X 3. But before passing any input to the model, we must preprocess it as per the models requirement. Otherwise, the model will not perform well enough. Use the preprocess_input() function ofkeras.applications.vgg16 to perform this step.We also need a validation set to check the performance of the model on unseen images. We will make use of the train_test_split() function of the sklearn.model_selection module to randomly divide images into training and validation set.The next step is to build our model. As mentioned, we shall be using the VGG16 pretrained model for this task. Let us first import the required libraries to build the model:We will now load the VGG16 pretrained model and store it as base_model:We will make predictions using this model for X_train and X_valid, get the features, and then use those features to retrain the model.The shape of X_train and X_valid is (208, 7, 7, 512), (90, 7, 7, 512) respectively. In order to pass it to our neural network, we have to reshape it to 1-D.We will now preprocess the images and make them zero-centered which helps the model to converge faster.Finally, we will build our model. This step can be divided into 3 sub-steps:Lets check the summary of the model using the summary() function:We have a hidden layer with 1,024 neurons and an output layer with 3 neurons (since we have 3 classes to predict). Now we will compile our model:In the final step, we will fit the model and simultaneously also check its performance on the unseen images, i.e., validation images:We can see it is performing really well on the training as well as the validation images. We got an accuracy of around 85% on unseen images. And this is how we train a model on video data to get predictions for each frame.In the next section, we will try to calculate the screen time of TOM and JERRY in a new video.First, download the video well be using in this section from here. Once done, go ahead and load the video and extract frames from it. We will follow the same steps as we did above:Done!After extracting the frames from the new video, we will now load the test.csv file which contains the names of each extracted frame. Download the test.csv file and load it:Next, we will import the images for testing and then reshape them as per the requirements of the aforementioned pretrained model:We need to make changes to these images similar to the ones we did for the training images. We will preprocess the images, use thebase_model.predict() function to extract features from these images using the VGG16 pretrained model, reshape these images to 1-D form, and make them zero-centered:Since we have trained the model previously, we will make use of that model to make prediction for these images.Recall that Class 1 represents the presence of JERRY, while Class 2 represents the presence of TOM. We shall make use of the above predictions to calculate the screen time of both these legendary characters:And there you go! We have the total screen time of both TOM and JERRY in the given video.I tried and tested many things for this challenge  some worked exceedingly well, while some ended up flat. In this section, I will elaborate a bit on some of the difficulties I faced, and then how I tackled them. After that, I have provided the entire code for the final model which gave me the best accuracy.First, I tried using the pretrained model without removing the top layer. The results were not satisfactory. The possible reason could be that these are the cartoon images and our pretrained model was trained on actual images and hence it was not able to classify these cartoon images. To tackle this problem, i retrained the pretrain model using few labelled images and the results were better from the previous results.Even after training on the labelled images, the accuracy was not satisfactory. The model was not able to perform well on the training images itself. So, i tried to increase the number of layers. Increasing the number of layers proved to be a good solution to increase the training accuracy but there was no sync between training and validation accuracy. The model was overfitting and its performance on the unseen data was not satisfactory. So I added a Dropout layer after every Dense layer and then there was good sync between training and validation accuracy.I noticed that the classes are imbalanced. TOM had more screen time so the predictions were dominated by it and most of the frames were predicted as TOM. To overcome this and make the classes balanced, i used compute_class_weight() function of sklearn.utils.class_weight module. It assigned higher weights to the classes with lower value counts as compared to the classes with higher value counts.I also used Model Checkpointing to save the best model, i.e. the model which produced lowest validation loss and then used that model to make the final predictions. I will summarize all the above mentioned steps and will give the final code now. The actual classes for the testing images can be found in testing.csv file.Done!Done!We got an accuracy of around 88% on the validation data and 64% on the test data using this model.One possible reason for getting a low accuracy on test data could be a lack of training data. As the model does not have much knowledge of cartoon images like TOM and JERRY, we must feed it more images during the training process. My advice would be to extract more frames from different TOM and JERRY videos, label them accordingly, and use them for training the model. Once the model has seen a plethora of images of these two characters, theres a good chance it will lead to a better classification result.Such models can help us in various fields:These are just a few examples where this technique can be used. You can come up with many more such applications on your own! Feel free to share your thoughts and feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/
MIT Open Sources Computer Vision Model that Teaches Itself Object Detection in 45 Minutes (with GitHub codes),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes)|Hey BMW, play some music  A Brilliant Use Case of Machine Learning in Vehicles|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computer Vision and deep learning techniques have so far produced incredible results, like sensing people and estimating their pose through walls, flipping burgers, etc. But there have been two primary caveats with them:So MITs researchers decided to work on a more generalized and less data greedy approach for solving this challenge. Their system, called Dense Object Nets (or DON, a more catchy name) makes robots capable of inspecting, analyzing, and manipulating objects they have not seen previously. Can you guess which learning technique is behind this system? Its self-supervised learning!Before we go further and understand the technique, watch the below video to see a robot integrated with this model in action:DON has been trained to generate descriptions of objects, but not in a way you would initially think. It generates these descriptions in the form of coordinates. Instead of feeding the system tons of images of objects from different angles, the robot is left unsupervised in a room and it automatically locates, analyzes and trains itself to manipulate these objects inside an hour (45 minutes on average!). Note here that the system does rely on a RGB-D sensor to detect objects in a room.You can even get started with implementing this technique on your own! There is a PyTorch implementation available on GitHub which has enough documentation, and even a tutorial, to get you on your way.If youre interested in reading about the approach and technique in more detail, the researchers have published their study in the form of a research paper. They will be presenting their findings at the Robot Learning conference in Zurich next month.That research paper is a great read to start this week. Self-supervised learning is definitely garnering attention in recent months, with the most popular use case coming from Googles CV model that tracks objects in videos. Weve seen plenty of supervised and unsupervised learning, it might be time to accept self-supervised as a part of that classification.Ill certainly be trying out the PyTorch implementation this week and I encourage you to do the same. I look forward to our community participating in such studies and advancing research.",https://www.analyticsvidhya.com/blog/2018/09/mit-computer-vision-teaches-object-detection-45-minutes/
"Hey BMW, play some music  A Brilliant Use Case of Machine Learning in Vehicles",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|MIT Open Sources Computer Vision Model that Teaches Itself Object Detection in 45 Minutes (with GitHub codes)|An End-to-End Guide to Understand the Math behind XGBoost|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Hey BMW, map the route to downtown Manhattan and turn on the AC.Sure, its done.This isnt a conversation conjured up from a dream or even from a movie. This is a reality thanks to BMW announcing that theyll be integrating a voice-controlled personal assistant in their fleet of cars. Following on from the likes of Google Assistant, Amazons Alexa and Apples Siri, personal assistants in your car have now become real-world use cases.BMWs personal assistant lets the driver communicate with the car, just like we communicate with our phones.Using this advanced voice-recognition system, the driver will be able to control vehicle settings, navigation and the in-built entertainment system. Moreover, the BMW designer team claims that the smart assistant can learn the drivers setting preferences over time and will be able to make a series of adjustments automatically! A classic case of machine learning.The assistant responds to the prompt Hey BMW (although it can be customized). One can choose to be specific with the commands, or simply say Im too tired or Im too cold and the assistant will adjust the settings accordingly. Cool, isnt it?Theres more. Based on the time of the day, it will be able to predict travel routes, sync with appointment calendars to offer directions to the next meeting, and integrate with Microsoft Office 365 to read emails. The team also mentioned that the assistant will offer fuel-saving driving tips, deliver alerts (such as low tire pressure warnings or send service appointment reminders) and even schedule the service.The personal assistant is expected to roll out in March 2019 (in 23 languages) to cars that support BMW operating system 0.7. The below video gives us a glimpse of the BMWs personal assistant:BMW already has countless features that differentiate it from the rest of the world, and here is another addition to it. For a car nerd like me, this is quite an exciting development.What caught my eye was that the assistant will be able to learn the setting preferences.Its amazing how far machine learning has come in recent years, isnt it? Im curious to figure out the nitty-gritties of the technique but I can guess it will be similar to how Google uses NLP and speech recognition in its services.",https://www.analyticsvidhya.com/blog/2018/09/hey-bmw-play-some-music-a-brilliant-use-case-of-machine-learning-in-vehicles/
An End-to-End Guide to Understand the Math behind XGBoost,Learn everything about Analytics|Introduction|Table of Contents|The Power of XGBoost|Why ensemble learning?|Demonstrating the Potential of Boosting|Using gradient descent for optimizing the loss function|Unique features of XGBoost|Python Code for XGBoost|End Notes,"Bagging|Boosting|About the author|Share this:|Like this:|Related Articles|Hey BMW, play some music  A Brilliant Use Case of Machine Learning in Vehicles|Google Launches Search Engine for Finding Datasets on the Internet|
Guest Blog
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ever since its introduction in 2014, XGBoost has been lauded as the holy grail of machine learning hackathons and competitions. From predicting ad click-through rates to classifying high energy physics events, XGBoost has proved its mettle in terms of performance  and speed.I always turn to XGBoost as my first algorithm of choice in any ML hackathon. The accuracy it consistently gives, and the time it saves, demonstrates how useful it is. But how does it actually work? What kind of mathematics power XGBoost? Well figure out the answers to these questions soon.Tianqi Chen, one of the co-creators of XGBoost, announced (in 2016) that the innovative system features and algorithmic optimizations in XGBoost have rendered it 10 times faster than most sought after machine learning solutions. A truly amazing technique!In this article, we will first look at the power of XGBoost, and then deep dive into the inner workings of this popular and powerful technique. Its good to be able to implement it in Python or R, but understanding the nitty-gritties of the algorithm will help you become a better data scientist.Note: We recommend going through the below article as well to fully understand the various terms and concepts mentioned in this article:The beauty of this powerful algorithm lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage.Its no wonder then that CERN recognized it as the best approach to classify signals from the Large Hadron Collider. This particular challenge posed by CERN required a solution that would be scalable to process data being generated at the rate of 3 petabytes per year and effectively distinguish an extremely rare signal from background noises in a complex physical process. XGBoost emerged as the most useful, straightforward and robust solution.Now, lets deep dive into the inner workings of XGBoost.XGBoost is an ensemble learning method. Sometimes, it may not be sufficient to rely upon the results of just one machine learning model. Ensemble learning offers a systematic solution to combine the predictive power of multiple learners. The resultant is a single model which gives the aggregated output from several models.The models that form the ensemble,also known as base learners, could be either from the same learning algorithm or different learning algorithms. Bagging and boosting are two widely used ensemble learners. Though these two techniques can be used with several statistical models, the most predominant usage has been with decision trees.Lets briefly discuss bagging before taking a more detailed look at the concept of boosting.While decision trees are one of the most easily interpretable models, they exhibit highly variable behavior. Consider a single training dataset that we randomly split into two parts. Now, lets use each part to train a decision tree in order to obtain two models.When we fit both these models, they would yield different results. Decision trees are said to be associated with high variance due to this behavior. Bagging or boosting aggregation helps to reduce the variance in any learner. Several decision trees which are generated in parallel, form the base learners of bagging technique. Data sampled with replacement is fed to these learners for training. The final prediction is the averaged output from all the learners.In boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. Each tree learns from its predecessors and updates the residual errors. Hence, the tree that grows next in the sequence will learn from an updated version of the residuals.The base learners in boosting are weak learners in which the bias is high, and the predictive power is just a tad better than random guessing. Each of these weak learners contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners. The final strong learner brings down both the bias and the variance.In contrast to bagging techniques like Random Forest, in which trees are grown to their maximum extent, boosting makes use of trees with fewer splits. Such small trees, which are not very deep, are highly interpretable. Parameters like the number of trees or iterations, the rate at which the gradient boosting learns, and the depth of the tree, could be optimally selected through validation techniques like k-fold cross validation. Having a large number of trees might lead to overfitting. So, it is necessary to carefully choose the stopping criteria for boosting.Boosting consists of three simple steps:To improve the performance of F1, we could model after the residuals of F1 and create a new model F2:This can be done for m iterations, until residuals have been minimized as much as possible:Here, the additive learners do not disturb the functions created in the previous steps. Instead, they impart information of their own to bring down the errors.Consider the following data where the years of experience is predictor variable and salary (in thousand dollars) is the target. Using regression trees as base learners, we can create a model to predict the salary. For the sake of simplicity, we can choose square loss as our loss function and our objective would be to minimize the square error.As the first step, the model should be initialized with a function F0(x). F0(x) should be a function which minimizes the loss function or MSE (mean squared error), in this case:Taking the first differential of the above equation with respect to , it is seen that the function minimizes at the mean i=1nyin. So, the boosting model could be initiated with:F0(x) gives the predictions from the first stage of our model. Now, the residual error for each instance is (yi  F0(x)).We can use the residuals from F0(x) to create h1(x). h1(x) will be a regression tree which will try and reduce the residuals from the previous step. The output of h1(x) wont be a prediction of y; instead, it will help in predicting the successive function F1(x) which will bring down the residuals.The additive model h1(x) computes the mean of the residuals (y  F0) at each leaf of the tree. The boosted function F1(x) is obtained by summing F0(x) and h1(x). This way h1(x) learns from the residuals of F0(x) and suppresses it in F1(x).This can be repeated for 2 more iterations to compute h2(x) and h3(x). Each of these additive learners, hm(x), will make use of the residuals from the preceding function, Fm-1(x).The MSEs for F0(x), F1(x) and F2(x) are 875, 692 and 540. Its amazing how these simple weak learners can bring about a huge reduction in error!Note that each learner, hm(x), is trained on the residuals. All the additive learners in boosting are modeled after the residual errors at each step. Intuitively, it could be observed that the boosting learners make use of the patterns in residual errors. At the stage where maximum accuracy is reached by boosting, the residuals appear to be randomly distributed without any pattern.Plots of Fnand hnIn the case discussed above, MSE was the loss function. The mean minimized the error here. When MAE (mean absolute error) is the loss function, the median would be used as F0(x) to initialize the model. A unit change in y would cause a unit change in MAE as well.For MSE, the change observed would be roughly exponential. Instead of fitting hm(x) on the residuals, fitting it on the gradient of loss function, or the step along which loss occurs, would make this process generic and applicable across all loss functions. Gradient descent helps us minimize any differentiable function. Earlier, the regression tree for hm(x) predicted the mean residual at each terminal node of the tree. In gradient boosting, the average gradient component would be computed.For each node, there is a factor  with which hm(x) is multiplied. This accounts for the difference in impact of each branch of the split. Gradient boosting helps in predicting the optimal gradient for the additive model, unlike classical gradient descent techniques which reduce error in the output at each iteration.The following steps are involved in gradient boosting:XGBoost is a popular implementation of gradient boosting. Lets discuss some features of XGBoost that make it so interesting.Heres a live coding window to see how XGBoost works and play around with the code without leaving this article!So that was all about the mathematics that power the popular XGBoost algorithm. If your basics are solid, this article must have been a breeze for you. Its such a powerful algorithm and while there are other techniques that have spawned from it (like CATBoost), XGBoost remains a game changer in the machine learning community.If you have any feedback on the article, or questions on any of the above concepts, connect with me in the comments section below.Ramya Bhaskar Sundaram  Data Scientist, Noah DataIts safe to say my forte is advanced analytics. The charm and magnificence of statistics have enticed me, all through my journey as a Data Scientist. There is a definite beauty in how the simplest of statistical techniques can bring out the most intriguing insights from data. My fascination for statistics has helped me to continuously learn and expand my skill set in the domain.My experience spans across multiple verticals: Renewable Energy, Semiconductor, Financial Technology, Educational Technology, E-Commerce Aggregator, Digital Marketing, CRM, Fabricated Metal Manufacturing, Human Resources.",https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/
Google Launches Search Engine for Finding Datasets on the Internet,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An End-to-End Guide to Understand the Math behind XGBoost|Classifying and Decoding Historical Texts and Images using CNNs!|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Weve all been there  you have a great project idea but cant find the right dataset to get started. A comprehensive Google search lands you on page 20 and you feel like giving up. If its not on page 20 of Googles search, where then will it be?Well, Google has figured out a solution to this as well. The tech giant has launched Dataset Search, a search engine for finding open datasets stored anywhere on the internet. Dataset Search enables users to find datasets stored across thousands of repositories on the Web, making these datasets universally accessible and useful.You can try it out for yourself here.Dataset Search is available for multiple languages, with even more languages coming soon.You can find more details in Googles official blog post here. The search engine is still in beta mode so you might find a glitch here and there (but theres a good chance you wont).You can also view the below short video that quickly demonstrates this search engine in action:How awesome is this? Pretty much everyone in the machine learning sphere has struggled to find the right data at one point or another. And here we are  the perfect solution from everyones favourite search engine. I wont go so far as to say this is a one-stop shop for datasets..but really how long before it truly becomes that?I tried searching for a few datasets and it gave me a pretty useful results. Go ahead and try it out.",https://www.analyticsvidhya.com/blog/2018/09/dataset-search-google-search-engine-finding-datasets/
Classifying and Decoding Historical Texts and Images using CNNs!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Launches Search Engine for Finding Datasets on the Internet|MITs Neural Network uses Text & Audio to Perform Sentiment Analysis for Healthcare|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Historical texts and images hold a special fascination for me. Ancient writings and codes have a mysterious aura about them, and as a data scientist thats something Im automatically drawn to. Could machine learning be the answer? Could our algorithms really decode texts written as back as thousands of years?A team of researchers from the National Technical University of Ukraine and Huizhou Universitys School of Information Science and Technology have designed an algorithm with the aim of detecting, isolating and classifying ancient Graffiti. Sounds really interesting, right? Stay with me, because were about to take a dive into the technique theyve used!Current techniques used for handwriting recognition give a very high accuracy on the hand written texts, but did not show a similar performance for graffiti images. According to the team, one of the reasons for this could be the difference in quality of text written with on a paper and text written on a stone. The quality of stone carved handwriting is comparatively poor, which is understandable gives were talking about centuries ago!The first step was to preprocess the data and then train a model on this dataset. Broadly two datasets were used  CGCl and notMNIST. The CGCL dataset consists of carved Glagolitic and Cyrillic letters (CGCL) of graffiti from the St. Sophia Cathedral of Kyiv. These images were assembled and preprocessed to provide glyphs for recognition and prediction.The dataset consists of 4000 images (34 types of letters  classes). Another dataset , notMNIST, was used to compare the results obtained with GCGL. The notMNISt dataset includes publicly available fonts from 10 classes.A multinomial logistic regression model was applied to a subset of 10 classes. The AUC-ROC for individual letters was approximately 0.92 for notMNIST and 0.60 for CGCL (refer to the image below). The averaged AUC values were 0.99 and 0.82 for notMNIST and CGCL respectively. A 2D-CNN showed an accuracy of 0.94 on CGCL and 0.91 on notMNIST.These details are mentioned in much more depth in the paper published by the team on arxiv.org  Open Source Dataset and Machine Learning Techniques for Automatic Recognition of Historical Graffiti.4000 images is a relatively small subset of images so expectations from this algorithm are still tempered. Its a good start but ancient hieroglyphics had complex codes and texts, not something that one single algorithm will be able to crack open in a matter of weeks.Having said that, this study still shows that the potential is there. Its a good start, and while interpretability remains a question, I am looking forward to getting my hands on the dataset and performing some cool exploratory analysis.",https://www.analyticsvidhya.com/blog/2018/09/classify-decode-machine-learning-graffiti-recognition/
MITs Neural Network uses Text & Audio to Perform Sentiment Analysis for Healthcare,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Classifying and Decoding Historical Texts and Images using CNNs!|Baidu Releases EZDL, an Automated Machine Learning Tool for Non-Programmers|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Depression is an overwhelmingly common illness in society these days. It has been exacerbated in recent years with the advent of social media and is especially prevalent in young people. Doctors typically ask a set of pre-defined questions to the patient, based on which they diagnose depression.And now MIT researchers have designed a neural network model that doesnt need all these questions to detect depression in a person. Instead, the model focuses on an individuals way of speaking and his/her writing style.Think about it for a second  how do we tell if a person is depressed? We typically analyze their way of talking, whether they sound low, etc. And thats essentially what this neural network does. It does away with any of these constraints on the data, making it free from context.The researchers have based their model on a technique called sequence modelling. They obtained samples of audio recordings and text and trained the neural network on those. These samples were from both depressed and non-depressed people. According to the lead researcher, Tuka Alhanai, the dataset contained 142 interactions from the Distress Analysis Interview Corpus (DAIC).The job of the neural network model was to analyze sequences of words (or the speaking style) and then predict whether the individual was depressed or not. The model gave results of 71% precision and 83% recall when tested on a held-out dataset from DAIC itself.It might not surprise you to know that the model had a much tougher time detecting depression from the audio recordings as compared to the writing style. The model required an average of seven sequences to precisely diagnose depression, while this number jumped to 30 for audio recordings.Ive recently been reading about sequence modeling and can attest to its usefulness. Its potential (at least in my opinion) lies in the audio/speech processing space and this study further reinforces my thoughts. A great place to start learning about this technique is here  A Must-Read Introduction to Sequence Modeling.The results may not jump out at you, but theyre still better than most research in this area. The models ability to analyze any kind of conversation is significant, rather than working with only a specific set of questions (as has been the case with other models).",https://www.analyticsvidhya.com/blog/2018/09/mit-model-text-and-audio-healthcare/
"Baidu Releases EZDL, an Automated Machine Learning Tool for Non-Programmers",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|MITs Neural Network uses Text & Audio to Perform Sentiment Analysis for Healthcare|The 5 Best Machine Learning GitHub Repositories & Reddit Threads from August 2018|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Automated machine learning has carved a massive space for itself in the industry and it shows no signs of slowing down. Led by Google Clouds AutoML platform, most of the big tech companies have launched their own tools to spice up the competition. AutoML can no longer be considered a fad, its extremely useful and its here to stay.The latest release comes from Chinese giant Baidu, who have previously made the news for breakthroughs in deep learning. From open sourcing the worlds largest self-driving dataset to building a Deep Voice AI system that can clone your voice within seconds, they are well respected in this domain.And now theyve announced their entry into the autoML space with the launch of EZDL. EZDL is a simple drag-and-drop platform that allows users to design and build custom machine learning models. Even if you dont have any programming background (and no time to learn it either), this tool will feel like a godsend. Their aim is to help those companies which are struggling to scale up their AI operations and dont have access to a ton of data (which, frankly, is most companies these days).Currently EZDL offers the option of building two kinds of custom models:One of the best features of EZDL is that it takes just four steps to build and deploy a model:The company claims that it takes less than 15 minutes on average to train models and that more than 2/3rds of the models give an accuracy score of more than 90%. Quite a bold claim, and one that will certainly be tested as the user base grows.The below video is a nice concise summary of what EZDL does (the audio is in Chinese with English subtitles):The offerings of EZDL are still fairly limited but Im impressed with whats on show so far. I tried out the Object Detection service and it went like a breeze. Granted the set of images in my data was small, but it clearly shows EZDL has potential and Baidu will most certainly be hard at work to expand and improve the platform.AutoML tools are a boon for all non-programmers and the competition in the market makes it easy to try and choose what suits you and your business. What data scientists think of this tool is another matter, as it takes away the mathematical aspect of the domain completely.",https://www.analyticsvidhya.com/blog/2018/09/baidu-ezdl-automated-machine-learning-non-programmers/
The 5 Best Machine Learning GitHub Repositories & Reddit Threads from August 2018,Learn everything about Analytics|Introduction|GitHub Repositories|NVIDIAs vid2vid Technique|Dopamine by Google|Automating Object Detection|Human Pose Estimation|Chorrrds|Reddit Discussions|OpenAI Five Lose their First Professional Dota Game|A Different Perspective on using Notebooks for Machine Learning Tasks|TensorFlow 2.0 is coming|Review of Julia for Machine Learning|Data Leakage Stories in Real-World ML Projects|End Notes,"Share this:|Like this:|Related Articles|Baidu Releases EZDL, an Automated Machine Learning Tool for Non-Programmers|DataHack Radio Episode #9: Data Science at Airbnb & Lyft with Dr. Alok Gupta|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When I started using GitHub early last year, I had never imagined how useful it would become for me. Initially I only used it to upload my own code, assuming that was the extent to which GitHub would prove its usefulness. But as I joined Analytics Vidhya and my scope of research expanded, I was enthralled by how vast this platform really is.Apart from allowing me access to open source codes and projects from top companies like Google, Microsoft, NVIDIA, Facebook, etc., it opened up avenues to collaborate on existing projects with fellow machine learning enthusiasts. I cannot tell you how amazing it feels to have contributed to a project that other people use. Its a feeling like no other. And this, of course, led me to write this monthly series which I hope you have found beneficial in your own line of work.This months article contains some pretty sweet repositories. Theres a project from NVIDIA which looks at video-to-video translations, a neat Google repository that makes reinforcement learning way easier to learn than ever before, and Ive also included a useful automated object detection library. Theres a ton of more information below, including an entertaining R package.In our Reddit section, we have diverse discussions ranging from multiple expert reviews of Julia to real-life data leakage stories. As a data scientist, you need to be on top of your game at all times, and that includes being updated with all the latest developments. Reddit, and AVBytes, should definitely be on your go-to list.You can check out the top GitHub repositories and top Reddit discussions (from April onwards) we have covered each month below:There has been tremendous progress in the image-to-image translation field. However the video processing field has rarely seen many breakthroughs in recent times. Until now.NVIDIA, already leading the way in using deep learning for image and video processing, has open sourced a technique that does video-to-video translation, with mind-blowing results. They have open sourced their code on GitHub so you can get started with using this technique NOW. The code is a PyTorch implementation of vid2vid and you can use it for:Check out our coverage of this repository here.If youve worked or researched in the field of reinforcement learning, you will have an idea of how difficult (if not impossible) it is to reproduce existing approaches. Dopemine is a TensorFlow framework that has been created and open sourced with the hope of accelerating progress in this field and making it more flexible and reproducible.If youve been wanting to learn reinforcement learning but were scared by how complex it is, this repository comes as a golden opportunity. Available in just 15 Python files, the code comes with detailed documentation and a free dataset!You can additionally read up on this repository here.Object detection is thriving in the deep learning community, but it can be a daunting challenge for newcomers. How many pixels and frames to map? How to increase the accuracy of a very basic model? Where do you even begin? You dont need to fret too much about this anymore  thanks to MITs algorithm that automates object detection with stunning precision.Their approach is called Semantic Soft Segmentation (SSS). What takes an expert, say 10 minutes to manually edit, you can now do in a matter of seconds! The above image is a nice illustration of how this algorithm works, and how itll look when you implement it on your machine.View our coverage of this technique in more detail here.Pose estimation is seeing a ton of interest from researchers this year and publications like MIT have published studies marking progress in this field. From helping elderly people receive the right treatment to commercial applications like making a human virtually dance, pose estimation is poised to become the next best thing commercially.This repository is Microsofts official PyTorch implementation of their popular paper Simple Baselines for Human Pose Estimation and Tracking. They have offered baseline models and benchmarks that are good enough to hopefully inspire new ideas in this line of research.This one is for all the R users out there. We usually download R packages from CRAN so I personally havent felt the need to go to GitHub, but this package is one that I found very interesting. chorrrds helps you extract, analyze, and organize music chords. It even comes pre-loaded with several music datasets.You can actually directly install it from CRAN, or use the devtools package to download it from GitHub. Find out more about how to do this, and more details, in this article.In case you havent been following OpenAI in the last couple of months, their team has been hard at work trying to hype up their latest innovation  OpenAI Five. Its a team of five neural network working together to become better at playing Dota. And these neural networks were doing extremely well, until they ran into the first professional Dota playing team.This Reddit thread looks at the teams defeat from all angles, and the machine learning perspective really stands out. Even if you havent read their research paper, this thread has enough information to get you up to speed in a jiffy. There are well over 100 comments on this topic, a truly knowledge-rich discussion.Most of us in the data science and machine learning space have used Notebooks for various tasks, like data cleaning, model building, etc. Im actually yet to meet someone who hasnt used Notebooks at some point in their data science journey. We dont usually question the limitations of these notebooks, do we?Now heres an interesting take on why Notebooks arent actually as useful as we think. Make sure you scroll through the entire discussion, there are some curious as well as insightful comments from fellow data scientists. And as a bonus, you can also check out the really well made presentation deck.TensorFlow 2.0 was teased a couple of weeks ago by Google and is expected to be launched in the next few months. This thread is equal parts funny and serious. TensorFlow users from around the world have given their take on what they are expecting, and what they want to see added. Quite a lot of comments are around the usefulness of Eager Execution.This has been a long awaited update so big things are being expected. Will Google deliver?The Julia programming language has been doing the rounds on social media lately after a few articles were written on how it might replace Python in the future. Ive had requests to review the language and have directed everyone to this thread. What better place to check out the pros and cons of a programming language than a hardcore ML Reddit thread?Rather than reading one perspective, you get access to multiple reviews, each adding a unique point of view. What I liked about this discussion was that plenty of existing Julia users have added their two cents. The consensus seems to be that it is showing a lot of promise (especially the latest release, Julia 1.0), but it has a while to go before it catches up with Python.We are all caught up in trying to solve real-world problems that we tend to forget issues that might crop up in existing projects. You might be surprised at the kind of stories people have told here  including one where they had duplicate entries for one row, and that was making the model overfit the training data massively. There are some useful links as well for further reading on the kind of data leakage problems that have come up in the industry.Have you ever been a victim of data leakage? Share your story in this Reddit thread and participate in the discussion!I thoroughly enjoy putting together this article every month as I trawl through hundreds of libraries and tens of Reddit discussion threads to bring the best of them to you. In this process I get to learn and try out tons of new techniques and tools.Enjoy this months article and I hope you experiment with a few repositories mentioned above! In case you feel there are any other libraries or Reddit threads that the community should know about, let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/09/best-machine-learning-github-repositories-reddit-threads-august-2018/
DataHack Radio Episode #9: Data Science at Airbnb & Lyft with Dr. Alok Gupta,Learn everything about Analytics|Introduction|Dr. Alok Guptas Background|Data Scientist vs. Quant Trader Similarities and Differences|Data Science at Airbnb|Tools and Techniques used at Airbnb|Data Science at Lyft|End Notes,"Share this:|Related Articles|The 5 Best Machine Learning GitHub Repositories & Reddit Threads from August 2018|Sketch2Code Transforms Handwritten Notes into Working Code within Seconds  and its Open Source!|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Airbnb and Lyft have transformed their respective industries in recent years using data science as their guiding light.In episode 9 of our DataHack Radio series, Dr. Alok Gupta gave us some very interesting insights into how Airbnb and Lyft use data science.For instance, did you know that Spark is Airbnbs machine learning tool of choice?Dr. Alok is currently working as the Director of Data Science and Head of Growth Science at Lyft. He has a deep passion for mathematics and has used that throughout his career, including his four year stint at Airbnb. You will learn a lot in this podcast about how a data science leader thinks about challenging problems, and how leading tech start-ups scale up their operations from the ground up.This article summarizes the key points Dr. Alok discussed during this podcast. This is another valuable addition to the DataHack Radio podcast series, and I highly recommend listening to it as soon as possible!Subscribe to DataHack Radio NOW and listen to this, as well as all previous episodes, on any of the below platforms:Dr. Alok completed his undergraduate in mathematics from Cambridge University and proceeded to do his Masters in Finance and Mathematics from Imperial College, London. During his time there, he developed an interest in stochastic finance and statistics and decided to pursue this as his Ph.D at Oxford University, which he successfully completed in 2010.During his Ph.D years, the infamous recession struck and created chaos in the industry so he wasnt sure which industry to apply to. He ended up in financial trading at Deutsche Bank where he had an opportunity to design and build algorithms with profit and loss objectives.As part of his role at Deutsche Bank, he moved from London to New York, where he worked for around a year and a half. He discovered the role of a data scientist while in New York, and realized the similarities between that, and his own role as a Quant Trader in finance. This led to him applying at a number of companies and he finally got his break in 2014 at Airbnb as a data scientist and the rest, as he said, is history.The overlaps between a data scientist and a quant trader were plenty, including understanding the problem and framing it in a way that made business sense. There were other intersections, like opportunity sizing, detective analysis, impact estimation, etc. Of course one of the most interesting commonalities was actually solving the problem  deciding which mathematical, or statistical, techniques do we need to apply, what is the objective function, how do we get to the optimal solution, among others.But there were a couple of crucial differences between these two roles as well, as Alok discovered in his initial days at Airbnb. The metric that youre trying to optimize in finance is taken as given (for example, trying to optimize PnL is a concrete objective). Whereas in the technology space, this was vague and needed to be understood at a far more granular level before performing any data science task.Experimentation is another tricky and challenging aspect in technology (there are a number of assignment units, different methodologies for measurements, etc.), whereas in finance you run an algorithm, see how much money it makes, and youre done!When Alok joined Airbnb in 2014, the entire company was some 1,000 employees strong, with the data science team consisting of just 10 people (when he left earlier this year, the team had grown to around 110!). He started as the Data Scientist on their Risk and Safety fraud prediction team, where he built models for both online and offline fraud detection.One year into his role, Alok started to build his own data science team in the Customer Support optimization space. Airbnb has some 10,000 customer support employees globally that use channels like phone, chat, email, SMS, etc. to help their customers resolve issues. This, as you can see, was a challenge ripe for machine learning. Alok has explained how his team took this as an optimization problem in the podcast and the different features they considered for the final model. A very fascinating section, this.In his last 2 years at Airbnb he switched focus completely to work on acquisition of new guests. This included sourcing different marketing channels, working on search engine optimization, recommendation systems, etc.Alok has described the acquisition process in a lot of depth which will benefit anyone who works in data science, regardless of the industry. The way he and his team approached the problem and worked their way through it can serve as a roadmap for all aspiring data scientists.Most of the data scientists at Airbnb use tools and services like Amazon Web Services (AWS), HIVE, etc. to pull or extract the data they needed. Python and R are used to perform local analysis and Alok saw an increasing number of data scientists moving to Python as its easier to productionize Python scripts.For building models and solutions when confronted with large datasets, Airbnbs machine learning tool of choice was Spark.Airbnb has also invested in building its own centralized machine learning platform that can enable non-data scientists and non-engineers to spin up their own ML models without needing to have a lot of programming experience.Alok led the way in pioneering a knowledge sharing tool within the organization which was shared between data scientists and non-data scientists. The idea behind it was to get everyone on the same page regarding the happenings internally, and it was almost always written in Python or R as a Markdown document. This also helped them get peer reviews on any technical stuff and the quality of analysis was raised to unprecedented levels.At Lyft, all the folks working in the analytics and data science domain are grouped under the umbrella of scientists. Acquisition, engagement, and retention (of passengers and drivers) are some of the problems they are currently working on simultaneously. In his role as the head of Growth Science, Alok has been exposed to the supply side of things, a new and exciting challenge for him.The data science team under Alok currently consists of 40 people (at the time of recording this podcast). Quiet a few challenges he is facing in his current role, which he started just four months ago, he has already seen at Airbnb, so he feels at home in that respect.The details covered in this podcast about Airbnbs data science operations is eemplary and exhaustive. I found out far more about how a leading tech start-up operates, thinks, the structures involved, etc. than I had initially anticipated. Anyone involved in data science will benefit from listening to Dr. Alok.If you have any suggestions for us on who you would like to see as a guest in the future, or any feedback on the nine episodes we have released so far, use the comments section below and let us know!",https://www.analyticsvidhya.com/blog/2018/09/datahack-radio-lyft-dr-alok-gupta/
Sketch2Code Transforms Handwritten Notes into Working Code within Seconds  and its Open Source!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #9: Data Science at Airbnb & Lyft with Dr. Alok Gupta|A Neural Network Algorithm Analyzes Eye Blinking to Detect Fake Videos (with 95% accuracy!)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How many times have you brainstormed and come up with solutions on whiteboards, only to then write the entire code to make it workable on your machine? Ive done this hundreds of times, and it eventually wears on you. How useful would it be if you could simply copy the whiteboard design into your machine without needing to code through it?This isnt magic or the stuff of dreams anymore, thanks to the combination of machine learning and Microsoft. Sketch2Code is a web based solution that uses machine learning to transform your handwritten notes from an image to a working HTML markup code. And its open source!The below image, taken from Microsofts official blog post announcing this offering, shows the application workflow:As someone who is interested in data science, youll be wondering how machine learning works in this particular application. Wonder no more, because below Ive listed the elements that went into designing Sketch2Code:You can access all the code and documentation for Sketch2Code on GitHub. This repository contains very detailed steps on each element we saw above, and is more than enough to (at the very least) get your feet wet.Amazing! I tried this out myself and can attest to how useful this is. Note that this release is still a bit raw and might not give you the perfect result every time, but the concept itself deserves praise. This will save money and time for any business looking for quick web design solutions.Computer vision just continues to amaze me. Were living in a glorious age of machine learning and we should take a moment to appreciate that fact. And when youre done, go ahead and clone the Sketch2Code repository and get started with it!",https://www.analyticsvidhya.com/blog/2018/08/sketch2code-ml-transforms-notes-working-html-code/
A Neural Network Algorithm Analyzes Eye Blinking to Detect Fake Videos (with 95% accuracy!),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Sketch2Code Transforms Handwritten Notes into Working Code within Seconds  and its Open Source!|Build High Performance Time Series Models using Auto ARIMA in Python and R|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Manipulating facial expressions and body movements in videos has become so advanced that most people struggle to tell the difference between fake and real. A fake video of Barack Obama went viral last year where you see the former President addressing the camera. If you turn off the sound, you will not even realize its a fake video!Neural networks have become the go-to technique for generating results, but their black box nature has created certain problems we dont have a solution to yet. With elections soon to happen, how do we get a handle on this? The previous US elections were mired in fake news scandals, and it is only expected to get worse next time out.Thankfully, a lot of work is being done to detect fake videos, thanks to machine learning. I have previously covered XceptionNet, an algorithm that detects face swaps. And now another unique technique has been pioneered that can detect a fake video by analyzing the subjects eye blinking pattern in the video. The above image illustrates this point clearly  the top images are real, and the bottom ones are fake and have been generated using neural networks.According to the researchers, adults blink between every 2-10 seconds. This, surprisingly, is not taken into account in most fake videos. And now his flaw has been detected and applied in this latest algorithm.One neural network scans each and every frame in a given video, is trained to detect all the faces in it, and then zooms in on each persons eyes. Another neural network then identifies if the eyes are open or closed, using the movement, appearance and other features of human eyes. This algorithm has been trained on GBs of images of both open and closed eyes and is so far performing with a very impressive 95% accuracy.You can read about it in more detail here and also go through the fullresearch paper, which includes a step-by-step explanation of this technique.The research paper made for quite an interesting read. Fake videos are a pervasive problem and will continue to be so for a long time. With each solution, the adversaries come up with an even better neural network that circumvents any previous detection algorithm.For example, fake videos can include blinking by training their algorithms on images with both closed and open eyes. This is akin to a game of cat and mouse, and solutions will constantly need to be found at each turn. For now, this algorithm seems to be working but how long? Thats a question we are afraid to find the answer to.",https://www.analyticsvidhya.com/blog/2018/08/algorithm-analyzes-eye-blinking-detect-fake-video/
Build High Performance Time Series Models using Auto ARIMA in Python and R,Learn everything about Analytics|Introduction|Table of content|1. What is a time series ?|2. Methods for time series forecasting|3. Introduction to ARIMA||4. Steps for ARIMA implementation|5. Why do we need Auto ARIMA?|6. Implementation in Python and R||7. How does Auto Arima select the best parameters|8. End Notes and Further Reads,"Share this:|Like this:|Related Articles|A Neural Network Algorithm Analyzes Eye Blinking to Detect Fake Videos (with 95% accuracy!)|A Simple Introduction to Facial Recognition (with Python codes)|
Aishwarya Singh
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picture this  Youve been tasked with forecasting the price of the next iPhone and have been provided with historical data. This includes features like quarterly sales, month-on-month expenditure, and a whole host of things that come with Apples balance sheet. As a data scientist, which kind of problem would you classify this as? Time series modeling, of course.From predicting the sales of a product to estimating the electricity usage of households, time series forecasting is one of the core skills any data scientist is expected to know, if not master. There are a plethora of different techniques out there which you can use, and we will be covering one of the most effective ones, called Auto ARIMA, in this article.We will first understand the concept of ARIMA which will lead us to our main topic  Auto ARIMA. To solidify our concepts, we will take up a dataset and implement it in both Python and R.If you are familiar with time series and its techniques (like moving average, exponential smoothing, and ARIMA), you can skip directly to section 4. For beginners, start from the below section which is a brief introduction to time series and various forecasting techniques.Before we learn about the techniques to work on time series data, we must first understand what a time series actually is and how is it different from any other kind of data. Here is the formal definition of time series  It is a series of data points measured at consistent time intervals. This simply means that particular values are recorded at a constant interval which may be hourly, daily, weekly, every 10 days, and so on. What makes time series different is that each data point in the series is dependent on the previous data points. Let us understand the difference more clearly by taking a couple of examples.Example 1:Suppose you have a dataset of people who have taken a loan from a particular company (as shown in the table below). Do you think each row will be related to the previous rows? Certainly not! The loan taken by a person will be based on his financial conditions and needs (there could be other factors such as the family size etc., but for simplicity we are considering only income and loan type) . Also, the data was not collected at any specific time interval. It depends on when the company received a request for the loan.Example 2:Lets take another example. Suppose you have a dataset that contains the level of CO2 in the air per day (screenshot below). Will you be able to predict the approximate amount of CO2 for the next day by looking at the values from the past few days? Well, of course. If you observe, the data has been recorded on a daily basis, that is, the time interval is constant (24 hours).You must have got an intuition about this by now  the first case is a simple regression problem and the second is a time series problem. Although the time series puzzle here can also be solved using linear regression, but that isnt really the best approach as it neglects the relation of the values with all the relative past values. Lets now look at some of the common techniques used for solving time series problems.There are a number of methods for time series forecasting and we will briefly cover them in this section. The detailed explanation and python codes for all the below mentioned techniques can be found in this article: 7 techniques for time series forecasting (with python codes).In this section we will do a quick introduction to ARIMA which will be helpful in understanding Auto Arima. A detailed explanation of Arima, parameters (p,q,d), plots (ACF PACF) and implementation is included in this article : Complete tutorial to Time Series.ARIMA is a very popular statistical method for time series forecasting. ARIMA stands for Auto-Regressive Integrated Moving Averages. ARIMA models work on the following assumptions ARIMA has three components  AR (autoregressive term), I (differencing term) and MA (moving average term). Let us understand each of these components The general steps to implement an ARIMA model are Although ARIMA is a very powerful model for forecasting time series data, the data preparation and parameter tuning processes end up being really time consuming. Before implementing ARIMA, you need to make the series stationary, and determine the values of p and q using the plots we discussed above. Auto ARIMA makes this task really simple for us as it eliminates steps 3 to 6 we saw in the previous section. Below are the steps you should follow for implementing auto ARIMA:We completely bypassed the selection of p and q feature as you can see. What a relief! In the next section, we will implement auto ARIMA using a toy dataset.We will be using the International-Air-Passenger dataset. This dataset contains monthly total of number of passengers (in thousands). It has two columns  month and count of passengers. You can download the dataset from this link.Below is the R Code for the same problem:In the above code, we simply used the .fit() command to fit the model without having to select the combination of p, q, d. But how did the model figure out the best combination of these parameters? Auto ARIMA takes into account the AIC and BIC values generated (as you can see in the code) to determine the best combination of parameters. AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) values are estimators to compare models. The lower these values, the better is the model.Check out these links if you are interested in the maths behind AIC and BIC.I have found auto ARIMA to be the simplest technique for performing time series forecasting. Knowing a shortcut is good but being familiar with the math behind it is also important. In this article I have skimmed through the details of how ARIMA works but do make sure that you go through the links provided in the article. For your easy reference, here are the links again:I would suggest practicing what we have learned here on this practice problem: Time Series Practice Problem. You can also take our training course created on the same practice problem,Time series forecasting, to provide you a head start.Good luck, and feel free to provide your feedback and ask questions in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/
A Simple Introduction to Facial Recognition (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Understanding how Face Recognition works|Case Study|Implementation in Python|Understanding the Python code|Face Recognition Applications|End Notes,"About the author|Share this:|Like this:|Related Articles|Build High Performance Time Series Models using Auto ARIMA in Python and R|Google Clouds Machine Learning Powered Text-to-Speech is Available for Everyone!|
Guest Blog
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you know that every time you upload a photo to Facebook, the platform uses facial recognition algorithms to identify the people in that image? Or that certain governments around the world use face recognition technology to identify and catch criminals? I dont need to tell you that you can now unlock smartphones with your face!The applications of this sub-domain of computer vision are vast and businesses around the world are already reaping the benefits. The usage of face recognition models is only going to increase in the next few years so why not teach yourself how to build one from scratch?In this article, we are going to do just that. We will first understand the inner workings of face recognition, and then take a simple case study and implement it in Python. By the end of the article you will have built your very first facial recognition model!In order to understand how Face Recognition works, let us first get an idea of the concept of a feature vector.Every Machine Learning algorithm takes a dataset as input and learns from this data. The algorithm goes through the data and identifies patterns in the data. For instance, suppose we wish to identify whose face is present in a given image, there are multiple things we can look at as a pattern:Clearly, there is a pattern here  different faces have different dimensions like the ones above. Similar faces have similar dimensions. The challenging part is to convert a particular face into numbers  Machine Learning algorithms only understand numbers. This numerical representation of a face (or an element in the training set) is termed as a feature vector. A feature vector comprises of various numbers in a specific order.As a simple example, we can map a face into a feature vector which can comprise various features like:Essentially, given an image, we can map out various features and convert it into a feature vector like:So, our image is now a vector that could be represented as (23.1, 15.8, 255, 224, 189, 5.2, 4.4). Of course there could be countless other features that could be derived from the image (for instance, hair color, facial hair, spectacles, etc). However, for the example, let us consider just these 5 simple features.Now, once we have encoded each image into a feature vector, the problem becomes much simpler. Clearly, when we have 2 faces (images) that represent the same person, the feature vectors derived will be quite similar. Put it the other way, the distance between the 2 feature vectors will be quite small.Machine Learning can help us here with 2 things:Now that we have a basic understanding of how Face Recognition works, let us build our own Face Recognition algorithm using some of the well-known Python libraries.We are given a bunch of faces  possibly of celebrities like Mark Zuckerberg, Warren Buffett, Bill Gates, Shah Rukh Khan, etc. Call this bunch of faces as our corpus. Now, we are given image of yet another celebrity (new celebrity). The task is simple  identify if this new celebrity is among those present in the corpus.Here are some of the images in the corpus:As you can see, we have celebrities like Barack Obama, Bill Gates, Jeff Bezos, Mark Zuckerberg, Ray Dalio and Shah Rukh Khan.Now, here is the new celebrity:Note: all of the above images have been taken from Google images.It is obvious that this is Shah Rukh Khan. However, for a computer this is a challenging task. The challenge is because of the fact that for us humans, it is easy to combine so many features of the images to see which one is which celebrity. However, for a computer, it isnt straightforward to learn how to recognize these faces.There is an amazingly simple Python library that encapsulates all of what we learn above  creating feature vectors out of faces and knowing how to differentiate across faces. This Python library is called as face_recognition and deep within, it employs dlib  a modern C++ toolkit that contains several machine learning algorithms that help in writing sophisticated C++ based applications.face_recognition library in Python can perform a large number of tasks:Here, we will talk about the 3rd use case  identify faces in images.You can find the source code of face_recognition library here on Github: https://github.com/ageitgey/face_recognition.In fact, there is also a tutorial on how to install face_recognition library: https://github.com/ageitgey/face_recognition#installation-options. Before you install face_recognition, you need to install dlib as well. You can find the instructions to install dlib over here: https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf.This section contains the code for a building a straightforward face recognition system using theface_recognition library. This is the implementation part, we will go through the code to understand it in more detail in the next section.The folder structure is as follows:facialrecognition:Our root directory, facialrecognition contains:When you create the folder structure as above and run the above code, here is what you get as the output:Clearly, the new celebrity is Shah Rukh Khan and our face recognition system is able to detect it!Now, let us go through the code to understand how it works:These are simply the imports. We will be using the built-in os library to read all the images in our corpus and we will use face_recognition for the purpose of writing the algorithm.This simple code helps us identify the path of all of the images in the corpus. Once this line is executed, we will have:Now, the code below loads the new celebritys image:To make sure that the algorithms are able to interpret the image, we convert the image to a feature vector:The rest of the code now is fairly easy:Here, we are:The output as shown above clearly suggests that this simple face recognition algorithm works amazingly well. Let us try replacing my_image with another image:When you run the algorithm again, you will see the following output:Clearly, the system did not identify Jack Ma as any of the above celebrities. This indicates that our algorithm is quite good in both:Face Recognition is a well researched problem and is widely used in both industry and in academia. As an example, a criminal in China was caught because a Face Recognition system in a mall detected his face and raised an alarm. Clearly, Face Recognition can be used to mitigate crime. There are many other interesting use cases of Face Recognition:To summarize, Face Recognition is an interesting problem with lots of powerful use cases which can significantly help society across various dimensions. While there will always be an ethical risk attached to commercialzing such techniques, that is a debate we will shelve for another time.I hope you found this article useful. Please provide you feedback and suggestions in the comments section below!",https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/
Google Clouds Machine Learning Powered Text-to-Speech is Available for Everyone!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Simple Introduction to Facial Recognition (with Python codes)|Google AI Open Sources TensorFlow Code & Dataset for Reinforcement Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Text-to-speech and Speech-to-text are fascinating concepts and ones that have seen a ton of research thanks to machine learning. We are no longer limited to hearing mechanical voices emanating from machines. If youre still skeptical, a look at the Google Duplex demo will quickly convince you otherwise.Source: Financial ExpressGoogle Clouds Text-to-Speech and Speech-to-Text offerings have been around for almost a year but were still fairly limited in their ability to synthesize speech and doing so in multiple languages. However, all bets are off in the latest release. A bunch of updates have been added, making it far easier to hear natural sounding voices from machines and generating much more accurate transcripts.And of course, the Text-to-Speech API is now available to the general public!This Text-to-Speech API now works in 14 languages and supports 30 standards voices along with 26 WaveNet voices. A demo is available for you to try out here. The below table shows the entire list:The key takeaway for data scientists in this release is surely the launch of 17 new WaveNet voices. WaveNet is a model developed by DeepMind that uses machine learning to generate these text-to-speech audios. Its a deep neural network that is capable of producing incredible human-like sound from machines. It is the algorithm that powers the voice you hear in the Google Assistant. You can read more about WaveNet here.On the Speech-to-Text front, Google Cloud can now recognize the different speakers in the audio thanks to machine learning. You need to specify how many speakers are there in the audio, and Googles service then gets to work. It even has the ability to tag each word with a unique speaker number.You dont even need to wait for Google to release any research paper detailing each step  head over to this GitHub repository and download the TensorFlow implementation of WaveNet!This is a brilliant example of combining NLP with audio processing. When Id heard the Google Duplex demo earlier this year I was blow away and instantly wanted to figure out how I could create this technology on my machine. And with WaveNets TensorFlow implementation that part becomes easier than ever before.Once youre done playing around with the demo, you can use the full services. The pricing et all is available on Googles page. Its not very expensive so I would recommend trying it out at least once if NLP is your field of choice.",https://www.analyticsvidhya.com/blog/2018/08/google-cloud-text-to-speech-available/
Google AI Open Sources TensorFlow Code & Dataset for Reinforcement Learning,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Clouds Machine Learning Powered Text-to-Speech is Available for Everyone!|AI World Cup is a Fully Simulated Football Tournament Powered by Neural Networks!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Progress in reinforcement learning has chugged along at a far slower rate than deep learning. While there have been notable news-worthy breakthroughs like OpenAI Five and Googles AlphaGo, getting reinforcement learning practices into practical scenarios has just not happened.As Google AIs team mentions in this blog post, developing these kind of algorithms requires tons of experimentation without any clear direction. Unfortunately most of the existing frameworks out there just do not have that kind of flexibility. If youve worked or researched in this field, you know exactly how difficult (if not impossible) it is to reproduce existing approaches.So to help speed along research and with the hope of getting the community more involved in reinforcement learning, the Google AI team has open sourced a TensorFlow framework, called Dopamine, that aims to create research by making it more flexible and reproducible. According to the teams official documentation, their design principles are:Realizing how important is for new folks to check their results against a benchmark, the researchers have also released the entire training data. It is available as Python pickle files, JSON files, and a website where users can visualize each training iteration.The code available on GitHub is just 15 Python files and comes packaged with detailed documentations.What are you waiting for? Get started on this NOW!Note that DeepMinds research on dopamine was unrelated to this work by Google AI. While both are rooted in reinforcement learning to quite an extent, Google AI has involved the entire community by open sourcing its efforts. It certainly helps that its TensorFlow based, a framework everyone in the deep learning community is familiar with.Reinforcement learning can be a daunting subject to start with but I encourage you all to give this a try. This is a field that is still ripe with potential and will see a lot of progress in the coming years. This is a great resource to get started and you can also refer to to our article for beginners.",https://www.analyticsvidhya.com/blog/2018/08/google-ai-open-source-tensorflow-reinforcement-learning/
AI World Cup is a Fully Simulated Football Tournament Powered by Neural Networks!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google AI Open Sources TensorFlow Code & Dataset for Reinforcement Learning|The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"24 teams. 12 countries. 1 trophy. This was the 2nd edition of the AI World Cup, a tournament that combines the power of artificial intelligence and football. And it was as intriguing as it sounds!The tournament was hosted by the Korea Institute of Science and Technology (Kaist) last week and saw participation from around the globe. The hosts, Kaist, ran out winners in the end, seeing off competition from the big giants like Google and MIT (according to this report).Below is a high-level overview of how the tournament was structured:This was reinforcement learning and neural networks being put to the team on a virtual football field, and it passed the intense examination with flying colors. According to the winning teams leader, they used an algorithm based on Q-Learning. It was designed to maximize rewards if the agent (in this case the virtual player) took the right action, and correct itself if it did not. If you want to learn more about reinforcement learning and and how to implement it, check out this beginner-friendly article.In their particular algorithm, the player could take five steps in a second. The team simulated three million different steps to get the agent to understand the environment and optimize the final results. The tournament even had journalists and commentators powered by AI!Intrigued? Now watch the below video from last years event to understand how the virtual players were trained using neural networks. The video contains different cases which further illustrate how crucial these algorithms are to clinching the AI World Cup:This is quite an entertaining use of AI, wouldnt you agree? Not even single breakthrough or development has to be groundbreaking, its refreshing to see the light side of AI once in a while. Being a huge advocate from AI and ML in sports, it was a joy covering this world cup event here.However, it adds to the belief that researchers are not quite able to find a practical use of reinforcement learning algorithms outside a simulation environment. We seem to still be quite a while away from that scenario. Watch this space for more!",https://www.analyticsvidhya.com/blog/2018/08/ai-world-cup-powered-by-neural-networks/
The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|1. What is Dimensionality Reduction?|2. Why is Dimensionality Reduction required?|3. Common Dimensionality Reduction Techniques|4. BriefSummary of when to use each Dimensionality Reduction Technique|End Notes,"3.1 Missing Value Ratio|3.2 Low Variance Filter|3.3 High Correlation filter|3.4 Random Forest|3.5 Backward Feature Elimination|3.6 Forward Feature Selection|3.7Factor Analysis|3.8 Principal Component Analysis (PCA)|3.9 Independent Component Analysis|3.10 Methods Based on Projections|3.11 t- Distributed Stochastic Neighbor Embedding (t-SNE)|3.12 UMAP|Share this:|Like this:|Related Articles|AI World Cup is a Fully Simulated Football Tournament Powered by Neural Networks!|The Ultimate Data Science and Machine Learning Blogathon  More than $2500 up for grabs!|
Pulkit Sharma
|42 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever worked on a dataset with more than a thousand features? How about over 50,000 features? I have, and let me tell you its a very challenging task, especially if you dont know where to start! Having a high number of variables is both a boon and a curse. Its great that we have loads of data for analysis, but it is challenging due to size.Its not feasible to analyze each and every variable at a microscopic level. It might take us days or months to perform any meaningful analysis and well lose a ton of time and money for our business! Not to mention the amount of computational power this will take. We need a better way to deal with high dimensional data so that we can quickly extract patterns and insights from it. So how do we approach such a dataset?Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the models performance. Its a really powerful way to deal with huge datasets, as youll see in this article.This is a comprehensive guide to various dimensionality reduction techniques that can be used in practical scenarios. We will first understand what this concept is and why we should use it, before diving into the 12 different techniques I have covered. Each technique has its own implementation in Python to get you well acquainted with it.We are generating a tremendous amount of data daily. In fact, 90% of the data in the world has been generated in the last 3-4 years! The numbers are truly mind boggling. Below are just some of the examples of the kind of data being collected:As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. One of the most common ways of doing visualization is through charts. Suppose we have 2 variables, Age and Height. We can use a scatter or line plot between Age and Height and visualize their relationship easily:Now consider a case in which we have, say 100 variables (p=100). In this case, we can have 100(100-1)/2 = 5000 different plots. It does not make much sense to visualize each of them separately, right? In such cases where we have a large number of variables, it is better to select a subset of these variables (p<<100) which captures as much information as the original set of variables.Let us understand this with a simple example. Consider the below image:Here we have weights of similar objects in Kg (X1) and Pound (X2). If we use both of these variables, they will convey similar information. So, it would make sense to use only one variable. We can convert the data from 2D (X1 and X2) to 1D (Y1) as shown below:Similarly, we can reduce p dimensions of the data into a subset of k dimensions (k<<n). This is called dimensionality reduction.Here are some of the benefits of applying dimensionality reduction to a dataset:Time to dive into the crux of this article  the various dimensionality reduction techniques! We will be using the dataset from AVsPractice Problem: Big Mart Sales III(register on this link and download the dataset from the data section).Dimensionality reduction can be done in two different ways:We will now look at various dimensionality reduction techniques and how to implement each of them in Python.Suppose youre given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isnt set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable.Lets implement this approach in Python.First, lets load the data:Note: The path of the file should be added while reading the data.Now, we will check the percentage of missing values in each variable. We can use .isnull().sum() to calculate this.As you can see in the above table, there arent too many missing values (just 2 variables have them actually). We can impute the values using appropriate methods, or we can set a threshold of, say 20%, and remove the variable having more than 20% missing values. Lets look at how this can be done in Python:So the variables to be used are stored in variable, which contains only those features where the missing values are less than 20%.Consider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance.So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset. The reason for doing this, as I mentioned above, is that variables with a low variance will not affect the target variable.Lets first impute the missing values in the Item_Weight column using the median value of the known Item_Weight observations. For theOutlet_Size column, we will use the mode of the known Outlet_Size values to impute the missing values:Lets check whether all the missing values have been filled:Voila! We are all set. Now lets calculate the variance of all the numerical variables.As the above output shows, the variance of Item_Visibility is very less as compared to the other variables. We can safely drop this column. This is how we apply low variance filter. Lets implement this in Python:The above code gives us the list of variables that have a variance greater than 10.High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).As a general guideline, we should keep those variables which show a decent or high correlation with the target variable.Lets perform the correlation calculation in Python. We will drop the dependent variable (Item_Outlet_Sales) first and save the remaining variables in a new dataframe (df).Wonderful, we dont have any variables with a high correlation in our dataset. Generally, if the correlation between a pair of variables is greater than 0.5-0.6, we should seriously consider dropping one of those variables.Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you dont need to program that separately. This helps us select a smaller subset of features.We need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs. Lets also drop the ID variables (Item_Identifier and Outlet_Identifier) as these are just unique numbers and hold no significant importance for us currently.After fitting the model, plot the feature importance graph:Based on the above graph, we can hand pick the top-most features to reduce the dimensionality in our dataset. Alernatively, we can use the SelectFromModel of sklearn to do so. It selects the features based on the importance of their weights.Follow the below steps to understand and use the Backward Feature Elimination technique:This method can be used when building Linear Regression or Logistic Regression models. Lets look at its Python implementation:We need to specify the algorithm and number of features to select, and we get back the list of variables obtained from backward feature elimination. We can also check the ranking of the variables using the rfe.ranking_ command.This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows:Lets implement it in Python:This returns an array containing the F-values of the variables and the p-values corresponding to each F value. Refer tothis link to learn more about F-values. For our purpose, we will select the variables having F-value greater than 10:This gives us the top most variables based on the forward feature selection algorithm.NOTE : Both Backward Feature Elimination and Forward Feature Selection are time consuming and computationally expensive.They are practically only used on datasets that have a small number of input variables.The techniques we have seen so far are generally used when we do not have a very large number of variables in our dataset. These are more or less feature selection techniques. In the upcoming sections, we will be working with the Fashion MNIST dataset, which consists of images belonging to different types of apparel, e.g. T-shirt, trousers, bag, etc. The dataset can be downloaded from the IDENTIFY THE APPAREL practice problem.The dataset has a total of 70,000 images, out of which 60,000 are in the training set and the remaining 10,000 are test images. For the scope of this article, we will be working only on the training images. The train file is in a zip format. Once you extract the zip file, you will get a .csv file and a train folder which includes these 60,000 images. The corresponding label of each image can be found in the train.csv file.Suppose we have two variables: Income and Education. These variables will potentially have a high correlation as people with a higher education level tend to have significantly higher income, and vice versa.In the Factor Analysis technique, variables are grouped by their correlations, i.e., all variables in a particular group will have a high correlation among themselves, but a low correlation with variables of other group(s). Here, each group is known as a factor. These factors are small in number as compared to the original dimensions of the data. However, these factors are difficult to observe.Lets first read in all the images contained in the train folder:NOTE: You must replace the path inside the glob function with the path of your train folder.Now we will convert these images into anumpy array format so that we can perform mathematical operations and also plot the images.(60000, 28, 28, 3)As you can see above, its a 3-dimensional array. We must convert it to 1-dimension as all the upcoming techniques only take 1-dimensional input. To do this, we need to flatten the images:Let us now create a dataframe containing the pixel values of every individual pixel present in each image, and also their corresponding labels (for labels, we will make use of the train.csv file).Now we will decompose the dataset using Factor Analysis:Here, n_components will decide the number of factors in the transformed data. After transforming the data, its time to visualize the results:Looks amazing, doesnt it? We can see all the different factors in the above graph. Here, the x-axis and y-axis represent the values of decomposed factors. As I mentioned earlier, it is hard to observe these factors individually but we have been able to reduce the dimensions of our data successfully.PCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. You can refer to this article to learn more about PCA. For your quick reference, below are some of the key points you should know about PCA before proceeding further:Before moving further, well randomly plot some of the images from our dataset:Lets implement PCA using Python and transform the dataset:In this case, n_components will decide the number of principal components in the transformed data. Lets visualize how much variance has been explained using these 4 components. We will use explained_variance_ratio_ to calculate the same.In the above graph, the blue line represents component-wise explained variance while the orange line represents the cumulative explained variance. We are able to explain around 60% variance in the dataset using just four components. Let us now try to visualize each of these decomposed components:Each additional dimension we add to the PCA technique captures less and less of the variance in the model. The first component is the most important one, followed by the second, then the third, and so on.We can also use Singular Value Decomposition(SVD) to decompose our original dataset into its constituents, resulting in dimensionality reduction. To learn the mathematics behind SVD, refer tothis article.SVD decomposes the original variables into three constituent matrices. It is essentially used to remove redundant features from the dataset. It uses the concept of Eigenvalues and Eigenvectors to determine those three matrices. We will not go into the mathematics of it due to the scope of this article, but lets stick to our plan, i.e. reducing the dimensions in our dataset.Lets implement SVD and decompose our original variables:Let us visualize the transformed variables by plotting the first two principal components:The above scatter plot shows us the decomposed components very neatly. As described earlier, there is not much correlation between these components.Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors.If two variables are uncorrelated, it means there is no linear relation between them. If they are independent, it means they are not dependent on other variables. For example, the age of a person is independent of what that person eats, or how much television he/she watches.This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data.Lets compare PCA and ICA visually to get a better understanding of how they are different:Here, image (a) represents the PCA results while image (b) represents the ICA results on the same dataset.The equation of PCA is x = W.Here,Now we have to find an un-mixing matrix such that the components become as independent as possible. Most common method to measure independence of components is Non-Gaussianity:The above distribution is non-gaussian which in turn makes the components independent. Lets try to implement ICA in Python:Here, n_components will decide the number of components in the transformed data. We have transformed the data into 3 components using ICA. Lets visualize how well it has transformed the data:The data has been separated into different independent components which can be seen very clearly in the above image. X-axis and Y-axis represent the value of decomposed independent components.Now we shall look at some of the methods which reduce the dimensions of the data using projection techniques.To start off, we need to understand what projection is. Suppose we have two vectors, vector a and vector b, as shown below:We want to find the projection of a on b. Let the angle between a and b be . The projection (a1) will look like:a1 is the vector parallel to b. So, we can get the projection of vector a on vector b using the below equation:Here,By projecting one vector onto the other, dimensionality can be reduced.In projection techniques, multi-dimensional data is represented by projecting its points onto a lower-dimensional space. Now we will discuss different methods of projections:Once upon a time, it was assumed that the Earth was flat. No matter where you go on Earth, it keeps looking flat (lets ignore the mountains for a while). But if you keep walking in one direction, you will end up where you started. That wouldnt happen if the Earth was flat. The Earth only looks flat because we are minuscule as compared to the size of the Earth.These small portions where the Earth looks flat are manifolds, and if we combine all these manifolds we get a large scale view of the Earth, i.e., original data. Similarly for an n-dimensional curve, small flat pieces are manifolds and a combination of these manifolds will give us the original n-dimensional curve. Let us look at the steps for projection onto manifolds:Let us understand manifold projection technique with an example.If a manifold is continuously differentiable to any order, it is known as smooth or differentiable manifold. ISOMAP is an algorithm which aims to recover full low-dimensional representation of a non-linear manifold. It assumes that the manifold is smooth.It also assumes that for any pair of points on manifold, the geodesic distance (shortest distance between two points on a curved surface) between the two points is equal to the Euclidean distance (shortest distance between two points on a straight line). Lets first visualize the geodesic and Euclidean distance between a pair of points:Here,ISOMAP assumes both of these distances to be equal. Lets now look at a more detailed explanation of this technique. As mentioned earlier, all these techniques work on a three-step approach. We will look at each of these steps in detail:Lets implement it in Python and get a clearer picture of what Im talking about. We will perform non-linear dimensionality reduction through Isometric Mapping. For visualization, we will only take a subset of our dataset as running it on the entire dataset will require a lot of time.Parameters used:Visualizing the transformed data:You can see above that the correlation between these components is very low. In fact, they are even less correlated as compared to the components we obtained using SVD earlier!So far we have learned that PCA is a good choice for dimensionality reduction and visualization for datasets with a large number of variables. But what if we could use something more advanced? What if we can easily search for patterns in a non-linear way? t-SNE is one such technique. There are mainly two types of approaches we can use to map the data points:You can refer tothis article to learn about t-SNE in more detail.We will now implement it in Python and visualize the outcomes:n_components will decide the number of components in the transformed data. Time to visualize the transformed data:Here you can clearly see the different components that have been transformed using the powerful t-SNE technique.t-SNE works very well on large datasets but it also has its limitations, such as loss of large-scale information, slow computation time, and inability to meaningfully represent very large datasets. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can preserve as much of the local, and more of the global data structure as compared to t-SNE, with a shorter runtime. Sounds intriguing, right?Some of the key advantages of UMAP are:This method uses the concept of k-nearest neighbor and optimizes the results using stochastic gradient descent. It first calculates the distance between the points in high dimensional space, projects them onto the low dimensional space, and calculates the distance between points in this low dimensional space. It then uses Stochastic Gradient Descent to minimize the difference between these distances. To get a more in-depth understanding of how UMAP works, check outthis paper.Refer hereto see the documentation and installation guide of UMAP. We will now implement it in Python:Here,Let us visualize the transformation:The dimensions have been reduced and we can visualize the different transformed components. There is very less correlation between the transformed variables. Let us compare the results from UMAP and t-SNE:We can see that the correlation between the components obtained from UMAP is quite less as compared to the correlation between the components obtained from t-SNE. Hence, UMAP tends to give better results.As mentioned in UMAPs GitHub repository, it often performs better at preserving aspects of the global structure of the data than t-SNE. This means that it can often provide a better big picture view of the data as well as preserving local neighbor relations.Take a deep breath. We have covered quite a lot of the dimensionality reduction techniques out there. Lets briefly summarize where each of them can be used.In this section, we will briefly summarize the use cases of each dimensionality reduction technique that we covered. Its important to understand where you can, and should, use a certain technique as it helps save time, effort and computational power.This is as comprehensive an article on dimensionality reduction as youll find anywhere! I had a lot of fun writing it and found a few new ways of dealing with high number of variables I hadnt used before (like UMAP).Dealing with thousands and millions of features is a must-have skill for any data scientist. The amount of data we are generating each day is unprecedented and we need to find different ways to figure out how to use it. Dimensionality reduction is a very useful way to do this and has worked wonders for me, both in a professional setting as well as in machine learning hackathons.Im looking forward to hearing your feedback and ideas in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/
The Ultimate Data Science and Machine Learning Blogathon  More than $2500 up for grabs!,Learn everything about Analytics|Introduction|How do I Participate?|Lucrative Prizes to bewon,"Terms & Conditions|Share this:|Like this:|Related Articles|The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes)|UC Berkeleys Jaw Dropping Pose Detection and Translation Technique will leave you Speechless|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you want to change the world, pick up a pen and write.Martin LutherWe are delighted to announce the launch ofAnalytics Vidhyas Blogathon, the ultimate competition which combines your writing prowess with your machine learning skills!Analytics Vidhya has always been at the forefront of knowledge sharing, and we want to continue this trend among our community members. We will not only publish the best articles on Analytics Vidhyas Medium page, but also provide feedback to every writer on their article.Along with recognition in front of a broad audience, there are lucrative prizes to be won! The focus of the articles can be on any data related fielddata science, machine learning, deep learning, Artificial Intelligence, Business Analytics, etc.The blogathon starts today, 25th August, and will conclude on 23rd September. Beyond this, you will be given an extra week to work on any feedback we have provided on an already submitted article.We will announce the winners in each category on 30th September.To enter the competition, you need tofill in your details here. We will then add you as a writer to our Medium publication and you can start sending your drafts to us. It really is that simple!Every article which meets Analytics Vidhyas standards will get published on our Medium page.There are $2500 + bonuses to be won!Prizes will be given in 3 categories:Most popular article:This will be decided by the number of unique fans the article gets.Most number of articles with more than 10 unique fans:As the name suggests, the more articles you submit and the more fans you gather, the better your chance of grabbing this prize!Editors Prize:This category will be judged by AVs in-house editing team and the winner will receive $500!Theres more..The top 25 bloggers will each receive free access for 6 months to AVs Computer Vision using Deep Learning course!It is a one-of-a-kind course that will introduce you to the world of CV and ensure you come out a master of the field.The top 50 participants will get access to DataHack 2017day 1andday 2talks a collection of all the sessions that happened in Indias premier analytics conference last year. Hear from business leaders, domain experts, senior data scientists and other eminent personalities in the analytics domain. An unmissable opportunity!For those articles which do not meet our standards, we will ensure you are provided with feedback on how to make those articles better in the future.Sounds too good an opportunity to pass up? Thats the attitude were looking forgo ahead andfill in your details hereand we will add you as a writer on our Medium publication. Happy writing!",https://www.analyticsvidhya.com/blog/2018/08/data-science-machine-learning-blogathon-lucrative-prizes-bonus/
UC Berkeleys Jaw Dropping Pose Detection and Translation Technique will leave you Speechless,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Ultimate Data Science and Machine Learning Blogathon  More than $2500 up for grabs!|MITs Open Source Algorithm Automates Object Detection in Images (with GitHub link)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are certain developments that, every once in a while, tend to re-shape the landscape of how I look at things. DeepMinds AlphaGo and NVIDIAs vid2vid approach are just some examples of these occurrences. And now a research released by UC Berkeley has joined this list.One look at the video of the technique (shown at the end of the article) will show you exactly why. UC Berkeleys researchers have pioneered a method that transfers motion between human objects in different videos (let that sink in for a few seconds).The approach requires two videos  one of the target person whose appearance needs to be synthesized, and the other of the source subject whose dance poses are imposed on the target person. Pose detection is used to estimate the source subjects movements, which are then mapped accordingly on the targets appearance. Imagine how complex this is  the two humans are invariably of different shape and size, with different body movements.The above image illustrates this perfectly. The frame on the top left is the source subject doing the dance moves, the one below shows pose detection, and the right frame shows the motion translated to the target. The accuracy of the motions is incredible. And its not just one pose at a time  the poses change dynamically every fraction of a second and the technique does not waver.Check out the aforementioned video below. Notice the sheer amount of minute details that Berkeleys technique covers, like wrinkles on the clothes, reflection in the glass, etc.:I have gone through the video multiple times now and Im still stunned by the amazing complexity and accuracy of Berkeleys technique. Earlier this week I covered NVIDIAs vid2vid technique and thought that was game changing, and here we are, already setting the benchmark and bar ever higher.It wouldnt surprise you to know that GANs are at the heart of the technique. Do read through the research paper as well that gives a step-by-step approach the researchers took, and also includes a bunch of useful resources.",https://www.analyticsvidhya.com/blog/2018/08/pose-detection-translation-technique-uc-berkeley/
MITs Open Source Algorithm Automates Object Detection in Images (with GitHub link),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|UC Berkeleys Jaw Dropping Pose Detection and Translation Technique will leave you Speechless|A Hands-On Guide to Automated Feature Engineering using Featuretools in Python|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Fixing corrupt or bad images, filling the gaps in existing images, translating the background from one image to another  these are all applications of computer vision that have transcended imagination to become a reality this year. And now researchers from MITs Computer Science and Artificial Intelligence Laboratory (CSAIL) have thrown their hat into this ring with their latest study.They have unveiled an approach called Semantic Soft Segmentation (SSS) that uses machine learning to automate certain parts of the image editing process, including detecting objects! What takes an expert editor several minutes (or even hours) and involves tweaking and analyzing images pixel-by-pixel can now be done in a matter of seconds thanks to SSS. The below image shows how the algorithm works to detect objects in a given image:SSS works by analyzing the texture and color of the given image. It then combines these attributes with data provided by a trained neural network model that information about what kind of objects are present in the image.As mentioned in the research paper (link below), the algorithm generates soft segments that correspond to semantically meaningful regions in the image by fusing the high-level information from a neural network with low-level image features fully automatically. This makes tasks like parsing objects, editing backgrounds, etc. quite trivial and removed the need for expertise (at least as far as casual users are concerned!).While the current version of SSS is heavily focused on static images, there is an acknowledgement by the researchers that this will be fine-tuned for video applications in the future.I have mentioned a few resources below to help you get acquainted with this study and also try it out by yourself:Also do check out the short video below which shows SSS in its full glory:Another week, another breakthrough study in computer vision. Deep learning has carved a niche for itself in this field and you can expect to see more and more of these projects coming soon, especially in video editing. CGI effects that we see in movies could easily be done using techniques like SSS (once they have been improved a bit more).Should expert artists be worried? Deep learning does have the potential to generate a decent work of art but that human touch and intuition continues to elude even the finest works of machines.If this field interests you, you can take our Computer Vision using Deep Learning course which aims to help you dip your toes and come out a master in this exciting and upcoming field.",https://www.analyticsvidhya.com/blog/2018/08/mits-open-source-algorithm-automates-object-detection-images/
A Hands-On Guide to Automated Feature Engineering using Featuretools in Python,Learn everything about Analytics|Introduction|Table of Contents||1. What is a feature?||2. What is Feature Engineering?|||||3. Why is Feature Engineering required?|4. Automating Feature Engineering|5. Introduction to Featuretools|6. Implementation of Featuretools|7. Featuretools Interpretability|End Notes,"|6.1. Installation||6.2. Loading required Libraries and Data|6.3. Data Preparation|6.4. Data Preprocessing|6.5. Feature Engineering using Featuretools|6.6. Model Building|Share this:|Like this:|Related Articles|MITs Open Source Algorithm Automates Object Detection in Images (with GitHub link)|A Practical Introduction to K-Nearest Neighbors Algorithm for Regression (with Python code)|
Prateek Joshi
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anyone who has participated in machine learning hackathons and competitions can attest to how crucial feature engineering can be. It is often the difference between getting into the top 10 of the leaderboard and finishing outside the top 50!I have been a huge advocate of feature engineering ever since I realized its immense potential. But it can be a slow and arduous process when done manually. I have to spend time brainstorming over what features to come up, and analyze their usability them from different angles. Now, this entire FE process can be automated and Im going to show you how in this article.                                                                          Source: VentureBeatWe will be using the Python feature engineering library called Featuretools to do this. But before we get into that, we will first look at the basic building blocks of FE, understand them with intuitive examples, and then finally dive into the awesome world of automated feature engineering using the BigMart Sales dataset.In the context of machine learning, a feature can be described as a characteristic, or a set of characteristics, that explains the occurrence of a phenomenon. When these characteristics are converted into some measurable form, they are called features.For example, assume you have a list of students. This list contains the name of each student, number of hours they studied, their IQ, and their total marks in the previous examinations. Now you are given information about a new student the number of hours he/she studied and his IQ, but his/her marks are missing. You have to estimate his/her probable marks.Here, youd use IQ and study_hours to build a predictive model to estimate these missing marks. So, IQ and study_hours are called the features for this model.Feature Engineering can simply be defined as the process of creating new features from the existing features in a dataset. Lets consider a sample data that has details about a few items, such as their weight and price.Now, to create a new feature we can use Item_Weight and Item_Price. So, lets create a feature called Price_per_Weight. It is nothing but the price of the item divided by the weight of the item. This process is called feature engineering.This was just a simple example to create a new feature from existing ones, but in practice, when we have quite a lot of features, feature engineering can become quite complex and cumbersome.Lets take another example. In the popular Titanic dataset, there is a passenger name feature and below are some of the names in the dataset:These names can actually be broken downinto additional meaningful features. For example, we can extract and group similar titles into single categories. Lets have a look at the unique number of titles in the passenger names.It turns out that titles likeDona, Lady, the Countess, Capt, Col, Don, Dr, Major, Rev, Sir, and Jonkheer are quite rare and can be put under a single label. Lets call it rare_title. Apart from this, the titles Mlle and Ms can be placed under Miss, and Mme can be replaced with Mrs.Hence, the new title feature would have only 5 unique values as shown below:So, this is how we can extract useful information with the help of feature engineering, even from features like passenger names which initially seemed fairly pointless.The performance of a predictive model is heavily dependent on the quality of the features in the dataset used to train that model. If you are able to create new features which help in providing more information to the model about the target variable, its performance will go up. Hence, when we dont have enough quality features in our dataset, we have to lean on feature engineering.In one of the most popular Kaggle competitions, Bike Sharing Demand Prediction, the participants are asked to forecast the rental demand in Washington, D.C based on historical usage patterns in relation with weather, time and other data.As explained in this article, smart feature engineering was instrumental in securing a place in the top 5 percentile of the leaderboard. Some of the features created are given below:Creating such features is no cakewalk  it takes a great deal of brainstorming and extensive data exploration. Not everyone is good at feature engineering because it is not something that you can learn by reading books or watching videos. This is why feature engineering is also called an art. If you are good at it, then you have a major edge over the competition. Quite like Roger Federer, the master of feature engineering when it comes to Tennis shots.Analyze the two images shown above. The left one shows a car being assembled by a group of men during early 20th century, and the right picture shows robots doing the same job in todays world. Automating any process has the potential to make it much more efficient and cost-effective. For similar reasons, feature engineering can, and has been, automated in machine learning.Building machine learning models can often be a painstaking and tedious process. It involves many steps so if we are able to automate a certain percentage of feature engineering tasks, then the data scientists or the domain experts can focus on other aspects of the model. Sounds too good to be true, right?Now that we have understood that automating feature engineering is the need of the hour, the next question to ask is  how is it going to happen? Well, we have a great tool to address this issue and its called Featuretools.Featuretools is an open source library for performing automated feature engineering. It is a great tool designed to fast-forward the feature generation process, thereby giving more time to focus on other aspects of machine learning model building. In other words, it makes your data machine learning ready.Before taking Featuretools for a spin, there are three major components of the package that we should be aware of:a) An Entitycan be considered as a representation of a Pandas DataFrame. A collection of multiple entities is called an Entityset.b) Deep Feature Synthesis(DFS) has got nothing to do with deep learning. Dont worry. DFS is actually a Feature Engineering method and is the backbone of Featuretools. It enables the creation of new features from single, as well as multiple dataframes.c) DFS create features by applying Feature primitives to the Entity-relationships in an EntitySet. These primitives are the often-used methods to generate features manually. For example, the primitive mean would find the mean of a variable at an aggregated level.The best way to understand and become comfortable with Featuretools is by applying it on a dataset. So, we will use the dataset from our BigMart Sales practice problemin the next section to solidify our concepts.The objective of the BigMart Sales challenge is to build a predictive model to estimate the sales of each product at a particular store. This would help the decision makers at BigMart to find out the properties of any product or store, which play a key role in increasing the overall sales. Note that there are 1559 products across 10 stores in the given dataset.The below table shows the features provided in our data:You can download the data from here.Featuretools is available for Python 2.7, 3.5, and 3.6. You can easily install Featuretools using pip.To start off, well just store the target Item_Outlet_Sales in a variable called sales and id variables in test_Item_Identifier andtest_Outlet_Identifier.Then we will combine the train and test set as it saves us the trouble of performing the same step(s) twice.Lets check the missing values in the dataset.Quite a lot of missing values in the Item_Weight and Outlet_size variables. Lets quickly deal with them:I will not do an extensive preprocessing operation since the objective of this article is to get you started with Featuretools.It seems Item_Fat_Content contains only two categories, i.e., Low Fat and Regular  the rest of them we will consider redundant. So, lets convert it into a binary variable.Now we can start using Featuretools to perform automated feature engineering! It is necessary to have a unique identifier feature in the dataset (our dataset doesnt have any right now). So, we will create one unique ID for our combined dataset. If you notice, we have two IDs in our dataone for the item and another for the outlet. So, simply concatenating both will give us a unique ID.Please note that I have dropped the feature Item_Identifier as it is no longer required. However, I have retained the featureOutlet_Identifier because I plan to use it later.Now before proceeding, we will have to create an EntitySet. An EntitySet is a structure that contains multiple dataframes and relationships between them. So, lets create an EntitySet and add the dataframe combination to it.Our data contains information at two levelsitem level and outlet level. Featuretools offers a functionality to split a dataset into multiple tables. We have created a new table outlet from the BigMart table based on the outlet ID Outlet_Identifier.Lets check the summary of our EntitySet.As you can see above, it contains two entities  bigmart and outlet. There is also a relationship formed between the two tables, connected by Outlet_Identifier. This relationship will play a key role in the generation of new features.Now we will use Deep Feature Synthesis to create new features automatically. Recall that DFS uses Feature Primitives to create features using multiple tables present in the EntitySet.target_entity is nothing but the entity ID for which we wish to create new features (in this case, it is the entity bigmart). The parameter max_depthcontrols the complexity of the features being generated by stacking the primitives. The parameter n_jobs helps in parallel feature computation by using multiple cores.Thats all you have to do with Featuretools. It has generated a bunch of new features on its own.Lets have a look at these newly created features.DFS has created 29 new features in such a quick time. It is phenomenal as it would have taken much longer to do it manually. If you have datasets with multiple interrelated tables, Featuretools would still work. In that case, you wouldnt have to normalize a table as multiple tables will already be available.Lets print the first few rows of feature_matrix.There is one issue with this dataframe  itis not sorted properly. We will have to sort it based on the id variable from the combi dataframe.Now the dataframe feature_matrix is in proper order.It is time to check how useful these generated features actually are. We will use them to build a model and predict Item_Outlet_Sales. Since our final data (feature_matrix) has many categorical features, I decided to use the CatBoost algorithm. It can use categorical features directly and is scalable in nature. You can refer to this article to read more about CatBoost.CatBoost requires all the categorical variables to be in the string format. So, we will convert the categorical variables in our data to string first:Lets split feature_matrix back into train and test sets.Split the train data into training and validation set to check the models performance locally.Finally, we can now train our model. The evaluation metric we will use is RMSE (Root Mean Squared Error).1091.244The RMSE score on the validation set is ~1092.24.The same model got a score of 1155.12 on the public leaderboard. Without any feature engineering, the scores were ~1103 and ~1183 on the validation set and the public leaderboard, respectively. Hence, the features created by Featuretoolsare not just random features, they are valuable and useful. Most importantly, the amount of time it saves in feature engineering is incredible.Making our data science solutions interpretable is a very important aspect of performing machine learning. Features generated by Featuretools can be easily explained even to a non-technical person because they are based on the primitives, which are easy to understand.For example, the featuresoutlet.SUM(bigmart.Item_Weight)and outlet.STD(bigmart.Item_MRP)mean outlet-level sum of weight of the items and standard deviation of the cost of the items, respectively.This makes it possible for those people who are not machine learning experts, to contribute as well in terms of their domain expertise.The featuretools package is truly a game-changer in machine learning. While its applications are understandably still limited in industry use cases, it has quickly become ultra popular in hackathons and ML competitions. The amount of time it saves, and the usefulness of feature it generates, has truly won me over.Try it out next time you work on any dataset and let me know how it went in the comments section!",https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/
A Practical Introduction to K-Nearest Neighbors Algorithm for Regression (with Python code),Learn everything about Analytics|Introduction|Table of contents|1. A simple example to understand the intuition behind KNN|2. How does the KNN algorithm work?|3. Methods of calculating distance between points|4. How to choose the k factor?|5. Work on a dataset (Python codes)|6. End Notes and additional resources,"Share this:|Like this:|Related Articles|A Hands-On Guide to Automated Feature Engineering using Featuretools in Python|chorrrds  A Superb R Package for Analyzing and Working with Music Data|
Aishwarya Singh
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Out of all the machine learning algorithms I have come across, KNN has easily been the simplest to pick up. Despite its simplicity, it has proven to be incredibly effective at certain tasks (as you will see in this article).And even better? It can be used for both classification and regression problems! Its far more popularly used for classification problems, however. I have seldom seen KNN being implemented on any regression task. My aim here is to illustrate and emphasize how KNN can be equally effective when the target variable is continuous in nature.In this article, we will first understand the intuition behind KNN algorithms, look at the different ways to calculate distances between points, and then finally implement the algorithm in Python on the Big Mart Sales dataset. Lets go!Let us start with a simple example. Consider the following table  it consists of the height, age and weight (target) value for 10 people. As you can see, the weight value of ID11 is missing. We need to predict the weight of this person based on their height and age.Note: The data in this table does not represent actual values. It is merely used as an example to explain this concept.For a clearer understanding of this, below is the plot of height versus age from the above table:In the above graph, the y-axis represents the height of a person (in feet) and the x-axis represents the age (in years). The points are numbered according to the ID values. The yellow point (ID 11) is our test point.If I ask you to identify the weight of ID11 based on the plot, what would be your answer? You would likely say that since ID11 is closer topoints 5 and 1, so it must have a weight similar to these IDs, probably between 72-77 kgs (weights of ID1 and ID5 from the table). That actually makes sense, but how do you think the algorithm predicts the values? We will find that out in this article.As we saw above, KNN can be used for both classification and regression problems. The algorithm uses feature similarity to predict values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. From our example, we know that ID11 has height and age similar to ID1 and ID5, so the weight would also approximately be the same.Had it been a classification problem, we would have taken the mode as the final prediction. In this case, we have two values of weight  72 and 77. Any guesses how the final value will be calculated? The average of the values is taken to be the final prediction.Below is a stepwise explanation of the algorithm:2. The closest k data points are selected (based on the distance). In this example, points 1, 5, 6 will be selected if value of k is 3. We will further explore the method to select the right value of k later in this article.3. The average of these data points is the final prediction for the new point. Here, we have weight of ID11 = (77+72+60)/3 = 69.66 kg.In the next few sections we will discuss each of these three steps in detail.The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are  Euclidian, Manhattan (for continuous) and Hamming distance (for categorical).Once the distance of a new observation from the points in our training set has been measured, the next step is to pick the closest points. The number of points to be considered is defined by the value of k.The second step is to select the k value. This determines the number of neighbors we look at when we assign a value to any new observation.In our example, for a value k = 3, the closest points are ID1, ID5 and ID6.The prediction of weight for ID11 will be:For the value of k=5, the closest point will be ID1, ID4, ID5, ID6, ID10.The prediction for ID11 will be :We notice that based on the k value, the final result tends to change. Then how can we figure out the optimum value of k? Let us decide it based on the error calculation for our train and validation set (after all, minimizing the error is our final goal!).Have a look at the below graphs for training error and validation error for different values of k.For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train and validation set. If you observe closely, the validation error curve reaches a minima at a value of k = 9. This value of k is the optimum value of the model (it will vary for different datasets). This curve is known as an elbow curve (because it has a shape like an elbow) and is usually used to determine the k value.You can also use the grid search technique to find the best k value. We will implement this in the next section.By now you must have a clear understanding of the algorithm. If you have any questions regarding the same, please use the comments section below and I will be happy to answer them. We will now go ahead and implement the algorithm on a dataset. I have used the Big Mart sales dataset to show the implementation and you can download it from this link.The full Python code is below but we have a really cool coding window here where you can code your own k-Nearest Neighbor model in Python:1. Read the file2. Impute missing values3. Deal with categorical variables and drop the id columns4. Create train and test set5. Preprocessing  Scaling the features6. Let us have a look at the error rate for different k valuesOutput :As we discussed, when we take k=1, we get a very high RMSE value. The RMSE value decreases as we increase the k value. At k= 7, the RMSE is approximately 1219.06, and shoots up on further increasing the k value. We can safely say that k=7 will give us the best result in this case.These are the predictions using our training dataset. Let us now predict the values for test dataset and make a submission.7. Predictions on the test datasetOn submitting this file, I get an RMSE of 1279.5159651297.8. Implementing GridsearchCVFor deciding the value of k, plotting the elbow curve every time is be a cumbersome and tedious process. You can simply use gridsearch to find the best value.Output :In this article, we covered the workings of the KNN algorithm and its implementation in Python. Its one of the most basic, yet effective machine learning techniques. For KNN implementation in R, you can go through this article : kNN Algorithm using R.In this article, we used the KNN model directly from the sklearn library. You can also implement KNN from scratch (I recommend this!), which is covered in the this article: KNN simplified.If you think you know KNN well and have a solid grasp on the technique, test your skills in this MCQ quiz:30 questions on kNN Algorithm. Good luck!",https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/
chorrrds  A Superb R Package for Analyzing and Working with Music Data,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Practical Introduction to K-Nearest Neighbors Algorithm for Regression (with Python code)|Facebook and NYUs Powerful AI System will Make MRI Scans up to 10 Times Faster!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning and music  it may sound like an odd combination at first, but the two fit extremely well together. Music, or audio, generates tons of data points that can be extracted, analyzed, and worked with. Thats what Google did with NSynth, an algorithm that uses a deep neural network technique to generate sound. Artists these days are leaning on ML to generate music, and even create videos.Music is a universal language, while machine learning is quickly becoming a universally accepted (or at least acknowledged) field of work. So far all the open source codes I have seen or covered for music related ML work have been done in Python which is understandable, given how popular the language has become. Finally, we have a simple-to-use and intuitive package for R that deals with music data.Called chorrrds, the package scrapes data from the Cifraclub site and extracts it to your machine. The aim ofchorrrds is to help R users analyze and organize music chords and it can be considered a music information retrieval (MIR) package. You can check out this article in case youre wondering what MIR means and how it works. Its a fairly broad field which encompasses both structured and unstructured data. The package comes pre-loaded with several datasets whichYou can install chorrrds directly from CRAN by typing the below command:If you prefer downloading and installing it from GitHub, use the below code (you need to have the devtools package installed for this):The package comes pre-loaded with several datasets. The results are long so Im not listing them here, but you can copy the below code and check it out yourself:The official documentation of the package can be found here. There are enough details on the page to get you up and running with the code. If you have any ideas or suggestions on how you want to use this package, let us know in the comments section below.Weve been waiting for a while for something like to come along in R! As a music buff, it is a pleasure to dig deep into the musical chords and analyze them. You definitely need to have a bit of knowledge about music in order to extract the most out of this package but there surely isnt a better time to start than now!Remember the main function of the package is called get_chords() which you can use to extract chords for specific artists. The documentation has excellent examples of how to use it. Be sure to check that out!",https://www.analyticsvidhya.com/blog/2018/08/chorrrds-r-package-analyzing-working-music-data/
Facebook and NYUs Powerful AI System will Make MRI Scans up to 10 Times Faster!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|chorrrds  A Superb R Package for Analyzing and Working with Music Data|NVIDIA RTX 2080 Ti set for Enabling Faster Deep Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Getting a Magnetic Resonance Imaging (MRI) scan is a notoriously long process. For anyone who has been through the wringer, you know how harrowing that experience can be. It can take up to an hour for the entire process to work out while you sit inside that claustrophobic environment.Facebook (yes, Facebook!) and the NYU School of Medicine have joined hands with the aim of designing an AI system that will make these MRI scans up to 10 times faster! Not only that, the researchers are hoping that this will make MRI technology available to even more people. The project, called FastMRI, was started back in 2015 by the NYU researchers.In the current scenario, a MRI scan can take from anywhere between 15 minutes to an hour. This poses a significant problem for children, elderly people and especially those who suffer from claustrophobia. Additionally, the patient needs to hold his breath for prolonged periods (when scanning of the heart, liver, and other organs is ongoing). You might be getting an idea as to why this research has garnered everyones attention.MRI scanners operate by creating a series of 2D images which need to be converted to 3D. This is done by stacking up the 2D images. The more in-depth the data needs to be (depending on what has to be scanned), the longer the scanning process lasts. This is the part where the AI system being designed will be aiming to make the difference.The AI system should be able to capture less data, and hence accelerate the scanning process. The key, according to this blog post by Facebook, is to train artificial neural networks (ANNs) in order to recognize the underlying structure of the scanned images.The dataset being used for this research contains 10,000 cases and around 3 million MRIs of various patients liver, knee and brain. Training neural networks on these might yield good results, but there are a few major challenges that could derail the whole project, like a few wrongly modeled pixels. Its a fine line between success and failure in this field and one that the researchers will be well aware of.Its wonderful that machine learning and artificial intelligence are the heart of a truly revolutionary period in healthcare. When big companies like Facebook enter the fray, they garner the worlds attention. Lets hope that this research comes to fruition soon and sees a quick deployment in hospitals worldwide.Though as I mentioned, using neural networks to accelerate scanning has its own set of obstacles. I suspect it will a bit of time for them to perfect and open source their approach like theyre promised, but it should be worth the wait.",https://www.analyticsvidhya.com/blog/2018/08/facebook-and-nyu-designing-ai-system-mri-scans-quicker/
NVIDIA RTX 2080 Ti set for Enabling Faster Deep Learning,Learn everything about Analytics|Overview|Introduction,"Our take on this|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Facebook and NYUs Powerful AI System will Make MRI Scans up to 10 Times Faster!|NVIDIA Open Sourced a Video-to-Video Translation Technique using PyTorch  and it is Super Impressive|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Nvidia unveiled its new GeForce RTX 2000 series of graphics cards at Gamescom earlier today. While there has been a lot of anticipation in the gaming community, my eyes are gleaming with the possibilities in Deep Learning as I am writing this post.Nvidia announced RTX 2070, which is claimed to be 40% faster than GTX 1070.The beast  RTX 2080 Ti comes with 11 GB GDDR6, 4352 CUDA cores (yes  you read it right), that is 21% more CUDA cores than GTX 1080 Ti. I think that this would result in a 40%+ performance improvement over GTX 1080 Ti  although only time will tell.The cards are up for pre-orders and will be delivered from 20th September 2018. Here is a brief summary of the specifications of the new cards against the older ones:We think NVIDIA is set to have a big hardware impact on Deep Learning. A 20%  40% increase in hardware performance combined with the advancements happening in the algorithms should accelerate the Deep Learning innovations and have huge impact on real world applications in coming 6  12 months. We cant wait to get our hands on this new beast.",https://www.analyticsvidhya.com/blog/2018/08/nvidia-rtx-2080-ti-faster-deep-learning/
NVIDIA Open Sourced a Video-to-Video Translation Technique using PyTorch  and it is Super Impressive,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIA RTX 2080 Ti set for Enabling Faster Deep Learning|Launching Analytics Vidhyas Medium Publication and AV Editors club!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Progress in the field of deep learning and reinforcement learning relies on our capability to recreate the dynamics of real-world scenarios in a simulation environment. I have previously written about an algorithm that transforms images into a completely different category, and another technique that fixes corrupt images in the blink of an eye. Progress, at least in the image processing field, has been constant and promising.But research in the area of video processing has been painstakingly difficult. For example, can you take a video sequence and predict what will happen in the next frame? Its been explored, but not to any great avail. At least until now.NVIDIA, already leading the way in using deep learning for image and video processing, has open sourced a technique that does video-to-video translation with impressive results. The goal of this research, as described by the researchers in their paper, is to learn a mapping function from a given input video in order to produce an output video which depicts the contents of the input video with incredible precision (as you can see in the above GIF).They have released the code on GitHub, which is a PyTorch implementation of the technique for a high resolution translation of videos. This code can currently be used for:The above image is a wonderful illustration of different models (or techniques) used to perform the same task. On the top left is the input source video. Adjacent to that is the pix2pixHD model, the state-of-the-art image-to-image translation approach. On the bottom left is theCOVST model and on the bottom right is NVIDIAs vid2vid technique.You can browse through the below links to read more about this novel technique and even implement it on your own machine:Also, be sure to check out the below video which encapsulates all that the open sourced PyTorch code can do:If you were impressed with our last NVIDIA article on converting a standard video into slow-motion, this latest research will leave you stunned. And its not just limited to recreating real-world scenarios, it can even predict what will happen in the next few frames! When compared to baseline models like PredNet and MCNet, the vid2vid model produced far superior results.There are still a few issues with the model like not being able to map a turning car, but these will be overcome in due course. If this field of research interests you, go through the research paper I linked above and also download the PyTorch code and try to replicate the technique on your own end.",https://www.analyticsvidhya.com/blog/2018/08/nvidia-open-sourced-video-to-video-translation-pytorch/
Launching Analytics Vidhyas Medium Publication and AV Editors club!,Learn everything about Analytics||Introduction|Analytics Vidhyas Medium Publication!|Why start a Medium Publication?|What can you expect from our Medium Publication?|Open Call for Writers|Join the AV Editors Club|Are you ready?,"Share this:|Like this:|Related Articles|NVIDIA Open Sourced a Video-to-Video Translation Technique using PyTorch  and it is Super Impressive|DataHack Radio Episode #8: How Self-Driving Cars Work with Drive.ais Brody Huval|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Vidhya started as a blog in 2013 and has grown into a thriving community of data science professionals. We have evolved the platform based on our community needs and feedback. Today Analytics Vidhya offers Meetups, Webinars,DataHack platform,job portal,training portaland our flagship event DataHack Summit.Across these engagement points offered by Analytics Vidhya  the most popular continues to be our blog. Our blog continues to play an anchor in the way we have built this community. Thousands of people have benefitted from the resource provided on our blog. We are planning to step up our efforts in this direction today.We are extremely excited to announce the launch ofNow you might ask, why start a Medium publication?Simply because we believe that we can grow our community and its impact multi-fold by creating a channel where people can contribute in the same way they learnt from the platform.Medium offers seamless manner for a community run publication and we intend to run it that way. We want community members to come and share their knowledge so that everyone in the community benefits further.Here are a few things you can expect from our Medium publication:Excited? Here is what you need to do nextAs we launch this publication, we are thrilled to open the floor for budding and experienced writers who want to showcase their work to our broad and thriving community. Our aim has always been to spread data science knowledge and make it accessible to as many people as possible, around the globe.We have played an active role in this regard in the last 5 years, and continue to do so proudly. You bring the creativity and your passion for data science, and we now provide you with a platform to display it to the world.Some of the key benefits you will receive as a writer:Interested? Go ahead and fill this form for us.You can read more about what benefits we offer, and what we expect from you, in this detailed article.Wait, theres more..Yes, thats right  there is an opportunity to work with us in the capacity of an Editor! We are looking for Editors and Content creators who can drive our blog and content platform to the next level.Are you passionate about data science journalism? Do you have an itch to try and experiment with the latest tools, techniques and frameworks as soon as they come out? Can you take complex data ideas and explain them in simple easy to understand content? Do you love to share your knowledge and skill with the rest of the universe?Do you think your writing skills can enable and inspire the next generation of data scientists across the globe?If the answer to the above questions was yes, then you arewelcome to become the Founding member of AVs Editor Club.Apply here to submit your application and we will be in touch with you.Got questions? Head over to this article on Medium which covers a lot of what you might be wondering. If theres anything else, please go ahead and connect with us in the comments sectionSo, are you ready to answer the call and bring your A-game? This is a golden opportunity to collaborate and work with Analytics Vidhyas awesome team and launch yourself on a global platform. See you on AVs Medium page!",https://www.analyticsvidhya.com/blog/2018/08/analytics-vidhyas-medium-publication-av-editors-club/
DataHack Radio Episode #8: How Self-Driving Cars Work with Drive.ais Brody Huval,Learn everything about Analytics|Introduction|Brodys Background|Interest in Self-Driving Cars and Founding Drive.ai|The Challenges of Annotating Data|Camera Based Systems v LIDAR|Other Aspects of Drive.ais Self-Driving Cars|Techniques used in Self-Driving Cars|The Different Components of a Self-Driving Car Team|End Notes,"Share this:|Related Articles|Launching Analytics Vidhyas Medium Publication and AV Editors club!|How the Data Science Team at Netflix uses Jupyter Notebooks|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Self-driving cars are expected to rule the streets in the next few years. In fact, countries like the USA, China and Japan have already started using them in real-world situations! One of the leaders in this space is Andrew Ng backed Drive.ai, a self-driving car start-up based in California.So how do these autonomous cars work? How difficult is it making one from scratch? What kind of machine learning techniques are used? In this podcast, Drive.ais co-founder Brody Huval sheds light on these questions put forward by Kunal, along with other really intriguing facets of autonomous vehicles. Its a podcast you better not miss!Check out all the key takeaways from this really cool podcast below. You can check our the research paper Brody has co-authored on highway driving and deep learning (explained in the podcast) here. Happy listening!You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms:Brody completed his graduation in mechanical engineering from the University of Florida in 2010. Back then, AI had just started to become popular and enter the mainstream industry space. He became interested in the field and started applying to computer science programs in the United States.His application was accepted by Stanford in 2011 for their mechanical engineering program but Brody almost immediately switched to the computer science stream to work with Andrew Ng. His first couple of projects there were in deep learning and natural language processing (NLP). In total, he spent 4 years at Stanford doing various projects and research in machine learning and deep learning. Post that, in 2015, he co-founded Drive.ai along with fellow students from Andrew Ngs Stanford AI lab.Brodys interest in self-driving cars dates back almost 6 years ago to 2012. He and his fellow researchers worked on an autonomous driving project where they tried to replicate Googles work in this space. Google had used 16,000 CPU cores to create neural networks that watched YouTube videos all day in order to learn everything about autonomous cars. Brodys group decided to use 12 GPUs instead to replicate the power of those 16,000 CPUs.At the end of this project, the team was left with a cluster of approximately 64 machines (each with 4 GPUs). Brody and his team knew at this point that they wanted to work with tons of data, and eventually they landed on the idea of self-driving cars. They would use a camera-only approach, along with other hardware equipment to automatically annotate the data. In short, they were looking to solve a meaningful problem with tons of data, and thats how Drive.ai was born.Unsurprisingly, it took a significant amount of time to label the data! For lanes, they had a special GPS unit using which they could map a path of where the car was driving. Based on this route, the team could understand how far away the lanes were from the car.For dynamic obstacles like pedestrians, Brody set up a mechanical pipeline and thats when he faced the difficulties of annotating data manually. This process involved a lot of logistics and other overhead, something only folks who have set up a ML project from scratch will truly understand.The initial testing produced mixed results. Because they had collected a ton of highway driving data, the testing on highways went pretty well. But when it came to urban areas, the system did not perform as well because of a lack of proper data. One of the biggest weak points during their testing process was the huge number of false positives regarding gauging the side of the road.After trying out various camera based approaches, the Drive.ai team started exploring LIDAR and RADAR based solutions. They tested out different sensors with the aim of getting a much better and improved precision and recall for their system.Brody mentioned that he believes camera based approaches will definitely get better in urban areas with time. You need a lot of data to understand all the nuances of the various images the cameras collect, and thus it comes down to how much computational power you have.This section was a really insightful explanation of LIDAR, RADAR and camera sensors. If youre interested in self-driving cars, make sure you listen to this part especially carefully.Right now, Drive.ai has 7 fully functional self-driving cars on the streets in Texas which can service up to 10,000 people. They have been set up to try and solve the micro-transit problem, which means distances that are too far for walking but too close for driving. This is especially useful in Texas where it can get blisteringly hot during the summer months.Some of the challenges Brody and his team currently face are with perception and performance. When he mentions perception, it refers to understanding where certain objects are placed, and how the system goes around these dynamic objects given the uncertainty in predictions.One of the ways of dealing with constantly changing scenarios (like re-painting roads, or avoiding construction areas) is to be in touch with the local government and understanding well in advance what changes are expected to happen. Another option Drive.ai have explored is tele-operators, who operate through networks to guide the car using different routes.Rain is also a major problem for LIDAR units, while cameras are not at their best during the night. These are acknowledged weaknesses that AI has not been able to fully solve so far. Currently Drive.ais cars operate during daylight and if theres inclement weather, the company pauses the service until it clears up.Simulators are also becoming a major part of any self-driving car setup. They help the team understand where certain things are going awry, or how long a certain scenario takes to run, etc. These are all machine learning problems.There are certain aspects where classic machine learning or deep learning techniques fit. For example, deep learning algorithms work really well in the perception facet (like geospatial data) of these cars. The motion planning system, on the other hand, is a combination of classic ML and learned ML.Brody made a great point about how reinforcement learning, as powerful as it is, hasnt made as many advancements as deep learning. Its getting better and the Drive.ai team do experiment with it, but to use it in a real-world use case is not a feasible option right now.Below are the general components Brody listed down which go into making a full-fledged self-driving car project:An absolute goldmine for all self-driving car enthusiasts! There are a ton of details in the podcast about these cars  how theyre deployed, how theyre made, the technical machine learning aspects that go into designing these cars, the major obstacles every startup in this space faces, Brodys take on how to overcome them, etc. I personally found the camera sensors v LIDAR section particularly insightful.",https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-8-how-self-driving-cars-work-with-drive-ais-brody-huval/
How the Data Science Team at Netflix uses Jupyter Notebooks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Data Access|Notebook Templates|Scheduling Notebooks|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #8: How Self-Driving Cars Work with Drive.ais Brody Huval|Salesforce Open Sources TransmogrifAI  An AutoML Library that will Change the way you do Machine Learning!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Jupyter notebooks are powerful and compelling tools, and have become ultra popular among data scientists. I have personally tried out a ton of different IDEs from Spyder to RStudio, but I keep coming back to Jupyter Notebooks because of how neat, multi-faceted, and easy-to-use the whole environment is. In fact, I wrote an entire article on how to get started with them!So when the data team at Netflix posted a blog about how they use these wonderful notebooks, my interest was piqued. In this AVBytes article, I have briefly summarised their different use cases for using Jupyter (if you are interested in reading their entire post, the link is at the bottom of this section).Netflixs use of Jupyter notebooks can broadly be classified into three use cases:Notebooks were initially used at Netflix with the aim of supporting the different data science workflows. But as their popularity grew, the team realised there were a ton of benefits to reap by extending their usage for general data access.According to the team, they started this transition in Q3 2017. They also created and actively maintain a Python library that consolidates access to platform APIs. This enables their users to have access to the entire platform from within their own notebook!Once the team started expanding the use of these notebooks, brand new tasks were created to meet the different use cases. Out of this whole operation came parametrised notebooks which basically allow you to specify parameters in your code blocks and provide input values while the code is running.Data scientists use it to run experiments with different parameters, data engineers use it for part of the deployment process and data analysts use it perform queries and visualizations.Netflix uses notebooks as a unifying layer for scheduling workflows. This helps bridge the gap between constructing an entire workflow and getting that into deployment. When a Spark job in executed, the source code is sent into a brand new notebook and run there. This notebook essentially becomes an archive which can be referred to during the troubleshooting process.If youre curious about how the data science team is structured at Netflix, below are the different roles there:You can check out Netflixs blog post in full here.Netflix have emerged as a leader in how to scale up data science workflows when working with copious amounts of data. Their blog post contains a lot more detail about other aspects of their data science process so make sure you go through it.Coming to Jupyter notebooks, I cannot stress enough on how awesome they are. If you havent used them yet (where have you been?), you should give them a try immediately. They transformed the way I code and have saved me a lot of time and headache with their really useful shortcut functions.",https://www.analyticsvidhya.com/blog/2018/08/how-the-data-science-team-at-netflix-uses-jupyter-notebooks/
Salesforce Open Sources TransmogrifAI  An AutoML Library that will Change the way you do Machine Learning!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|How the Data Science Team at Netflix uses Jupyter Notebooks|TensorFlow 2.0 Announced! Here are the Top Highlights of Whats Coming|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Salesforce is one of the leaders in the analytics/data science space. Automated machine learning (Auto ML) is easily the hottest tool in that industry. When you combine them both, you get a game-changing experience that has the potential to disrupt the way businesses approach machine learning projects!Einstein AI is Salesforces flagship ML platform that is powered by their ownTransmogrifAIlibrary. It is an automated machine learning library, written in Scala and running on top of Spark, built for dealing with structured data. It is used by Salesforce to build machine learning solutions for production-ready scenarios, in a scalable and time efficient manner. If youre wondering how its pronounced, Salesforce says itstrans-mog-ri-phi.In a blog post by Shubha Nabar, Senior Director, Data Science, she shared the general workflow ofTransmogrifAI, its advantages, how YOU can use it for your individual use case (or your business), and also elaborated on the wayTransmogrifAI has been designed. It really is an impressive solution to building enterprise-level ML systems, a process way more difficult than what other blog posts would have you believe!The below image illustrates the workflow ofTransmogrifAI:Their GitHub page (link below) has links to documentations and Wikis to help you get started with the library. It even includes a small code example run on the popular Titanic dataset. In just a few lines of code, the model gave an accuracy of 85%, an impressive start. And dont worry, you dont lose interpretability at any point TransmogrifAI lets you manually specify which features you want, the algorithm you prefer to run, etc.To install it on your machine, follow the below steps, as mentioned on TransmogrifAI home page:Excited yet? You can get started withTransmogrifAI NOW by heading over to the librarys GitHub page and downloading the code.A game changer in ML? Quite possibly. With Auto-Keras being launched last week, and now this, open-source Auto ML is having its day in the sun right now. Unlike Auto-Keras (which is incredibly useful for deep learning by the way), TransmogrifAI has a bunch of examples, proper documentations and a detailed wiki of terms (what else did you expect from an industry leader like Salesforce?).Its inspiring to see market leaders take the initiative and enable the entire community with powerful libraries. I will definitely be using this for my next hackathon, and encourage you to do the same!",https://www.analyticsvidhya.com/blog/2018/08/salesforce-open-sources-transmogrifai-auto-ml-library/
TensorFlow 2.0 Announced! Here are the Top Highlights of Whats Coming,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Salesforce Open Sources TransmogrifAI  An AutoML Library that will Change the way you do Machine Learning!|Top 7 Sectors where Data Science can Transform India (with Free Datasets)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow is one of the most popular and widely used machine learning frameworks in the industry. While it has its limitations and PyTorch has definitely given it a run for its money, TensorFlow has embedded itself into many a product, and continues to do so unabated.Developed and pioneered by Google, TensorFlow v1.0 was launched last year and since then we have seen many releases of this library. Each version has been incrementally better than the previous one, with a number of improvements and bug fixes.v2.0 has been eagerly awaited for quite a while now and we finally have some good news for you. TensorFlow 2.0 is officially in the works, and Google has released the first details around it this week.As mentioned by one of the Google Brain Engineers, Martin Wicke, here is what we can expect from TensorFlow 2.0:Wicke also mentioned that TensorFlow 2.0 needs to go through a public review process and hence a series of public design reviews (that will cover the planned changes) will be conducted. After the process, the community can give feedback and propose changes for the version.In order to simplify the transition for current users, a conversion tool will be created which updates the codes in python to use TensorFlow compatible APIs. Otherwise, it will issue a warning in case the conversion is not automatically possible.Another major change as a part of TensorFlow 2.0 will be that the distribution to tf.contrib will be ceased. For each currently existing contrib module, some will be integrated into the core project or moved to a separate repository while others will simply be removed. For any queries you can get in touch with the team directly by at [emailprotected]. You can also subscribe to [emailprotected] for regular updates.Exciting news! Like most of the ML community, I have been waiting for this version for a while now. While the developer team hasnt exactly announced too many features yet, its still something to go on.I personally like the conversion tool that will be bundled with the release. It proved to be extremely helpful at the time of the the version 1.0 release as well.",https://www.analyticsvidhya.com/blog/2018/08/tensorflow-2-announced-what-you-can-expect/
Top 7 Sectors where Data Science can Transform India (with Free Datasets),Learn everything about Analytics|Introduction|Tables of Contents|Agriculture|Electricity|Water|Healthcare|Education|Traffic/Road Accidents|Air Pollution Levels|End Notes,"Resources|Resources|Resources|Resources|Resources|Resources|Resources|Share this:|Like this:|Related Articles|TensorFlow 2.0 Announced! Here are the Top Highlights of Whats Coming|DataHack Radio Episode #7: Tackling Data Science Challenges in India with NITI Aayogs Dr. Avik Sarkar (Independence Day Special!)|
Pranav Dar
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data science is a powerful tool and has the ability to transform the world. We are already seeing massive changes to the way industries work in the Western world and data science is powering that change.But when it comes to India, we are still a long, long way from reaching that stage. As Dr. Avik mentioned in his insightful DataHack Radio podcast, most of the data we collect is highly unstructured and difficult to make sense of. But optimism abounds  there is a growing expectation that people are realizing how crucial data is to the economy.In this article, we look at the top sectors in this wonderful nation of ours that are ripe for applying data science. We have also provided resource links for each sector with the hope that our AV community can take up the challenge and make this country a better place, all with the help of data science!The agriculture industry needs the use of data science more than any other right now. 40% of our population is employed in this field but unfortunately, agricultures contribution to the nations economy is a paltry 16% of the overall GPD. Given how critical this sector is, should that number not be significantly higher?There are a lot of facets in agriculture that can be worked upon  predicting monthly/quarterly/annual yield, forecasting demand, analyzing weather patterns to decide when to sow, predicting the prices of vegetables so as to pick which crop to sow, etc.Open datasets on agriculture: There are three datasets on this page  two are monthly and one is annual. These deal with the stock of different food grains in a year, the production of these food grains, and the central statistics of food and beverages.Dataset on the crop production in India. Its a fairly straightforward dataset but excellent for producing visualizations and basic insights.Rainfall in India dataset. Another crucial aspect to agriculture, and one that decides the livelihood of farmers. Predicting rainfall is essential to farming, and with this dataset, you can do just that! It contains monthly rainfall data from 1901-2015.This article, written by Shweta Gupta, is an excellent resource on the state of the agriculture industry in India, challenges that we face right now, and how we can use data to improve it. It should be a mandatory read for all Indians who are into data science, its just that important!The average electricity use in India during the 2016-17 FY was a staggering 1,122 kWh per capita. Out of this, the industrial consumption was 40%, followed by residential consumption at 24%. This is all to say that the power demand is surging beyond expectations as the population increases year-on-year.Predicting power supply and demand, understanding the consumption pattern of households, classifying this by region/district/blocks, etc. are just some of the ways we can use data science in this sector. The resources I have mentioned below are enough to get you started and even go beyond that.A collection of open datasets by the Government. Datasets on pattern of electricity consumption, per capita consumption, consumption by sectors, and more are available here. Check it out!This Wikipedia article has up-to-date statistics on the electricity sector in India. This should be a compulsory read for all Indians, from students to working professionals. It is an eye opener to the fact that we are consuming power at a never-before-seen rate. It also contains granular details about rural areas. If you know even the basics of web scraping, this page is a goldmine.Dataset on individual household electric power consumption. While this isnt strictly Indian data, it sheds light on the kind of data we do need to collect in the first place. I encourage you to download this dataset, play around with it and come up with solutions as to how we, as a community, can utilize and maximize power consumption to our benefit.The most critical resource of all, and one of the most misused in India. It seems we see a drought every summer in quite a lot of rural areas, and the situation does not seem to be improving. The water usage is increasing each year and unless we properly assess the usage, it could end up turning into a crisis very soon.You can predict things like the predicted water level, the usage in certain areas in order to send adequate water supply tanks there in time, etc. You can come up with more ideas as you think about the challenges in this sector.Open datasets from the Central Ground Water Board. This contains granular information about water levels in every district in India. The variables it includes are the district name, latitude and longitude, type of site, the year the data was observed, and the water level during, after and before monsoons. Its a good place for you to understand the kind of data collected by the Government, and even work on it on your own!Open datasets on water quality in 2014. Plenty of datasets to download and work with here. It is available for different states so pick and choose as per your interest.Did you know that the Indian constitution guarantees free healthcare for all citizens? And thats the practice Government hospitals follow, at least for those who are below the poverty line. But the truth is that the private healthcare sector takes care of the majority of the healthcare business in the country. With the amount of people populating government hospital, it is not easy to get proper attention there.Which is why people who can afford it tend to turn to the private hospitals. They prefer paying from their own pockets than putting themselves through the rigors of a government hospital. According to Wikipedia, 58% of the hospitals in India are private along with a mind-boggling 81% doctors.The current infrastructure is just not good enough to handle the growing demands and the surging population. This is where data science can step in and ease the burden. Predicting things like how many days will a patient be admitted so as to calculate the proper allotment of beds, child mortality rate, heart issues, diabetes, etc. are some of the points you can work with for starters. The NITI Aayog initiative is already working on quite a lot of these points.Dataset on key indicators of annual health survey. These are survey results for nine Indian states from 2012-13. It is a very comprehensive dataset and contains 1,287 columns. If you are serious about analyzing and working with Indian healthcare data, this is as good a place as any to start.Multiple datasets on the governments data site. If you wish to analyze the state of healthcare at a more granular level, check out this link. It contains all sorts of information about the various aspects of healthcare, from OPD attendance to the comparison of various health indicators around the country.Datasets curated by the World health Organization. This is a treasure trove of data on healthcare in India, collected by WHO. It contains datasets on infant mortality rate, life expectancy at birth, hospital beds, etc.The state of education in India is appalling, to say the least. While more Indians are enrolled in schools than ever before, they are not really being educated. Outside the cream of the crop private schools, there is no proper structure, focus or attention given to the majority of children in rural areas.Almost 95% children have enrolled in primary school, 69% in secondary and a shockingly meager 25% in post secondary. Where is it all going wrong? Why cant one of the biggest school systems in the world improve upon this? What is the expected years of schooling education?Using data from national surveys, you can analyze and try to find answers to these pressing questions. As with any data science project, curiosity will help you a lot. This is a field thats very close to me so any progress, however minor, has the potential to start a ripple effect.Comprehensive district-level dataset. This is a really detailed dataset covering the length and breadth of report card information categorized by district. It contains 439 columns with zero milling values. What a great place to begin!Open datasets from the government. These are not so neat and tidy. You will require a bit of preprocessing and research to work with these properly, but they highlight the true nature of education here, including data on teaching staff and education loans.Ah, one of the most frustrating things we encounter on an almost daily basis. As more and more people flock to metro cities, the state of traffic on the roads is getting worse. Long traffic jams are an accepted part of our lives, but should they be? The NITI Aayog team is working on understanding why this happens, and how to deal with them.Aspects like choke points, narrow or broken roads, lack of traffic personnel, and failure of traffic lights, are just some of the features you can look at when trying to solve this problem. Cities like Kuala Lumpur and Toronto are already being converted into smart cities, with CCTV cameras and sensors everywhere to monitor traffic and imediaetely solve the problem.India is a fair way off that, though we saw earlier this year how the Kolkata police is trying to use Google Maps with the aim of dealing with long jams.Another aspect of road transport is the number of accidents on the road. India records some of the worlds largest road fatalities every year. According to an Economic Times article, more than 150,000 people are killed in these accidents every year! This is a terribly distressing number and I hope data science can be used to analyze patterns and take immediate action on this.Datasets on road accidents. These are quite a few in number and cover features like accidents due to intake of alcohol/drugs, overspeeding, over crowding, over loading of trucks, etc.Accidents in India by month (2001-2014) dataset. You will need to carefully import this data but its a good starting point for analyzing and extracting any patterns, if you can.Traffic Data. Unfortunately theres no single resource that contains the traffic data for India. Thankfully there are simple ways to get it. You can head over to Google Maps and export the data in a jiffy. Check out this article which explains how to do it. Once you download it, you can get to analyzing where the traffic jams regularly occur, at what time that happens, and can come up with ways to mitigate it. The possibilities are vast and making your city a smart one is now in your hands!Anyone with access to news will be aware how bad the air pollution levels are in certain parts on India. It is beyond the out of hand stage. Despite taking precautions and trying out different measures, the pollution level has not really come down to a manageable state.According to a WHO report from 2016, 11 of the top 12 most polluted cities come from India (Kanpur leads the way). The Environment Performance Index ranked India 141 out of 180 nations. All this is to say that the problem is grave, and we need a permanent solution to this in double-quick time.Variables like crop burning, pollution from vehicles, industry fuel and biomass burning, etc. are major contributors to the alarming rise in air pollution. While there have been recent studies done using data science on the topic, none have so far been able to bring down the numbers.Air Quality Data for India. This contains historical data on Indias air pollution levels and has spawned many a projects. Its a brilliant starting point for anyone looking to work with this kind of data.Daily Ambient Air Quality Data. Coming from the Government itself, this is a location wise dataset measuring the air quality in 2015. You can also check out their entire catalog on air pollution here if you so wish. Its a little unstructured so patience is key!Air Quality Info Site. A really cool website displaying the different statistics associated with air quality indices in India. It has forecasts for daily, monthly, and hourly numbers. Bookmark this site!The resources I have mentioned here are enough to get you started in each sector. There are other datasets and resources out there which you can get your hands on to practice more. There are government sites where you can request more data, if required.There is so much scope for improving each of these sectors with the help of data science. Im looking forward to our community making a huge impact (in a positive way of course!) soon! If there are any other datasets you are aware of and want to share with the community, please feel free to do so in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/top-7-sectors-where-data-science-can-transform-india-with-free-datasets/
DataHack Radio Episode #7: Tackling Data Science Challenges in India with NITI Aayogs Dr. Avik Sarkar (Independence Day Special!),Learn everything about Analytics|Introduction|Dr. Aviks Background Story|Details about his masters thesis|Data Science at NITI Aayog|The Challenges of Data Collection in India|Tools/Languages used by Dr. Aviks Team|Indias Current and Future State of AI|End Notes,"Share this:|Like this:|Related Articles|Top 7 Sectors where Data Science can Transform India (with Free Datasets)|DeepMinds AI System Identified over 50 Eye Diseases as Accurately as a Doctor|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data science is still a very nascent field in India, despite the recent surge in interest. From agriculture to healthcare, there are a plethora of challenges the Government faces on a day-to-day basis, and that was the primary reason for founding a data science department under the NITI Aayog initiative. On this Independence Day, we thought what better way to acquaint our community with these challenges and how our Goverment is using data science to tackle them, than bring NITI Aayogs Head of Data Science straight to you?It was a thrilling experience to have one of Indias foremost data science leaders, Dr. Avik Sarkar, on our DataHack Radio podcast. He is an eloquent speaker and he talked about various topics, from his love of mathematics to his masters and Ph.D thesis techniques. He also provided details about the work performed by the data science team under the NITI Aayog initiative, a must-listen for all Indians.In this article, we look at the top key points Dr. Avik made during his conversation with Kunal. Happy listening!You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms:Dr. Aviks penchant for numbers can be traced back to his childhood. He was interested in mathematics since his school days and that led him to do his bachelors in statistics and masters (from IIT-Bombay) in applied statistics and informatics. He also holds a Ph.D in computer science and statistics. As you can surmise from this, he was the perfect candidate for data science!Before he joined NITI Aayog as the Head of the Data Science cell, Dr. Avik worked in senior roles at companies like Accenture, IBM and Nokia Siemens, among others. A trend emerges when you look at his profile  he worked with data well before data science become a buzzword, and thus has a very strong background in this domain.When Dr. Avik was learning and working on Artificial Intelligence, it was a different experience to how we see AI these days. This is what he had to say about how quickly the world of data science, machine learning and AI is advancing:In this domain, learning new things is something you have to do every year. Its a rapidly evolving field  new technologies, new platforms and new coding languages come every year, so getting acquainted with these is very important.The subject of Dr. Aviks masters thesis was around multi-topic text classification. He took this up because it was an important topic due to the hierarchical information arrangement that was prevalent at that time (early 2000s). The main aim of the hierarchy was to arrange whatever text data you had into categories  it could be news articles, blogs, etc.The internet was getting democratized as more and more Indians (and global users) started getting online in the late 90s/early 2000s. So, suddenly we went from seeing a few editors putting content online to a plethora of writers gaining access to the internet. The amount of content spiked, nothing close to what we see now, but enough to ensure that one could not manually categorize the articles into a hierarchy.Dr. Avik saw a need for an automatic classification system that would identify these topics and put them into a hierarchy model. The more challenging problem, which he took up, was that some articles might be relevant for multiple topics.His Ph.D was in text mining and statistical modeling on text distribution. If you are interested in NLP, do listen to this section where Dr. Avik explains why and how he took up this topic.He discusses the nuances of various techniques he used and how they helped him build up his study. It makes for fascinating listening!We are trying to make sense of the operational data to get a good picture about the state of the economy.The data science team at NITI Aayog, as Dr. Avik put it, is more of a horizontal organization. The type of analytics he and his team perform are vast in nature. Even though he had over fifteen years of experience working with data prior to joining the Government, this was an almost new body of work for him.There is a lot of simulation and scenario modeling that they need to perform. He gave some really intuitive examples of how the team thinks about certain industries (like oil and automobiles), and the variables to consider when forecasting production and manufacturing. This qualifies as long term forecasting.The team also uses analytics for short-term challenges as well, which are operational in nature. For example, malnutrition is a major problem in India (and has been for decades). They extract insights about which districts need more funds to deal with the issue and this has helped the people on the ground.There are other aspects where data science is helping the government tackle long standing challenges. Taking the example of surveys, Dr. Avik explained how there is a lag of 2-3 years between initiating surveys and finally extracting meaningful insights from them. His current team at NITI Aayog is trying to do more of a real-time analysis of these things, especially critical fields like healthcare, education and agriculture.80% of my day goes in phone calls!Data collection, as Kunal pointed out, would be a major obstacle for Dr. Aviks team. All of the things they are doing are fairly new from an Indian perspective and nothing so far has been done in a systematic or structured manner. As the above quote summarized, he spends most of his day trying to convince people to share their data.Often there are data quality issues. Since most of the data is operational, people assume it might not be used anywhere and hence its stored in a very unfocused manner. A lot of the fields need to be dropped because of the serious gaps in data quality. The hope is that with time, as Dr. Avik continues his work, departments will soon realize the need to properly store this data.A lack of data also inevitably leads to biases in the model you build. Unfortunately this is a problem India faces in almost all sectors. Mitigating these issues has become a big challenge as well and Dr. Avik pointed out this is the biggest obstacle he had to deal with.For energy modeling, a long term initiative (takes up to 1-2 years), Message Models and Times Markel Model are the teams tools of choice. For generating visualizations and dashboards to be shared with state governments, the team uses popular tools like:Different countries have their unique challenges when it comes to adopting AI. For India, Dr. Avik believes its the obstacle of inclusion, or AI for all. This is what his team is piloting throughout the country.Taking the example of healthcare, he explained how automating certain parts of a nurse and/or doctors job will help cut down on the time it takes to make a diagnosis, as well as spread the benefits of healthcare to rural places. Intriguing is the only word I can think of to describe the task Dr. Avik and his team are dealing with.The podcast also includes details about how the team is working on certain agricultural issues throughout the country. This includes factors like yield, fertilizer, weather patterns, etc. Where does all the data comes from, you ask? Most of it is collected through satellite imagery and then broken down to analyze and extract certain patters. This helps them inform the farmers 2-3 weeks in advance that, for example, potato prices are going down so dont sow potatoes for now.If you are a data scientist (or aspiring to be one) working in India, this podcast is like a treasure trove of information. This article only covers the very key takeaways  there are a ton of awesome nuggets in the podcast you are sure to find useful, like the reception to data science at NITI Aayog, the ranking system Dr. Aviks team has pioneered, etc.The power of data science isnt just limited to research labs and big tech companies. It was truly inspiring to hear the different issues Dr. Aviks team is trying to solve. I hope to see our community leverage data science for good causes in the foreseeable future.  And of course, happy Independence Day to everyone!",https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-7-dr-avik-sarkar/
DeepMinds AI System Identified over 50 Eye Diseases as Accurately as a Doctor,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #7: Tackling Data Science Challenges in India with NITI Aayogs Dr. Avik Sarkar (Independence Day Special!)|Your Code Leaves Fingerprints, and Machine Learning can now Identify it|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The field of healthcare has been inching towards automation in recent years. In fact, this year itself has seen some groundbreaking studies  Predicting psychosis, detecting damage in knee joints, detecting cancer, among others. At the heart of these studies? Machine learning, of course.Given the amount of data points that are generated each minute in this industry, its not surprising to see the amount of research being done. The latest comes from London-based DeepMind, and it tackles the ever-growing problems of eyesight.Their AI system can interpret eye scans from routine tests with an accuracy that will blow your mind. The system is so advanced that it can recommend how patients should be referred for treatment for over 50 eyesight related diseases. And it does all this as accurately as renowned doctors. Score one for the machines!In a detailed blog post, DeepMinds researchers have laid down the approach they used to come up with their AI system. There are two neural networks are play, which are combined by the system. The first neural network (called the segmentation network) analyses the scan to come up with the features of the disease(s) it sees, likehaemorrhages, lesions, irregular fluid, etc.The second neural network (called the classification network), analyzes he results of the first neural network to present healthcare professionals with diagnosis and recommendations. Check out the below infographic DeepMind created to illustrate the difference between the current process and their own AI system:The sheer number of scans that healthcare professionals process during the day is in the thousands and the aim of DeepMinds AI system is to cut that down significantly. Not only does it automatically detect the features of eye diseases in a matter of seconds, it also prioritizes patients who need urgent treatment. Who said machines are bad for humans?As always, DeepMind are at the forefront of breakthrough technology. But the next challenge for them is to convert this into a practical product which can be distributed to hospitals and clinics. As I mentioned above, their partnership with Moorsfield has yielded excellent results so that should encourage others to take it up.Coming to the approach used, notice how they have made the entire system interpretable. There is no black box associated with the neural networks used. The entire algorithm is transparent so that the results can be explained to patients. Its truly a milestone research.",https://www.analyticsvidhya.com/blog/2018/08/deepminds-ai-identified-over-50-eye-diseases-accurately-doctor/
"Your Code Leaves Fingerprints, and Machine Learning can now Identify it",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DeepMinds AI System Identified over 50 Eye Diseases as Accurately as a Doctor|Complete tutorial on Text Classification using Conditional Random Fields Model (in Python)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing is a challenging field because of how unstructured the data within it is. Finding and analyzing hidden patterns among the noise is where a data scientist earns the big bucks.Recent progress in this field has included identifying the names of authors of wrote a certain piece of literature. This has been automated to quite an extent but how can we apply this to a programmers code? Its also a collection of text and numbers, albeit in a very different manner.A couple of researchers from Drexel University and the George Washington University have revealed (in this brilliantWIRED article) that code, just like literature, can also be analyzed to identify and pinpoint the author. They will be presenting their work at the DefCon hacking conference later this week.So how did these researchers design their system? First, the features present in samples of code are identified by the algorithm. The two researchers then narrowed the features to only include those ones which helped them distinguish individual developers. This cut down the number of features significantly.The researchers created an abstract syntax tree which was used to recognize the codes underlying structure. As you can imagine, the algorithm requires a few examples/samples to train. In this research paper, the researchers along with others showed that its possible to identify the programmer using just their compiled binary code.The researchers picked up code samples from Googles annual Code Jam competition to test their algorithm. It achieved an impressive 96% accuracy when analyzing 100 individual coders (each had eight code samples). But the accuracy dropped a bit to 83% when the number of programmers was increased to 600.The curious (though not altogether surprising) finding was that it was far easier to recognize experienced programmers from their code, as compared to newcomers. I imagine this must be because of the number of samples present plus the fact that each programmer must embed his/her own unique style in each piece of their code.I previously covered DeepCodes efforts to clean up a programmers code, but this latest project in a whole different beast. It tackles a variety of problems, like plagiarism and identity theft. It could also help in cyber security by identifying who created a specific piece of malware.I believe we are still a fair bit away from seeing this algorithm being used in practical scenarios given how complex the problem is. Lets wait and watch where this study leads us in the near future.",https://www.analyticsvidhya.com/blog/2018/08/machine-learning-identify-author-code-deanonymize/
Complete tutorial on Text Classification using Conditional Random Fields Model (in Python),Learn everything about Analytics|Introduction|Table of contents|What is Entity Recognition?|Case Study Objective & Understanding Different Approaches|Formulating Conditional Random Fields (CRF)|Annotating training data|Building and Training a CRF Module in Python|End Notes|References,"Annotations using GATE|About the Author|Share this:|Like this:|Related Articles|Your Code Leaves Fingerprints, and Machine Learning can now Identify it|18 Minutes, $40  Fast.ais Algorithm Beat Googles Model in a win for Every Data Scientist!|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The amount of text data being generated in the world is staggering. Google processes more than 40,000 searches EVERY second! According to a Forbes report, every single minute we send 16 million text messages and post 510,00 comments on Facebook. For a layman, it is difficult to even grasp the sheer magnitude of data out there?News sites and other online media alone generate tons of text content on an hourly basis. Analyzing patterns in that data can become daunting if you dont have the right tools. Here we will discuss one such approach, using entity recognition, called Conditional Random Fields (CRF).This article explains the concept and python implementation of conditional random fields on a self-annotated dataset. This is a really fun concept and Im sure youll enjoy taking this ride with me!Entity recognition has seen a recent surge in adoption with the interest in Natural Language Processing (NLP). An entity can generally be defined as a part of text that is of interest to the data scientist or the business. Examples of frequently extracted entities are names of people, address, account numbers, locations etc. These are only simple examples and one could come up with ones own entity for the problem at hand.To take a simple application of entity recognition, if theres any text with London in the dataset, the algorithm would automatically categorize or classify that as a location (you must be getting a general idea of where Im going with this).Lets take a simple case study to understand our topic in a better way.Suppose that you are part of an analytics team in an insurance company where each day, the claims team receives thousands of emails from customers regarding their claims. The claims operations team goes through each email and updates an online form with the details before acting on them.Source: mugo.caYou are asked to work with the IT team to automate the process of pre-populating the online form. For this task, the analytics team needs to build a custom entity recognition algorithm.To identify entities in text, one must be able to identify the pattern. For example, if we need to identify the claim number, we can look at the words around it such as my id is or my number is, etc. Let us examine a few approaches mentioned below for identifying the patterns.The bag of words (BoW) approach works well for multiple text classification problems. This approach assumes that presence or absence of word(s) matter more than the sequence of the words. However, there are problems such as entity recognition, part of speech identification where word sequences matter as much, if not more. Conditional Random Fields (CRF) comes to the rescue here as it uses word sequences as opposed to just words. Let us now understand how CRF is formulated.Below is the formula for CRF where Y is the hidden state (for example, part of speech) and X is the observed variable (in our example this is the entity or other words around it). Broadly speaking, there are 2 components to the CRF formula: Now that you are aware of the CRF model, let us curate the training data. The first step to doing this is annotation. Annotation is a process of tagging the word(s) with the corresponding tag. For simplicity, let us suppose that we only need 2 entities to populate the online form, namely the claimant name and the claim number.The following is a sample email received as is. Such emails need to be annotated so that the CRF model can be trained. The annotated text needs to be in an XML format. Although you may choose to annotate the documents in your way, Ill walk you through the use of the GATE architecture to do the same. Email received:Hi, I am writing this email to claim my insurance amount. My id is abc123 and I claimed it on 1st January 2018. I did not receive any acknowledgement. Please help. Thanks,randompersonAnnotated Email:<document>Hi, I am writing this email to claim my insurance amount. My id is <claim_number>abc123</claim_number> and I claimed on 1st January 2018. I did not receive any acknowledgement. Please help. Thanks, <claimant>randomperson</claimant></document>Let us understand how to use the General Architecture for Text Engineering (GATE). Please follow the below steps to install GATE.Once the installation is complete, you are ready to train and build your own CRF module. Lets do this!Lets define and build a few functions.Now we will import the annotated training data.Generate features. These are the default features that NER algorithm uses in nltk. One can modify it for customization.Now well build features and create train and test data frames.Lets test our model.You can inspect any predicted value by selecting the corresponding row number i.Check the performance of the model.Print out the classification report. Based on the model performance, build better features to improve the performance.By now, you would have understood how to annotate training data, how to use Python to train a CRF model, and finally how to identify entities from new text. Although this algorithm provides some basic set of features, you can come up with your own set of features to improve the accuracy of the model.To summarize, here are the key points that we have covered in this article:Sidharth Macherla  Independent Researcher, Natural Language Processing",https://www.analyticsvidhya.com/blog/2018/08/nlp-guide-conditional-random-fields-text-classification/
"18 Minutes, $40  Fast.ais Algorithm Beat Googles Model in a win for Every Data Scientist!",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Complete tutorial on Text Classification using Conditional Random Fields Model (in Python)|Independence Day Bonanza with Analytics Vidhyas Offers and Launches!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anyone familiar with deep learning has heard of Fast.ai. Its an open platform built by Rachel Thomas and Jeremy Howard with the aim of teaching coders to pick up deep learning concepts through a series of videos.And now students from Fast.ai, along with Jeremy, have designed an algorithm that has outperformed Googles code according to the DAWNBench benchmark. This is a very popular benchmark that measures the training time, cost and other aspects of deep learning models. In this particular case, the Imagenet database was used by the Fast.ai researchers.The researchers managed to train their model on Imagenet to a 93% accuracy is an impressive 18 minutes. The hardware they used, detailed in their blog post, was 16 public AWS cloud instances, each with 8 NVIDIA V100 GPUs. They built the algorithm using the fastai and PyTorch libraries.Fast.ai entered Stanfords DAWNBench competition in the first place because they wanted to show the community that you dont need to have tons of resources to drive innovation. In fact four months ago, a bunch of researchers from Fast.ai performed very impressively on both the CIFAR-10 and Imagenet datasets, coming second only to Googles TPU Pod cluster (which is, unsurprisingly, not available to the general public).So this time around, Jeremy and his team used multiple publicly available machines. The total cost of putting the whole thing together came out to be just $40!Jeremy has described their approach, including techniques, in much more detail here.Why is Fast.ais win so important? Because it dispels the notion (at least for now) that you need tons of computational power to build workable deep learning models. There is a perception that the likes of Google, with their almost unlimited access to GPUs and TPUs, are the only ones who can truly rule the roost when it comes to ML innovation.But the question is how does this translate into real world scenarios. Its great to see algorithms outperforming other competitors but until that can be put into a practical use case, it just remains stuck in the research stage. Im curious to see how Jeremy and his team plan to utilize this.",https://www.analyticsvidhya.com/blog/2018/08/fast-ais-algorithm-beat-googles-code-in-a-popular-image-recognition-challenge/
Independence Day Bonanza with Analytics Vidhyas Offers and Launches!,"Learn everything about Analytics|Introduction|Podcast with Dr. Avik Sarkar, Head of Data Science Cell, NITI Aayog|60% off on all training courses!!|Flat Rs. 1,000 off on DataHack Summit 2018 tickets!|Resources and Ideas on how Data can help us solve problems in India|Launch of AV Editors Club","This podcast will be released on the evening of August 14th on all the below platforms:|Simply use the code IDAY60 when you checkout to avail this offer on all the below courses:|Introduction to Data Science|Computer Vision using Deep Learning|AI and ML for Business Leaders|Microsoft Excel: Beginners to Advanced|To get flat Rs. 1,000 off on DHS 2018 tickets, use the code IDAY1000.|Share this:|Like this:|Related Articles|18 Minutes, $40  Fast.ais Algorithm Beat Googles Model in a win for Every Data Scientist!|Perform Automated Time Series Modeling with DataRobots Latest Tool|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"India is celebrating its 72nd Independence Day and so are we! Its day to celebrate our past, enjoy the present and be optimistic about the future.What would be the best way to do this? The best way is to build things for future and enable more and more people to do so. Thats what we will be doing this Independence Day.Let me tell what is in store.Dr. Avik Sarkar is the Head of the Data Science Cell as part of the Indian governments NITI Aayog initiative. He is involved in the effective use of artificial intelligence, big data and data science techniques for social good in areas of real-time governance, healthcare, agriculture monitoring, education, energy, etc.This exclusive podcast will feature Kunal in conversation with Dr. Avik about the state of the data science field in India, what and how NITI Aayog is using data science to work on various challenges in multiple Indian sectors, among other things.Happy listening!We will offer incredible deals on our existing as well as upcoming training courses.It all kicks off on 11th August midnight. Check them out.Whether you want to learn data science, machine learning, computer vision, MS Excel, or want to understand what Artificial Intelligence is from a business leaders perspective, we are offering each of these courses at flat 60% off. An unmissable offer!Analytics Vidhyas flagship event DataHack Summit 2018 is one of the most anticipated and eagerly awaited machine learning, deep learning artificial intelligence and IoT conferences this year. It will be held on November 22-24 in Bengaluru.These are just some of the sectors we will be exploring in our Independence Day article (to be published on 14th November). Data science is a powerful tool so why shouldnt we use it for the betterment of this wonderful nation?We will provide links to resources like open datasets which you can download and contribute back to the community. Its both an informational piece as well as a call to action. All aboard the Data Science for good express!Analytics Vidhya is known for its content and the kind of effort we put in creating that. We want to democratise this and get more and more community members to learn the way we work. That is why, we are launching AV Editors Club. Here are a few benefits you will get as a member of this club:Stay tuned for more details. We will announce this on 14th August.Excited? We are as well. Enjoy Analytics Vidhyas offerings and once again, a very happy Independence Day to our community!",https://www.analyticsvidhya.com/blog/2018/08/independence-day-bonanza-analytics-vidhyas-offers-launches/
Perform Automated Time Series Modeling with DataRobots Latest Tool,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Independence Day Bonanza with Analytics Vidhyas Offers and Launches!|Do Not Miss RStudios Game Changing Package Manager Tool!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time Series modeling is one of the most complex and tricky tasks in data science. There are so many facets and intricate details a data scientist needs to consider before getting to the model building stage.The end goal of building a time series model is to predict future performance based on historical data that is time-dependent. This includes examples such as predicting the sales of a restaurant in a holiday season, or the rise and fall of stock prices, etc. It might sound easy to read, but once you look at the data and the noise within it, the complexity goes to an entirely new level.Data Robot, a leading company in the automated machine learning (AutoML) space, has taken all these things into consideration and designed a platform that automates the entire time series modeling process. Their aim is to help business leaders improve forecasts for sales volumes, product demands, staffing, inventory and whole lot more.There are a lot of tasks this tool automates, including:The core time series models like ARIMA and Facebook Prophet are present and are supported by advanced models including eXtreme gradient boosting models. The tool also offers full API support to integrate your models into the existing business processes and products.DataRobot has also released a short video about the tool which you can view here.This release, as with all automated machine learning platform releases, will help democratize machine learning and take the technical aspect out of the equation (as much as possible). AutoML is seeing a massive spike in interest with multiple tool releases lately.I particularly like this release because I have seen even experienced data scientists get stuck with time series problems. This tool, if your business can afford it, will be a huge boost.Data scientists worried about their jobs with AutoML dont need to worry. These tools have been built to assist, rather than replace, existing jobs. It allows you to focus on the problem rather than the programming part. You can still tune and perform hyperparameter optimization, etc. but the tool does that for you to quite an extent.",https://www.analyticsvidhya.com/blog/2018/08/perform-automated-time-series-modeling-datarobots/
Do Not Miss RStudios Game Changing Package Manager Tool!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Perform Automated Time Series Modeling with DataRobots Latest Tool|Ultimate guide to handle Big Datasets for Machine Learning using Dask (in Python)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"RStudio has just laid to rest one of the biggest hurdles R has previously faced in terms of enterprise adoption with the release of the Package Manager tool.Anyone who has used R is well aware of CRAN and the multiple mirrors available to download packages. More often than not desktop systems in the organization will have full access to download packages from any available mirror, but server systems usually are not given the required permissions (due to security concerns).Given the widespread usage of R, there have been previous attempts to circumvent this issue  creating an internal CRAN-like system, building entirely new or attempting to modify existing packages, etc. But none of these were truly satisfactory enough answers.Package Manager is a game changer, theres no other way to put it. It is a repository management server to organize and centralize R packages in your organization. In other words, it is a single CRAN-esque interface that eliminates the security risk on server systems.Source: R-BloggersThe above image shows how the Package manager works and reduces the risk of a security breach. You only need a single server system for external access to a CRAN mirror built for this purpose. All the other server systems in the organization can then connect to this individual system instead of multiple unmonitored mirrors. Of course individual desktops/laptops can also make use of Package Manager.Check out the below resources to understand and get started with using Package Manager:Having been a R user before I learned Python, I always keenly follow any R updates. And this is as big as they come. If your organization or business had been dithering on using R, this should tip the scales in Rs favor significantly.If you have any questions around this and how to enable your organization to use it, check out the links I have mentioned above. They are comprehensive in nature and will take care of your queries. You can also use the comments section below and I will try to help out as well.",https://www.analyticsvidhya.com/blog/2018/08/do-not-miss-rstudios-game-changing-package-manager-tool/
Ultimate guide to handle Big Datasets for Machine Learning using Dask (in Python),"Learn everything about Analytics|Introduction|Table of contents|1. A Simple Example to Understand Dask|2. Challenges with Common Data Science Python Libraries (Numpy, Pandas, Sklearn)|3. Introduction to Dask|4. Set up your system: Dask Installation||5. Dask Interface|6. Solving a machine learning problem|7. Spark vs Dask|End Notes","  4.1 Using conda|  4.2 Using pip|  4.3 From source|5.1 Dask Arrays|5.2 Dask Dataframe|5.3 Dask ML|5.3.1 ML models|5.3.2 Dask-Search CV|Share this:|Like this:|Related Articles|Do Not Miss RStudios Game Changing Package Manager Tool!|Infographic  A Complete Guide on Getting Started with Deep Learning in Python|
Aishwarya Singh
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever tried working with a large dataset on a 4GB RAM machine? It starts heating up while doing simplest of machine learning tasks? This is a common problem data scientists face when working with restricted computational resources.When I started my data science journey using python, I almost immediately realized that the existing libraries have certain limitations when it comes to handling large datasets.Pandas and Numpy are great libraries but they are not always computationally efficient, especially when there are GBs of data to manipulate. So what can you do to get around this obstacle?This is where Dask weaves its magic! It works with Pandas dataframes and Numpy data structures to help you perform data wrangling and model building using large datasets on not-so-powerful machines. Once you start using Dask, you wont look back.In this article, we will look at what Dask is, how it works, and how you can use it for working on large datasets. We will also take up a dataset and put Dask to good use. Lets begin!Let me illustrate these aforementioned limitations with a simple example. Suppose you have 4 balls (of different colors) and you are asked to separate them within an hour (based on the color) into different buckets.What if you are given a hundred balls and you have to separate them in an hours time? That would be a tedious task but still sounds feasible. Imagine you are given a thousand balls and an hour to separate them into buckets. It is impossible for an individual to complete the task within the given time (in this case, the data is huge and the resources are limited). How would you accomplish this?The best bet would be to ask a few other people for help. You can call 9 other friends, give each of them 100 balls and ask them to separate these based on the color. In this case, 10 people are simultaneously working on the assigned task and together would be able to complete it faster than a single person would have (here you had a huge amount of data which you distributed among a bunch of people).Currently we use common libraries like pandas, numpy and scikit-learn for data preprocessing and model building. These libraries are not scalable and work on a single CPU. Dask however can scale up to a cluster of machines. To sum up, pandas and numpy are like the individual trying to sort the balls alone, while the group of people working together representDask.Python is one of the most popular programming languages today and is widely used by data scientists and analysts across the globe. There are common python libraries (numpy, pandas, sklearn) for performing data science tasks and these are easy to understand and implement.But when it comes to working with large datasets using these python libraries, the run time can become very high due to memory constraints. These libraries usually work well if the dataset fits into the existing RAM. But if we are given a large dataset to analyze (like 8/16/32 GB or beyond), it would be difficult to process and model it. Unfortunately, these popular libraries were not designed to scale beyond a single machine. It is like asking a single person to separate a thousand balls in a limited time frame, its quite unfair to ask!What should one do when faced with a dataset larger than what a single machine can process? This is where Dask comes into the picture. It is a python library that can handle moderately large datasets on a single CPU by using multiple cores of machines or on a cluster of machines (distributed computing).If you are familiar with pandas and numpy, you will find working with Dask fairly easy. Dask is popularly known as a parallel computing python library that has been designed to run across multiple systems. Your next question would understandably be  what is parallel computing?As in our example of separating the balls, 10 people doing the job simultaneously can be considered analogous to parallel computation. In technical terms, parallel computation is performing multiple tasks (or computations) simultaneously, using more than one resource.Dask can efficiently perform parallel computations on a single machine using multi-core CPUs. For example, if you have a quad core processor, Dask can effectively use all 4 cores of your system simultaneously for processing. In order to use lesser memory during computations, Dask stores the complete data on the disk, and uses chunks of data (smaller parts, rather than the whole data) from the disk for processing. During the processing, the intermediate values generated (if any) are discarded as soon as possible, to save the memory consumption.In summary, Dask can run on a cluster of machines to process data efficiently as it uses all the cores of the connected machines. One interesting fact here is that it is not necessary that all machines should have the same number of cores. If one system has 2 cores while the other has 4 cores, Dask can handle these variations internally.Dask supports the Pandas dataframe and Numpy array data structures to analyze large datasets. Basically, Dask lets you scale pandas and numpy with minimum changes in your code format. How great is that?Before we go ahead and explore the various functionalities provided by Dask, we need to setup our system first. Dask can be installed with conda, with pip, or directly from the source. This section explores all three options.Dask is installed in Anaconda by default. You can update it using the following command:To install Dask using pip, simply use the below code in your command prompt/terminal window:To install Dask from source, follow these steps:1. Clone the git repository2. Use pip to install all dependenciesNow that we are familiar with Dask and have set up our system, let us talk about the Dask interface before we jump over to the python code. Dask provides several user interfaces, each having a different set of parallel algorithms for distributed computing. For data science practitioners looking for scaling numpy, pandas and scikit-learn, following are the important user interfaces:The dataset used for implementation in this article is AVsBlack Friday practice problem . You can download the dataset from the given link and follow along with the code blocks below. Lets get started!A large numpy array is divided into smaller arrays which, when grouped together, form the Dask array. In simple words, Dask arrays are distributed numpy arrays! Every operation on a Dask array triggers operations on the smaller numpy arrays, each using a core on the machine. Thus all available cores are used simultaneously enabling computations on arrays which are larger than the memory size.Below is an image to help you understand what a Dask array looks like:As you can see, a number of numpy arrays are arranged into grids to form a Dask array. While creating a Dask array, you can specify the chunk size which defines the size of the numpy arrays. For instance, if you have 10 values in an array and you give the chunk size as 5, it will return 2 numpy arrays with 5 values each.In summary, below are a few important features of Dask arrays below:We will now have a look at some simple cases for creating arrays using Dask.As you can see here, I had 11 values in the array and I used the chunk size as 5. This distributed my array into three chunks, where the first and second blocks have 5 values each and the third one has 1 value.Dask arrays support most of the numpy functions. For instance, you can use .sum() or .mean(), as we will do now.Here, we simply converted our numpy array into a Dask array and used .mean() to do the operation.In all the above codes, you must have noticed that we used .compute() to get the results. This is because when we simply use dask_array.mean(), Dask builds a graph of tasks to be executed. To get the final result, we use the.compute() function which triggers the actual computations.We saw that multiple numpy arrays are grouped together to form a Dask array. Similar to a Dask array, a Dask dataframe consists of multiple smaller pandas dataframes. A large pandas dataframe splits row-wise to form multiple smaller dataframes. These smaller dataframes are present on a disk of a single machine, or multiple machines (thus allowing to store datasets of size larger than the memory). Each computation on a Dask dataframe parallelizes operations on the existing pandas dataframes.Below is an image that represents the structure of a Dask dataframe:The APIs offered by the Dask dataframe are very similar to that of the pandas dataframe.Now, lets perform some basic operations on Dask dataframes. Time to load up the Black Friday dataset you had downloaded earlier!The Black Friday dataset used here has 5,50,068 rows. On using Dask, the read timereduced more than ten times as compared to using pandas!Dask ML provides scalable machine learning algorithms in python which are compatible with scikit-learn. Let us first understand how scikit-learn handles the computations and then we will look at how Dask performs these operations differently.A user can perform parallel computing using scikit-learn (on a single machine) by setting the parameter njobs = -1. Scikit-learn uses Joblibto perform these parallel computations. Joblib is a library in python that provides support for parallelization. When you call the .fit() function, based on the tasks to be performed (whether it is a hyperparameter search or fitting a model), Joblibdistributes the task over the available cores. To understand Joblibin detail, you can have a look at this documentation.Even though parallel computations can be performed using scikit-learn, it cannot be scaled to multiple machines. On the other hand, Dask works well on a single machine and can also be scaled up to a cluster of machines.Dask has a central task scheduler and a set of workers. The scheduler assigns tasks to the workers. Each worker is assigned a number of cores on which it can perform computations. The workers provide two functions:Below is an example that explains how a conversation between a scheduler and workers looks like (this has been given by one of the developers of Dask, Matthew Rocklin):The central task scheduler sends jobs (python functions) to lots of worker processes, either on the same machine or on a cluster:This should give you a clear idea about how Dask works. Now we will discuss about machine learning models and Dask-search CV!Dask-ML provides scalable machine learning in python which we will discuss in this section. Implementation for the same will be covered in section 6. Let us first get our systems ready. Below are the installation steps for Dask-ML.1. Parallelize Scikit-Learn DirectlyAs we have seen previously, sklearn provides parallel computing (on a single CPU) using Joblib. In order to parallelize multiple sklearn estimators, you can directly use Dask by adding a few lines of code (without having to make modifications in the existing code).The first step is to import client from dask.distributed. This command will create a local scheduler and worker on your machine.To read more about the Dask client, you can refer tothis document.The next step will be to instantiate dask joblib in the backend. You need to import parallel_backend from sklearn joblib like I have shown below.2. Reimplement Algorithms with Dask ArrayFor simple machine learning algorithms which use Numpy arrays, Dask ML re-implements these algorithms. Dask replaces numpy arrays with Dask arrays to achieve scalable algorithms. This has been implemented for:A. Linear model exampleB. Pre-processing exampleC. Clustering exampleHyperparameter tuning is an important step in model building and can greatly affect the performance of your model. Machine learning models have multiple hyperparameters and it is not easy to figure out which parameter would work best for a particular case. Performing this task manually is generally a tedious process. In order to simplify the process, sklearn provides Gridsearch for hyperparameter tuning. The user is required to give the values for parameters and Gridsearch gives you the best combination of these parameters.Consider an example where you choose a random forest technique to fit the dataset. Your model has three important tunable parameters  parameter 1, parameter 2 and parameter 3. You set the values for these parameters as:Parameter 1  Bootstrap = TrueParameter 2  max_depth  [8, 9]
Parameter 3  n_estimators : [50, 100 , 200]
sklearn Gridsearch : For each combination of the parameters, sklearn Gridsearch executes the tasks, sometimes ending up repeating a single task multiple times. As you can see from the below graph, this is not exactly the most efficient method:Dask-Search CV:Parallel to Gridsearch CV in sklearn, Dask provides a library called Dask-search CV (Dask-search CV is now included in Dask ML). It merges steps so that there are less repetitions. Below are the installation steps for Dask-search.The following graph explains the working of Dask-Search CV:We will implement what we have learned so far on the Black Friday dataset and see how it works. Data exploration and treatment is out of the scope of this article as I will only illustrate how to use Dask for a ML problem.In case you are interested in these steps, you can check out the below mentioned articles:1. Using a simple logistic regression model and making predictionsThis will give you the predictions on the given test set.2. Using grid search and random forest algorithm to find the best set of parameters.On printing grid_search.best_params_ you will get the best combination of parameters for the given mode. I have varied only a few parameters here but once you are comfortable with using dask-search, I would suggest experimenting with more parameters while using multiple varying values for each parameter.One very common question that I have seen while exploring Dask is: How is Dask different from Spark and which one is preferred? There is no hard and fast rule that says one should use Dask (or Spark), but you can make your choice based on the features offered by them and whichever one suits your requirements more.Here are some important differences between Dask and Spark :I have recently started using Dask and am still exploring this amazing library. It is comforting to know that I dont have to explore a whole new tool in order to build my models when faced with large datasets. The best part about Dask is that it offers an interface very similar to pandas and there is a very slight (sometimes negligible) difference in the code.There are innumerable tasks that one can perform using Dask thanks to the drastic reduction in processing time. Go ahead and explore this library and share your experience in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/
Infographic  A Complete Guide on Getting Started with Deep Learning in Python,Learn everything about Analytics|Introduction,"Share this:|Like this:|Related Articles|Ultimate guide to handle Big Datasets for Machine Learning using Dask (in Python)|Feedzai AutoML  A ML Platform for Fraud Prevention thats 50 Times Faster!|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You seem to come across the term Deep Learning everywhere these days. Its all pervasive and seems to be at the heart of all AI related research. It has even spawned new and never-thought-of-before innovations!But how can you learn it? There are way too many resources out there, spread in a very unstructured and not a very beginner friendly manner. You complete a course on one platform, move to another course on a different platform, and so on. You learn, but not in any logical or sequential manner. Thats a bad idea.We have put together a comprehensive learning path for any person wanting to get into the field of deep learning. This path contains plenty of resources, links, ideas and suggestions to get you on your way! I encourage you to check out the full article here, which contains these resources.The below infographic is a very handy resource every aspiring and even established data science professional should keep handy with them at all times.You can download a high resolution PDF (just 1.5MB) from this link.I highly recommend doing this and taking a printout of the infographic  its a very handy pocket guide.This is your ticket to deep learning  use it wisely!",https://www.analyticsvidhya.com/blog/2018/08/infographic-complete-deep-learning-path/
Feedzai AutoML  A ML Platform for Fraud Prevention thats 50 Times Faster!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Infographic  A Complete Guide on Getting Started with Deep Learning in Python|A Fascinating Machine Learning Approach to Generating Faces in Advertisements|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning is THE buzzword in the industry these days as more and more professionals try to jump on the bandwagon. But theres an obstacle most people cant seem to cross  getting to grips with the technical nature of the field. Add to that the domain knowledge required, and the majority of aspiring data scientists seem to fall away.But with the introduction of automated machine learning (AutoML), organizations like Google and IBM are attempting to democratize the field. The aim of AutoML is to lower the barrier for entry in ML so that domain experts can also leverage it, without having to study an entirely new field.Feedzai is the latest organization to make an entry in this space. They have released Feedzai AutoML, a platform for fighting fraud in double quick time that frees up data scientists from tedious data wrangling tasks. Dealing with risk and fraud cases is a must in the banking industry and while existing data science techniques are adequate, performing the same task at a fraction of the time is a welcome sight.Feedzai claim that their their AutoML will let data scientist generate one thousand new features (feature engineering) in a matter of minutes. And the most intriguing part? It speeds up fraud prevention workflows by as much as 50 times!The company realized that businesses were struggling to adopt Google AutoML because of its operational difficulty and need for GPUs. So they took a different approach to building their own platform. The developers first used it on non-neural network models like LightGBM and XGBoost so as to speed up the training time. As you can see in the above graph, the model built by 1 machine in 1 day is virtually indistinguishablefrom a model built by 2 people working for 2 months.But what truly sets this release apart is the use of an advanced semantic-based automated feature engineering approach. This means that the machine recognizes the semantics associated with each variable in your dataset. The platform can automate the below tasks currently:For a more in-depth explanation of the platform and how to use it for your needs, head over to Feedzais blog post.This is quite an interesting release. While I have previously seen AutoML platforms being released, they were not created with a singular task in mind. The results shown by the company are encouraging and promising. AutoML seems to be something that will grow into a big service in the next couple of years.If you are curious about AutoML and dont have the money to splurge on learning it, I would recommend checking out Auto-Keras. Its a recently released open source library that REALLY helps you dig deep into deep learning without getting bogged down by code.",https://www.analyticsvidhya.com/blog/2018/08/feedzai-automl-a-ml-platform-for-fraud-prevention-thats-50-times-faster/
A Fascinating Machine Learning Approach to Generating Faces in Advertisements,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Feedzai AutoML  A ML Platform for Fraud Prevention thats 50 Times Faster!|Want to Speed Up your Model Building Process in Python? Try Studio.ML|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Advertisements are a powerful tool that affect our habits and decisions. For years, we have been watching them everyday on television, and splattered across posters and print media. These ads have become even more pervasive in the digital age.What makes these ads different is the way they are portrayed. Regardless of what the ad is about, we see all kinds of faces attached to it  from beauty products to beverages, there is usually a face next to the actual product. So instead of splurging out money, what if you could use machine learning to generate faces according to the advertisement requirements?Thats exactly what a couple of researchers from the Computer Science department of the University of Pittsburgh went about doing in their research. They took a multi-step approach to understanding what makes advertisements persuasive and then generating faces which appear as if they are coming from different types of ads.The two researchers built a conditional variational autoencoder for this purpose. This techniques makes use of predicted semantic attributes and facial expressions as the supervisory signals during the training process. As you can see in the above and below images, the model was able to generate unique facial images according to the category of the advertisement. In the below image, the models you see on the x-axis are the ones trained and/or modified by the researchers.This model outperformed several baseline models, including two previous state-of-the-art GANs built for transforming faces. Interestingly, the researchers extended their study to generate objects as well. The results were pretty impressive in terms of how detailed each image looked.You can also read about this study in more detail, including the mathematical notations, in the recently published research paper.The first question that came to my mind was  will this replace humans in the long run? Its too early to say but this research certainly encourages that line of thinking. It gives business, especially those geared towards marketing and advertising, food for thought.GANs have been around for a few years but are rapidly gaining popularity these days. Its intriguing to see how researchers are using ML to first analyze what makes ads persuasive, and then leverage deep learning algorithms to take advantage of the said analysis. It gives you a window into the thinking of top experts in this field.",https://www.analyticsvidhya.com/blog/2018/08/machine-learning-approach-gans-generate-faces-advertisements/
Want to Speed Up your Model Building Process in Python? Try Studio.ML,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Fascinating Machine Learning Approach to Generating Faces in Advertisements|DataHack Radio Episode #6: Exploring Techniques and Strategy with Courseras Head of Data Science, Emily Glassberg Sands|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ever been in a situation where you need to design models and put them into deployment with a quick turnaround time? It is as challenging as it sounds and there are not many easy solutions for it out there.But a good option to try is Sentients recently released first version of Studio.ml, an open source framework written in Python to simplify and accelerate ML model development. This project has been designed for data scientists and ML practitioners to help them speed up their experiments.Studio.ml aims to minimize the time taken in scheduling, running, monitoring and managing artifacts of the machine learning experiments, according to Sentients blog post. The current tools and software available in the market cannot ensure reproducibility of ML experiments. Studio.ml has been built to overcome this obstacle.With Studio, you can keep a track of all your experiments in a single location. Studios web interface lets you share your experiments and view others as well. It also provides you the flexibility for privately saving up your experiments. Give it a try here.Studio.ml tools are compatible with Keras, TensorFlow, PyTorch and scikit-learn. The team has made every effort to keep Studio.mls code syntax similar to the aforementioned frameworks so that the user has to make very less modifications in their codes in order to run studio.Install studio.ml on your machine via pip using the following command:According to Studio.mls GitHub page, the tool offers the below features:Here are a couple of short videos to help you get acquainted with Studio.ml:One of its kind, a framework that lets you save (and share) your experiments, all in one location. More interestingly, you dont have to make an effort at learning a new tool or language, the developers have made it as easy for you as possible.Sure there are other alternate options out there but the open source nature of Studio.ml is what makes it an attractive option to data scientists and especially ML researchers. If you have used this tool before or are planning to use it now, let us know your experience in the comments section below!",https://www.analyticsvidhya.com/blog/2018/08/want-to-speed-up-your-model-building-process-in-python-try-studio-ml/
"DataHack Radio Episode #6: Exploring Techniques and Strategy with Courseras Head of Data Science, Emily Glassberg Sands",Learn everything about Analytics|Introduction|Emilys Background|Courseras Data Science Team|Techniques and Approaches used by the Data Science Team|The Thinking behind Courseras Certificate Pricing|Analyzing and Dealing with Course Dropouts|Building a Data Science Team|Women in Data Science|The Difference in having an Economics v Computer Science Background|End Notes,"Share this:|Like this:|Related Articles|Want to Speed Up your Model Building Process in Python? Try Studio.ML|Quicksilver  A Natural Language Processing System that Writes Wikipedia Entries|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever wondered how Coursera uses data science? Sure we have all taken (or heard of) their courses but how does the platform make use of the amount of data thats generated? How does their pricing strategy work for certificates? How is their data science team structured? What tools and techniques do they use?These are just some of the questions answered in this phenomenal podcast. We had the pleasure of hosting Courseras Head of Data Science, Emily Glassberg Sands, who gave us a detailed and thorough explanation of how Coursera functions behind the scenes. It makes for very fascinating listening and you will learn a whole bunch of data science and machine learning stuff throughout.In this article, we have listed a few snippets from Kunals conversation with Emily. Click the above SoundCloud button to listen to the podcast!You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms:Emily joined Coursera over four years ago after successfully completing her Ph.D in Economics from Harvard University. Before that, she completed her undergraduate degree from Princeton University in Economics as well. She co-authored two papers during her Ph.D which make for very interesting reads (they are available on her LinkedIn profile and are very relevant for data scientists).She had not initially applied to be a data scientist at Coursera, instead opting to apply to the Partnerships division. She liked Courseras mission and during her initial few months there, she worked with various machine learning folks (half of the original team was comprised of machine learning researchers from Stanford).Emily was the first data science hire at Coursera and is now their Head of Data Science!Her research during her graduation years was exclusively in MATLAB. She trained herself in R and Python using books (courses back then werent as good as they are now) before joining Coursera and learned quite a lot on the job as well.Emily elaborated on Courseras growth since she joined in 2014. The platform has seen a rapid rise in both the number of users and the amount of content they have. The team has grown to over 300 people across the organization, with the product and engineering department making up half of that number.Emilys data science team is currently comprised of the below three roles, along with their functions:So which language(s) does Courseras data science team use for day-to-day tasks? R and Python! Everyone on the team is expected to know and work with these two popular languages.For the content discovery aspect that we see on Courseras platform, its important for the data science team to establish the right metadata (understanding features of the content and Courseras users). This is followed by building relatively lightweight logistic regression models to recommend the best content to the end user.There are also a few black box models which the team experiments with.But the biggest boon has been what Emily calls skills graphs or knowledge graphs. This contains a bunch of metadata about content and learners. When you combine these two, you get very powerful predictions that Coursera leverages in its end product. Building the graph however, is a bit complex. It maps about 40,000 skills to the available content types and this is what powers various machine learning models that work on recommendations.The team has been working on this knowledge graph since almost two years. At this point Emily offered some valuable insights into what a data scientist should expect  it wasnt a one step process. They built out one part of the knowledge graph, tried it out in a practical situation, and then went ahead with building the rest, piece by piece. If you are interested in NLP or just want to know how a data science project functions from scratch, you should listen to this section in the podcast.Collaborative filtering, one of the most popular recommendation techniques, was initially being used by Coursera before they started using tons of metadata to refine and improve on it. If you have the amount of information and data that the team has collected over the years, why not use it to improve the experience for its users?Emily wanted to improve the credential value of Courseras certificates when she started working there. Her working hypothesis, before she had a look at the data, was that people in developing countries would value the certificate as it would help them in their job applications. The data revealed otherwise  people in these countries were not taking these certificates.Using various econometric and data science techniques including regression analysis, fixed effects model, etc. she discovered that learners in developing markets were far more price sensitive than learners in developed markets. It sounds pretty intuitive now but was not something the team had thought of back then.Now came the question of how to price these certificates. Emily and her team ruled out trying A/B testing from the outset because of how unfair that could be for certain people. Instead, they went with a quasi experimental design. Emily took a bundle of developing markets and applied synthetic control methods and difference-in-difference analysis. Emily has described this in much more detail in the podcast and it makes for really interesting listening.This is a pretty common issue observed in todays MOOC dominated world. People start a course but leave halfway through due to various reasons. Emily discussed how Coursera divides their approach into two parts to handle this:After experimenting with a predictive model that identified at-risk learners, the team then built several advanced models to recognize the reason why learners were dropping off at certain points. They have built several intervention points which help them understand where the user might need a nudge, or extra help, and then take action accordingly.On the instructor side of things, Coursera lets them do A/B testing to understand their audience. Two versions of a course are created at the backend and the instructor can edit one of those to make it a bit different. Then when learners enroll in the course, they are shown one of these two versions by a randomization method.Team building, for me, is about collecting a superstar team.Emily looks at a number of components when building her data science team. I have mentioned a few crucial ones below:Analytics Vidhya has featured Emily in the most influential women in data science list two years running now. She is an advocate of diversity in data science and enlightened us with her opinions on the subject in this podcast. Below are a few pointers she mentioned which hiring companies should take note of.When crafting a job description, she makes sure its inclusive and gender neutral. She also believes its important to invest in diversity early when you have a small team. Co-hosting events with the Women who Code organization has also worked out really well for Coursera.Small things in the interview process also make a world of difference  making sure that all candidates are asked the same questions, ensuring feedback from interviewers is not being shown to each other until they have submitted their own feedback, etc. There are a lot more really thoughtful points in the podcast.Both are tremendously valuable in isolation, and particularly valuable when combined.Emily agreed with Kunals assessment that people who comes from an economics background have more intuitive insights and hypothesis as compared to folks who come from computer science degrees. She mentioned that when taking these two separately, they have their own unique value. But they become truly valuable when theyre combined.Her approach while building the data science team at Coursera has been to focus on a mix of backgrounds. The current set up has mathematicians, statisticians, computer scientists, and people from a host of diverse quantitative backgrounds. She firmly believes that focusing on one field is a recipe for failure. Data science is not a domain where you have obvious solutions. It takes a mix of ideas to extract insights from the challenges data poses.This article isnt enough to describe how insightful the podcast is. Emily is a great speaker and that clearly reflected in her conversation with Kunal. I had personally wondered how Courseras recommendation engine worked and this podcast emphatically answered my question.We found out so much more about the platform and it adds a fresh perspective on how a leading education provider thinks and works using the power of data science and machine learning. You will learn a LOT of new things in this hour long podcast. Take out the time to hear Emily Glassberg Sands and get ready to be inspired by a true thought leader.",https://www.analyticsvidhya.com/blog/2018/08/datahack-radio-episode-4-coursera-data-science-emily-glassberg-sands/
Quicksilver  A Natural Language Processing System that Writes Wikipedia Entries,Learn everything about Analytics|Overview|Introduction|Our take on this,"How Quicksilver works|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #6: Exploring Techniques and Strategy with Courseras Head of Data Science, Emily Glassberg Sands|DeepMind Open Sources Dataset to Measure the Reasoning Ability of Neural Networks|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whenever we search for a famous personality on Google, their Wikipedia page is usually the first thing that pops up. The free-for-all encyclopedia has become the go-to tool for people of all ages, from students looking for homework material to journalists looking for confirmation on their research. But a disturbing trend has emerged of late.Quite a few people pointed out that Wikipedia was suffering from gender bias, that is, a lot of the popular female personalities did not have their dedicated page. Take the example of Mirian Adelson. She is a well known physician and has published tons of research papers in her career. Until recently, she did not even have a Wikipedia entry!Thousands of such names were flagged by Quicksilver, a software tool introduced by Primer, a startup in San Fransisco. Particularly targeting women in science, Quicksilver found that only 18% biographies on Wikipedia were of women. Further digging also revealed that approximately 84-90% of the Wikipedia editors are male.Wikipedia has pages and subsequent details for innumerable topics and figuring out the blind spots would understandably be an impossible task for the editors. Quicksilver has been designed with the aim of helping the editors accomplish this task. As stated by the developers, Quicksilver uses machine-learning algorithms to scour news articles and scientific citations to find notable scientists missing from Wikipedia. Not only this, it can also write fully sourced draft entries for them.Their algorithm was trained on 30,000 Wikipedia articles about scientists. It detects signals in the articles that correlate with a researcher having an entry on the site which is used by Quicksilver to find notable missing names. This is done by cross-referencing existing Wikipedia entries with a list of 200,000 scientific authors drawn from an academic search engine (called Semantic Scholar). The software sources the facts needed to write missing entries from a collection of 500 million news articles and feeds them into a system trained to generate biographical entries from past examples.Below is a draft created for Miriam Adelson Miriam Adelson is a doctor and chairman of The Dr. Miriam & Sheldon G. Adelson Clinic for Drug Abuse Treatment and Research.[1] With her husband, Sheldon Adelson, she owns the Las Vegas Review-Journal and Israel Hayom.[2] She was listed by Forbes in June 2015 as having a fortune of $28 billion, making him[sic] the 18th richest person in the world.[3] She has frequently been cited in media reports as the newspapers owner, including by JTA.[4]Quicksilver has already prepared 40,000 summaries for both male and female scientists which was missing from Wikipedia. You can see a sample of hundred entrieshere. These summaries are not directly published on the site but are meant to be a starting point for Wikipedias editors.Quicksilver can also help the editors at keeping the present articles up to date. It was tested in New York during an edit-a-thon, conducted by the American Museum of Natural History, with the aim of improving the existing eateries on women scientists. Quicksilver created sparse Wikipedia bio for women scientists by scraping facts from the web and providing source links along with them. This helped 35 first-time editors in updating their pages for 70 women scientists within 2 hours! Pretty impressive, isnt it?Given how popular Wikipedia is and the wide-ranging implications any bias related to it might have, its good to see machine learning bridging that gap. This research not only helps bring under-represented scientists in the spotlight, it is also a shining example of how useful ML can be.I personally feel that we are just scraping the surface as far as the uses of NLP are concerned. The algorithm behind Quicksilver is fairly easy to understand (though of course equally complex to design). Do check out Primer.ai and see the different domains they are breaching using NLP.There is so much text out there  both existing and being generated every day. Im looking forward to seeing more of these researches soon!",https://www.analyticsvidhya.com/blog/2018/08/machine-learning-algorithm-fix-wikis/
DeepMind Open Sources Dataset to Measure the Reasoning Ability of Neural Networks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Quicksilver  A Natural Language Processing System that Writes Wikipedia Entries|Auto-Keras  A Must-Use Open Source Python Package for Automated Machine Learning!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial General Intelligence (AGI) has long been the stuff of science fiction, rather than hardcore research. One of the primary reasons for that is the black box nature of deep neural networks. These neural networks are making great headway in certain areas, but they are limited to doing the one task they were designed for.The current state of neural networks prevents these models from generalizing to other tasks. For example, an algorithm that is designed for picking up objects cannot be re-programmed to also drive a vehicle. This has been a significant roadblock that is preventing researchers from reaching the AGI stage and unifying systems.If anyone could achieve a breakthrough though, I would imagine it could be DeepMind. They have the required computational power, and the best research scientists are working there to get us closer to AGI. Their latest effort comes in the form of an approach and a challenge. They have attempted to measure the reasoning ability of neural networks in order to understand the nature of generalization.Instead of trying to transfer knowledge from real-world scenarios to visual reasoning problems, the DeepMind research team studied knowledge transfer from one set of visual reasoning problems to another. They have designed a novel architecture that does significantly better than popular models like ResNet.Their research makes for an intriguing read, because they have admitted that while their model is definitely proficient at certain things, it tends to be weak in a few areas as well. When the training and test questions focused on similar factors, the model did moderately well with a 75% accuracy rate. But when these two sets differed, the model had a nightmare getting the right answer.Their research paper (link below) also shows how the models ability to generalize improves quite a bit when it is trained to predict symbolic explanations. The researchers have also publicly released their abstract reasoning dataset in the hopes of progressing this study with the help of the community.You can delve into a more in-depth explanation of DeepMinds approach, and try it out yourself using the below links:While the results released by DeepMind are not what one would hope for, they still offer a lot of promise. No one till date has been able to crack the AGI challenge so even a hint of a solution has to be taken as progress. Neural networks are such a powerful thing, and the sooner we can dispel their black box image, the better it will be for machine learning and AI research.The good news is that we at least have a way to measure work in this field. Im sure this will be modified as well going forward. Meanwhile, make sure you play around with the dataset and see if you can come up with insights of your own!",https://www.analyticsvidhya.com/blog/2018/08/deepmind-open-sources-dataset-measure-neural-networks-reason/
Auto-Keras  A Must-Use Open Source Python Package for Automated Machine Learning!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DeepMind Open Sources Dataset to Measure the Reasoning Ability of Neural Networks|The Best Machine Learning GitHub Repositories & Reddit Threads from July 2018|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google has been championing AutoML (and with good reason) as the go-to tool for all your AI tasks. They have even released real-world scenarios where AutoML has proven to be more effective than traditional programming tools. But theres a caveat  its a paid service (like so many similar services out there). And while these services may produce good results, theres always a risk that an open source competitor might supersede it.With Auto-Keras, thats exactly what might happen. Auto-Keras is an open source library built for doing automated machine learning based on the popular Keras package. Automated machine learning (AutoML) has recently gained popularity because it makes ML techniques and usage available to non-data science folks. It reduces the technical barrier and allows domain experts to utilise the power of machine learning for their business.The current version of Auto-Keras provides functions to automatically search for the hyperparameters of deep learning models. It is easy to install, understand, use and has a few examples to help you get started. And of course you can play around with the code and manually change the hyperparameters and other model settings (remember, its open source!).To install Auto-Keras on your machine, paste the below command in your Terminal/Command Prompt window:Check out the full code for Auto Keras on GitHuband also make sure you view all the examples here.Try it out! Its free so you have nothing to lose here. Automated ML will be a big thing in the future so its good, as a data scientist, to understand what all the current hype is about. Auto-Keras will help you build models with easy and unerring accuracy.Note that this is not to condone the other AutoML tools out there. These tools were built by data scientists to help other data scientists so they should not be ignored. But when an open source opportunity like this comes along, it instantly becomes a hit in the community. If you do use Auto-Keras, share your experience with us in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/auto-keras-open-source-python-package-automated-machine-learning/
The Best Machine Learning GitHub Repositories & Reddit Threads from July 2018,"Learn everything about Analytics|Introduction|GitHub Repositories|Image Outpainting|Text Classification Models with TensorFlow|MatchZoo|GANimation|GAN Stability|Reddit Discussions|Which deep learning papers should I implement to learn?|Use of Science at Organizations like Google Brain/FAIR/DeepMind|Some Good Books to Gain a Theoretical Understanding|Discussion on how AI will Impact Jobs, both Present and in the Future|Common Mistakes People make in Data Visualization|End Notes","Share this:|Like this:|Related Articles|Auto-Keras  A Must-Use Open Source Python Package for Automated Machine Learning!|A Brilliant Example of How Automated Deep Learning is Reshaping the Healthcare Analytics Industry|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you ever imagine you could become an artist without knowing how to paint or even hold a paintbrush? This is what you can do now, thanks to computer vision techniques. And whats even better, the ML community is so awesome that the code to do this has been open sourced! This is the power of GitHub and why I encourage all data scientists, aspiring or established, to use it regularly.GitHub has been at the heart of open source data science and machine learning.Whether you are contributing to an existing repository or building one of your own, you are sure to gain a ton of knowledge.There are some really cool repositories below  deep learning and GANs specific, natural language processing (NLP) related text matching, and computer vision (as mentioned above) to extend and re-imagine existing images. Theres something here for everyone!Coming to Reddit, we have selected a mix of deep learning and artificial intelligence related discussions. These will help you assess and understand the current state of certain technologies in the industry and where we might be headed in the near future.You can check out the top GitHub repositories and top Reddit discussions (from April onwards) for the first 6 months of the year below:This is one of the coolest repositories I have covered in this series. Inpainting has been a trending concept recently but this technique, designed by a couple of researchers from Stanford, does the opposite.Outpainting extends the use of GANs for inpainting to estimate and imagine what the existing image might look like beyond what can be seen. Then the algorithm expands the image beyond its existing boundaries. The results, as you can see in the image above, are outstanding.This repository is an open source implementation using Keras in Python. You can either build a model from scratch or use the one provided by this repositorys author. Either way, try it out!Be sure to check out Analytics Vidhyas article on this approach here.This repository does what it says  its a TensorFlow implementation of various text classification models. What I liked about this repository is that it contains links to each model that has been discussed. This provides an understanding of what you are doing, which is extremely helpful. The models implemented here are:While not strictly a library created last month, this repository got a big update recently. MatchZoo is basically a toolkit for text matching. It has been created in order to design, compare and share the various deep text matching models. Potential tasks MatchZoo can do are document retrieval, conversational response ranking, question answering, and paraphrase identification, among others.Some of the deep matching methods out there are DRMM, MatchPyramid, MV-LSTM, aNMM, DUET, etc. Check out the repository to get details on how to install and take advantage of this extremely useful library.Does the above ensemble of faces get you excited about this repository? The image inside the green border is the original one, the rest of the images use GANimation to anatomically change the facial expressions of the subject(s). This is a slightly complex approach but is something you must explore if you are interested in deep learning.The authors have provided everything you need to get started  a beginners guide, prerequisites, data preparation resources, and of course, the Python code. What are you waiting for? Dig in!This excellent repository contains Python codes for various experiments conducted as part of the here paper. This was presented at the International Conference on Machine Learning 2018 last month. Its a fascinating case study for anybody interested in deep learning and especially GANs.Why I have included this repository is because it gives you a really good idea of the level of research and thinking that goes into papers that are accepted and presented at top class ML conferences. You can also view the best papers from ICML 2018 here.                                           Source: WikipediaIf you are a newcomer to deep learning, this instantly becomes a must-read thread for you. Plenty of DL experts have provided their views (and a plethora of links) on recently published papers that you should read and implement. This reinforces what youve learned and has the additional advantage of keeping you up-to-date with a breakthrough technique.If you are a deep learning veteran, this will either refresh your concepts or teach you about all thats happening in this diverse field. You can never get enough knowledge so I encourage you to check out all the resources provided. You should also read through all the opinions provided by other data scientists which will add to your own perspective.The title of this thread is enough to grab a data scientists attention. This discussion spawned from a Twitter debate on how science is being used by the big technology organizations. While the debate started from a pessimistic viewpoint, it jumped to more positive or assertive views from people who have worked with these companies.You will not only learn how science is defined and used at Google Brain, et all, but also what fellow data science people think about the current state of science in the industry.If you want to get into the research side of machine learning, you need to know the theory behind how things work. Thin includes topics like core mathematics, probability, etc. This thread lists down some of the more advanced books on various machine learning concepts.There are tons and tons of suggestions (almost a 100 comments!) in there along with links so you cannot complain about lack of resources. From advanced ML to introduction to reinforcement learning, this thread is a goldmine of top notch resources.This has been an ongoing discussions since decades, and has gained even more prominence with the recent interest in ML and AI. The concern is real despite experts doing their best to allay fears. Go through this thread end-to-end  it contains opinions from AI enthusiasts and experts about how they see AI impacting jobs in different countries.There are also plenty of statistics and links shared which help in gauging where AI is headed. Make sure you contribute with your valuable opinion to the overall discussion as well. The more you put yourself out there, the more confident you will be in your data science skin.Data visualization is a critical aspect of any machine learning project. But it has its own standalone applications as well, like dashboards, reports, etc. Business intelligence is a thriving field these days and as more folks get into it, they need to be aware of some of the most common mistakes people make. The given image is a great illustration of this.One of the more fun but important threads you will come across in your data science journey. You dont need to religiously adhere to each point that has been showcased, but its good to have an overall idea of how leaders in the field think.This months article was geared more towards deep learning but I have tried to maintain balance by sharing some beginner friendly Reddit discussions. I repeat again  please try to contribute to both GitHub repositories and Reddit discussions because these will help you immensely in your career. The more you read and share, the better your own knowledge becomes.If you know of any other links that the community should know about, go ahead and share them with us in the comments section below.",https://www.analyticsvidhya.com/blog/2018/08/best-machine-learning-github-repositories-reddit-threads-july-2018/
A Brilliant Example of How Automated Deep Learning is Reshaping the Healthcare Analytics Industry,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Best Machine Learning GitHub Repositories & Reddit Threads from July 2018|Dactyl  OpenAIs Robot Hand trained itself without any Human Learning (Video + Research Paper)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Healthcare analytics in a booming industry.Recent applications of artificial intelligence and machine learning in healthcare includeScanning Brain Anomalies faster than humans, Predicting Heart Diseases by Retina Scans, etc. Google even designed a system to predict the likelihood of a patients death with 95% accuracy!Recently, a research project led by Fang Liu alongside the University of Wisconsin School of Medicine and Public Health, focused on using an automated deep learning based system to improve patient care. The researchers have created a model that can accurately identify knee joint cartilage and detect wear or injury with impressive accuracy.The team has used segmentation and classification convolutional neural networks (CNNs) to train the deep learning system. To test this model, a dataset consisting of MRI images from 175 patients was used. These patients underwent fat-suppressed T2-weighted fast spin-echo MRI.As a part of the test, two separate evaluations were performed and ROC (receiver operating curve) was used to measure performance. Below are the results of the two evaluations.ROCEvaluation 10.917Evaluation 20.914As you can see, the deep learning system showed an overall high accuracy. It had a ROC score of 0.917 for evaluation one and ROC score of 0.914 for the second evaluation. Also, a sensitivity of 84.1% and a specificity of 85.2 % was achieved during the first evaluation, followed by 80.5 % sensitivity and 87.9 % specificity in the second evaluation.SpecificitySensitivityEvaluation 185.284.1Evaluation 287.980.5As mentioned by the experienced radiologists, the main drawback of examining MRI for articular cartilages is a comparatively lower sensitivity. Looking at the high sensitivity provided by the system, it is expected to be the reason for bringing this into the healthcare industry and use it for practical scenarios. But before it is fully implemented in clinical practice, the model needs to be optimized.The healthcare industry is always in need of more manpower and various researchers are working on providing AI and ML systems to help improve the current state of affairs. After having models detecting brain anomalies, early signs of diabetes and heart diseases, we now have a deep learning-based system that detects injury in knee joints (with a higher sensitivity than experts).There is still tons of skepticism around AI but the hope remains that it will be used to assist clinical experts, rather than replace them.",https://www.analyticsvidhya.com/blog/2018/08/automated-deep-learning-healthcare/
Dactyl  OpenAIs Robot Hand trained itself without any Human Learning (Video + Research Paper),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Brilliant Example of How Automated Deep Learning is Reshaping the Healthcare Analytics Industry|Become a Computer Vision Artist with Stanfords Game Changing Outpainting Algorithm (with GitHub link)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picking up an object and analyzing it may be an arbitrary task for humans, but dont tell a machine that! Teaching a computer to detect objects, pick them up and analyze them has turned out to be way harder than anybody had initially imagined. What a few months old toddler can do is something that takes years of training for a machine to learn (thats just one simple example of why we are nowhere near general artificial intelligence).Robot hands have become the primary application machine learning researchers use to showcase their projects. And OpenAI, always at the cutting edge of AI research, have trained a robot hand that can manipulate objects with mind boggling dexterity. The system, which OpenAI is calling Dactyl, has been trained entirely using round after round of simulations. Dactyl learns to do tasks from scratch using the same reinforcement learning techniques that power the popular OpenAI Five system.The task OpenAI researchers gave Dactyl was to reposition a given object (like a letter block) such that a new position is visible every time. Three cameras monitor how the hand works while the position and movement of fingertips is tracked in real-time. As more and more simulations were performed, Dactyl used human-level strategies to achieve the desired results. Again, this wasnt labelled or taught, it came as a result of the simulations.The below image, posted by OpenAI, shows how they built this system:OpenAIs blog post and research paper cover Dactyl in more technical detail. You can also check out the video below to see the robot hand in action:This may seem like arbitrary research at first glance but it might be the first step towards general AI. Sure we have seen tons of robot hands before (), but what makes Dactyl different is that it isnt programmed to perform any one single task. Place any object in that hand, and it will learn by itself how to change its orientation.This goes to show that robots can adapt to human-like behavior. It will take a lot more experiments and research to perfect this and make it useful in a practical scenario, but at least the stepping stone has been laid down.",https://www.analyticsvidhya.com/blog/2018/07/dactyl-openais-machine-learning-robot-hand-trains-without-human-learning/
Become a Computer Vision Artist with Stanfords Game Changing Outpainting Algorithm (with GitHub link),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Dactyl  OpenAIs Robot Hand trained itself without any Human Learning (Video + Research Paper)|Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Becoming an artist these days is easier than ever before (especially if youre a programmer!) thanks to recent advances in computer vision algorithms. Your computer is your canvas and machine learning techniques are your paint brushes, and theres really no reason to dally on your interests any longer  its time to get started on your artistic dreams!If you are a keen follower of AVBytes, you must have read about a technique called inpainting (read up on that in case you havent yet, its really worth it). It is a popular computer vision technique that aims to restore missing parts in an image and has produced some exquisite results, as you will see in that article. Current state-of-the-art methods for inpainting involve GANs (Generative Adversarial Networks) and CNNs (Convolutional Neural Networks).It was only a matter of time before someone from the ML community figured out a technique that goes beyond the scope of inpainting. This breakthrough has come from a couple of Stanford researchers, Mark Sabini and Gili Rusak, and the new technique is appropriately named outpainting.This approach extends the use of GANs for inpainting to estimate and imagine what the existing image might look like beyond what can be seen. Then the algorithm expands the image and paints what it has estimated  and the results, as you can see in the image below, are truly astounding.For the dataset, the researchers used 36,500 images of 256256 size, which were downsampled to 128128. 100 images were held out for the validation set.Even the research paper for outpainting has been written in a user-friendly format. Instead of the usual page after page of theory, the paper is of just 2 pages  one which lists down how the technique was derived and how it works, and the second which contains a list of references. Check out the image of the first page below which lists down a step-by-step approach for designing and executing outpainting:Wondering how to implement this on your own? Wonder no more  use this GitHub repository as your stepping stone. It is a Keras implementation of outpainting in Python. It gives you the option to either build your model from scratch or use the pertained model the creator has uploaded. Get started now!What an awesome concept! If this doesnt get your interest in computer vision going, I dont know what will. Take this course to learn all about computer vision with deep learning, and get started on your path toward becoming a CV expert!For the Keras model, theres a caveat here  as youll read in this Reddit discussion thread, theres a chance that the Keras model was overfitted. The model was trained on images that were present in the training set itself so it was able to convincingly extrapolate the generated image. The model still did fairly well when tested on unseen data, but not as well as first imagined. But dont let that dissuade you! The Stanford technique is still solid, and there will be far more refined frameworks coming soon using outpainting. Hope to see one from our Analytics Vidhya community as well!",https://www.analyticsvidhya.com/blog/2018/07/become-computer-vision-artist-stanfords-outpainting-algorithm-github-keras/
Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code,Learn everything about Analytics|Introduction|Table of Contents||1. Understand the Problem Statement|2.Tweets Preprocessing and Cleaning|3. Story Generation and Visualization from Tweets|4.Extracting Features from Cleaned Tweets|5. Model Building: Sentiment Analysis|6. Whats Next?|End Notes,"A) Removing Twitter Handles (@user)|B) Removing Punctuations, Numbers, and Special Characters|C) Removing Short Words|D) Tokenization|E) Stemming|A) Understanding the common words used in the tweets: WordCloud|B) Words in non racist/sexist tweets|C) Racist/Sexist Tweets|D) Understanding the impact of Hashtags on tweets sentiment|Bag-of-Words Features||TF-IDF Features|A) Building model using Bag-of-Words features|B) Building model using TF-IDF features|Share this:|Like this:|Related Articles|Become a Computer Vision Artist with Stanfords Game Changing Outpainting Algorithm (with GitHub link)|An AI Algorithm Detects your Personality by analyzing Eye Movement!|
Prateek Joshi
|58 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing (NLP) is a hotbed of research in data science these days and one of the most common applications of NLP is sentiment analysis. From opinion polls to creating entire marketing strategies, this domain has completely reshaped the way businesses work, which is why this is an area every data scientist must be familiar with.Thousands of text documents can be processed for sentiment (and other features including named entities, topics, themes, etc.) in seconds, compared to the hours it would take a team of people to manually complete the same task.In this article, we will learn how to solve the Twitter Sentiment Analysis Practice Problem.We will do so by following a sequence of steps needed to solve a general sentiment analysis problem. We will start with preprocessing and cleaning of the raw text of the tweets. Then we will explore the cleaned text and try to get some intuition about the context of the tweets. After that, we will extract numerical features from the data and finally use these feature sets to train models and identify the sentiments of the tweets.This is one of the most interesting challenges in NLP so Im very excited to take this journey with you!Lets go through the problem statement once as it is very crucial to understand the objective before working on the dataset. The problem statement is as follows:The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.Formally, given a training sample of tweets and labels, where label 1 denotes the tweet is racist/sexist and label 0 denotes the tweet is not racist/sexist, your objective is to predict the labels on the given test dataset.Note: The evaluation metric from this practice problem is F1-Score.Personally, I quite like this task because hate speech, trolling and social media bullying have become serious issues these days and a system that is able to detect such texts would surely be of great use in making the internet and social media a better and bully-free place. Lets look at each step in detail now.Take a look at the pictures below depicting two scenarios of an office space  one is untidy and the other is clean and organized.You are searching for a document in this office space. In which scenario are you more likely to find the document easily? Of course, in the less cluttered one because each item is kept in its proper place. The data cleaning exercise is quite similar. If the data is arranged in a structured format then it becomes easier to find the right information.The preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which dont carry much weightage in context to the text. In one of the later stages, we will be extracting numeric features from our Twitter text data. This feature space is created using all the unique words present in the entire data. So, if we preprocess our data well, then we would be able to get a better quality feature space.Lets first read our data and load the necessary libraries. You can download the datasets from here.Lets check the first few rows of the train dataset.The data has 3 columns id, label, and tweet. label is the binary target variable and tweet contains the tweets that we will clean and preprocess. Initial data cleaning requirements that we can think of after looking at the top 5 records:As mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they dont convey much information.For our convenience, lets first combine train and test set. This saves the trouble of performing the same steps twice on test and train.Given below is a user-defined function to remove unwanted text patterns from the tweets. It takes two arguments, one is the original string of text and the other is the pattern of text that we want to remove from the string. The function returns the same input string but without the given pattern. We will use this function to remove the pattern @user from all the tweets in our data.Now lets create a new column tidy_tweet, it will contain the cleaned and processed tweets. Note that we have passed @[\w]* as the pattern to the remove_pattern function. It is actually a regular expression which will pick any word starting with @.As discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles. Here we will replace everything except characters and hashtags with spaces.We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like hmm, oh are of very little use. It is better to get rid of them.Lets take another look at the first few rows of the combined dataframe.You can see the difference between the raw tweets and the cleaned tweets (tidy_tweet) quite clearly. Only the important words in the tweets have been retained and the noise (numbers, punctuations, and special characters) has been removed.Now we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.Stemming is a rule-based process of stripping the suffixes (ing, ly, es, s etc) from a word. For example, For example  play, player, played, plays and playing are the different variations of the word  play.Now lets stitch these tokens back together.In this section, we will explore the cleaned tweets text. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Do not limit yourself to only these methods told in this tutorial, feel free to explore the data as much as possible.Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:Now I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.Lets visualize all the words our data using the wordcloud plot. We can see most of the words are positive or neutral. Withhappyandlovebeing the most frequent ones. It doesnt give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes(racist/sexist or not) in our train data.We can see most of the words are positive or neutral. With happy, smile,and love being the most frequent ones. Hence, most of the frequent words are compatible with the sentiment which is non racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms.As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data.Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis task, i.e., they help in distinguishing tweets into the different sentiments. For instance, given below is a tweet from our dataset:The tweet seems sexist in nature and the hashtags in the tweet convey the same feeling.We will store all the trend terms in two separate lists  one for non-racist/sexist tweets and the other for racist/sexist tweets.Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top n hashtags. So, first lets check the hashtags in the non-racist/sexist tweets.Non-Racist/Sexist TweetsAll these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Lets check the most frequent hashtags appearing in the racist/sexist tweets.Racist/Sexist TweetsAs expected, most of the terms are negative with a few neutral terms as well. So, its not a bad idea to keep these hashtags in our data as they contain useful information.Next, we will try to extract features from the tokenized tweets.To analyze a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques  Bag-of-Words, TF-IDF, and Word Embeddings. In this article, we will be covering only Bag-of-Words and TF-IDF.Bag-of-Words is a method to represent text into numerical features. Consider a corpus (a collection of texts) called C of D documents {d1,d2..dD} and N unique tokens extracted out of the corpus C. The N tokens (words) will form a list, and the size of the bag-of-words matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).Let us understand this using a simple example. Suppose we have only 2 documentD1: He is a lazy boy. She is also lazy.D2: Smith is a lazy person.The list created would consist of all the unique tokens in the corpus C. = [He,She,lazy,boy,Smith,person]Here, D=2, N=6The matrix M of size 2 X 6 will be represented as Now the columns in the above matrix can be used as features to build a classification model. Bag-of-Words features can be easily created using sklearns CountVectorizer function. We will set the parameter max_features = 1000 to select only top 1000 terms ordered by term frequency across the corpus.This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account, not just the occurrence of a word in a single document (or tweet) but in the entire corpus.TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.Lets have a look at the important terms related to TF-IDF:We are now done with all the pre-modeling stages required to get the data in the proper form and shape. Now we will be building predictive models on the dataset using the two feature set  Bag-of-Words and TF-IDF.We will use logistic regression to build the models. It predicts the probability of occurrence of an event by fitting data to a logit function.The following equation is used in Logistic Regression:Read this article to know more about Logistic Regression. Note: If you are interested in trying out other machine learning algorithms like RandomForest, Support Vector Machine, or XGBoost, then we have a free full-fledged course on Sentiment Analysis for you.
Output: 0.53We trained the logistic regression model on the Bag-of-Words features and it gave us an F1-score of 0.53 for the validation set. Now we will use this model to predict for the test data.The public leaderboard F1 score is 0.567. Now we will again train a logistic regression model but this time on the TF-IDF features. Lets see how it performs.Output: 0.544The validation score is 0.544 and the public leaderboard F1 score is 0.564. So, by using the TF-IDF features, the validation score has improved and thepublic leaderboard score is more or less the same. If you are interested to learn about more techniques for Sentiment Analysis, we have a well laid out video course on NLP for you.This course is designed for people who are looking to get into the field of Natural Language Processing. It provides you everything you need to know to become an NLP practitioner.
Key topics covered in the course:In this article, we learned how to approach a sentiment analysis problem. We started with preprocessing and exploration of data. Then we extracted features from the cleaned text using Bag-of-Words and TF-IDF. Finally, we were able to build a couple of models using both the feature sets to classify the tweets.Did you find this article useful? Do you have any useful trick? Did you use any other method for feature extraction? Feel free to discuss your experiences in comments below or on the discussion portal and well be more than happy to discuss.Full Code: https://github.com/prateekjoshi565/twitter_sentiment_analysis/blob/master/code_sentiment_analysis.ipynb",https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/
An AI Algorithm Detects your Personality by analyzing Eye Movement!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Comprehensive Hands on Guide to Twitter Sentiment Analysis with dataset and code|Making AI Systems Faster and More Efficient with Google Edge TPUs|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Psychologists and researchers all over the world have been trying to determine an individuals personality based on characteristics, posture, handwriting, etc since times immemorial. Thousands upon thousands of tests and studies have been done on the subject but now finally artificial intelligence might have a concrete answer.Researchers claim that eye movement can be used as an indicator of an individuals personality. They have used state-of-the-art machine learning algorithms to demonstrate their findings. The result of the study undertaken by the neuroscience researchers from the University of Stuttgart in Germany and Flinders University in Australia have shown mind blowing results.As a part of this study, researchers tracked the eye movements of 42 participants, while they undertook everyday tasks in the university. On feeding the data into the AI algorithm, the researchers found that computers running the algorithm were able to record human eye movements and immediately determine a persons major personality traits.Well established userquestionnaires were used to assess these findings. The team claims that their algorithm successfully recognizes four of the Big Five personality traits, namely: neuroticism, extroversion, agreeableness, and conscientiousness.The study has been published in a scientific journal calledFrontiers in Neuroscience. The results show that AI is able to identify the most important personality types. A few previously undiscovered links between specific eye movements and personality characteristics have been revealed in the final report. The team is positive that it can be useful for providing inferences in social signal processing and social robots. The researchers believe that this will be able to improve the human-machine interactions and improvise personalized services.This is something very different from the usual research papers we come across. The team has proved that there exists a strong correlation between the eye movements of an individual and what they think. While this has been hypothesized earlier, its nice to finally have some substantial proof.As mentioned by one of the team members in an interview, the results of the study could be used to improve the human-robot interactions. Does this put us closer to general artificial intelligence? Its too early to make that claim but its a step in the right direction.",https://www.analyticsvidhya.com/blog/2018/07/artificial-intelligence-eye-tracking-identify-individuals-personality/
Making AI Systems Faster and More Efficient with Google Edge TPUs,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An AI Algorithm Detects your Personality by analyzing Eye Movement!|Top 10 Pretrained Models to get you Started with Deep Learning (Part 1  Computer Vision)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In 2016, Google unveiled Tensor Processing Units, or TPUs are theyre more commonly known  chips specifically designed for Googles TensorFlow framework. Taking this a step further, the tech behemoth has now introduced Edge TPU, a small artificial intelligence accelerator that enables machine learning jobs in IoT (Internet of Things) devices.The Edge TPU is designed to perform tasks that the machine learning algorithm are trained for. For example, it will be able to recognize an object in a picture. This part of preforming task which the algorithm is trained for, is known as inference. While the Edge TPUs are designed to perform the inference, Goggles server based TPUs are responsible for training the algorithm.As the team mentioned in their blog post, the newly designed chips are actually meant to be used in various enterprise jobs such as for automating the check for quality in factories. If you were hoping to see it in your smart devices, sorry to disappoint! Currently, the hardwares that are used send the data over the internet for analysis. These hardwares will now be replaced with the devices which will eliminate this process. This means lesser downtime and faster results.This is not the first attempt at creating AI chips for on-device tasks. Other companies like ARM, Qualcomm and Mediatek have their own AI accelerators and of course Nvidias GPUs are one of the best in the business. Then how is Google different from any of these?Here is the interesting part  with Google, one can store the data on Google Cloud, train their algorithms using TPUs, and then carry out on-device inference using the new Edge TPUs. Google can ensure that all the processes mentioned run as efficiently and smoothly as possible so as to make it a seamless experience for the end user.Google is also making the Edge TPU available as a development kit for customers. The idea is to let the customers test the hardwares capability and see how it fits into their existing product catalog.Google continues to stamp its authority in the IoT space. Having on-device machine learning is expected to be comparatively more secure and provide faster results.Also, for the end user, storing data, training algorithms and performing the required task(s) will all become more simpler as they will not have to switch to different platforms. Googles Cloud offerings, TPU and Edge TPU will cover all of this!",https://www.analyticsvidhya.com/blog/2018/07/google-unveils-ai-chips-on-device-machine-learning/
Top 10 Pretrained Models to get you Started with Deep Learning (Part 1  Computer Vision),Learn everything about Analytics|Introduction|Table of Contents|Object Detection|Mask R-CNN|YOLOv2|MobileNet|Ripe/Unripe Tomato Classification|Car Classification|Facial Recognition and Regeneration|VGG-Face Model|3D Face Reconstruction from a Single Image|Segmentation|Semantic Image Segmentation  Deeplabv3+|Robot Surgery Segmentation|Miscellaneous|Image Captioning|End Notes,"Share this:|Like this:|Related Articles|Making AI Systems Faster and More Efficient with Google Edge TPUs|Infographic  13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Pretrained models are a wonderful source of help for people looking to learn an algorithm or try out an existing framework. Due to time restrictions or computational restraints, its not always possible to build a model from scratch which is why pretrained models exist! You can use a pretrained model as a benchmark to either improve the existing model or test your own model against it. The potential and possibilities are vast.Source: Facebook AIIn this article, we will look at various pretrained models in Keras that have applications in computer vision. Why Keras? First, because I believe its a great library for people starting out with neural networks. Second, I wanted to stick with one framework throughout the article. This will help you in moving from one model to the next without having to worry about frameworks.I encourage you to try each model out on your own machine, understand how it works, and how you can improve or tweak the internal parameters.We have segmented this topic into a series of articles. Part II will focus on Natural Language Processing (NLP) and Part III will look at Audio and Speech models. The aim is to get you up and running in these fields with existing solutions that will fast track your learning process.Object detection is one of the most common applications in the field of computer vision. It has applications in all walks of life, from self-driving cars to counting the number of people in a crowd. This section deals with pretrained models that can be used for detecting objects. You can also check out the below articles to get familiar with this topic:Mask R-CNN is a flexible framework developed for the purpose of object instance segmentation. This pretrained model is an implementation of this Mask R-CNN technique on Python and Keras. It generates bounding boxes and segmentation masks for each instance of an object in a given image (like the one shown above).This GitHub repository features a plethora of resources to get you started. It includes the source code of Mask R-CNN, the training code and pretrained weights for MS COCO, Jupyter notebooks to visualize each step of the detection pipeline, among other things.YOLO is an ultra popular object detection framework for deep learning applications. This repository contains implementations of YOLOv2 in Keras. While the developers have tested the framework on all sorts of object images  like kangaroo detection, self-driving car, red blood cell detection, etc., they have released the pretrained model for raccoon detection.You can download the raccoon dataset here and get started with this pretrained model now! The datasetconsists of 200 images (160-training, 40-validation). You can download the pretrained weights for the entire model here. According to the developers, these weights can be used for an object detector for one class.As the name suggests, MobileNet is an architecture designed for mobile devices. It has been built by none other than Google. This particular model, which we have linked above, comes with pretrained weights on the popular ImageNet database (its a database containing millions of images belonging to more than 20,000 classes).As you can see above, the applications of MobileNet are not just limited to object detection but span a variety of computer vision tasks  like facial attributes, landmark recognition, finegrain classification, etc.If you were given a few hundred images of tomatoes, how would you classify them  say defective/non-defective, or ripe/unripe? When it comes to deep learning, the go-to technique for this problem is image processing. In this classification problem, we have toidentify whether the tomato in the given image is grown or unripe using a pretrained Keras VGG16 model.The model was trained on 390 images of grown and unripe tomatoes from the ImageNet dataset and was tested on 18 different validation images of tomatoes.The overall result on these validation images is given below:There are numerous ways of classifying a vehicle  by its body style, number of doors, open or closed roof, number of seats, etc. In this particular problem, we have to classify the images of cars into various classes. These classes include make, model, year, e.g. 2012 Tesla Model S. To develop this model, the car dataset from Stanford was used which contains16,185 images of 196 classes of cars.The model was trained using pretrained VGG16, VGG19 and InceptionV3 models. The VGG network is characterized by its simplicity, using only 33 convolutional layers stacked on top of each other in increasing depth. The 16 and 19 stand for the number of weight layers in the network.As the dataset is small, the simplest model, i.e. VGG16, was the most accurate. Training the VGG16 network gave an accuracy of 66.11% on the cross validation dataset.More complex models like InceptionV3 were less accurate due to bias/variance issues.Facial recognition is all the rage in the deep learning community. More and more techniques and models are being developed at a remarkable pace to design facial recognition technology. Its applications span a wide range of tasks  phone unlocking, crowd detection, sentiment analysis by analyzing the face, among other things.Face regeneration on the other hand, is the generation of a 3D modelled face from a closeup image of a face. The creation of 3D structured objects from mere two dimensional information is another thought out problem in the industry. The applications of face regeneration are vast in the film and gaming industry. Various CGI models can be automated thus saving tons of time and money in the process.This section of our article deals with pretrained models for these two domains.Creating a facial recognition model from scratch is a daunting task. You need to find, collect and then annotate a ton of images to have any hope of building a decent model. Hence using a pretrained model in this domain makes a lot of sense.VGG-Face is adataset that contains 2,622 unique identities with more than two million faces. This pretrained modelhas been designed through the following method:This is a really cool implementation of deep learning. You can infer from the above image how this model works in order to reconstruct the facial features into a 3 dimensional space.This pretrained model was originally developed using Torch and then transferred to Keras.Semantic image segmentation is the task of assigning a semantic label to every single pixel in an image. These labels can be sky, car, road, giraffe, etc. What this technique does is it finds the outlines of objects and thus places restrictions on the accuracy requirements (this is what separates it from image level classification which has a much looser accuracy requirement).Deeplabv3 is Googles latest semantic image segmentation model. It was originally created using TensorFlow and has now been implemented using Keras. This GitHub repository also has code for how to get labels, how to use this pretrained model with custom number of classes, and of course how to trail your own model.This model attempts to address the problem of image segmentation of surgical instruments in a robot-assisted surgery scenario. The problem is further divided into two parts, which are as follows:This pretrained model is based on the U-Net network architecture and is further improved by using state-of-the-art semantic segmentation neural networks known as LinkNet and TernausNet. The model was trained on 8  225-frame sequences of high resolution stereo camera images.Remember those games where you were given images and had to come up with captions? Thats basically what image captioning is. It uses a combination of NLP and Computer Vision to produce the captions.This task has been a challenging one for a long time as it requires huge datasets with unbiased images and scenarios. Given all these constraints, the algorithm must be generalized for any given image.A lot of businesses are leveraging this technique nowadays but how can you go about using it?The solution lies in converting a given input image into a short and meaningful description. The encoder-decoder framework is widely used for this task. The image encoder is a convolutional neural network (CNN).This is aVGG 16 pretrained model on the MS COCO dataset where the decoder is a long short-term memory (LSTM) network predicting the captions for the given image. For detailed explanation and walk through its recommended that you follow up with our article on Automated Image Captioning.Deep learning is a tricky field to get acclimated with, thats why we see researchers releasing so many pretrained models. Having personally used them to understand and expand my knowledge of object detection tasks, I highly recommend picking a domain from the above and using the given model to get your own journey started.In the next article, we will dive into Natural Language Processing. If you have any feedback or suggestions for this article and the series as a whole, use the comments section below to let me know.",https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/
Infographic  13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them,Learn everything about Analytics|Introduction,"Share this:|Like this:|Related Articles|Top 10 Pretrained Models to get you Started with Deep Learning (Part 1  Computer Vision)|Googles BigQuery ML Tool Enables you to do Machine Learning through SQL!|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The path to becoming a data scientist is one riddled with traps and challenges. If youre not careful, you might just fall into one of them! There are so many resources out there which aim to help aspiring data scientists become experts, but most of them are half-baked efforts that leave a gaping hole in your data scientist journey.This infographic lists down 13 common mistakes we have seen fresher data scientists make, and we even porvide tips on how to avoid them. To get a more comprehensive overview of this infographic, and to get free resources on how to avoid each mistake, head over to our popular article  13 Mistakes Amateur Data Scientists Make and How to Avoid Them.I highly recommend downloading this infographic as a pocket guide on your data scientist journey. You can download a high resolution PDF (just 1.5MB) from this link.",https://www.analyticsvidhya.com/blog/2018/07/infographic-common-mistakes-amateur-data-scientists-make-how-avoid-them/
Googles BigQuery ML Tool Enables you to do Machine Learning through SQL!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Infographic  13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them|PixieDebugger  A Visual Python Debugger for Jupyter Notebooks Every Data Scientist Should Use|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning is THE buzzword of this decade. But job vacancies for experts in this field keeps increasing despite a gigantic amount of people wanting to get a piece of the pie. This is because machine learning is a complex field that combines knowledge, skill, innovation and domain expertise  and most aspiring data scientists are found wanting in at least one of these areas.Source: TwitterMost of these aspiring data scientists are coming from an IT background, with a good (or at least manageable) grasp of SQL. If you are one of these people, theres good news  Google wants to help you get familiar with ML! The tech giant has released Google BigQuery ML, a cloud offering that enables developers to design and build models in BigQuery using standard SQL queries.Check out the below GIF, posted on Googles AI blog today, that gives a quick overview of how you can use the tool:Below are some of the advantages this tool currently offers:Google has even documented a getting started guide for both data analysts and data scientists who want to learn about this tool:Awesome news for SQL users! I personally know a lot of folks in IT who want to make the transition to machine learning, but are held back by the complexities this field has. BigQuery ML will clear at least a few of those hurdles for you. Learning and doing ML in a language you are absolutely familiar with is priceless.Currently, BigQuery ML supports both linear regression and logistic regression models but they are working to integrate other techniques into the tool. We should expect to see that happen in the near future.",https://www.analyticsvidhya.com/blog/2018/07/googles-bigquery-ml-tool-do-machine-learning-through-sql/
PixieDebugger  A Visual Python Debugger for Jupyter Notebooks Every Data Scientist Should Use,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Googles BigQuery ML Tool Enables you to do Machine Learning through SQL!|IIT Hyderabad Researchers are using Machine Learning to Identify Bike Drivers without Helmets|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picture this  you think youve designed a phenomenal model using tons of lines of code but theres a pesky error in there. Its holding back the model from achieving its full power but you cant quite figure out whats wrong. Sure, you could use Jupyters pdb but wouldnt it be awesome to visually debug that code block within an interactive environment? Ive got some awesome news for you!PixieDebugger, developed by the PixieDust team, claims to be the first visual debugger for Python that works exclusively with Jupyter Notebooks. Note that it doesnt yet work with JupyterLab but the developers might be working on fixing this soon.The PixieDebugger comes packaged with multiple features, including:Keep in mind that you need to install PixieDust as a prerequisite before using this debugger. To do so, simply go to command prompt/terminal, and paste the following:Before you start using it, youll need to import it first:Now youre ready! Invoke this awesome tool as a magic command using the simple below code:And voila! Now try this out on any code block you have open in your Jupyter notebook and youll start to see the magic.Check out the below video which shows how PixieDebugger works in a Jupyter Notebook environment:Ive been using Jupyter Notebooks more and more lately (Im started using Python recently after switching over from R), so this feels like the icing on the cake. Having tried PixieDebugger out extensively, I can vouch for how easy and helpful it make my coding experience. I encourage all data scientists to try it out as well, its sure to become a very handy tool for you.In case you are a beginner with Python and/or Jupyter Notebooks, go through this article which comprehensively covers all aspects of this wonderful IDE and why you should use it next time you start working on a new project or dataset.",https://www.analyticsvidhya.com/blog/2018/07/pixie-debugger-python-debugging-tool-jupyter-notebooks-data-scientist-must-use/
IIT Hyderabad Researchers are using Machine Learning to Identify Bike Drivers without Helmets,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|PixieDebugger  A Visual Python Debugger for Jupyter Notebooks Every Data Scientist Should Use|Protect your Machine Learning Model with IBMs Watermark Algorithm|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Getting drivers of two-wheeler vehicles to wear helmets in India has been an eternal struggle. Despite the law being made official, and the number of road accidents increasing, the adoption of helmets by these riders has sadly been a no-show. How does the police enforce this rule?According to a report by the Economic Times, researchers from IIT Hyderabad may have the answer. They have designed a machine learning algorithm that uses computer vision techniques to detect motorcyclists without helmets, using CCTV cameras. They have already filed to get this technology patented and this could soon be put into action on the Hyderabad roads.The algorithm will be installed in the CCTV cameras, which will be connected to the servers that will be monitored by the police. It will also be conncted to the existing RTO website so that appropriate fines and penalties can be handed out. Additionally, the violaters will be notified of their error through a SMS.As with almost all advanced research being done around the world in machine learning, the technique at the core of this system is a convolutional neural network. The current idea is to deploy these cameras at various intersections in the city, like toll bridges, checkpoints, traffic lights, etc.Along with this, the researchers have also developed a framework for detecting snatch thieves in the city.A great initiative and it feels wonderful to see this coming from India. This framework can also be applied to other road related incidents  traffic light violations, detection of tripling on two-wheelers, overspeeding, among other things.I hope this sees the light of day sooner rather than later because it has the potential to reduce the number of accidents by quite a margin. And of course, it will be another shining example of machine learning being put to use for the welfare of society.",https://www.analyticsvidhya.com/blog/2018/07/iit-hyderabad-researchers-using-machine-learning-identify-bike-drivers-without-helmets/
Protect your Machine Learning Model with IBMs Watermark Algorithm,Learn everything about Analytics|Overview|Introduction|Our take on this,"So how does this all work?|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|IIT Hyderabad Researchers are using Machine Learning to Identify Bike Drivers without Helmets|DataHack Radio Episode #5: Building High Performance Data Science teams with Kiran R|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Books, documents, images and videos are rightly considered sacred intellectual properties and are appropriately watermarked by the creators. So why shouldnt the same be done with machine learning models? Businesses spend so much time and effort in building them, so it makes sense to protect them, especially when it comes to commercial applications.This is the concept IBM has followed in their latest study. Their aim is to help businesses and data scientists protect their work, especially those complex deep learning models. Protected from what, you might be wondering. Well, there are unfortunately too many people who use technology for the wrong purpose, and machine learning has not been spared from that (fake images and videos come to mind).We recently published an article on how deep learning can be used to fight off adversaries and bolster cyber security, and IBMs research adds a different perspective to that. IBM has already applied to patent this approach.When youre applying watermarks to an image or video, there are essentially two stages to it  embedding and detection. In the embedding phase, the developer can overlay the watermark on the image. If it is indeed stolen, the detection stage comes into play. Here, the developer can extract the embedded watermark to prove his/her ownership. This exact idea is used by IBM to protect deep neural networks.The researchers developed three different algorithms to generate watermarks for these neural networks. As described by them in their blog post:These algorithms were then tested and verified on two popular datasets  MNIST, and CIFAR10.You can read about IBMs efforts with AI watermarking in their blog post hereand their full research paper here.A curious research by IBM, and certainly one I hadnt thought of, or read about before. I certainly appreciate the aim with which they have pursued this study, though it remains to be seen how the adversaries and attackers find a way around this as well. Currently, there are a few limitations to this approach. If the model is deployed as an internal service rather than online, this approach will not work.",https://www.analyticsvidhya.com/blog/2018/07/protect-machine-learning-model-ibms-watermark-ai-system/
DataHack Radio Episode #5: Building High Performance Data Science teams with Kiran R,"Learn everything about Analytics|Introduction|Kirans Profile|Kiran Rs Journey|Kaggle Competitions and Experience|Competitions and Real-World Job|Kirans take on how Data Science Works in the Industry|Day-to-day Schedule as a Director, Data Sciences CoE at VMware|Hiring and Recruitment Strategy|The Next 5 Years in Data Science|Advice to Graduates and Freshers in Data Science|End Notes","Share this:|Like this:|Related Articles|Protect your Machine Learning Model with IBMs Watermark Algorithm|Googles Latest AI Experiment is Fun, Free and Combines TensorFlow.js and Pose Estimation|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Very rarely do you come across someone who brings all of the following skills to the table:What do you do when you get some one with these skills?Well  you ask them to be your guest on DataHack Radio and be a speaker on DataHack Summit and you let them do the talking!Ladies and Gentleman  today we have a guest like this on the show  please give a round of applause to Kiran R,Director of the Data Sciences Center of Excellence at VMware.He has also agreed to do a talk at DataHack Summit 2018 this year  so stay tuned.Kiran is a computer science engineer from MSRIT and a post-graduate from the Indian Institute of Management. Kozhikode (IIM-K). He brings more than14 years of experience building high performance teams for several leading organizations. He is currently leading the Data Sciences Center of Excellence at VMware.Kiran is one of the few Grandmasters on Kaggle who has made his mark in the overall top 10 data scientists there. He is also a winner of the KDD 2014 Data Mining Cup. He is very passionate about data science, as you will discover in his conversation with Kunal.This is a MUST-LISTEN podcast that spans the length and breadth of the various components in the world of data science.This article is essentially a highlight reel of this incredibly knowledge-rich podcast. Happy listening! You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms: Kirans first job was at Motorola as a software engineer writing C/C++ code. After spending a year there, he decided to pursue higher studies and completed his MBA from IIM Kozhikode in 2006. He took up an offer from Dell as an Analytics Manager and worked on various tools like SQL, Access and SAS (to build propensity models).His brilliant work ethic and keen innovative mind led him to win Dells India Innovator of the Year award in 2012, handed to him directly by Michael Dell!He set up his own proprietorship under the name Chaotic Experiments which he used while participating in several Kaggle competitions. He regularly finished in the top 10 in various competitions and achieved his highest rank of 7 till date during this period. He also did some freelance work in data mining at the time.From there, Kiran had stints at Amazon and Flipkart. He has been in his current role at VMware since the last four years. His role involves managing a team of data scientists that work across various domains including marketing, pricing and strategy.One of the more fascinating things about Kirans career arc has been his choice to stay in a technical role even after getting his management degree. He was determined not to lose touch with his technical skills and carved out a path for himself accordingly.Kiran started off his Kaggle journey back when there werent too many data science enthusiasts around. The way he used to approach these problems was to build a diverse set of models  a linear model was always a good start, random forest, gradient boosting and then an ensemble model.He made a lot of acquaintances through the platform and these competitions always pushed him to get out of his comfort zone and learn new techniques and build frameworks. This is something he stressed on, because most data scientist jobs restrict you to only those tools and techniques which the business can support. Platforms like Kaggle and Analytics Vidhya allow you to spread your wings.If you have a hammer and only one tool, everything looks like a nail to you.Participating and learning through competitions helped Kiran a lot when it came to his professional job. There is obviously a difference between the two  like in the industry, you wont get a ready-made dataset and you will need to convert the business problem into a data mining problem.But competitions and platforms like Analytics Vidhya help you become an expert in techniques and tools, and finding out why and how things work (or dont work). Concepts like overfitting can really help you in the industry and competitions also expose you to problems from various domains, which is an added advantage.Kiran also used his experience to tell us that most problems in the industry are classification based. Regression and segmentation are used as well, but pale in comparison to classification problems. Some of the most widely used techniques are logistic regression and tree-based models.Kirans experience has mostly been in the B2C space (with the last 4 years in the B2B sector). He uses the following parameters to classify the data science work companies do:Kiran used these pointers to tell us his take on how Amazon, Flipkart and Dell leverage data science. His insights are fascinating and will enrich your knowledge of how data science works in the real-world.He also elaborated on how his current organization, VMware, uses data science. He went on tell us the difference between B2B and B2C, and how its easier to test your models and results in a B2B space than a B2C space (primarily because of the longer purchase cycle the former has).Kirans role is divided into 2 parts  90% of it is a functional role, which is to lead the Data Sciences Center of Excellence. The other 10% is to play a sight leader role for enterprise information management (BI, Analytics, and Data Science).His typical day involves a lot of operational activities, tons of people activities, project deliveries, data science technique brainstorming, interacting with stakeholders, among other things. He has a very hands-on leadership style which helps him juggle and manage these various aspects on a daily basis.Kiran believes hiring is the most important part of team building, because if you have the right people, the job will get done. When he hires people for his team, there are a few critical areas he looks at:I highly recommend listening to the podcast to understand the details of these pointers which Kiran has elaborated on in detail. This will give you a brilliant insight into what an industry leader looks for when hiring a data scientist.One thing that has stood out for Kiran is the pace at which data science has grown in recent years. The interest in this field is extremely high right now and he sees this continuing in the future.Advancement in the type of models we make will increase and the time it takes to train models will decrease, according to Kiran. This will of course be helped by far better computational power (GPUs could well get cheaper) and we should see deep learning being used more and more on structured data.Another change Kiran predicts in the near future is the increasing pressure on business leaders to adopt AI and data mining. As more and more techniques are developed and become interpretable, businesses will be forced into situations where they have the leverage AI to gain any competitive edge they can get.Most of the proprietor software, like SAS, will go away according to Kiran. Open source languages like R and Python have already eaten away into the lead SAS had and will continue to gain popularity in the coming years. Open source is the driving force behind machine learning and more and more researchers and organizations are realizing this.Kiran firmly believes one needs to be good in programming to make a career in machine learning. If you cant do programming, you will have an almighty struggle to get into this field. If youre new to this field, Kirans advice is to pick up Python and get very familiar with it. You should also eventually learn more than one language which will help expand your skillset.Learn the fundamentals of data science well. There are tons of excellent resources out there so theres no excuse not to do this. Once you build your base, you can look at applying for a job in this domain. Another critical aspect, often overlooked, is debugging skills. If you cant debug your program or code, you will end up getting frustrated with the shortcomings in your script.Also, develop some complementary skills (like web development or application development) to go along with the data science techniques. As a data scientist, you need to understand how the technical aspects of how to put your model output into a deployment friendly format (among other things).This was another quality addition to our DataHack podcast series. Kiran Rs incredible knowledge and his willingness to share it with the community was obvious in this conversation with Kunal. You will learn a LOT of new things about the data science domain.",https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-building-data-science-teams-kiran-r/
"Googles Latest AI Experiment is Fun, Free and Combines TensorFlow.js and Pose Estimation",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #5: Building High Performance Data Science teams with Kiran R|Evolutionary Algorithm  The Surprising and Incredibly Useful Alternative to Neural Networks|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Pose estimation is a hot research topic in machine learning these days. A few weeks back I covered how a Japanese firm is using it to detect shoplifters, and thats just one example of how useful this technique can be.Of course, Google has been heavily involved in pioneering this technique. They released an open-source version of PoseNet recently and have now used it, along with TensorFlow.js, to run a fun experiment, called Move Mirror. It tracks your movements via your computers webcam and looks for images that match the poses you make.Google has used over 80,000 images to build its catalog and it delves into each of these images to find the ones that match your pose. The result, especially if youre constantly changing your pose, can be dizzying. The experiment shows your image on side of the screen, and the matching pose (from its collection of images) on the adjacent side. The above photos illustrate this point.Pose estimation is, as the name suggests, the machine trying to predict what pose the human is in. Its far more tricky than it sounds. In a three-dimensional space, people have all sorts of features that can be difficult to track  body shape, joints, etc. The task is made doubly difficult when there are other objects in the frame. The algorithm has to be able to distinguish between that and the human, in addition to doing the tracking task.The experiment is done solely through the browser, thanks to TensorFlow.js.If youre worried about privacy, Google claims it does not store your photos on its servers since everything is done in the browser itself.A fun experiment! It makes a nice change from all the serious research going on that is being conducted globally. Trust Google to run funky and weird experiments like these to not only test out their research, but also keep people interested in machine learning.TensorFlow.js and pose estimation are fields that I have been wanting to try out since they came into mainstream popularity and this has certainly piqued my interest. If you have used this technology before, or want to now, get in touch with me and we can definitely work on this together.",https://www.analyticsvidhya.com/blog/2018/07/googles-latest-ai-experiment-combines-tensorflow-js-pose-estimation/
Evolutionary Algorithm  The Surprising and Incredibly Useful Alternative to Neural Networks,Learn everything about Analytics|Overview|Introduction|Our take on this,"How does it work?|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Googles Latest AI Experiment is Fun, Free and Combines TensorFlow.js and Pose Estimation|Elon Musk, DeepMind Co-Founders and other AI Leaders Pledge Against Developing Autonomous Weapons|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural networks have become the be all and end all of all machine learning models. No matter which research blog you read about, DeepMind, Google AI, Facebooks FAIR, etc., most of the latest research has neural networks at the core of the system.From facial recognition and object detection to beating humans in board and video games, neural networks have developed an aura and power of their own. The concept has been around for decades, but has gained massive popularity in recent years thanks to advanced in technology and hardware. These neural nets are essentially based on how our brain works.But a new type of algorithm, called Evolutionary Algorithm, has been developed that could significantly change the way we build and design deep learning models. Instead of trying to map the neurons like in a human brain, this approach is based on evolution  the process that has shaped the human brain itself. This evolutionary algorithm has been used to beat deep learning powered machines in various Atari games.The evolutionary algorithm approach begins with generating code at a completely random rate (tons of versions of code actually). These code pieces are then tested to check whether the intended goal has been achieved. As you can imagine, most of the code pieces are scrappy and make no sense because of their random nature.But eventually some pieces of code are found that are better than the rest. These pieces are then used to reproduce a new generation of code (which is not identical to the original code because that would defeat the purpose). As new code is generated, it is continuously tested and this process keeps repeating until such a code is found that is better than anything else at solving the problem. Can you now understand how this relates to the evolution of the human brain?The algorithm outperformed deep learning systems by a comfortable margin. The best part? It did so in a much quicker fashion than any deep learning system there!Read more about this algorithm in MITs Technology Review article and also ensure you read the highly detailed research paper. This paper was published by Dennis Wilson and his colleague at the University of Toulouse.Thisevolutionary approach has been around for a while but due to the advancements in deep learning, it has taken a back seat. This research has already brought some attention to it. Apart from taking less training time, the code is fairly easy to interpret because the evolved approach means smaller code blocks. And interpretability is a MAJOR issue these days.Are data scientists working on deep learning missing out on this technique? This research certainly puts the evolutionary algorithm right in the middle of the debate. Its definitely worth checking out.",https://www.analyticsvidhya.com/blog/2018/07/evolutionary-algorithm-perfect-useful-alternative-neural-network/
"Elon Musk, DeepMind Co-Founders and other AI Leaders Pledge Against Developing Autonomous Weapons",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Evolutionary Algorithm  The Surprising and Incredibly Useful Alternative to Neural Networks|FLASH SALE! Get Flat 55% off on our Popular Introduction to Data Science Course!|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In the past few years, we have seen artificial intelligence come a long way. From autonomous robots to AI bots that can talk like humans, knowingly and unknowingly we have created a whole new world which can work (mostly) without human assistance. Its an exciting yet scary time to be alive!Top AI researches have taken a step forward to ensure that does not happen anytime soon. Some of the top AI leaders, including Elon Musk, three co-founders of Googles DeepMind, and Stuart Russell, have pledged to not develop any AI autonomous weapons. This will include any AI system that has the ability to select and engage targets without human supervision.The pledge was published on 18th July 2018 at the International Joint Conference on Artificial Intelligence (IJCAI) in Stockholm. The conference was organized by the Future of Life Institute concerned with the growth of lethal autonomous weapons. Thousands of researchers unanimously agreed that machines should not be allowed to make decisions pertaining to life and death. More than 2,400 researchers and around 160 companies have signed the pledge.Until today, no hard limits were imposed  on the development of AI for military. The technology for developing AI weapons has taken good shape and various attempts supporting regulation of autonomous weapons have proven to be highly ineffective. Recently, Google was in the throes of a controversy after several employees resigned when it was revealed that the company was working to assist with building weapons using AI. The aim is to discourage the development of AI-enhanced killer robots.This is a much needed step, taken by the researchers and AI practitioners. The upcoming technologies and development of autonomous robots do amaze us, but also leave a question in our minds about our safety and ethics. Recently, we saw an AI system that can dream on its own, and another bot that could talk exactly like humans, and this possibilities are tantalizing and endless.This act will help draw a line which should help in regulating AI and its use for the wrong reasons.",https://www.analyticsvidhya.com/blog/2018/07/elon-musk-deepmind-top-ai-leaders-pledge-against-developing-deadly-autonomous-weapons/
FLASH SALE! Get Flat 55% off on our Popular Introduction to Data Science Course!,Learn everything about Analytics|Introduction|Introduction to Data Science!|Highlights of the Course,"Enroll NOW on this linkbefore the offer expires!|Identifying the best agents|Sales Prediction for a large Supermarket|Predict the Titanic Survivors|Share this:|Like this:|Related Articles|Elon Musk, DeepMind Co-Founders and other AI Leaders Pledge Against Developing Autonomous Weapons|Ultimate Guide: Building a Mask R-CNN Model for Detecting Car Damage (with Python codes)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Without data, youre just another person with an opinion.  W. Edwards DemingThe role of a data scientist is the most sought after in the industry these days. If you are one of those aspiring data scientists, but dont know where to start, youve come to the right place! We are excited to announce an exclusive 48-hours flash sale, starting today at 12.48pm till 21st July 12.48pm, on our most popular and best selling-course The course will be available for Rs. 5,310 ($77.48) on our trainings platform for the entirety of this flash sale.Apply your exclusive coupon code:FLASH55and enroll at a mind-boggling 55% discount (the list price for this course is Rs. 11,800).This is the ultimate unmissable opportunity to take your first step towards becoming a data scientist.This course covers the basics of Python, before diving into the world of statistics and probability and finally a detailed go-through of the various data science modeling techniques commonly used in the industry. The course has been curated and taught by experienced instructors from Analytics Vidhya and requires no prior knowledge of data science or Python.Below is a summary of the learning modules covered in this course:One of the most unique aspects of this course is the real-life projects we provide. We firmly believe in practicing what you learn, and our core belief is reflected in how we have structured the projects. Check out the projects below:Your client is a Financial Distribution company. Over the last 10 years, they have created an offline distribution channel across country. They sell Financial products to consumers by hiring agents in their network. These agents are freelancers and get commission when they make a product sale.The data scientists at BigMart have collected sales data for 1559 products across 10 stores in different cities for an entire year. Also, certain attributes of each product and store have been defined. You will build a predictive model to forecast the sales of each product at a particular store.You will analyse what kind of people were likely to survive in Titanic tragedy. You will apply machine learning tools to predict which passengers survived the tragedy.Check out a sample video from the course, which covers the difference between forecasting, predictive modeling and machine learning:Enroll NOW!",https://www.analyticsvidhya.com/blog/2018/07/flash-sale-flat-55-off-introduction-to-data-science-course/
Ultimate Guide: Building a Mask R-CNN Model for Detecting Car Damage (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|What is Mask R-CNN?|How Mask R-CNNworks|How to build a Mask R-CNN Model for Car Damage Detection|End Notes|References,"Collecting Data|Annotating the Data|Training a model|Validate your model|Run model on images and make predictions|About the Author|Share this:|Like this:|Related Articles|FLASH SALE! Get Flat 55% off on our Popular Introduction to Data Science Course!|MyStory: Step by Step process of How I Became a Machine Learning Expert in 10 Months|
Guest Blog
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The applications of computer vision continue to amaze. From detecting objects in a video, to counting the number of people in a crowd, there is no challenge that computer vision seemingly cannot overcome.One of the more intriguing applications of computer vision is identifying pixels in a scene and using them for diverse and remarkably useful purposes. We will be taking up one such application in this article, and trying to understand how it works using Python!The aim of this post is to build a custom Mask R-CNN model that can detect the area of damage on a car (see the image example above). The rationale for such a model is that it can be used by insurance companies for faster processing of claims if users can upload pics and they can assess damage from them. This model can also be used by lenders if they are underwriting a car loan especially for a used car.You can read an in-depth explanation of Mask R-CNN and how it works in more detail here.Sponsored: Check out our Computer Vision using Deep Learning Course for creating Computer Vision applicationsMask R-CNN is an instance segmentation model that allows us to identify pixel wise location for our class. Instance segmentation means segmenting individual objects within a scene, regardless of whether they are of the same typei.e, identifying individual cars, persons, etc. Check out the below GIF of a Mask-RCNN model trained on theCOCOdataset. As you can see, we can identify pixel locations for cars, persons, fruits, etc.Mask R-CNN is different from classical object detection models like Faster R-CNN where, in addition to identifying the class and its bounding box location, it can also color pixels in the bounding box that correspond to that class. When do you think we would be need this additional detail? Some examples I can think of are:The easiest way to try a Mask R-CNN model built on COCO classes is to use theTensorflow Object Detection API. You can refer to this article (written by me) that has information on how to use the API and run the model on YouTube videos.Before we build a Mask R-CNN model, lets first understand how it actually works.A good way to think about Mask R-CNN is that it is a combination of a Faster R-CNN that does object detection (class + bounding box) and FCN (Fully Convolutional Network) that does pixel wise boundary. See figure below:Mask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object maskwhich is a binary mask that indicates the pixels where the object is in the bounding box. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. To do this Mask R-CNN uses theFully Convolution Network (FCN) described below.FCN is a popular algorithm for doing semantic segmentation. This model uses various blocks of convolution and max pool layers to first decompress an image to 1/32th of its original size. It then makes a class prediction at this level of granularity. Finally it uses up sampling and deconvolution layers to resize the image to its original dimensions.So, in short, we can say that Mask R-CNN combines the two networksFaster R-CNN and FCN in one mega architecture. The loss function for the model is the total loss in doing classification, generating bounding box and generating the mask.Mask RCNN has a couple of additional improvements that make it much more accurate than FCN. You can read more about them in theirpaper.For building a custom Mask R-CNN, we will leverage theMatterport Github repository. The latest TensorFlow Object Detection repository also provides the option to build Mask R-CNN. However I would only recommend this for the strong-hearted! The versions of TensorFlow, object detection, format for mask, etc. can demand debugging of errors. I was able to successfully train a Mask R-CNN using it.But I have seen many people struggle with all kinds of errors. So I now highly recommend theMatterport Mask R-CNN repository to anyone venturing into this domain.For this exercise, I collected 66 images (50 train and 16 validation) of damaged cars from Google. Check out some examples below.A Mask R-CNN model requires the user to annotate the images and identify the region of damage. The annotation tool I used is the VGG Image Annotatorv 1.0.6. You can use the html version available at thislink. Using this tool you can create a polygon mask as shown below:Once you have created all the annotations, you can download the annotation and save it in a json format. You can look at my images and annotations on my repository here.Now we start the interesting work of actually training the model! Start by cloning the Matterport Mask R-CNN repositoryhttps://github.com/matterport/Mask_RCNN.Next we will load our images and annotations.I have used the balloon.py file shared by Matterport and modified it to create a custom code that loads images and annotations and adds them to a CustomDataset class. Check out the entire code here. Follow the same code block and update it for any specifics for your class. Please note that this code only works for one class.Further, you can use this notebookto visualize the mask on the given images. See an example of this below:To train the model, we use the COCO trained model as the checkpoint to perform transfer learning. You can download this model from the Matterport repository as well.To train the model, run the below code block:I am using a GPU and trained the model for 10 epochs in 2030 minutes.You can inspect the model weights using the notebookInspect Custom Weights. Please link your last checkpoint in this notebook. This notebook can help you perform a sanity check if your weights and biases are properly distributed. See a sample output below:Use the notebookinspect_custom_modelto run model on images from test/val set and see model predictions. See a sample result below:And there you have it! You just built a Mask R-CNN model to detect damage on a car. What an awesome way to learn deep learning.Mask-RCNN is the next evolution of object detection models which allow detection with better precision. A big thanks to Matterport for making their repository public and allowing us to leverage it to build custom models. This is just a small example of what we can accomplish with this wonderful model.If you have any questions, or feedback for me on this article, please share it using the comments section below.",https://www.analyticsvidhya.com/blog/2018/07/building-mask-r-cnn-model-detecting-damage-cars-python/
MyStory: Step by Step process of How I Became a Machine Learning Expert in 10 Months,Learn everything about Analytics|Introduction|Step 1: Understand the basics|Step 2: LearnsomeStatistics|Step 3: LearnPythonorR(orboth)fordataanalysis|Step 4: CompleteanExploratoryData AnalysisProject|Step 5: Create unsupervised learning models|Step 6:Createsupervisedlearningmodels|Step 7:UnderstandBigDataTechnologies|Step 8: ExploreDeepLearningModels|Step 9.Undertake and Complete a Data Project|End Notes,"About the Author|Share this:|Like this:|Related Articles|Ultimate Guide: Building a Mask R-CNN Model for Detecting Car Damage (with Python codes)|Plotly.py 3.0.0 Launched with Major Updates  a Must-Download for all Python & Visualization users!|
Guest Blog
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Not so long ago, using the pivot tables option in Excel was the upper limit of my skills with numbers and the word python was more likely to make me think about a dense jungle or a nature program on TV than a tool to generate business insights and create complex solutions.It took me ten months to leave that life behind and start feeling like I belonged to the exclusive world of people who can tell their medians from their means, their x-bars from the neighborhood pub, and who know how to teach machines what they need to learn.The transformation process was not easy and demanded hard work, lots of time, dedication and required plenty of help along the way. It also involved well over hundreds of hours of studying in different forms and an equal amount of time practicing and applying all that was being learnt. In short, it wasnt easy to transform from being data dumb to a data nerd, but I managed to do so while going through a terribly busy work schedule as well as being a dad to a one-year old.The point of this article is to help you if you are looking to make a similar transformation but do not know where to start and how to proceed from one step to the next. If you are interested in finding out, read on to get an idea about the topics you need to cover and also develop an understanding of the level of expertise you need to build at each stage of the learning process. There are plenty of great online and offline resources to help you master each of these steps, but very often, the trouble for the uninitiated can be in figuring out where to start and where to finish. I hope spending the next ten to fifteen minutes going through this article will help solve that problem for you. And finally, before proceeding any further, I would like to point out that I had a lot of help in making this transformation. Right at the end of the article, I will reveal how I managed to squeeze in so much learning and work in a matter of ten months. But thats for later.For now, I want to give you more details about the nine steps that I had to go through in my transformation process.Spend a couple of weeks enhancing your general knowledge about the field of data science and machine learning. You may already have ideas and some sort of understanding about what the field is, but if you want to become an expert, you need to understand the finer details to a point where you can explain it in simple terms to just about anyone.Suggested topics: Exercise to show that you know:I have a confession to make. Even though I feel like a machine learning expert, I do not feel that I have any level of expertise in statistics. Which should be good news for people who struggle with concepts in statistics as much as I do, as it proves that you can be a data scientist without being a statistician. Having said that, you cannot ignore statistical concepts  not in machine learning and data science! So what you need to do is to understand certain concepts and know when they may be applied or used. If you can also completely understand the theory behind these concepts, give yourself a few good pats on your back. Suggested topics: Suggested exercise to mark completion of this step:Programming turned out to be easier to learn, more fun and more rewarding in terms of the things it made possible, than I had ever imagined. While mastering a programming language could be an eternal quest, at this stage, you need to get familiar with the process of learning a language and that is not too difficult.Both Python and R are very popular and mastering one can make it quite easy to learn the other. I started with R and have slowly started using Python for doing similar tasks as well. Suggested topics:Know that you are set for the next step:In the first cricket test match ever played (see scorecard), Australian Charles Bannerman scored 67.35% (165 out of 245) of his teams total score, in the very first innings of crickets history. This remains a record in cricket at the time of writing, for the highest share of the total score by a batsman in an innings of a test match.What makes the innings even more remarkable is that the other 43 innings in that test match had an average of only 10.8 runs an innings, with only about 40% of all batsmen registering a score of ten or more runs. In fact, the second highest score by an Australian in the match was 20 runs. Given that Australia won the match by 45 runs, we can say with conviction that Bannermans innings was the most important contributor to Australias win.Just like we were able to build this story from the scorecard of the test match, exploratory data analysis is about studying data to understand the story that is hidden beneath it, and then sharing the story with everyone.Personally, I find this phase of a data project the most interesting, which is a good thing as quite a lot of the time in a typical project could be expected to be taken up by exploratory data analysis. Topics to cover:Project output:Lets say we had data for all the countries in the world across many parameters ranging from population, to income, to health, to major industries and more. Now suppose we wanted to find out which countries are similar to each other across all these parameters. How do we go about doing this, when we have to compare each country with all the others, across over 50 different parameters?That is where unsupervised machine learning algorithms come in. This is not the time to bore you with details about what these are all about, but the good news is that once you reach this stage, you have moved on into the world of machine learning and are already in elite company.Topics to cover:Milestone exercise:If you had data about millions of loan applicants and their repayment history from the past, could you identify an applicant who is likely to default on payments, even before the loan is approved?Given enough prior data, could you predict which users are more likely to respond to a digital advertising campaign? Could you identify if someone is more likely to develop a certain disease later in their life based on their current lifestyle and habits?Supervised learning algorithms help solve all these problems and a lot more. While there are a plethora of algorithms to understand and master, just getting started with some of the most popular ones will open up a world of new possibilities for you and the ways in which you can make data useful for an organization. Topics to cover:You have not really started with creating models till you have done this:Many of the machine learning models in use today have been around for decades. The reason why these algorithms are only finding applications now, is that we finally have access to sufficiently large amounts of data, that can be supplied to these algorithms for them to be able to come up with useful outputs.Data engineering and architecture is a field of specialization in itself, but every machine learning expert must know how to deal with big data systems, irrespective of their specialization within the industry.Understanding how large amounts of data can be stored, accessed and processed efficiently is important to being able to create solutions that can be implemented in practice and are not just theoretical exercises. I had approached this step with a real lack of conviction, but as I soon found out, it was driven more by the fear of the unknown in the form of Linux interfaces than any real complexity in finding my way around a Hadoop system. Topics to cover:Do this to know that you have understood the basics:Deep learning models are helping companies like Apple and Google create solutions like Siri or the Google Assistant. They are helping global giants test driverless cars and suggesting best courses of treatment to doctors.Machines are able to see, listen, read, write and speak thanks to deep learning models that are going to transform the world in many ways, including significantly changing the skills required for people to be useful to organizations.Getting started with creating a model that can tell the image of a flower from a fruit may not immediately help you start building your own driverless car, but it will certainly help you start seeing the path to getting there.Topics to cover:Milestone exercise:By now you are almost ready to unleash yourself to the world as a machine learning pro, but you need to showcase all that you have learnt before anyone else will be willing to agree with you. The internet presents glorious opportunities to find such projects. If you have been diligent about the previous eight steps, chances are that you would already know how to find a project that will excite you, be useful to someone, as well as help demonstrate your knowledge and skills.Topics to cover:Milestone exercise:Machine learning and artificial intelligence is a set of skills for the present and future. It is also a field where learning will never cease and very often you may have to keep running to stay in the same place, as far as being equipped with the most in-demand skills is concerned.However, if you start the journey well, you will be able to understand how to go about taking the next step in your learning path. As you must have gathered by now, starting the journey well is a pretty challenging exercise in itself. If you choose to start upon it, I hope this article will have been of some help to you and I wish you the very best.Finally, I will confess that I got a lot of help with my ten-month transition. The reason I was able to cover so much ground in this amount of time, along with a busy schedule at work and home, was that I enrolled for the Post Graduate Program in Data Science and Machine Learning offered by Jigsaw Academy and Graham School, University of Chicago.Investing in the course helped in keeping my learning hours focused, created external pressure that ensured that I was finding time for it irrespective of whatever else was going on in life, and gave me access to experts in the form of faculty and a great peer group through other students. Transforming from being non-technical to someone who is comfortable with the machine learning world has already opened up many new doors for me. Whatever path you choose to make this transformation, you can do so with the assurance that going through the rigor will reap rewards for a long time and will banish any fears of becoming irrelevant in tomorrows economy.Madhukar Jha, Founder  Blue Footed IdeasMadhukar Jhabelieves that great digital experiences are created by concocting a perfect mix of data driven insights, understanding of behavioural drivers, a design thinking approach, and cutting edge technology. He applies this philosophy to help businesses make world class products, run campaigns that rock and tell compelling stories.",https://www.analyticsvidhya.com/blog/2018/07/mystory-became-a-machine-learning-expert-10-months/
Plotly.py 3.0.0 Launched with Major Updates  a Must-Download for all Python & Visualization users!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|MyStory: Step by Step process of How I Became a Machine Learning Expert in 10 Months|NVIDIAs Machine Learning Model Converts a Standard Video into Stunning Slow Motion|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Python is undeniably the most popular language being used by data scientists globally. It has eaten up the ground on R in the last couple of years and is rapidly ascending to scale new heights. One of the best things about R has been its ggplot2 package, which has always been my go-to tool for visualization (even that has now been introduced in Python).Pythons data visualization library catalogue is also diverse, though nothing had been as good as ggplot2. Until now. Here comes the latest release in the popular plotly.py library,3.0.0, and its a big one! There are tons of new features and changes that will make working with this library even more interactive and enjoyable.You can download plotlyusing the below commands:In case you already have plotly installedin your machine, you can upgrade it by typing the below command:Lets look at what all is new in this release.Plotting thousands on points in your notebook used to take a bit of time. This has also been vastly improved in plotly 3.0.0 and you will notice how quickly charts are popping up on your machine.You can read more about these changes, and see them in action, via Plotlys Twitter account.Plotlys release came at approximately the same time that ggplot2 3.0.0 was launched. Its a great time to be a data visualization user! These changes in plotly will greatly enhance my experience with visualizations (I am already a heavy visualization user). Im really looking forward to using the Jupyter widget support and see how that functions in my notebooks.Another thing worth exploring is the Imperative Method which gives us far more flexibility in drawing the plots (and not just how they look). You can even use plotly with R if you with Shiny apps. If you do use this latest release, be sure to let me know your experience in the comments section below!",https://www.analyticsvidhya.com/blog/2018/07/plotly-3-0-0-launched-major-updates-must-download-python-users/
NVIDIAs Machine Learning Model Converts a Standard Video into Stunning Slow Motion,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Plotly.py 3.0.0 Launched with Major Updates  a Must-Download for all Python & Visualization users!|Build your own Computer Vision Model with the Latest TensorFlow Object Detection API Update|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Converting a standard video into slow motion may sound like a simple concept, but actually requires tons of effort, time and skill to master the art. Simply recording slow motion video (like on your phone), is also a tricky affair. If you dont record enough frames, the resulting slow-mo becomes blurry, choppy and frankly, unwatchable.This is where the wonderful field of computer vision (CV) steps in. Researchers from NVIDIA have delved into this field, and developed a CV algorithm that can convert standard videos into high quality slow motion videos. The deep learning model, powered by convolutional neural networks, turns a 30-frames-per-second video into a jaw-dropping 240-frames-per-second slow motion video.The system was trained (using NVIDIA Tesla V100 GPUs and cuDNN) using 1,132 video clips with 240 frames-per-second, containing 300,000 individual video frames. Post the training process, the convolutional neural network was able to predict extra frames in a 30-frames-per-second video. A separate dataset was used to validate the accuracy of the system. Using a series of clips from a popular YouTube series The slow mo guys, the system generated 4 times slower videos with a stunning high resolution accuracy.To get a more in depth feel for the technology, you can read NVIDIAs blog post and research paper. Also, check out the below video which shows how this deep learning system works:This is not the first attempt to use machine learning for manipulating videos, but its certainly unique in the way NVIDIA have approached the challenge.Of course since its NVIDIA, its no surprise that the resulting slow-mo is gorgeous. The research paper is a must-read for any data scientist interested in working in the computer vision field  it contains a detailed explanation of how the researchers arrived at the model after several experiments.Once NVIDIA polishes up the algorithm, and we see smartphones getting more and more computationally powerful, I expect a quick adoption rate for this technology. Its also a much cheaper alternative compared to the other options available in the market currently.",https://www.analyticsvidhya.com/blog/2018/07/nvidias-machine-learning-model-converts-standard-video-slow-motion/
Build your own Computer Vision Model with the Latest TensorFlow Object Detection API Update,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIAs Machine Learning Model Converts a Standard Video into Stunning Slow Motion|An Introductory Guide to Maximum Likelihood Estimation (with a case study in R)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computer vision (CV) is one of the hottest research topics in machine learning these days. From self-driving cars to Instagram and Facebooks object detection technology, it has seen a rapid rise in recent times thanks to advances in hardware.Object detection is easily one of the most common applications of computer vision. Thanks to the wonderful open-source community ML has, object detection has seen a lot of interest as more and more data scientists and ML practitioners line up to break new ground. Keep up with that trend, Google, one of the leaders in ML (perhaps THE leader in ML), has released the latest version of its popular TensorFlow Object Detection API framework.Ever since its release last year, the TensorFlow Object Detection API has regularly received updates from the Google team. These updates have included pretrained models trained on datasets like Open Images, among other things. We have seen the community embrace this framework with open arms  detecting objects on a football field, pedestrian counting, finding cracks in the streets, etc. There have been all sorts of amazing experiments performed using this API.And here comes the latest, and quite a major update released by Google. The changes were announced in a blog post and we have mentioned the highlights below:This is another quality example of Googles efforts to expand the reach of ML and help newcomers and practitioners get the most out of their work. This will undoubtedly help in advancing the research efforts in computer vision and object detection. They have even released a short tutorial on how to train a model on their Cloud TPUs, which you can check out in their blog post.On the other side of the story, Google of course benefits as more and more data scientists and ML folks are integrated into the TensorFlow community. Its a win-win situation for all sides! If you need help getting started with object detection, check out the below guide to get you on your way:Understanding and Building an Object Detection Model from Scratch in PythonYou can also enroll in Analytics Vidhyas soon-to-be-launched Computer Vision using Deep Learning course which will cover a whole host of topics using real-world case studies. Its a beginner friendly course and does not assume any familiarity with computer vision.",https://www.analyticsvidhya.com/blog/2018/07/build-computer-vision-model-tensorflow-object-detection-api/
An Introductory Guide to Maximum Likelihood Estimation (with a case study in R),Learn everything about Analytics|Introduction|Table of Contents|Why should I use Maximum Likelihood Estimation (MLE)?|Understanding MLE with an example|Getting to know the technical details|Determining model coefficients using MLE|MLE using R|End Notes,"Distribution Parameters |Likelihood|Log Likelihood|Maximizing the Likelihood|Share this:|Like this:|Related Articles|Build your own Computer Vision Model with the Latest TensorFlow Object Detection API Update|Microsoft has Released a Collection of Awesome Free Datasets|
Aanish Singla
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Interpreting how a model works is one of the most basic yet critical aspects of data science. You build a model which is giving you pretty impressive results, but what was the process behind it? As a data scientist, you need to have an answer to this oft-asked question.For example, lets say you built a model to predict the stock price of a company. You observed that the stock price increased rapidly over night. There could be multiple reasons behind it. Finding the likelihood of the most probable reason is what Maximum Likelihood Estimation is all about. This concept is used in economics, MRIs, satellite imaging, among other things.Source: YouTubeIn this post we will look into how Maximum Likelihood Estimation (referred as MLE hereafter) works and how it can be used to determine coefficients of a model with any kind of distribution. Understanding MLE would involve probability and mathematics, but I will try to make it easier with examples.Note: As mentioned, this article assumes that you know the basics of maths and probability. You can refresh your concepts by going through this article first 6 Common Probability Distributions every data science professional should know.Let us say we want to predict the sale of tickets for an event. The data has the following histogram and density.How would you model such a variable? The variable is not normally distributed and is asymmetric and hence it violates the assumptions of linear regression. A popular way is to transform the variable with log, sqrt, reciprocal, etc. so that the transformed variable is normally distributed and can be modelled with linear regression. Lets try these transformations and see how the results are:With Log transformation: With Square Root Transformation:With Reciprocal: None of these are close to a normal distribution. How should we model such data so that the basic assumptions of the model are not violated? How about modelling this data with a different distribution rather than a normal one? If we do use a different distribution, how will we estimate the coefficients? This is where Maximum Likelihood Estimation (MLE) has such a major advantage.While studying stats and probability, you must have come across problems like  What is the probability of x > 100, given that x follows a normal distribution with mean 50 and standard deviation (sd) 10. In such problems, we already know the distribution (normal in this case) and its parameters (mean and sd) but in real life problems these quantities are unknown and must be estimated from the data. MLE is the technique which helps us in determining the parameters of the distribution that best describe the given data.Lets understand this with an example: Suppose we have data points representing the weight (in kgs) of students in a class. The data points are shown in the figure below (the R code that was used to generate the image is provided as well):Figure 1This appears to follow a normal distribution. But how do we get the mean and standard deviation (sd) for this distribution? One way is to directly compute the mean and sd of the given data, which comes out to be 49.8 Kg and 11.37 respectively. These values are a good representation of the given data but may not best describe the population.We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized. In order to get an intuition of MLE, try to guess which of the following would maximize the probability of observing the data in the above figure?Clearly, it is not very likely well observe the above data shape if the population mean is 100.Now that you got an intuition of what MLE can do, we can get into the details of what actually likelihood is and how it can be maximized. But first, lets start with a quick review of distribution parameters.Let us first understand distribution parameters. Wikipedias definition of this term is as follows: It is a quantity that indexes a family of probability distributions. It can be regarded as a numerical characteristic of a population or a statistical model. We can understand it by the following diagram:Figure 2, Source: WikipediaThe width and height of the bell curve is governed by two parameters  mean and variance. These are known as distribution parameters for normal distribution. Similarly, Poisson distribution is governed by one parameter  lambda, which is thenumber of times an event occurs in an interval of time or space.Figure 3, Source: WikipediaMost of the distributions have one or two parameters, but some distributions can have up to 4 parameters, like a 4 parameter beta distribution. From Fig. 2 and 3 we can see that given a set of distribution parameters, some data values are more probable than other data. From Fig. 1, we have seen that the given data is more likely to occur when themean is 50, rather than100. In reality however, we have already observed the data. Accordingly, we are faced with an inverse problem: Given the observed data and a model of interest, we need to find the one Probability Density Function/Probability Mass Function (f(x|)), among all the probability densities that are most likely to have produced the data.To solve this inverse problem, we define the likelihood function by reversing the roles of the data vector x and the (distribution) parameter vector  in f(x| ),i.e.,L(;x) = f(x| )In MLE, we can assume that we have a likelihood function L(;x), where is the distribution parameter vector and x is the set of observations. We are interested in finding the value of  that maximizes the likelihood with given observations (values of x). The mathematical problem at hand becomes simpler if we assume that the observations (xi) are independent and identically distributed random variables drawn from a Probability Distribution, f0 (where f0= Normal Distribution for example in Fig.1). This reduces the Likelihood function to:To find the maxima/minima of this function, we can take the derivative of this function w.r.t and equate it to 0 (as zero slope indicates maxima or minima). Since we have terms in product here, we need to apply the chain rule which is quite cumbersome with products. A clever trick would be to take log of the likelihood function and maximize the same. This will convert the product to sum and since log is a strictly increasing function, it would not impact the resulting value of . So we have:To find the maxima of the log likelihood function LL(; x), we can:There are many situations where calculus is of no direct help in maximizing a likelihood, but a maximum can still be readily identified. Theres nothing that gives setting the first derivative equal to zero any kind of primacy or special place in finding the parameter value(s) that maximize log-likelihood. Its simply a convenient tool when a few parameters need to be estimated. As a general principle, pretty much any valid approach for identifying the argmax of a function may be suitable to find maxima of the log likelihood function. This is an unconstrained non-linear optimization problem. We seek an optimization algorithm that behaves in the following manner:Its very common to use optimization techniques to maximize likelihood; there are a large variety of methods (Newtons method, Fisher scoring, various conjugate gradient-based approaches, steepest descent, Nelder-Mead type (simplex) approaches, BFGS and a wide variety of other techniques). It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the ordinary least squares method.You can refer to the proof here.Let us now look at how MLE can be used to determine the coefficients of a predictive model.Suppose that we have a sample of n observations y1, y2, . . . , yn which can be treated as realizations of independent Poisson random variables, with Yi  P(i). Also, suppose that we want to let the mean i (and therefore the variance!) depend on a vector of explanatory variables xi . We could form a simple linear model as follows  whereis the vector of model coefficients. This model has the disadvantage that the linear predictor on the right-hand side can assume any real value, whereas the Poisson mean on the left-hand side, which represents an expected count, has to be non-negative. A straightforward solution to this problem is to model the logarithm of the mean using a linear model. Thus, we consider a generalized linear model with log link log, which can be written as follows  Our aim is to find  by using MLE. Now, Poisson distribution is given by:We can apply the log likelihood concept that we learnt in the previous section to find the . Taking logs of the above equation and ignoring a constant involving log(y!), we find that the log-likelihood function is where i depends on the covariates xi and a vector of coefficients.We can substitute i = exp(xi) and solve the equation to getthat maximizes the likelihood.Once we have thevector, we can then predict the expected value of the mean by multiplying the xi and  vector. In this section, we will use a real-life dataset to solve a problem using the concepts learnt earlier.You can download the dataset from this link.A sample from the dataset is as follows: Datetime  Count of tickets sold25-08-2012 00:00    825-08-2012 01:00  225-08-2012 02:00  625-08-2012 03:00  225-08-2012 04:00  225-08-2012 05:00  2It has the count of tickets sold in each hour from 25th Aug 2012 to 25th Sep 2014 (about 18K records). Our aim is to predict the number of tickets sold in each hour. This is the same dataset which was discussed in the first section of this article. The problem can be solved using techniques like regression, time series, etc. Here we will use the statistical modeling technique that we have learnt above using R.Lets first analyze the data. In statistical modelling, we are concerned more with how the target variable is distributed. Lets have a look at the distribution of counts: This could be treated as a Poisson distribution or we could even try fitting an exponential distribution. Since the variable at hand is count of tickets, Poisson is a more suitable model for this. Exponential distribution is generally used to model time interval between events.Lets plot the count of tickets sold over these 2 years:Looks like there is a significant increase in sale of tickets over time.In order to keep things simple, lets model the outcome by only using age as a factor, where age is the defined no. of weeks elapsed since 25th Aug 2012. We can write this as:where,  (Count of tickets sold) is assumed to follow the mean of Poisson distribution and 0and 1 are the coefficients that we need to estimate. Combining Eq. 1 and 2, we get the log likelihood function as follows:We can use the mle() function in R stats4 package to estimate the coefficients 0and 1. It needs the following primary parameters: For our example, the negative log likelihood function can be coded as follows:I have divided the data into train and test set so that we can objectively evaluate the performance of the model. idx is the indices of the rows which are in test set.Next lets call the mle function to get the parameters:This gives us the estimate of the coefficients. Lets use RMSE as the evaluation metric for getting results on the test set:Now lets see how our model fairs against the standard linear model (with errors normally distributed), modelled with log of count. As you can see, RMSE for the standard linear model is higher than our model with Poisson distribution.Lets compare the residual plots for these 2 models on a held out sample to see how the models perform in different regions:We see that the errors using Poisson regression are much closer to zero when compared to Normal linear regression. Similar thing can be achieved in Python by using thescipy.optimize.minimize() function which accepts objective function to minimize, initial guess for the parameters and methods like BFGS, L-BFGS, etc.Its further simpler to model popular distributions in R using the glm function from thestats package. It supports Poisson, Gamma, Binomial, Quasi, Inverse Gaussian, Quasi Binomial, Quasi Poisson distributions out of the box. For the example shown above, you can get the coefficients directly using the below command:Same can be done in Python using pymc.glm() and setting the family as pm.glm.families.Poisson(). One way to think of the above example is that there exist better coefficients in the parameter space than those estimated by a standard linear model. Normal distribution is the default and most widely used form of distribution, but we can obtain better results if the correct distribution is used instead. Maximum likelihood estimation is a technique which can be used to estimate the distribution parameters irrespective of the distribution used. So next time you have a modelling problem at hand, first look at the distribution of data and see if something other than normal makes more sense!The detailed code and data is present on my Github repository.Refer to the Modelling single variables.R file for an example that covers data reading, formatting and modelling using only age variables. I have also modelled using multiple variables, which is present in theModelling multiple variables.R file.",https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/
Microsoft has Released a Collection of Awesome Free Datasets,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An Introductory Guide to Maximum Likelihood Estimation (with a case study in R)|Learn and Improve your Machine Learning Skills with TensorFlows Free Seedbank Platform|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Open source has always been the overriding theme of the data science and machine learning community. Its hearing to see so many big research and tech companies like Google, Uber, Facebook, NVIDIA, and Intel open source their research to the wider audience. It not only benefits the companies, but expands the reach of ML to an already growing and thriving user base.I have also seen Microsoft make their shift to artificial intelligence and ML in recent years with their flagship product  Azure ML. So it comes as welcome news that Microsofts research arm has launched Microsoft Research Open Data, a platform that hosts a collection of tons of free datasets. These datasets span a variety of diverse domains and categories so you are free to pick your choice.The different categories currently available are listed below:These are all active research areas for Microsoft. The data they have provided has been curated and collected over a number of years for different studies and activities they have been involved in.Of course since this is Microsofts platform, they have provided us with an option to either download the dataset directly or use a virtual machine (VM) powered by Azure. This VM is preloaded with popular development tools to make it a seamless experience for the user. Below is an image shown in their blog post which shows the development tools:I love this move by Microsoft! They are not often spoken about when it comes to open source research, but they have certainly made their mark with this release. Theres a bit of something for every data scientist  natural language processing, computer vision, image processing, geospatial, among others.There is a very heavy focus on NLP in this collection. That might be because of Microsofts emphasis on improving their voice assistant and other chatbot-like related applications. Whatever the case, everyone wins! I cannot recommend enough browsing these datasets and downloading one to get started on your own project. This represents a real-life industry project you can excel at to improve or polish your skills and get noticed.Use the comments section below to ask any questions in case you get stuck anywhere or are unable to download any dataset.",https://www.analyticsvidhya.com/blog/2018/07/microsoft-has-released-a-collection-of-awesome-free-datasets/
Learn and Improve your Machine Learning Skills with TensorFlows Free Seedbank Platform,Learn everything about Analytics|Overview|Introduction|Our take on this ,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Microsoft has Released a Collection of Awesome Free Datasets|Facebook Open Sources Dataset on NLP and Navigation Every Data Scientist should Download|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Understanding machine learning concepts and then putting them into practice can be a challenging task. When it comes to deep learning, that challenge goes to a whole new level. If youre looking for a place to start applying these concepts, youve come to the right place!In order to help individuals through this journey, TensorFlow has launched an interactive in-browser platform called Seedbank. Seedbank enables you to learn and discover various new machine learning examples and assists you in understanding and implementing your ideas. The best part about it is that you can use it in your browser without having to go through the complicated steps of setting up your system and downloading frameworks!The various learning categories Seedbank is offering include classification, unsupervised learning, text and language, recurrent nets, etc., comprising machine learning examples, tutorials and algorithms. Apart from this, TensorFlow Hub provides numerous pretrained machine learning modules along with the Colaboratory notebooks (machine learning codes by Google).With Seedbank, you can explore Colab-powered Machine Learning examples. You can play around with the code already provided, or extend it to implement your own ideas. It also allows you to filter your search by typing relevant keywords present inside the notebooks. With each search, you can look at the preview and decide if you wish to end your search or look further. After you select the Colab notebook, youll be connected to a free GPU kernel where you can start working through the example or tutorial.At present, Seedbank is able to track only notebooks by Google though the team is planning to index user-created content in the near future.This is awesome news for newcomers and amateur data scientists. It will help develop your already fledgling ML skills on pretrained models and tasks. The fact that you dont need to install anything on your system and get free GPU support? Its like hitting the jackpot!It will simplify the problem and help in understanding various scenarios. You can also save copies to Google Drive and share your work with your friends or on social media. You can also read data from Google Drive which makes importing large datasets a snap. Get started NOW!",https://www.analyticsvidhya.com/blog/2018/07/learn-improve-machine-learning-skills-with-tensorflows-seedbank/
Facebook Open Sources Dataset on NLP and Navigation Every Data Scientist should Download,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Learn and Improve your Machine Learning Skills with TensorFlows Free Seedbank Platform|13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them?|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Existing Natural Language Techniques (NLP) focus mostly on transcribing what humans say, rather than understanding whats being said. Even with the release of advanced chatbot technologies like Google Duplex and MicrosoftsXiaoice, this is a challenge that has eluded researchers so far.This has prompted a group of researchers from the University of Montreal and Facebooks AI department (FAIR) to curate a dataset called Talk the Walk that aims to teach the machine to understand language in the same way that a human does. The researchers have of course open sourced the dataset and opened up the challenge to the wider machine learning community.The dataset is essentially made up of three elements:The idea behind this research is to get two agents talking to each other  a tourist, and a guide. The tourist has access to the 360 degree images of the locations but not the map, and the guide has access to the map but the images. Can you distinguish which is the human and the machine in this case? Below is a sample screenshot from the dataset:The researchers have also released baseline results of the experiments they ran. Watch the below video, released by Facebook, which illustrates their approach to the problem:(function(d, s, id) {  var js, fjs = d.getElementsByTagName(s)[0];  if (d.getElementById(id)) return;  js = d.createElement(s); js.id = id;  js.src = 'https://connect.facebook.net/mr_IN/sdk.js#xfbml=1&version=v3.0';  fjs.parentNode.insertBefore(js, fjs);}(document, 'script', 'facebook-jssdk'));Talk the Walk: AI ""tourist"" and ""guide"" demonstrationFacebook Engineering     ,  , Below are a few resources to get you started on this challenge:This is one the most difficult challenges youll see anywhere. It combines so many machine learning tasks that it can become daunting. One of the authors of the research paper himself admitted that breakthroughs in this study might be a few years away. But when it does happen, it has the potential to be a game changer in the NLP as well as navigational guidance domains.But dont let that deter you! Download the dataset, and try to understand all that it has. If you dont understand something, use the comments section below to ask. Play around with parts of the data and publish your findings and analysis online. You never know where inspiration might strike.",https://www.analyticsvidhya.com/blog/2018/07/facebook-open-sources-dataset-on-nlp-and-navigation-every-data-scientist-should-download/
13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them?,Learn everything about Analytics|Introduction|Table of Contents|1. Learning Theoretical Concepts without Applying Them|2. Heading Straight for Machine Learning Techniques without Learning the Prerequisites|3. Relying Solely on Certifications and Degrees|4. Assuming that what you see in ML Competitions is what Real-Life Jobs are Like|5. Focusing on Model Accuracy over Applicability and Interpretability in the Domain|6. Using too Many Data Science Terms in your Resume|7. Giving Tools and Libraries Precedence over the Business Problem|8. Not Spending Enough Time on Exploring and Visualizing the Data|9. Not Having a Structured Approach to Problem Solving|10. Trying to Learn Multiple Tools at Once|11. Not Studying in a Consistent Manner|12. Shying Away from Discussions and Competitions|13. Not Working on Communication Skills|End Notes,"Share this:|Like this:|Related Articles|Facebook Open Sources Dataset on NLP and Navigation Every Data Scientist should Download|NVIDIAs Noise2Noise Technique Helps you Fix Bad Images in Milliseconds|
Pranav Dar
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",How to avoid this mistake?|How to avoid this mistake?|How to avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How can you avoid this mistake?|How to avoid this mistake?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So youve decided data science is the field for you. More and more businesses are becoming data driven, the world is increasingly becoming more connected and looks like every business will need a data science practice. So, the demand for data scientists is huge. Even better,everyone acknowledges the shortfall of talent in the industry.However, becoming a data scientist does not come easy. It needs a mix of problem solving, structured thinking, coding and various technical skills among others to be truly successful. If you are from a non-technical and non-mathematical background, theres a good chance a lot of your learning happens through books and video courses. Most of these resources dont teach you what the industry is looking for in a data scientist.This is one of the reasons why aspiring data scientists are struggling to bridge the gap between self education and real-world jobs.In this article, I discuss the top mistakes amateur data scientists make (I have made some of them myself). I have also provided resources wherever applicable with the aim of helping you avoid these pitfalls on your data science journey.Additionally, if youre just starting out in data science or struggling to make headway, I would recommend this awesome and comprehensive program: Certified Program on Data Science for Beginners (with Interviews).Source: Cognitive Class  YouTubeAs I mentioned in my article on AVs practice problems its good to get a grasp of the theory behind machine learning techniques. But if you dont apply them, they are only theoretical concepts. When I started out learning data science, I made the same mistake  I studied books and online courses but didnt always apply them to solve a problem.So when I was faced with a challenge or problem where I had the chance to apply all that I had learned, I couldnt remember half of it! Theres so much to learn  algorithms, derivations, research papers, etc. Theres a high chance youll lose your motivation halfway through and give up. I have personally seen this happen to a lot of people who attempt to enter this field.Its imperative that your learning process should be a healthy balance between theoretical and practical. As soon as you learn a concept, head over to Google and find a dataset or problem where you can use it. Youll find that you are retaining that concept way better than before. You can also use AVs DataHack platform to take part in practice problems and ongoing competitions.You will have to accept that you cannot learn everything in one go. Fill in the gaps as you practice and you will learn a whole lot more!Source: Imperial College London  YouTubeThe majority of folks who want to become a data scientist are inspired by videos of robots, or awesome predictive models, and in some cases even the high salaries. Sadly (sorry to disappoint!), there is a long road you need to travel, before you reach there.You should get to know how techniques work before you apply them in a problem.Learning this will help you understand how an algorithm works, what you can do to fine tune it, and will also help you build on existing techniques. Mathematics plays an important role here so its always helpful to know certain concepts. In a day-to-day corporate data scientist role you may not need to know advanced calculus, but having a high-level overview definitely helps.In case you have a curious mind, or want to get into a research role, the four key components you need to know before diving into core machine learning are:Just as a house is built brick-by-brick, a data scientist is also the sum of all the individual parts. There are tons of resources out there which will help you learn these topics. I have mentioned one resource of each topic below which should get you started:You can also check out Analytics Vidhyas Introduction to Data Science course which includes a comprehensive module on statistics and probability.Source: CIO.comAh, the pet-peeve of hiring managers and recruiters. Ever since data science became ultra popular, certifications and degrees have cropped up just about everywhere. A glance through my LinkedIn feed shows up at least 5 certification images proudly being displayed. While achieving that certification is no easy feat, relying solely on it is a recipe for disaster.There are too many of these courses online being poured over and completed by thousands upon thousands of aspiring data scientists. If they ever added a unique value to your data science CV, that is no longer the case. Hiring managers do not care much for these pieces of paper  they place far more emphasis on your knowledge, and how youve applied it in real-life practical situations.This is because dealing with clients, handling deadlines, understanding how a data science project lifecycle works, how to design your model to fit into the existing business framework  these are just some of the things you will need to know to succeed as a data scientist. Just a certification or degree will not qualify you for it.Dont get me wrong  certifications are valuable, but only when you apply that knowledge outside the classroom and put it out in the open. Use real-world datasets and whatever analysis you do, make sure you write about it. Create your own blog, post it on LinkedIn, and ask for feedback from the community. This shows that you are willing to learn and are flexible enough to ask for suggestions and work them into your projects.You should be open to the idea of internships (regardless of your experience level). You will learn a lot about how a data science team works, which will benefit you when you sit for another interview.If youre looking for that next project, youve come to the right place. We have an awesome list of projects here divided by the degree of difficulty. Get started NOW.This is one of the biggest misconceptions aspiring data scientists have these days. Competitions and hackathons provide us with datasets that are clean and spotless (okay  I went a little overboard, but you get the hang of it). You download them, and start working on the problem. Even those datasets that have columns with missing values dont require you to work your brain cells off  figure out an imputation technique and fill in the blanks.Unfortunately, real-world projects dont work like that. There is an end-to-end pipeline which involves working with a bunch of people. You will almost always have to work with messy and unclean data. The old saying about spending 70-80% of your time just collecting and cleaning data is true. Its the grueling part and you will (most likely) not enjoy but its something that eventually becomes part of a routine.Also, and we will cover this in more detail in the next point, the simpler model will win precedence over any complex stacked ensemble model. Accuracy isnt always the end goal, and this is one of the most contrasting things youll learn on the job.One of the key factors to negate this misunderstanding is, ironically, experience. The more experience you gain (internships help a lot in this case), the better youll be able to distinguish between the two. This is where social media comes in handy  reach out to data scientists and ask them their experience.Additionally, I suggest going through this Quora thread where data scientists from around the world provide their input on this exact question. Getting a good score on a competition leaderboard is excellent for measuring your learning progress, but interviewers will want to know how you can optimize your algorithm for impact, not for the sake of increasing accuracy. Learn about how a data science project works, what different types of roles a team has (from a data engineer to a data architect), and structure your answer in that sense.Go through this LinkedIn post which explains the standard methodology for analytical models.Source: Design ShackAs mentioned above, accuracy isnt always what the business is after. Sure a model that predicts loan default with 95% accuracy is good, but if you cant explain how the model got there, which features led it there, and what your thinking was when building the model, your client will reject it.You will rarely, if ever, find a deep neural network being used in commercial applications. Its just not possible to explain to the client how a neural network (let alone a deep one) worked with hidden layers, convolutions layers, etc. The first preference is, and will always be, on ensuring that we are able to understand whats going on underneath the model. If you cant tell whether age, or number of family members, or previous credit history went into rejecting a loan application, how will the business run?Another key aspect is whether your model will fit within the organizations existing framework. Using 10 different types of tools and libraries will fail spectacularly if the production environment cannot support it. You will have to redesign and retrain the model from scratch with a simpler approach.The best way to prevent yourself from making this mistake is speaking to people working in the industry. There is no better teacher than experience. Pick a domain (finance, HR, marketing, sales, operations, etc.) and reach out to people to understand how their project works.Apart from that, practice making simpler models and then explaining them to non-technical people. Then add complexity to your model and keep doing this until even you dont understand whats going on beneath. This will teach you when to stop, and why simple models are always given preference in real-life applications.If you have done this before, you will know what Im talking about. If your resume currently has this problem, rectify it immediately! You may know a plethora of techniques and tools but simply listing them down will turn off potential hiring managers.Your resume is a profile of what you have accomplished and how you did it  not a list of things to simply jot down. When a recruiter looks at your resume, he/she wants to understand your background and what all you have accomplished in a neat and summarized manner. If half the page is filled with vague data science terms like linear regression, XGBoost, LightGBM, without any explanation, your resume might not clear the screening round.The simplest way to eliminate resume clutter is to use bullet points. Only list the techniques which you have used to accomplish something (could be a project or a competition). Write a line about how you used it  this helps the recruiter understand your thinking.When youre applying for fresher or entry-level jobs, your resume needs to reflect what potential impact you can add to the business. You will be applying to roles in different domains so perhaps having a set template will help  just change the story to relfect your interest in that particular industry.This article by Kunal Jain is an excellent resource for preparing an outstanding CV for data science roles.Source: Data Science LabLets take an example to understand why this is a mistake. Imagine youve been given a dataset on house prices and you need to predict the value of future real estate. There are over 200 variables, including number of buildings, rooms, number of tenants, family size, size of the courtyard, whether faucets are available, etc. Theres a good chance you might not be aware of what some variables mean. You can still build a model with a good accuracy, but you have no idea why a certain variable was dropped.As it turns out, that variable was a crucial element in a real-world scenario. Its a calamitous mistake.Having a solid knowledge of tools and libraries is excellent, but it will only take you so far. Combining that knowledge with the business problem posed by the domain is where a true data scientist steps in. You should be aware of at least the basic challenges in the industry you are interested in (or are applying to).There are plenty of options to explore here:Data visualization is such a wonderful facet of data science, yet a lot of aspiring data scientists prefer to skim over it and get to the model building stage. This approach might work out in competitions, but is bound to fail in a real job. Understanding the data youre given is the single most important thing you will do, and your models results will reflect that.By spending time on getting to know the dataset and trying out different charts, you will gain a deeper knowledge of the challenge or problem youve been tasked with solving. Youd be surprised to know how much insight you can gain just by doing this! Pattern and trends emerge, stories are told and the best part? Visualizations are the best way to present your findings to the client.As a data scientist, you need to be inherently curious. Its one of the great things about data science  the more curious you are, the more questions youll ask. This leads to a much better understanding of the data you are given and also helps solve problems you didnt know existed in the first place!Practice! Next time you work on a dataset, spend more time on this step. You will be stunned at the amount of insight it will generate for you. Ask questions! Ask your manager, ask domain experts, search for solutions on the internet and if you dont find any, ask on social media. So many options!To help you get started, I have mentioned a few resources below which you should refer to:Source: MindMatters.co.inStructured thinking helps a data scientist in many ways:There are many more reasons why having a structured thinking mindset helps. As you can imagine, not having a structured thinking mindset is counter intuitive. Your work and approach to a problem will be haphazard, you will lose track of your own steps when faced with a complex problem, etc.When you go for a data science interview, you will inevitably be given a case study, guess estimate and puzzle problem(s). Because of the pressure filled atmosphere in an interview room and the time constraint, the interviewer looks at how well you structure your thoughts to arrive at a final result. In many cases, this can be a deal breaker or deal sealer for getting the job.You can acquire a structured thinking mindset through simple training and and a disciplined approach. I have listed a few articles below which will help you get started on this crucial aspect:Ive seen this one too many times. Because of the dilemma and the unique features each tool offers, people tend to attempt learning all the tools at once. This is a bad idea  you will end up mastering none of them. Tools are a means to perform data science, they are not the end goal.Pick one tool and stick to it until you have mastery over it. If youve already started learning R, then dont be tempted by Python (yet). Stick with R, learn it end-to-end and only then try to incorporate another tool into your skillset. You will learn more with this approach.Each tool has a great user community which you can tap into whenever you get stuck. Use our discussion forum to ask questions, search stuff online, and dont give up. The aim is to learn data science through the tool, not the tool through data science.If you are still undecided on which tool you should use, check out this wonderful article which lists down each tools advantages and shortcomings (it also includes SAS in case you are interested in that).Source: The Brooks GroupThis one applies to all data scientists, not just freshers. We have a tendency to get distracted easily. We study for a period of time (say, a month), then we give it a break for the next 2 months. Trying to get back into the groove of things after that is a nightmare. Most of the earlier concepts are forgotten, notes are lost and it feels like we just wasted the last few months.I have personally experienced this as well. Due to various things we have going on, we find excuses and reasons not to get back to studying. But this is eventually our loss  if data science was as easy as opening a text book and cramming everything, everyone would be a data scientist today. It demands consistent effort and learning, something which people dont appreciate until its too late.Set goals for yourself. Map out a time table and stick it on your wall. Plan how and what you want to study and set deadlines for yourself. For example, when I wanted to learn about neural networks, I gave myself a couple of weeks and then tested what Id learned by competing in a hackathon.You have decided to become a data scientist so you should be ready to put in the hours. If you continually keep finding excuses not to study, this might not be the field for you.Source: Interview Skills ConsultingThis is a combination of a few things weve seen in the above points. Aspiring data scientists tend to shy away from posting their analysis online in fear of being criticized. But if you dont receive feedback from the community, you will not grow as a data scientist.Data science is a field where discussions, ideas and brainstorming is of utter importance. You cannot sit in a silo and work  you need to collaborate and understand other data scientists perspective. Similarly, people dont take part in competitions because they feel they wont win. This is a wrong mindset! You participate in these competitions to learn, not to win. Winning is a bonus, learning is the goal.Its fairly straightforward  start participating in discussions and competitions! Its okay to not come in the top 5%. If you learn a new technique out of the whole thing, you have won in your own right.Source: Jim HarveyCommunications skills are one of the most under-rated and least talked about aspects a data scientist absolutely MUST possess. I am yet to come across a course that places a solid emphasis on this. You can learn all the latest techniques, master multiple tools and make the best graphs, but if you cannot explain your analysis to your client, you will fail as a data scientist.And not just clients, you will also be working with team members who are not well versed with data science  IT, HR, finance, operations, etc.You can be sure that the interviewer will be monitoring this aspect throughout.Assume youve built a credit risk model using logistic regression. As a thought exercise, take a minute to think how you would explain to a non-technical person how you came to the final conclusion. If you used any technical words, you need to work on this ASAP!Most data scientists these days are coming from a computer science background so I understand this can be a daunting skill to acquire. But to become a successful data scientist and climb up the ladder, you dont have a choice but to polish this part of your personality.One of the things I find most helpful is explaining data science terms to a non-technical person. It helps me gauge how well I have articulated the problem. If youre working in a small to medium-sized company, find a person in the marketing or sales department and do this exercise with them. It will help you immensely in the long term.There are plenty of free resources available on the internet to get you started but remember, practice is key when it comes to soft skills. Ensure you start doing this TODAY.This is most definitely NOT an exhaustive list  there are plenty of other mistakes aspiring data scientists tend to make. But these were the most common ones I have seen and my aim, as stated earlier, is to help others avoid it (as much as possible).I would love to hear your thoughts on these pointers, and also your personal experience with similar problems. Use the comments section below to let me know!",https://www.analyticsvidhya.com/blog/2018/07/13-common-mistakes-aspiring-fresher-data-scientists-make-how-to-avoid-them/
NVIDIAs Noise2Noise Technique Helps you Fix Bad Images in Milliseconds,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|13 Common Mistakes Amateur Data Scientists Make and How to Avoid Them?|Samsungs ConZNet Algorithm just won Two Popular NLP Challenges (Dataset Links Inside)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How many times have we taken photos from our phone that turned out being really blurred out? Maybe the person or animal moved at the last minute, or perhaps the light was really low and the shot ended up being dark and grainy. I could go on and on.NVIDIA has been recently making waves with their work in computer vision and image processing so its no surprise to see them tackle this commonly faced issue. Using the power of deep learning, they have developed an algorithm, called Noise2Noise, that can fix bad images by learning from corrupted images only. Its a pretty unique approach to image processing.As you might have guessed, the popular ImageNet dataset was used to train the algorithm. 50,000 images were taken for this training process. And this is where NVIDIA flexed their deep learning muscles  the model was trained using the NVIDIA Tesla P100 GPUs along with the cuDNN-accelerated TensorFlow framework (you can read their blog post here).As you can see in the above image, the results are remarkably impressive, especially when compared to the Ground truth image. The neural network that was trained for this purpose learned from grainy or corrupt images. To validate the results, the researchers performed experiments by introducing different varieties of synthetic noise (Gaussian, Poisson, Bernoulli and Random-valued impulse noise). The results were still outstanding  the team claims that it took milliseconds for the model to perform its magic and restore quality to the images.The neural network can also remove any text present in the images which presents a significant problem for copyright images. If you can remove a watermark from the image, that presents a significant problem.Of course there is a research paper for this approach which you can read in full here. The researchers will be presenting this paper at the International Conference on Machine Learning (ICML) in Stockholm this week. You can also check out the best papers at ICML 2018 before the conference begins!You can see a video demonstrating this technique below:Previous related studies have used a neural network (or several) to clean up low-light photos, but those approaches required using clean images for training purposes. What sets NVIDIAs approach apart are 2 things:It can potentially be used in healthcare to restore MRI scan images, or in satellite imagery to clean up and see distant objects in an area, among various other things.",https://www.analyticsvidhya.com/blog/2018/07/nvidias-noise2noise-technique-helps-you-fix-bad-images-in-milliseconds/
Samsungs ConZNet Algorithm just won Two Popular NLP Challenges (Dataset Links Inside),Learn everything about Analytics|Overview|Introduction|Our take on this,"MS MARCO|TriviaQA|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIAs Noise2Noise Technique Helps you Fix Bad Images in Milliseconds|An Autonomous Car Learned how to Drive itself in 20 minutes using Reinforcement Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"2018 has been the year machine learning algorithms took control of NLP challenges once and for all. We saw Alibabas neural network approach beat out all human competitors on Stanfords Reading Comprehension Test at the start of the year and the trend has continued.This week, Samsungs AI research arm used their unique reinforcement learning algorithm, called ConZNet, won two extremely tough yet popular Natural Language Processing (NLP) competitions. One of the competitions was organized by the University of Washington, called TriviaQA, and the other one was hosted by none other than Microsoft, called MS MARCO (MAchine Reading COmprehension).                                                                            Source: BetanewsConZNet is a deep reinforcement learning algorithm. This means that each time the algorithm learns from its mistakes, it is rewarded accordingly. In case you are unfamiliar with this concept, I recommend going through this beginner friendly article.Lets take a quick look at the two datasets that Samsung applied their algorithm on.MS MARCO is Microsofts reading comprehension challenge dataset. It has neatly been divided into training, validation and test sets for you to dig into straightaway. It contains well over 1 million search queries from Bing and over 180,000 well formed answers.In this competition, an AI algorithm is presented with with ten web documents to answer a certain query.To increase the complexity,Microsoft insists that the contestants use its Bing search engine.Queries are randomly selected and answers are evaluated statistically by estimating how close they are with human answers.You can download the MS MARCO dataset here.TriviaQA is a massive reading comprehension dataset which contains over 650,000 questions and 95,000 question-answer pairs. The dataset also includes evidence documents that provide supervision help for answering the questions. The file size is 7.2GB so ensure youre on a solid internet connection before you start the download!You can download the dataset from here.I am a big NLP enthusiast so news like this definitely gets me excited! What I like about these two competitions, and the Stanford Question Answering Dataset, are that they are open source. Anyone can download and work on them. I would love to see folks from Analytics Vidhyas community develop algorithms that rival and compete in these challenges. Its the perfect opportunity to apply all those NLP concepts youve been learning and measure your progress against global data scientists.Coming to Samsung, this is quite a win for them because it props up their Bixby, their virtual assistant. Im sure well see ConZNet being applied within Bixby when the next update comes out.",https://www.analyticsvidhya.com/blog/2018/07/samsungs-conznet-algorithm-won-2-huge-nlp-competitions/
An Autonomous Car Learned how to Drive itself in 20 minutes using Reinforcement Learning,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Samsungs ConZNet Algorithm just won Two Popular NLP Challenges (Dataset Links Inside)|DataHack Radio #4  Data Privacy, Women in Data Science and More with Carla Gentry|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Self-driving cars are understandably the most attention-grabbing application of artificial intelligence. Until recently, weve just seen prototypes of these vehicles in showrooms or in sci-fi movies, with everything else left to our imagination. But with advances in technology, hardware and machine learning, this wonderful concept has got a life of its own.The autonomous vehicles we have seen so far have included a ton of training data including rules, hours upon hours of learning using that data, quite a lot of hardware, . And now a UK company, Wayve, has designed a first-ever autonomous car that works with the help of reinforcement learning. Their approach enables the car to learn how to drive in just 15-20 minutes!So how did they do this? Thats where it gets a bit more complicated. The researchers used a popular reinforcement learning algorithm called Deep Deterministic Policy Gradients (DDPG). This helped them solve the task of following the lane in front of the car. As mentioned in Wayves blog post, the architecture of the algorithm was a deep neural network with 4 convolutional layers and 3 fully connected layers with just under 10,000 parameters. In contrast, other state-of-the-art image classification networks have millions of parameters.Only a single camera was used with the autonomous car to capture the surroundings and to follow the road. All the processing requirements were done in the car on just a single GPU!Of course the trial and error process to train the cars system did not start on the road (too much of a safety hazard in public), but in Wayves workspace. A lot of testing was done in simulated environments to understand the task and fine tune the hyperparameters of the reinforcement learning algorithm. The below graph shows the distance traveled by the car against the number of times it took to train the system:As you can see in the video below, whenever the car veered off track, the human driver steered it back on to the road. The car was rewarded (a reinforcement learning term) every time it learned where it went wrong and corrected itself. You can read about the algorithm in more detail in this research paper.I was just talking to my colleague last week about how almost all reinforcement learning examples and studies are done using gaming environment (Alpha Go, Dota 2, etc.). So this is a welcome change in that regard and helps to expand the scope of where and how RL can be applied in real-life scenarios.The applications are HUGE  if the initial pilot phase of this technology goes well, one can imagine it being used for ferrying people around the city (perhaps as a fleet of taxis?). Given how quickly the reinforcement learning algorithm learns (95% accuracy in under 20 trials, compared to DeepMinds Atari algorithm which took millions of attempts), it could teach itself to be 99% accurate within a week!",https://www.analyticsvidhya.com/blog/2018/07/autonomous-car-learnt-drive-itself-20-minutes-using-reinforcement-learning/
"DataHack Radio #4  Data Privacy, Women in Data Science and More with Carla Gentry",Learn everything about Analytics|Introduction,"Carlas Background|The Importance of Data Mining and Dealing with Bias in the Data|Changes in the Last Few Years in the Data Science Domain|Changes to be Expected in the next Few Years|Women in Data Science|Social Media|Share this:|Like this:|Related Articles|An Autonomous Car Learned how to Drive itself in 20 minutes using Reinforcement Learning|Eye in the Sky? This Open-Source Model Creates Impressive Ground-Level Images using Satellite Data|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Carla Gentry is one of most popular social media influencers in the data science field. She has over 300,000 followers on LinkedIn and 48k followers on Twitter. Her experience in this field is unparalleled and we are grateful for leaders like her who consistently give back to the community.She has been a regular reader of Analytics Vidhyas articles for quite a while now. It was a pleasure to have her appear on the podcast and to hear her views on data privacy, how the domain has changed in the last few years, her advice for women in data science, and a whole host of other topics.This article contains highlights of Carlas conversation with Kunal Jain. You can listen to the podcast by clicking on the above SoundCloud link or on our iTunes channel. Happy listening!You can subscribe to DataHack Radio and listen to this, and all previous episodes, on any of the below platforms:Carla holds a number of degrees including one in advanced economics, one in advanced mathematics, among others. Right after college she got an internship at one of the few econometrics firms in the United States.There she worked with terabytes of data (this was well before Big Data was a buzzword). Her experience started with tools and platforms like SAS, Pico, etc. Her role was working with credit card data but the nature of the work was such that it became monotonous after a while.From there, she moved to the Weinstein Organization as a Senior Analyst and Data Specialist. Here Carlas role expanded to include direct marketing experience. This was followed by a year at the University of Chicago Booth School of Business as a Research Support Analyst where she taught Ph.D students how to work with data, how to do data mining, etc.Carla then moved on to work as the Marketing Information Manager at Career Education Corporation for the next 4 years. Post that she spent the next 3.5 years in the marketing and data field at PromoWorks, Tandus and Area203. She is now the owner and data scientist at Analytical Solution, where she has worked with clients like Kellogg, Johnson & Johnson, among various other organizations.Carla considers data mining a critical aspect in any data driven project. It helps you understand trends, see patterns, interpret reasons why a person was denied a loan or credit card, etc. Its at the core of the business and should be respected as such.But she stressed on the importance of privacy and the need to be clear with your clients and customers about where you are going to use their data, for what purposes, and how it might impact them (if at all). GDPR has of course changed the game in Europe with regards to being transparent with users but Carla feels everyone, regardless of laws, should have this as a best practice.With the amount of data thats being generated in the world, from websites to social media, its critical to have that level of sensitivity.Data scientists have a responsibility to be unbiased, have integrity and use their experience to add a positive background to the dataset, rather than let their feelings cloud the model building exercise.Carla recalled that if a business had terabytes of data in the 90s, running a program on that was next to impossible because the mainframe would have crashed. Now a mainframe isnt even required! If you have a good database architecture set up, you can access millions and billions of rows in a fraction of the time it used to take previously.COBOL, PASCAL, C++, SAS, Mathematica, MATLAB  these were the only programs available back when Carla started her journey. Of course now we have much more robust tools like R, Visual Studio (SQL), IBMs suite of tools, etc. One of the biggest reasons why the older programs have been phased out is because of their inability to handle gigantic datasets, which the new tools can.Of course with the rise of this data wave, and the advent of the digital era, the number of hackers and cyber thieves has also risen. So as things have gotten better in many ways, they have also gotten worse when it comes to security of your data.Carla expects more laws like GDPR to come to action in the next few years that will dictate how organizations collect and deal with your data. She has a warning for businesses that abuse data  people will leave and look for ways to go incognito, which will leave your business with no data at all.We have got to get rid of the thinking that its always been this way, so it should stay that way.Carla is a champion of women in data science. She strongly believes that in order to incorporate more females into this field, the change has to start from the top. The CEO should have an obligation to encourage diversity into the organization by going to the HR department and digging deeper to understand the percentage of women in the company, and how to further improve upon that.Her advice to aspiring female data scientists was to the point  stand your ground, be confident in yourself, find mentors, keep going and keep learning. You will find the perfect fit for you as long as you continue to believe in yourself and your abilities.Carla is an avid social media user and a HUGE influencer, especially in the data science field. Its very time consuming (2-3 hours per day at times) but if what she shares helps even one person, she definitely considers it worth that investment. She feels its her responsibility to give back to the community, given that she has gotten so much from it over the years.There are some awesome rapid fire questions at the end of the podcast  ensure you listen to those as well!*P.S.  All views expressed by the guests on DataHack Radio are their own, and not of Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2018/07/datahack-radio-episode-4-carla-gentry/
Eye in the Sky? This Open-Source Model Creates Impressive Ground-Level Images using Satellite Data,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio #4  Data Privacy, Women in Data Science and More with Carla Gentry|PixelPlayer  Identify and Extract Musical Instrument Sounds from Videos with MITs AI|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Remember our Eye in the Sky article about using machine learning with drones to detect violence in a crowd of people? Well, that kind of technology has been in existence since decades, in the form of satellites. But only in the last few years has it been truly commercially used. Its no coincidence that this has happened with the rise of machine learning in the industry.Now, the latest study comes from a group of researchers at the University of California. They have trained a machine learning model that looks at satellite images of the Earth, and then creates a ground-level image. This is basically imagining what that particular area (in the image) actually looks like on Earth. This can potentially be used for mapping and creating images of the places which are out of our reach.They have even published their research paper which you can read here. In it, the team has described the complete process of training the model and creating images. They used a machine learning system known as a conditional generative adversarial network (cGAN) which combines two neural networks: a generator and a discriminator.The cGAN was shown more than 16,000 pairs of images (comprising overhead images and ground-level images of the same location). It was trained to visualise what objects looked like on the ground based on the photographs which showed an aerial shot of that area.After training the system, the generator neural net was fed with 4,000 new satellite images. Based on the training data, it was able to create fake ground-level images for these 4,000 pictures. Note that the discriminator has access to the real ground-level views. By using the feedback from the discriminator, the generator gradually learns to produce more accurate images.The fake images generated by the machine learning model looked similar to the original images but lacked depth in the form of more granular details. As you can see in the above image, the system is able to cover major details, like whether the image has road, land, or water, but some tiny details are missing from them.Compared with the currently used human interpolation method (which is correct 65% of the times), this technique provided better results (correct 73% of the time). The researchers are determined to improve the systems performance even more, and explore other machine-learning methods.How cool is this? While most of the industry is looking at products and how to leverage machine learning to sell them, here comes a research totally isolated from that corporate environment. Its such a refreshing change of pace and encourages data scientists to try their hand in these fields as well.Go through the research paper and try to understand the way the model was designed. It will add a whole new perspective to way we usually go about building a model. Once done, check out the wonderful work Stanford is doing with satellite images and machine learning. Its also open source and the code is available on their GitHub page so you dont even need to search for ways to get started  its all here!",https://www.analyticsvidhya.com/blog/2018/07/ai-creates-ground-view-based-on-aerial-pictures/
PixelPlayer  Identify and Extract Musical Instrument Sounds from Videos with MITs AI,Learn everything about Analytics|Overview|Introduction|Our take on this,"So how does it all work?|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Eye in the Sky? This Open-Source Model Creates Impressive Ground-Level Images using Satellite Data|Top 3 AI Updates from Baidus Developer Conference 2018|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are countless times when I listen to music on YouTube and Im mesmerized by one of the instruments in the video. But isolating and extracting that instruments sound has so far been a difficult and cumbersome task for casual listeners and amateur musicians. Unless you owned and knew how to use a sophisticated tool, you were out of luck.This is where machine learning and AI have become so useful. Researchers from MITs Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a deep learning model that takes a video as input and identifies and isolates the sound of specific instruments. It even has the ability to make that instruments sound louder or softer.The model, or system, has been built using self-supervised learning, which doesnt require any pre-labelled data. Of course this makes it difficult to interpret fully how the system arrived at a certain result (how it isolated the instrument in this case) but this is something the researchers are working to understand.The system, called PixelPlayer, was trained on over 60 hours of videos and can identify 20 different instruments. The deep learning model first locates the image regions which are producing sound. It then separates the sound into a number of components that represent the sound from each pixel in the image (this is where the systems name comes from).There are a number of neural networks at play within the system  one that analyzes the visuals in the video, another that works on the audio part, and a third that first associates specific pixels with specific soundwaves, and then separates the different sounds.The part which surprised the researchers is that the system even recognizes actual musical elements. Their research found that certain harmonic frequencies seem to correlate to instruments like violin, while quick pulse-like patterns correspond to instruments like the xylophone.You should read the research paper which outlines PixelPlayer in more detail, including details of the experiments and their results. Check out the video below which shows this technology working its magic:To put things into context, this isnt the first attempt at using machine learning and AI in the music industry. We have previously seen Googles entry into this sphere with nSynth, a Data Science Music challenge from Michigan University, among other things. A lot of professional musicians are using AI to not only make music, but to create videos from scratch as well!This kind of AI can potentially be used to understand environmental sounds as well. I can see this being incorporated into the self-driving car technology to make it even safer. I personally cant wait for MIT to release the code on GitHub. Have you ever worked on any sound processing projects or datasets? Connect with me in the comments below.",https://www.analyticsvidhya.com/blog/2018/07/pixelplayer-mit-open-source-ai-identifies-separates-instrument-sounds-video/
Top 3 AI Updates from Baidus Developer Conference 2018,Learn everything about Analytics|Overview|Introduction|Our take on this,"Chinas first Autonomous Bus|Chinas first edge-to-cloud chip|Baidu Brain 3.0|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|PixelPlayer  Identify and Extract Musical Instrument Sounds from Videos with MITs AI|Learn and Test your Machine Learning Skills with AVs New Practice Problems and Free Courses!|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Labelled as Chinas Google, Baidu has made some really exciting announcements on AI-based innovations and product releases during their Create Developer conference in Beijing this week. We bring to you the top three AI focused announcements in this article.Source: VentureBeatBaidu announced Chinas first fully autonomous bus, Apolong, powered by the popular open-source Apollo autonomous driving platform. Apolong is equipped with Baidus voice-activated software and is able to perform driverless tasks (like obstacle avoidance or automatic transshipment). The first country to use this bus (outside of China) will be Japan.Baidu also released an open source platform, Apollo, in July 2017. It now has more than 100 partners including Ford, Intel, NVIDIA, and Microsoft, among others. The team announced in the conference that this Apollo platform has been upgraded to Apollo 3.0, with improved support for autonomous driving in geo-fenced areas.Since this is open-source, you can download the dataset that goes into making self-driving cars.Source: VentureBeatBaidu unveiled a new chip, Kunlun, that can handle AI models for edge computing. Built to assist high performance requirements, specifically for AI purposes, it is able to run intensive computing operations on both device-based edge computing, and cloud-based through datacentres. The Kunlun 818-300 model is being built to train AI, and the 818-100 model has been designed for inference purposes. Baidu claims that Kunlun is approximately 30 times faster than their first FPGA (field-programmablegate array) chip. It can reach 260 tera-operations per second with 512 GBps memory bandwidth.Baidu Brain was first introduced in September 2016 with the goal of expanding AI services and improving semantic understanding. Tt has now been updated to a new version, called Baidu Brain 3.0. This latest version features more than 110 AI services including, natural language processing, facial recognition, voice processing and recognition, among other things!DuerOS 3.0 has also been announced along with Baidu Brain, which is Baidus platform for conversational AI. It works with smart devices and appliances like televisions, fridges, mobiles, etc.The most attention garnering announcement was, you guessed it, the autonomous bus. Self-driving technology has been smack in the middle of the spotlight this year and Baidu has enhanced their name in the domain with this launch. Their popular open-source Apollo platform has always been a source of great use for the ML community and I encourage you to download their dataset and play around with it.Which of the three announcements excited you the most? Let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/07/top-3-ai-updates-from-baidus-developer-conference-2018/
Learn and Test your Machine Learning Skills with AVs New Practice Problems and Free Courses!,Learn everything about Analytics|Introduction|Practice Problems|Courses and Trainings|End Notes,"Identify the apparels|Jester  is this joke funny?|Creating Time Series Forecast using Python|Big Mart Sales Prediction using R|Loan Prediction using Python|Share this:|Like this:|Related Articles|Top 3 AI Updates from Baidus Developer Conference 2018|Using the Power of Deep Learning for Cyber Security (Part 1)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Knowledge is of no value unless you put it into practice.  Anton ChekhovGaining knowledge of new concepts is a critical aspect of data science and machine learning. But the real gold lies in putting these concepts into practice. The more you practice, the better your concepts become!I am excited to announce that Analytics Vidhya has launched two brand new practice problems for both machine learning and deep learning enthusiasts and experts. We have also added three new courses to our burgeoning training portal. These courses cover a variety of challenges machine learning folks will find useful.We believe in providing only top class content for our community and our trainings, hackathons, articles and practice problems reflect that commitment. Lets look at these practice problems and training courses in a bit more detail.Analytics Vidhyas practice problems bring out the data scientist within you. Our collection of practice problems span varying domains  performing sentiment analysis, building recommendation systems, prediction loan default, identifying digits from images, estimating the age of Indian actors, among a whole host of other challenges.We are excited to launch two new practice problems:This is an intriguing computer vision problem which has been recently gained a lot of traction in the deep learning community.The dataset we are providing for this is called Fashion MNIST. Its inspired from MNIST, a very popular dataset in the machine learning community (you can check out the MNIST practice problem in ourIdentify the digitschallenge). In Identify the apparels, instead of digits the images show a type of apparel, e.g. tee-shirt, trousers, bag, etc. The dataset used in this problem was created by Zalando Research.This practice problem is meant for beginners in deep learning. Intermediate or experts in this field can also work on this to refresh their concepts.This is quite a unique practice problem.You are challenged with predicting the ratings for jokes given by the users provided the ratings provided by the same users for another set of jokes. This dataset has been taken from the famous jester online Joke Recommender system dataset.This practice problem is meant for everyone in the machine learning field  from beginners to experts. I recommend getting familiar with recommendation systems to get the most from this challenge.Analytics Vidhyas aim has always been to build and help data scientists all over the globe by providing top notch training resources. So, we have expanded our training catalogue exponentially this year. We launched the Introduction to Data Science course which has quickly become our most popular training. We also have trainings on Excel and problem solving using data, and of course a comprehensive learning path to become a data scientist.We have recently added three more exciting trainings to this list!Time Series forecasts come in handy for creating simple forecasts like number of airline passengers, website traffic, etc. This course is a comprehensive guide to getting you started in this vast and intriguing domain. Time Series forecasting is a skill every data scientist should have in their skillset so ensure you take this course!This course is meant for newcomers in data science and machine learning. Predicting the sales of a business is one of the most common challenges in this field and this course will give you a very good idea of how to approach this challenge. This course will equip you with the skills and techniques required for solving a regression problem in R.This course is designed for people who want to learn how to solve binary classification problems. In this course, you will solve a real life case study of Dream Housing Finance. The company wants to automate the loan eligibility process (real-time) based on the customer details provided through an online application form.By the end of the course, you will have a solid understanding of classification problems and various approaches to solve them.",https://www.analyticsvidhya.com/blog/2018/07/learn-and-test-your-machine-learning-skills-with-avs-new-practice-problems-and-free-courses/
Using the Power of Deep Learning for Cyber Security (Part 1),Learn everything about Analytics|Introduction|Table of Contents|The Current State of Deep Learning Systems in InfoSec|A Brief Overview of Feed Forward Neural Network|Case Study: Tor Traffic Detection using Deep Learning|Data Experiments  Tor Traffic Detection|End Notes|References,"About the Authors|Share this:|Like this:|Related Articles|Learn and Test your Machine Learning Skills with AVs New Practice Problems and Free Courses!|IIT Roorkee Researchers are using Computer Vision to Monitor and Improve Railway Tracks|
Guest Blog
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The majority of the deep learning applications that we see in the community are usually geared towards fields like marketing, sales, finance, etc. We hardly ever read articles or find resources about deep learning being used to protect these products, and the business, from malware and hacker attacks.While the big technology companies like Google, Facebook, Microsoft, and Salesforce have already embedded deep learning into their products, the cybersecurity industry is still playing catch up.Its a challenging field but one that needs our full attention.In this article, we briefly introduce Deep Learning (DL) along with a few existing Information Security (hereby referred to as InfoSec) applications it enables. We then deep dive into the interesting problem of anonymous tor traffic detection and also present a DL-based solution to detect TOR traffic.The target audience for this article is data science professionals who are already working on machine learning projects. The content of this article assumes that you have foundation knowledge of machine learning and are currently either a beginner, or are exploring, deep learning and its use cases.The below pre-reads are highly recommended to get the most out of this article:Deep learning is not a silver bullet that can solve all the InfoSec problems because it needs extensive labeled datasets. Unfortunately, no such labeled datasets are readily available. However, there are several InfoSec use cases where the deep learning networks are making significant improvements to the existing solutions. Malware detection and network intrusion detection are two such areas where deep learning has shown significant improvements over the rule-based and classic machine learning-based solutions.Network intrusion detection systems are typically rule-based and signature-based controls that are deployed at the perimeter to detect known threats. Adversaries change the malware signatures and easily evade the traditional network intrusion detection systems. Quamar et al. [1], in their IEEE transaction paper, showed deep learning (DL)-based systems using self-taught learning to be promising in detecting unknown network intrusions. Traditional security use cases such as malware detection and spyware detection have been tackled with deep neural net-based systems [2].The generalization power of DL-based techniques is better compared to traditional ML-based approaches. Jung et al.s [3] DL based system can even detect zero-day malware. Daniel Gibert [2], a Ph.D. graduate from the University of Barcelona, has done extensive work related to convolutional neural networks (CNN, a type of DL architecture) and malware detection. In his Ph.D. thesis, he says that CNNs can detect even polymorphic malware.The DL-based neural nets are now getting used in User and Entity Behaviour Analytics (UEBA). Traditionally, UEBA employs anomaly detection and machine learning algorithms which distill the security events to profile and baseline every user and network element in the enterprise IT environment. Any significant deviations from the baselines were triggered as anomalies that further raised alerts to be investigated by the security analysts. UEBA enhanced the detection of insider threats, albeit to a limited extent.Now, deep learning-based systems are used to detect many other types of anomalies. Pawe Kobojek from Warsaw university, Poland [4] uses keystroke dynamics to verify the user using an LSTM network. Jason Trost, director of security data engineering at Capital One, has published several blogs [5] that have a list of technical papers and talks on applying deep learning in InfoSec.The artificial neural network is inspired from the biological neural network. Neurons are the atomic unit of a biological neural network. Each neuron consists of dendrites, nucleus, and axons. It receives signals through dendrites and is carried out through axons (Figure 1 below). The computations are performed in the nucleus. The entire network is made up of a chain of neurons.AI researchers borrowed this idea to develop the artificial neural network (ANN). In this setting, each neuron accomplishes three actions:Each neuron thus can classify whether a set of inputs belong to one class or another. This power is limited when only a single neuron is used. However, coining a set of neurons makes it a powerful machinery for classification and sequence labelling tasks.Figure 1: Greatest inspiration that we can get is from the nature  figure depicts a biological neuron and an artificial neuron.A set of neuron layers can be used to create a neural network. The network architecture differs based on the objective it needs to achieve. A common network architecture is a Feed Forward Neural Network (FFN). Neurons are arranged linearly without any cycles to form a FFN. It is called feed forward because information travels in the forward direction inside the network, first through the input neurons layer, then through the hidden neuron layers, and the output neurons layer (Figure 2 below).Figure 2: A feed forward network with two hidden layers Like any supervised machine learning model, the FFN needs to be trained using labeled data. The training is in the form of optimizing the parameters by reducing the error between the output value and the true value. One such important parameter to optimize is the weight each neuron gives to each of its input signals. For a single neuron, the weight can be easily computed using the error.However, when a set of neurons are collated in multiple layers, it is challenging to optimize the neuron weights in multiple layers based on the error computed at the output layer. The backpropagation algorithm helps to address this issue [6]. Backpropagation is an old technique which comes under the branch of computer algebra. Here, automatic differentiation is usedto calculate thegradientthat is needed in the calculation of theweightsto be used in the network.In aFFN, based on activation of each linked neuron, the output is obtained. The error is propagated layer by layer. Based on the correctness of the output with the final outcome, the error is calculated. This error is then in turn back propagated to fix errors of internal neurons. For each data instance, the parameters are optimized by going through multiple iterations. The primary goal of cyber-attacks is to steal the enterprise customer data, sales data, intellectual property documents, source codes and software keys. The adversaries exfiltrate the stolen data to remote servers in encrypted traffic along with the regular traffic.Most often adversaries use an anonymous network that makes it difficult for the security defenders to trace the traffic. Moreover, the exfiltrated data is typically encrypted, rendering rule-based network intrusion tools and firewalls to be ineffective. Recently, anonymous networks have also been used for C&C by specific variants of ransomware/malware. For instance, Onion Ransomware [7] uses the TOR network to communicate with its C&C.Figure 3: An illustration of TOR communication between Alice and a destination server. The communication starts with Alice requesting a path to the server. TOR network gives the path which is AES encrypted. The randomization of the path happens inside the TOR network. The encrypted path of the packet is shown in red. Upon reaching the exit node, which is the periphery node of the TOR network, the plain packet is transferred to the server. Anonymous network/traffic can be accomplished through various means. They can be broadly classified into:Among them, TOR is one of the more popular choices. TOR is a free software that enables anonymous communication over the internet through a specialized routing protocol known as the onion routing protocol [9]. The protocol depends on redirecting internet traffic over various freely hosted relays across the world. During the relay, like the layers of an onion peel, each HTTP packet is encrypted using the public key of the receiver.At each receiver point, the packet can be decrypted using the private key. Upon decryption, the next destination relay address is revealed. This carries on until the exit node of the TOR network is met, where the decryption of the packet ends, and a plain HTTP packet is forwarded to the original destination server. An example routing scheme between Alice and the server is depicted in the above Figure 3 for illustration. The original intent of launching TOR was to safeguard the privacy of users. However, adversaries have hijacked the good Samaritan objective to use it for various nefarious means instead. As of 2016, around 20% of the Tor traffic accounts for illegal activities. In an enterprise network, TOR traffic is curtained by not allowing the installation of the TOR client or blocking the Guard or Entry node IP address.However, there are numerous means through which adversaries and malware can get access to the TOR network to transfer data and information. The IP blocking strategy is not a sound strategy. Adversaries can spawn different IPs to carry out the communication. A bad bot landscape report by distil networks [5] shows that 70% of automated attacks in 2015 used multiple IPs, and 20% of automated attacks used over 100 IPs.TOR traffic can be detected by analyzing the traffic packets. This analysis can be on the TOR node, or in between the client and the entry node. The analysis is done on a single flow of packet. Each flow constitutes a tuple of source address, source port, destination address, and destination port.Network flows for different time intervals are extracted and analysis is carried on them. G. He et al. in their paper Inferring Application Type Information from Tor Encrypted Traffic extracted burst volumes and directions to create a HMM model to detect the TOR applications that might be generating that traffic. Most of the popular works in this area leverage time-based features along with other features like size and port informationto detect TOR traffic.We take inspiration from Habibi et als Characterization of Tor Traffic using Time based Features paper and follow a time-based approach over extracted network flow to detect TOR traffic for this article. However, our architecture uses a plethora of other meta-information that can be obtained to classify the traffic. This is inherently due to the Deep Learning architecture that has been chosen to solve this problem.We obtained the data from Habibi Lashkari et al. [11] at the University ofNew Brunswickfor the data experiments done in this article. Their data consists of features extracted from the analysis of the university internet traffic. Extracted meta information from the data is given in the table below:Table 1: Meta information parameters obtained from [1]Apart from these parameters, other flow-based parameters are also included. A sample instance from the dataset is shown in Figure 4 below:Figure 4: An instance of the dataset used for this article.Please note that source IP/port and destination IP/port, along with the protocol field, have been removed from the instance as they overfit the model. We process all other features using a deep feed forward neural network with N hidden layers. The architecture of the neural network is shown in Figure 5 below.Figure 5: Deep learning network representation used for TOR traffic detection.The hidden layers vary between 2 to 10. We found N=5 to be optimal. For activation, Relu is used for all the hidden layers. Each layer of the Hidden layers is dense in nature and of dimension 100.Figure 6: A Python Code Snippet of the FFN in Keras. The output node is activated by a sigmoid function. This was used as the output is a binary classification  Tor or Non-Tor. We usedKeras with Tensorflow in the backend to train the DL module. Binary cross entropy loss was used for optimizing the FFN. The model was trained for different epochs. Figure 7 below shows training simulation for a run depicting the increasing performance and decreasing loss value as the number of epochs increase.Figure 7: Tensorboard generated statics depicting the network training processThe results of the deep learning system were compared with various other estimators. Standard classification metrics of Recall, Precision and F-Score were used to measure the efficacy of the estimators. Our DL-based system was able to detect the TOR class well. However, it is the Non-Tor class that we need to give more importance to. It is seen that a Deep Learning-based system can reduce the false positive cases for Non-Tor category samples. The results are shown in the table below:Table 2: The output of ML and DL Models for the Tor Traffic Detection experimentAmong various classifiers, Random Forest and Deep learning based approaches perform better than the rest. The result shown is based on 55,000 training instances. The dataset used in this experiment is comparatively smaller than typical DL-based systems. As the training data increases, performance would increase further for both DL-based and Random forest classifier.However, for large datasets, a DL-based classifier typically outperforms other classifiers, and it can be generalised for similar types of applications. For example, if one needs to train a classifier to detect the application used by TOR, then only the output layer needs retraining, and all the other layers can be kept the same. Whereas other ML-classifiers will need to be retrained for the entire dataset. Keep in mind that retraining the model may take significant computing resources for large datasets.Anonymized traffic detection is a nuanced challenge that every enterprise faces. The adversaries use TOR channels to exfiltrate data in anonymous mode. Current approaches by tor traffic detection vendors depend on blocking known entry nodes of the TOR network. This is not a scalable approach and can be easily bypassed. A generic method is to use deep learning-based techniques.In this article, we presented a deep learning-based system to detect the TOR traffic with high recall and precision. Let us know your take on the current state of deep learning, or if you have any alternate approaches, in the comments section below.Dr. Satnam Singh, Chief Data Scientist  Acalvio TechnologiesDr Satnam Singh is currently leading security data science development at Acalvio Technologies. He has more than a decade of work experience in successfully building data products from concept to production in multiple domains. In 2015, he was named as one of the top 10 data scientists in India. To his credit, he has 25+ patents and 30+ journal and conference publications.Apart from holding a PhD degree in ECE from University of Connecticut, Satnam also holds a Masters in ECE from University of Wyoming. Satnam is a senior IEEE member and a regular speaker in various Big Data and Data Science conferences.Balamurali A R, Member Technical Staff (Data Science) at AcalvioBalamurali A R is a member of the data science team at Acalvio. He is a graduate from IIT Mumbai, holds a Ph.D in Computer Science and has previously worked with companies like Samsung and IBM.",https://www.analyticsvidhya.com/blog/2018/07/using-power-deep-learning-cyber-security/
IIT Roorkee Researchers are using Computer Vision to Monitor and Improve Railway Tracks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Using the Power of Deep Learning for Cyber Security (Part 1)|Google Brain Researchers Unveil ML Model that Manipulates Neural Networks|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Railways are the lifeblood of millions of Indians. You only need to hop on to the Mumbai local to understand how popular and necessary trains are in India. But negligence of infrastructure over the years has unfortunately led to frequent mishaps and accidents.So researchers from IIT Roorkee have proposed a solution that combines the power of computer vision with drone based technology. Their approach essentially monitors the health of railway tracks to ensure that they meet the compliance standards and more importantly, prevent accidents.So how does their approach work? As with any machine learning project, it works in several steps:Below is an image illustrating the technique:Once the approach was finalized, the researchers then set about testing it on more than thousand images that spanned different weather conditions. The results, shown in the paper (link below), clearly show that this is an effective technique and is not influenced by weather or altitude.You can read their research paper in full here.While there has been research in this area using drones previously (also mentioned in the research paper), this novel approach is cost effective and and takes up far less time. The current system being used by the railway authorities is manual, cumbersome and even inconsistent at times. I hope we see a rapid adoption of this technique, which can work alongside the current process to improve and hopefully prevent accidents from happening down the line.This is a shining example of academia being used to solve a real-life challenge. It serves as an inspiration to all the students and data science enthusiasts that this diverse field has a lot to offer outside of the usual corporate data science work.",https://www.analyticsvidhya.com/blog/2018/07/iit-roorkee-researchers-using-computer-vision-monitor-railway-tracks/
Google Brain Researchers Unveil ML Model that Manipulates Neural Networks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|IIT Roorkee Researchers are using Computer Vision to Monitor and Improve Railway Tracks|Formula 1 will use AWS and Machine Learning to Build Race Strategies and Design Cars|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picture this  you feed a picture of a cat to your computer vision algorithm and it misreads it as a bunch of squares, or in an even worse scenario, a dog. You made all the right tweaks to your algorithm so what went so wrong? Turns out it isnt very difficult to manipulate computer vision techniques.We have previously covered Google Brains research when they demonstrated how a CNN could be fooled into misreading the object in an image. And now Google Brain researchers have developed an even smarter technique, called adversarial reprogramming, that reprograms the entire machine learning model. This technique performs a task chosen by the attacker; it does not need the attacker to specify or compute what he wants to perform. This is what differentiates it from other research studies in this field.You can read the research paper in full here.This clearly illustrates the urgent need for a more robust deep learning security framework. Its all well and good developing an awesome deep neural net but if it can be manipulated with ease, then youre in big trouble. Kudos to the Google Brain team for continuously working on these scenarios and open sourcing their research.The researchers mention that future studies will involve possible ways to defend against these kind of attacks. While we wait for that, I recommend reading up on information security. Meanwhile, we will also soon publish a blog post on using deep learning for shoring up against adversarial attacks so keep an eye out for that.",https://www.analyticsvidhya.com/blog/2018/07/google-brain-researchers-unveil-ml-model-that-manipulates-neural-networks/
Formula 1 will use AWS and Machine Learning to Build Race Strategies and Design Cars,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Brain Researchers Unveil ML Model that Manipulates Neural Networks|The Top GitHub Repositories & Reddit Threads Every Data Scientist should know (June 2018)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Formula 1 is very much a numbers game. As an enraptured audience watches each race wondering which driver and car will outperform the rest, theres a huge team of engineers and data scientists behind the scenes using data to maximise the cars advantage. Given how close these races are, and how much is at stake, making the most of what you have becomes critical.So it comes as no surprise that Formula 1 recently announced itll be switching the vast majority of its infrastructure from on-premise data centers to Amazons premier ML offering, Amazon Web Services (AWS). Its a huge victory for AWS as itll help the racing conglomerates in building race strategies, data tracking systems and of course, increasing fan engagement.Formula 1s data scientists are already hard at work  theyre using Amazons Sagemaker (part of the AWS toolkit) to train deep learning models on more than 65 years of race data! Where is this data stored? Amazon DynamoDB and Amazon Glacier, of course. To know more about Sagemaker and how it works, read our article here.When you watch a race on your television, you see a bunch of statistics thrown on the screen every few minutes. These numbers will get even more in depth as the data scientists will use various algorithms and techniques to give fans insights into what the team is planning and where you can expect each driver to finish. As an example, these data scientists typically use ML to understand when a driver should come in for a pit stop so that he doesnt lose any ground to his competition. Other variables each team uses are brake wear, tyre pressure and condition, drivers health stats, etc. Its a treasure trove of data.Another huge advantage of moving to AWS is the power of Amazon Kinesis. The plan is to stream real-time data to AWS using Kinesis which will help Formula 1 teams understand the above variables at a very granular level. Its a game changer in more ways than one.You can read more about this announcement on Formula 1s official blog post here.Cars and machine learning? Sounds like a match made in heaven! This one is a way for the fans of both Formula 1 as well as sports analytics. A lot of data scientists are familiar with how AWS works so this is guaranteed to increase engagement with them as well. You can expect to see a lot more advanced stats on your screen in the coming races this year.Machine learning will also play a big part in the aerodynamics that go into designing cars. Quite fascinating, isnt it?",https://www.analyticsvidhya.com/blog/2018/07/formula-1-aws-machine-learning-build-race-strategies-design-cars/
The Top GitHub Repositories & Reddit Threads Every Data Scientist should know (June 2018),Learn everything about Analytics|Introduction|GitHub Repositories|Facebooks DensePose|NLP Progress|MLflow|Salesforces decaNLP|Reinforcement Learning Notebooks|Reddit Threads|Playing Card Detection with YOLOv3|OpenAI Five|What ML Hypothesis are you Curious About but are Hoping Someone Else will Research it?|Setup that Data Scientists use for Machine Learning|Practical Use Cases for Reinforcement Learning|End Notes,"Share this:|Like this:|Related Articles|Formula 1 will use AWS and Machine Learning to Build Race Strategies and Design Cars|Databricks and RStudio Launch Platform to make R Simpler than Ever for Big Data Projects!|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Half the year has flown by and that brings us to the June edition of our popular series  the top GitHub repositories and Reddit threads from last month. During the course of writing these articles, I have learned so much about machine learning from either open source codes or invaluable discussions among the top data science brains in the world.What makes GitHub special is not just its code hosting and social collaboration features for data scientists. It has lowered the entry barrier into the open source world and has played a MASSIVE role in spreading knowledge and expanding the machine learning community.We saw some amazing open source code being released in June. One of the most intriguing repositories was NLP Progress  created with the aim of keeping everyone updated regarding the latest updates in this field. Facebook also released the code for its popular DensePose framework which could be a game changer in the pose estimation field.When it comes to Reddit, it is so rich in knowledge and perspective from data scientists and ML experts from around the globe. In this article youll see discussions on reinforcement learning applications, machine learning setups, a wonderful computer vision example, and much more. I highly recommend participating in this discussions to enhance your skillset.You can check out the top GitHub repositories and top Reddit discussions (from April onwards) for the last five months below:Human pose estimation has garnered a lot of attention in the deep learning community this year. And Facebook took things to a new level when they open sourced the code to DensePose, their popular pose estimation framework. This technique identifies more than 5000 nodes in the human body (for context, other approaches operate with 10 or 20 joints). You can get an idea of this node mapping technique in the above image.DensePose has been created in the Detectron framework and is powered by Caffe2. Apart from the code, this repository also contains notebooks to visualize the DensePose-COCO dataset. Read more details about this release here.Natural Language Processing (NLP) is an often difficult field to get into, despite its attractions. There is tons of unstructured text lying around which you have to work with, and that is no easy task. This repository has been created especially to track the progress in the NLP field. Its a very informative list of datasets and current state-of-the-art tasks, like dependency parsing, part-of-speech tagging, reading comprehension, etc.Make sure you star this and follow the progress if youre even vaguely interested in NLP. There is still a lot that can (and will) be added to this list, like information extraction, relation extraction, grammatical error correction, etc. If you have anything to contribute to this repository, the creator is open to ideas and suggestions so feel free to do that.Getting your model into production is one of the biggest challenges data scientists face when they enter this field. Designing and building the model is what attracts most people to machine learning but if you cant get that model into production, it essentially becomes just a piece of useless code.So Databricks (founded by the Apache Spark creators) decided to build and open source a solution to all ML framework challenges. Called MLflow, it is a platform that manages the entire machine learning lifecycle (from start to production) and has been designed to work with any library. Ever since its release, it has gained a huge following (1,355 stars on GitHub) and you can check out our coverage of the library here.Another NLP entry in this article! When it comes to NLP tasks like sentiment analysis, or machine translation, the norm has been to build models specific to that task. Have you ever built a sentiment analysis model that can also do semantic parsing and question answering at the same time? Thats what Salesforce researchers intend to do with this repository.They have published a research paper that outlines a model which can do 10 different NLP tasks at the same time. In this paper, they have thrown a chalenge (which they are calling decaNLP) to the community  can you build such a model and improve on the approach theyve provided? The model Salesforce have built is being called the Swiss Army Knife for Natural Language Processing.Read more details about this on AVs post here.Reinforcement learning is becoming popular by the day and so is the open source community for it. This repository is a collection of reinforcement learning algorithms from Richard Sutton and Andrew Bartos book and other research papers. These algorithms are presented in the form pf Python notebooks.The creator of the repository recommends using these notebooks when you read the book as they will significantly enhance your understand of whats being presented. The notes are detailed and anyone entering this field should definitely refer to this collection.Source: WikipediaYour interest in this thread will be piqued by the above video, which is put together and presented really neatly. It sent the machine learning subreddit into overdrive and received almost 100 comments! The thread has a lot of useful information on how the technique was created (theres a step-by-step explanation from the developer), how long it took, what kind of other things it can do, etc. Youll learn a lot about computer vision in this thread.The creator of this technique and video has also open sourced his code on GitHub. So open your Jupyter notebooks and get cracking!OpenAI Five is a group of 5 neural networks designed and developed to beat human opponents in the popular Dota 2 game. It has been developed by the Elon Musk co-founded OpenAI venture, which explains the immediate popularity it has received since its release.Why Im recommending this thread is the rich discussion around what else data scientists want to see from this technique, its comparison with the popular DeepMind AlphaGo algorithm, and how much computational power it required to pull this off. There is a lot of perspective in this thread that will greatly benefit you.Additionally, you can also read our article on OpenAI Five here.If this topic didnt get your attention, the first few comments surely will. This discussion is like a wish list of what data scientists and machine learning practitioners want to see from the community. This thread made my list because of the discussion each idea spawned. Once a person added their idea to the thread, multiple folks replied with their ideas on how to implement it and if similar research was already present.This is a MUST-READ discussion  for both enthusiasts and practitioners. Take some time out to go through it and youll come out with a lot of knowledge (and perhaps even more questions).What hardware you use for machine learning plays a critical role in determining how good your model will perform, especially when the amount of data to be trained is huge. Read this thread to find out what other data scientists use for building their ML processes and models. The original poster has listed down a structured list of questions which helped keep the thread neat and understandable. There questions are as below:You can also take part in the discussion or use the comments section below this article to let us know your setup!As I mentioned above, reinforcement learning is a popular field these days. But due to the complex nature of the work, most of the research and use cases are limited to games and lab environments. In this thread, people already working in this field give their take on where they see RL penetrating in the near future. Some of the comments are of a more skeptical nature but are worth reading to understand what experts and enthusiasts feel about RL.Phew! So much to read and learn in this past month. This list has something for everybody  NLP, reinforcement learning, open source code you can download and start working on, computer vision, discussions on various machine learning related things, and much more. Use the comments section below to let us know which repository and/or discussion you found the most interesting!",https://www.analyticsvidhya.com/blog/2018/07/top-github-reddit-data-science-machine-learning-june-2018/
Databricks and RStudio Launch Platform to make R Simpler than Ever for Big Data Projects!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Top GitHub Repositories & Reddit Threads Every Data Scientist should know (June 2018)|The Best Research Papers from ICML 2018  A Must-Read for Data Scientists|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anyone who uses R programming typically does so using the wonderful RStudio IDE. Its a neat and intuitive tool with excellent and regular maintenance updates. A lot of tools from other languages have tried to copy RStudios style but to no avail  it stands out as one of the best coding tools in the community (not to mention its open source!).But performing Big Data tasks with R has been a little challenging. Sure there exist a few packages like Sparklyr that make things easier but scaling up has been an obstacle for many an organization. This gap is now being addressed through an integrated platform developed by Databricks and RStudio. Databricks was founded by the creators of Apache Spark and has recently been in the news thanks to MLflow  their open source platform that works with any language, tool and algorithm.The platform, provided by Databricks, integrates seamlessly with RStudio and enables data scientists and data engineers to automatically execute R code at an unprecedented scale. Both the popular R packages currently used for connecting and interacting with Apache Spark, sparklyr and SparkR, can be used inside RStudio on Databricks. Awesome!A demo of this platform has also been provided by Databricks which shows a KNN Regression problem. You can either view it using the HTML version or download the R Markdown file and watch the magic unfold inside RStudio itself.As mentioned in this Databricks blog post, R users can get access to the full ETL capabilities of Databricks to provide access to relevant datasetsincluding optimizing data formats, cleaning up data, and joining datasets to provide the perfect dataset for your analytics.Any data engineer (or to a certain extent a data scientist) who currently works with R will love this release. Despite recent advances in R, performing Big Data tasks has always been a challenge. Most of the data engineers prefer working with Python. It helps massively that a R Markdown file is available to get you started. Theres a free trial available so you can test it out on your machine before applying it in your current project.All the data engineers out there  what do you make of this release? Will it make your current job easier? Let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/06/databricks-rstudio-launch-platform-make-r-simpler-big-data-projects/
The Best Research Papers from ICML 2018  A Must-Read for Data Scientists,Learn everything about Analytics|Overview|Introduction|Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples|Delayed Impact of Fair Machine Learning|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Databricks and RStudio Launch Platform to make R Simpler than Ever for Big Data Projects!|Googles Amazing Self-Supervised Computer Vision Model can Track Objects in any Video!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The thirty-fifth edition of the International Conference on Machine Learning (ICML) is almost here! Some of the best minds in the machine learning industry come together at this well known summit to present their research and discuss new ideas. Its an event every data scientist and ML practitioner should have circled on their calendar!Each year, hundreds of research papers are submitted to the conference but only a few make the cut. A panel of hand-picked judges run the rule over these papers and pick out what the conference calls the Best Paper Awards. Its quite a prestigious award to win given that its picked from some of the best research in the ML space.This year the competition was tougher than ever before. With more and more research being funded, papers are being churned out at an unprecedented rate. Without any further ado, below are the two papers that have won the Best Paper Award at ICML 2018:Three runner-up awards have also been announced:Lets look at the two best papers in a bit more detail.Obfuscated gradients are a kind of gradient masking that often lead to a false sense of security against adversarial attacks. The researchers found a way to circumvent defences that used and relied on these obfuscated gradients. They discovered three types of these gradients, and managed to design techniques to attack each of them. Using a case study to illustrate their point, the team found 7 out of 9 defences using obfuscated gradients  their technique circumvented 6 of those completely and 1 partially.Why is this important? It offers organizations using this kind of defence to shore up their current methods and look for more robust measures. A very worthy co-winner of the best paper award.Bias has always been a very pressing issue in machine learning models. Recent examples of facial recognition software failing to recognize people of certain regions has been in the news but there are examples from other fields as well, like loan lending, recruitment, advertising, etc. Researchers from Berkeleys Artificial Intelligence Research lab have published this paper in which they talk about their work on making ML work with long term social welfare ideas.They have introduced a one-step feedback model that looks at decision-making. The results of this model show how certain decisions change the underlying population over a period of time. An example of credit score in terms of loans has been discussed extensively.These are two really fascinating research papers that explore two different but pressing issues in machine learning. I implore you to take time out and read them. While there are thousands of research papers out there these days, conferences like ICML and ICLR pick out the cream of the crop to make things easy for us.Is there any other research paper you feel we should know about? Let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/06/best-research-papers-icml-2018/
Googles Amazing Self-Supervised Computer Vision Model can Track Objects in any Video!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Best Research Papers from ICML 2018  A Must-Read for Data Scientists|Understanding and Building an Object Detection Model from Scratch in Python|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Self-supervised learning has so much untapped potential in deep learning. Where supervised learning requires tons of labelled data to come up with an accurate and precise solution, self-supervised learning only needs a sliver of that labelled data (if any at all). Which is what makes it such a challenging and difficult line of work.But self-supervised learning has been garnering attention recently, especially in the field of computer vision (which notoriously requires more labelled data than most fields to give a proper output). And now the Google AI team has developed a model that can track objects in videos without requiring labelled at all.The team has designed a convolutional neural network that adds color to grayscale videos. While doing this, the network learns by itself to visually track objects in the video. The team admits in a blog post that the model was never trained with the singular aim of tracking, but it managed to learn without supervision and can follow multiple objects and remain robust without requiring ANY labelledtraining data!The researchers used videos from the public Kinetics dataset to train the model. Keep in mind that all these videos are in color so they were first converted to grayscale, except the very first frame in each video. The convolutional network was then trained to predict the original colors in all the remaining frames. The below collection of images illustrates this technique well:You might we wondering why did they decolor the videos in the first place? This is because theres a good chance that there might be multiple objects in the video with the same color and by converting it to greyscale and then adding color again, the team was able to teach the machine to track specific objects.An important part of designing and using models in deep learning is their interpretability, which isnt easy given the complexity associated with them. According to their blog post, they used a standard trick to visualize the embeddings learned by the model by projecting them down to three dimensions usingPrincipal Component Analysis(PCA) and plotting it as an RGB movie.Another finding from the model was that its even able to track the pose of humans. See the below image that shows the poses of different humans being tracked (this was tested on the JHMDB dataset).You can read about this technique in more detail in Googles research paper here.If you read the paper (and you really should!) youll see that the results of this model dont outperform high-end supervised models. But since this is just the starting point for self-supervised video tracking, I think we can expect that gap to shrink significantly soon.I especially liked that the model is doing multiple things  colorization, pose estimation, and of course object tracking. It turns out that the failures of the model are correlated with a failure to colorize videos, which pinpoints where the team needs to work on. This is definitely something we should keep our eye on in the foreseeable future as the potential and possibilities of using this technique are vast.",https://www.analyticsvidhya.com/blog/2018/06/googles-amazing-self-supervised-computer-vision-model-can-track-objects-in-any-video/
Understanding and Building an Object Detection Model from Scratch in Python,Learn everything about Analytics|Introduction|Table of Contents|What is Object Detection?|Different Approaches to Solve an Object Detection Problem|Getting Technical: How to build an Object Detection model using the ImageAI library|End Notes,"Approach 1: Naive way (Divide and Conquer)|Approach 2: Increase the number of divisions|Approach 3: Performing structured divisions|Approach 4: Becoming more efficient|Approach 5: Using Deep Learning for feature selection and to build an end-to-end approach|Share this:|Like this:|Related Articles|Googles Amazing Self-Supervised Computer Vision Model can Track Objects in any Video!|AI Guardman  A Machine Learning Application that uses Pose Estimation to Detect Shoplifters|
Faizan Shaikh
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When were shown an image, our brain instantly recognizes the objects contained in it. On the other hand, it takes a lot of time and training data for a machine to identify these objects. But with the recent advances in hardware and deep learning, this computer vision field has become a whole lot easier and more intuitive.Check out the below image as an example. The system is able to identify different objects in the image with incredible accuracy.Object detection technology has seen a rapid adoption rate in various and diverse industries. It helps self-driving cars safely navigate through traffic, spots violent behavior in a crowded place, assists sports teams analyze and build scouting reports, ensures proper quality control of parts in manufacturing, among many, many other things. And these are just scratching the surface of what object detection technology can do!In this article, we will understand what object detection is and look at a few different approaches one can take to solve problems in this space. Then we will deep dive into building our own object detection system in Python. By the end of the article, you will have enough knowledge to take on different object detection challenges on your own!Note: This tutorial assumes that you know the basics of deep learning and have solved simple image processing problems before. In case you havent, or need a refresher, I recommend reading the following articles first:Before we dive into build a state-of-the-art model, let us first try to understand what object detection is. Lets (hypothetically) build a pedestrian detection system for a self-driving car. Suppose your car captures an image like the one below. How would you describe this image?The image essentially depicts that our car is near a square, and a handful of people are crossing the road in front of our car. As the traffic sign is not clearly visible, the cars pedestrian detection system should identify exactly where the people are walking so that we can steer clear of them.So what can the cars system do to ensure this happens? What it can do is create a bounding box around these people, so that the system can pinpoint where in the image the people are, and then accordingly make a decision as to which path to take, in order to avoid any mishaps.Our objective behind doing object detection is two folds:Now that we know what our problem statement is, what can be a possible approach (or multiple approaches) to solve it? In this section, well look at a few techniques that can be used to detect objects in images. We will start from the simplest approach and find our way up from there. If you have any suggestions or alternate approaches to the ones we will see below, do let me know in the comments section!The simplest approach we can take is to divide the image into four parts:Now the next step is to feed each of these parts into an image classifier. This will give us an output of whether that part of the image has a pedestrian or not. If yes, mark that patch in the original image. The output will be somewhat like this:This is a good approach to try out first, but we are looking for a much more accurate and precise system. It needs to identify the entire object (or a person in this case) because only locating parts of an object could lead to catastrophic results.The previous system worked well but what else can we do? We can improve upon it by exponentially increasing the number of patches we input into the system. This is how our output should look like:This ended up being a boon and a curse. Of course our solution seems a bit better than the naive approach, but it is riddled with so many bounding boxes which approximate the same thing. This is an issue, and we need a more structured way to solve our problem.In order to build our object detection system in a more structured way, we can follow the below steps:Step 1:Divide the image into a 1010 grid like this:Step 2:Define the centroids for each patchStep 3:For each centroid, take three different patches of different heights and aspect ratio:Step 4:Pass all of the patches created through the image classifier to get predictionsSo how does the final output look like? A bit more structured and disciplined for sure  take a look below:But we can further improve on this! Read on to see yet another approach that will produce even better results.The previous approach we saw is acceptable to quite a good degree, but we can build a system a little more efficient than that. Can you suggest how? Off the top of my mind, I can propose an optimization. If we think about approach #3, we can do two things to make our model better.This again, has its pros and cons. Sure both of the methods will help us go to a more granular level. But it will again create an explosion of all the patches that we have to pass through our image classification model.What we can do is, take selective patches instead of taking all of them. For example, we could build an intermediate classifier which tries to predict if the patch actually has background, or potentially contains an object. This would exponentially decrease the patches that our image classification model has to see.One more optimization that we can do, is to decrease the predictions which say the same thing. Lets take the output of approach 3 again:As you can see, both the bounding box predictions are basically of the same person. We have an option to choose any one of them. So to make predictions, we consider all the boxes which say the same thing and then pick whichever one has the most probability of detecting a person.All of these optimizations have so far given us pretty decent predictions. We almost have all the cards in our hands, but can you guess what is missing? Deep Learning of course!Deep learning has so much potential in the object detection space. Can you recommend where and how can we leverage it for our problem? I have listed a couple of methodologies below:Now instead of training different neural networks for solving each individual problem, we can take a single deep neural network model which will attempt to solve all the problems by itself. The advantage of doing this, is that each of the smaller components of a neural network will help in optimizing the other parts of the same neural network. This will help us in jointly training the entire deep model.Our output would give us the best performance out of all the approaches we have seen so far, somewhat similar to the image below. We will see how to create this using Python in the next section.Now that we know what object detection is and the best approach to solve the problem, lets build our own object detection system! We will be using ImageAI, a python library which supports state-of-the-art machine learning algorithms for computer vision tasks.Running an object detection model to get predictions is fairly simple. We dont have to go through complex installation scripts to get started. We dont even need a GPU to generate predictions! We will use this ImageAI library to get the output prediction we saw above in approach #5. I highly recommend following along with the code below (on your own machine) as this will enable you to gain the maximum knowledge out of this section.Please note that you need to set up your system before creating the object detection model. Once you have Anaconda installed in your local system, you can get started with the below steps.Step 1: Create an Anaconda environment with python version 3.6.Step 2: Activate the environment and install the necessary packages.Step 3: Then install the ImageAI library.Step 4: Now download the pretrained model required to generate predictions. This model is based on RetinaNet (a subject of a future article). Click on the link to download RetinaNet Pretrained modelStep 5: Copy the downloaded file to your current working folderStep 6: Download the image from this link. Name the image as image.pngStep 7: Open jupyter notebook (type jupyter notebook in your terminal) and run the following codes:This will create a modified image file named image_new.png, which contains the bounding box for your image.Step 8: To print the image use the following code:Congratulations! You have created your own object detection model for pedestrian detection. How awesome is that?In this article, we learned what is object detection, and the intuition behind creating an object detection model. We also saw how to build this object detection model for pedestrian detection using the ImageAI library.By just tweaking the code a bit, you can easily transform the model to solve your own object detection challenges. If you do solve such a problem using the approach above, especially for a social cause, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/06/understanding-building-object-detection-model-python/
AI Guardman  A Machine Learning Application that uses Pose Estimation to Detect Shoplifters,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Understanding and Building an Object Detection Model from Scratch in Python|OpenAI Five  A team of 5 Algorithms is Beating Human Opponents in a Popular Game|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The potential of machine learning and AI in the world of surveillance is being tapped into with gusto by companies globally. We have already seen a machine learning project that can spot violence in a crowd of people, which will go a long way towards handling security at events like concerts and sports.And now a Japanese telecom company has partnered with a startup to build an AI that can be used to detect shoplifters in any store. This AI, being called AI Guardman, is built into the CCTV cameras that you see in every store these days. Check out the below video that illustrates this technology in action:How does this technology work, however? The machine learning part of this AI uses a technique called OpenPose. It was developed by researchers at the Carnegie Mellon University to estimate the pose of a person in real-time. It is able to detect a persons body, hand, and facial points on 2D and 3D images. OpenPose has been met with an overwhelmingly positive response in the ML community and you can download the code to try it out yourself from CMUs GitHub repository.There are several pre-defined poses for suspicious behavior. When the AI camera detects such behavior, it instantly send an alert to the shopkeeper via a phone application, also developed for this purpose.The developers released the results on initial trials and the feedback has been excellent. According to a report in IT Media, they have seen a drop in shoplifting incidents by around 40 percent!AI is penetrating further and further into the surveillance market. As I mentioned earlier, its already being developed for spotting violent behavior in crowds, is being used in China by the police to catch criminals, and even in US homes as a security measure. Facial recognition has also been a game changer in this field  imagine walking past a CCTV camera and being identified in a matter of milliseconds.Its a little difficult to image how accurate this particular AI can be. As we saw above, there are pre-defined poses so if someone behaves outside of those parameters, that person might not be labelled as suspicious. Quite an alarming thought, isnt it?This does raise challenges of privacy and discrimination. Let me know your thoughts on this technology  are you waiting eagerly for it or are your concerns trumping that excitement? Use the comments section below!",https://www.analyticsvidhya.com/blog/2018/06/ai-guardman-machine-learning-application-estimates-poses-detect-shoplifters/
OpenAI Five  A team of 5 Algorithms is Beating Human Opponents in a Popular Game,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AI Guardman  A Machine Learning Application that uses Pose Estimation to Detect Shoplifters|Researchers at Adobe are using Machine Learning to Detect Image Manipulation|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence competing against humans in a game has become the norm these days. At this point in the evolution of AI, we assume that AI is advanced enough to figure out every single move in a game, find loop holes, and then create world record scores.But so far these games have been geared more towards the strategic side  like chess and Go. The team at OpenAI, a venture co-founded by Elon Musk, has developed a team of algorithms (called OpenAI Five) that competes against humans in the popular game Dota 2. What makes their approach towards AI different is that Dota 2 requires real-time decision making, instead of pondering on what move to make next. According to a blog post by OpenAI, this game runs at30 frames per second for an average of 45 minutes, resulting in 80,000 ticks per game.OpenAI Five is basically a group of 5 neural networks. It plays 180 years worth of games against itself every single day! Of course this amount and level of training doesnt come without proper computational resources. OpenAI Five trains itself using a scaled-up version of a class of reinforcement learning algorithms called Proximal Policy Optimization (PPO). This training process is executed on 256 GPUs and 128,000 CPU cores.So how does this group of neural networks recognize and build strategies in real-time? Each of the five neural networks contain a single-layer, 1024 unit LSTM (Long Short Term Memory) that analyzes the real-time state of the game and then performs actions. OpenAI Five views the world as a list of 20,000 numbers, and takes an action by emitting a list of 8 enumeration values.OpenAI Five has a few restrictions placed on it, which you can read about in their blog post. OpenAI will be playing a match against top Dota 2 players on July 28th and then participating in a tournament in August to benchmark their progress.Check out the below video released by the OpenAI team, where they show OpenAI Five in action:While its definitely a good sign that reinforcement learning is moving towards real-time decision making applications, expectations are tempered at this point. As the team repeatedly mentions in the post, Dota is an incredibly complex game with dozens of decisions possible in a single frame. Its going to take a lot more learning and tweaking before OpenAI Five (or whatever comes next) is able to comprehensively get one over human players.I recommend (again) going through OpenAIs post to read about the algorithm in detail.",https://www.analyticsvidhya.com/blog/2018/06/openai-five-a-team-of-5-algorithms-is-beating-human-opponents-in-a-popular-game/
Researchers at Adobe are using Machine Learning to Detect Image Manipulation,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|OpenAI Five  A team of 5 Algorithms is Beating Human Opponents in a Popular Game|DataHack Radio Episode #3  Marios Michailidis Inspiring Story of a Non-Programmer to No. 1 on Kaggle|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dealing with fake news has become one of the most pressing needs of the digital age. There are so many fake videos and images flying around on social media sites, it has become extremely difficult to stem the tide. Facebook has been in the news in recent times because of various scandals but companies like Amazon have been fighting this battle since a long time (like weeding out fake reviews).Adobe knows better than most about photoshopped images. In their latest blog post, they have acknowledged that while Photoshop has always shown its upside, people have also used it to doctor images for deceptive purposes. So Adobe decided to invest in machine learning and fight back against this rising menace.Researchers at the organization have built a model that that is able to differentiate between authentic and tampered images using image manipulation detection. The team focused on the three most common tampering methods:In order to train the R-CNN (convolutional neural network) to recognize manipulated image, tens of thousands of images were used as examples. Two different techniques were meshed together to make this neural network. The first technique makes use of an RGB stream, while the second uses a noise stream filter.The below collection of images shows how the final model works:Note that this technique is not the same as the traditional object detection techniques we have seen previously. Image manipulation detection focuses far more on tampering artefacts than the content in the image.The team has also published a full research paper describing this technique in detail which you can read here.Even using machine learning, this is quite an ambitious project from Adobe. Adobe themselves admit that this does not solve the problem of absolute truth of an image. Its a step in the right direction but one feels that we are still quite far away from what Adobe had in mind when they started this project.I would like to see other features (and not just the 2 streams described above) be used in the neural network. Like they mention in the paper, illumination in the entirety of the image, and compression factors can and should be included if this is to be truly effective. Let me know your take on this technique in the comments section below.",https://www.analyticsvidhya.com/blog/2018/06/scammers-beware-researchers-at-adobe-are-using-machine-learning-to-detect-image-manipulation/
DataHack Radio Episode #3  Marios Michailidis Inspiring Story of a Non-Programmer to No. 1 on Kaggle,Learn everything about Analytics||Introduction|End Notes,"Marioss background|Programming Languages|Kaggle Journey|Difference between ML Competitions and Real-Life Data Science Projects|Learning Resources|Framework/Approach to Machine Learning Competitions|Driverless AI and Automated ML Tools|Marios Ph.D and Future in Teaching|Rapidfire Questions!|Share this:|Like this:|Related Articles|Researchers at Adobe are using Machine Learning to Detect Image Manipulation|Facebook has Open Sourced the Python Code for DensePose  Download it now!|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Marios Michailidis is an experienced data scientist who is currently working at H2O.ai. One of most fascinating facts about Marios is that he had no programming background till he finished his Masters degree. He is the very definition of an inspiring self-taught data scientist!He is a popular figure in the world of machine learning competitions. He loves competing in Kaggle competitions, and has even won several of them. He holds the title of Kaggle Grandmaster and has previously held the number 1 rank globally!In this third episode of DataHack Radio, Kunal chats with him about his background, his approach to machine learning competitions, his Kaggle journey, his appreciation for Analytics Vidhya, and a whole lot more. Marios even gave us some terrific analogies throughout the podcast, like the one below when asked about the difference between competitions and real-life projects:Competitions is a bit like running in the Olympics. Its a good skill to be able to run really fast but are you going to need this kind of running ability your whole life?Below are the key excerpts from this fun and knowledge-filled podcast. Happy listening!You can subscribe to DataHack Radio or listen to previous episodes on any of the below platforms:Marios is originally from Greece and he got his accounting and finance degree from there. Due to the economic condition in Greece at the time, he decided to get his Masters degree in Risk Management from the University of Southampton, UK. At this point, he was still looking at various fields and was undecided which one to pick.What sparked his interest in data science and machine learning was a horse racing expert who used to collect data to predict whod win. This led to Marios reading more about data science and learning programming languages. He built an open source freeware called Kazanova (listen to the podcast to understand the thinking behind this name  youll love it) which led to job offers and things smoothened out from there.After his Masters, Marios started learning C. He spent some time studying it but was disappointed in it and switched to Java. It took him a month to properly understand the ins and outs of the language and from there it was smooth sailing.He believes learning programming is essential and if one dedicates proper time to it, anyone can pick it up.It took a lot of hard work and perseverance to reach the top of the rankings.When Marios joined dunnhumby back in 2013, the organization had already hosted 2 Kaggle competitions. So speaking to other people there, he decided to check out what it was. His first Kaggle competition was the Amazon Employee Classification challenge which was pretty popular back then.When he started, he tried his own techniques but that did not work out on the leaderboard. He altered his approach  spending more time on seeing other peoples work on the discussion forums, how they structured their thinking, etc. He ended up finishing in the top 10 of his very first Kaggle competition!From there, it took him around 3 years to get to #1 on Kaggles leaderboard. In this intervening period, he set short-term goals for himself, like jumping into the top 100 in the global rankings. It also helped that some of the competitions were similar to what the work he was doing at dunnhumby. An important he mentioned, which most of us should follow, is that he kept adding to his knowledge. He went from the basics to image classification, audio classification, etc.There are certainly fundamental differences between the two. As Marios put it quite eloquently, winning a competition is not the same as putting a model into production. A few things are definitely not replicable in either situation.However, Marios vehemently believes that there is a lot of value attached to competing in these hackathons. While you can always tweak a few parameters to achieve a better score, its always helpful to know how the theory behind how you can achieve the best score.Kaggle and you have done a fantastic job of helping everyone in this space, to become better, me included.One of the things Marios stressed about is going back to previous competitions and looking at how the winning solutions were built. Since other people have already put in their research, done their due diligence and shared their work, why not learn from them? Analytics Vidhya was also a useful reference point for him when he was looking for . Other things he picked up from books and his business environment.The first thing Marios does is try to understand and break down the problem statement into parts. Then he usually moves on to exploring the data  looking at the distribution of variables, differences between training and test datasets, if there are temporal variables, etc. This helps him decide his cross validation strategy.Once he pens down his hypothesis, he is then free to try various things with the dataset  feature selection, elimination, hyperparameter tuning, etc. This is a critical stage which goes a long way towards determining the success of the final model.Once this has been accomplished, he then moves onto trying different algorithms and feature transformations. Once he has saved his results (another often overlook but important step), he moves onto the final step  stacking.On the topic of forming teams, he usually looks for people who have a wider range of knowledge than him, or a different skill set. This helps in combing models and also looking at the problem with a different perspective.Automated ML is an empowering tool, according to Marios. He doesnt feel this will replace data scientists, but instead will help them focus and take more strategic decisions. You can find out a lot about the data through tools like Driverless AI. For example, Marios uses it for various tasks including exporting the best features of a model. Then he can run his own model(s) using those features.You also get a lot of interpretability through automated ML tools, which might not always be the case when developing your own model manually. He made a good point that when building complex models, or trying to find solutions to multi-layered problems, youll always lose something but Driverless AI helps to break down and understand most of the models inner workings.His thesis for his Ph.D (in financial accounting) was on using machine learning methods in recommender systems. He had already worked at dunnhumby in the recommender system space for grocery sales so this seemed like a logical move. His role wasnt just limited to modeling, but encompassed feature engineering as well. As far as teaching goes, he feels he has an obligation to teach and give back to the community. He has learned so much from open source resources, that he wants to share his experience and knowledge with the global community whatever way he can.Kunal Jain: If you could only apply 1 algorithm in a competition, which one would you use?Marios Michailidis: A type of gradient boosting, like LightGBM or XGBoost, since it requires the least preparation.KJ: What topic would you choose for your Ph.D thesis, if you had that option today?MM: I like this work about interpretability, so maybe something like de-stacking. Going from a stacked model to something really simple without losing any accuracy.There are a few more questions that you can (and should) listen to in the podcast!What an awesome podcast! I highly recommend listening to this one because not only will you learn about how a winner approaches machine learning competitions and how he structures his thinking, but will be inspired by Marios story of going from a non-technical background to a full blown data scientist and the no. 1 Kaggler in the world thanks to hard work and perseverance.Happy listening!",https://www.analyticsvidhya.com/blog/2018/06/datahack-radio-episode-3-marios-michailidis/
Facebook has Open Sourced the Python Code for DensePose  Download it now!,Learn everything about Analytics|Overview|Introduction|Our Take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #3  Marios Michailidis Inspiring Story of a Non-Programmer to No. 1 on Kaggle|DIVE  MITs Open Source Tool for Data Exploration and Visualization for Data Scientists|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine a world where you open an apparel application on your phone, tap on clothes you like, and the app shows images of you with those clothes on. Sounds like magic, right? On the contrary, we are very close to seeing this kind of technology turning into a real-life application.Currently, data scientists are able to annotate images, but the existing approaches locate a sparse set of joints, like the wrists or elbows, which are often used for applications like gesture or action recognition. Facebooks AI Research division (FAIR) has taken this technique to another level altogether.In order to map all human pixels in 2D images to a 3D surface-based model of the body, they have pioneered a new approach called DensePose. The current approaches in human pose estimation operate with 10 or 20 human joints (such as wrists, elbows, etc.) whereas DensePose identifies the human body in more than 5000 nodes! The below image illustrates my point pretty well.As mentioned by the researchers in the paperDensePose: Dense Human Pose Estimation In The Wild, presented at the Computer Vision and Pattern Recognition conference (CVPR) 2018 in Utah, the DensePose project includes:DensePose-COCO: A large-scale dataset with image-to-surface correspondences. The team has gathered annotations for 50K humans, collecting more than 5 million manually annotated correspondences. The exact same train/validation/test split as in the COCO challenge has been followed. Below is an example of a visualization of annotation from the validation set.DensePose-RCNN: This is a variant of Mask-RCNN, with Feature Pyramid Network and Region-of-Interest Pooling followed by fully-convolutional processing (architecture shown below). This is done to obtain dense part labels and coordinates within each of the selected regions.  The team has shared a GitHubrepository in which they have open sourced the code to train and evaluate DensePose-RCNN. Also the notebooks used to visualize the collected DensePose-COCO dataset have been provided. This technique has been implemented using Facebooks own Detectron framework and is powered by Caffe2.Below is a video in which they have provided an overview of the technique.I can see this technique being put to good use for improving virtual reality experiences or for motion capture devices. And not just that, it can help doctors make decisions regarding physical ailments in patients, accelerate the recent advancement in sports science, among other things.As usual, the code is available on code for you to play around with. Can you improve on what theyve released?Where else can you apply this transcendent technique? Share your thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/06/facebook-has-open-sourced-the-python-code-for-densepose-download-now/
DIVE  MITs Open Source Tool for Data Exploration and Visualization for Data Scientists,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Facebook has Open Sourced the Python Code for DensePose  Download it now!|Salesforce has Developed One Single Model to Deal with 10 Different NLP Tasks|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data cleaning is the most time consuming process in the data science lifecycle. But data exploration might be the most important one when it comes to building a good model. I have personally seen the accuracy of models drop significantly when the dataset at hand was not explored properly. Its critical that we know what the data represents, if there are any biases, what features can we engineer, etc. All of this falls under data exploration. And now you dont even have to write code to do this!MITs research team has built a web-based data exploration system called DIVE, that lets you create stories from your data without having to write any code. You can have a look at the public version of DIVE here. Below is a brief summary of what you can expect from DIVE:When it comes to analysis, the tool currently offers the below 4 options:Below is a demo video by the team presenting the working of DIVE from uploading the dataset to exploring the tool. Have a look.Here are the links to Front-end repository and Back-end repository provided by the team. For more information about DIVE, you can read their paper published in the proceedings of HILDA 2018.Of course this is not the first automated tool in this space. The competition for automated ML is fierce but what makes DIVE stand out is its relatively lightweight appearance for quick exploration.I took DIVE for a test run and it has impressed me a lot. Its easy to use, is extremely efficient and the fact that I dont have to install anything (its web based) is a major positive. I found the overall process extremely intuitive. Check out the below screenshots where I uploaded the dataset and analysed the data. This one is a simple statistical analysis of the variables in the dataset.The below one is a summary of the linear regression model:If youre from a non-technical background, I would suggest trying out this tool. You dont have to write a single line of code! Let me know your experience using it in the comments below.",https://www.analyticsvidhya.com/blog/2018/06/perform-data-exploration-with-a-single-click-dive/
Salesforce has Developed One Single Model to Deal with 10 Different NLP Tasks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DIVE  MITs Open Source Tool for Data Exploration and Visualization for Data Scientists|Comprehensive Guide to build a Recommendation Engine from scratch (in Python)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When we need to build a model for performing sentiment analysis, we build a model exclusively for that. If we need to perform machine translation on the same data, we would then need to build a different model for it. And if a chatbot had to be created, another model would have to be developed! Even though all of this falls under the Natural Language Processing (NLP) domain, we need to sketch out different approaches to deal with different problems.This is the challenge Salesforce researchers wanted to crack. They have published a research paper where theyve outlined a model that can deal with 10 different types of NLP tasks at once. The paper is essentially laying down a challenge called decaNLP (Natural Language Decathlon). They have also presented a multitask question answering network (MQAN) that jointly learns all the 10 tasks.You dont need to build a separate model when you need to switch from, say, sentiment analysis to a question answering chatbot. This model will be a one-stop shop for most NLP tasks and its rightly being labelled as the Swiss Army Knife for Natural Language Processing.These 10 tasks that the model should be able to solve are mentioned below:The below image gives an overview of the different tasks the model can perform.The words in red in the Answer column are generated by pointing to the context, in green from the question, and in blue if they are generated from a classifier.As you can see, all tasks have been framed in a question answering form. This is where MQAN comes into play. It has been structured on zero-shot learning, which enables it to learn tasks it hasnt seen or done before.To know more about this model and the challenge laid down by the researchers, read their research paper.While deep learning has made significant impact in fields like image generation and audio processing, NLP has been a little behind the curve. Sure we have still seen some fascinating research in this field but not at this level. This is a HUGE development that will change the way most data scientists spend time on NLP related tasks.The most obvious application I can see will be in chatbots, which will surely become more intelligent and able to carry a conversation in a human-like manner. It also helps that Salesforce has open sourced their research and laid down a challenge to the community. I hope you go through the research paper and participate in this challenge  the knowledge youll gain will be priceless!",https://www.analyticsvidhya.com/blog/2018/06/salesforce-has-developed-one-single-model-to-deal-with-10-different-nlp-tasks/
Comprehensive Guide to build a Recommendation Engine from scratch (in Python),Learn everything about Analytics|Introduction|Project to build your Recommendation Engine|Problem Statement|Table of Contents|1. What are recommendation engines?|2. How does a recommendation engine work?|3. Case study in Python using the MovieLens Dataset|4. Building collaborative filtering model from scratch|5. Building a simple popularity and collaborative filtering model using Turicreate|6. Introduction to matrix factorization|7. Building a recommendation engine using matrix factorization|8. Evaluation metrics for recommendation engines|9. What else can be tried?|End Notes,"2.1 Data collection|2.2 Data storage|2.3 Filtering the data|Share this:|Related Articles|Salesforce has Developed One Single Model to Deal with 10 Different NLP Tasks|Hiring the Right Data Scientist  The Needle in a Haystack Problem|
Pulkit Sharma
|78 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",2.3.1 Content based filtering|2.3.2 Collaborative filtering|User-User collaborative filtering|Item-Item collaborative filtering,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In todays world, every customer is faced with multiple choices. For example,If Im looking for a book to read without any specific idea of what I want, theres a wide range of possibilities how my search might pan out. I might waste a lot of time browsing around on the internet and trawling through various sites hoping to strike gold. I might look for recommendations from other people.But if there was a site or app which could recommend me books based on what I have read previously, that would be a massive help. Instead of wasting time on various sites, I could just log in and voila! 10 recommended books tailored to my taste.This is what recommendation engines do and their power is being harnessed by most businesses these days. From Amazon to Netflix, Google to Goodreads, recommendation engines are one of the most widely used applications of machine learning techniques.In this article, we will cover various types of recommendation engine algorithms and fundamentals of creating them in Python. We will also see the mathematics behind the workings of these algorithms. Finally, we will create our own recommendation engine using matrix factorization.Many online businesses rely on customer reviews and ratings. Explicit feedback is especially important in the entertainment and ecommerce industry where all customer engagements are impacted by these ratings. Netflix relies on such rating data to power its recommendation engine to provide the best movie and TV series recommendations that are personalized and most relevant to the user.This practice problem challenges the participants to predict the ratings for jokes given by the users provided the ratings provided by the same users for another set of jokes. This dataset is taken from the famous jester online Joke Recommender system dataset.Practice NowTill recently, people generally tended to buy products recommended to them by their friends or the people they trust. This used to be the primary method of purchase when there was any doubt about the product. But with the advent of the digital age, that circle has expanded to include online sites that utilize some sort of recommendation engine.A recommendation engine filters the data using different algorithms and recommends the most relevant items to users.It first captures the past behavior of a customer and based on that, recommends products which the users might be likely to buy.If a completely new user visits an e-commerce site, that site will not have any past history of that user. So how does the site go about recommending products to the user in such a scenario? One possible solution could be to recommend the best selling products, i.e. the products which are high in demand. Another possible solution could be to recommend the products which would bring the maximum profit to the business.If we can recommend a few items to a customer based on their needs and interests, it will create a positive impact on the user experience and lead to frequent visits. Hence, businesses nowadays are building smart and intelligent recommendation engines by studying the past behavior of their users.Now that we have an intuition of recommendation engines, lets now look at how they work.Before we deep dive into this topic, first well think of how we can recommend items to users:Both of the above methods have their drawbacks. In the first case, the most popular items would be the same for each user so everybody will see the same recommendations. While in the second case, as the number of users increases, the number of features will also increase. So classifying the users into various segments will be a very difficult task.The main problem here is that we are unable to tailor recommendations based on the specific interest of the users. Its like Amazon is recommending you buy a laptop just because its been bought by the majority of the shoppers. But thankfully, Amazon (or any other big firm) does not recommend products using the above mentioned approach. They use some personalized methods which help them in recommending products more accurately.Lets now focus on how a recommendation engine works by going through the following steps.This is the first and most crucial step for building a recommendation engine.The data can be collected by two means: explicitly and implicitly.Explicit data is information that is provided intentionally, i.e. input from the users such as movie ratings. Implicit data is information that is not provided intentionally but gathered from available data streams like search history, clicks, order history, etc.In the above image, Netflix is collecting the data explicitly in the form of ratings given by user to different movies.Here the order history of a user is recorded by Amazon which is an example of implicit mode of data collection.The amount of data dictates how good the recommendations of the model can get. For example, in a movie recommendation system, the more ratings users give to movies, the better the recommendations get for other users. The type of data plays an important role in deciding the type of storage that has to be used. This type of storage could include a standard SQL database, a NoSQL database or some kind of object storage.After collecting and storing the data, we have to filter it so as to extract the relevant information required to make the final recommendations.Source: intheshortestrunThere are various algorithms that help us make the filtering process easier. In the next section, we will go through each algorithm in detail.This algorithm recommends products which are similar to the ones that a user has liked in the past. Source: MediumFor example, if a person has liked the movie Inception, then this algorithm will recommend movies that fall under the same genre. But how does the algorithm understand which genre to pick and recommend movies from?Consider the example of Netflix. They save all the information related to each user in a vector form. This vector contains the past behavior of the user, i.e. the movies liked/disliked by the user and the ratings given by them. This vector is known as the profile vector. All the information related to movies is stored in another vector called the item vector. Item vector contains the details of each movie, like genre, cast, director, etc.The content-based filtering algorithm finds the cosine of the angle between the profile vector and item vector, i.e. cosine similarity. Suppose A is the profile vector and B is the item vector, then the similarity between them can be calculated as:Based on the cosine value, which ranges between -1 to 1, the movies are arranged in descending order and one of the two below approaches is used for recommendations:Other methods that can be used to calculate the similarity are:A major drawback of this algorithm is that it is limited to recommending items that are of the same type. It will never recommend products which the user has not bought or liked in the past. So if a user has watched or liked only action movies in the past, the system will recommend only action movies. Its a very narrow way of building an engine.To improve on this type of system, we need an algorithm that can recommend items not just based on the content, but the behavior of users as well.Let us understand this with an example. If person A likes 3 movies, say Interstellar, Inception and Predestination, and person B likes Inception, Predestination and The Prestige, then they have almost similar interests. We can say with some certainty that A should like The Prestige and B should like Interstellar. The collaborative filtering algorithm uses User Behavior for recommending items.This is one of the most commonly used algorithms in the industry as it is not dependent on any additional information. There are different types of collaborating filtering techniques and we shall look at them in detail below.This algorithm first finds the similarity score between users. Based on this similarity score, it then picks out the most similar users and recommends products which these similar users have liked or bought previously.Source: MediumIn terms of our movies example from earlier, this algorithm finds the similarity between each user based on the ratings they have previously given to different movies. The prediction of an item for a user u is calculated by computing the weighted sum of the user ratings given by other users to an item i.The prediction Pu,i is given by:Here,Now, we have the ratings for users in profile vector and based on that we have to predict the ratings for other users. Following steps are followed to do so:Consider the user-movie rating matrix:Here we have a user movie rating matrix. To understand this in a more practical manner, lets find the similarity between users (A, C) and (B, C) in the above table. Common movies rated by A/[ and C are movies x2 and x4 and by B and C are movies x2, x4 and x5.The correlation between user A and C is more than the correlation between B and C. Hence users A and C have more similarity and the movies liked by user A will be recommended to user C and vice versa.This algorithm is quite time consuming as it involves calculating the similarity for each user and then calculating prediction for each similarity score. One way of handling this problem is to select only a few users (neighbors) instead of all to make predictions, i.e. instead of making predictions for all similarity values, we choose only few similarity values. There are various ways to select the neighbors:This algorithm is useful when the number of users is less. Its not effective when there are a large number of users as it will take a lot of time to compute the similarity between all user pairs. This leads us to item-item collaborative filtering, which is effective when the number of users is more than the items being recommended.In this algorithm, we compute the similarity between each pair of items.Source: MediumSo in our case we will find the similarity between each movie pair and based on that, we will recommend similar movies which are liked by the users in the past. This algorithm works similar to user-user collaborative filtering with just a little change  instead of taking the weighted sum of ratings of user-neighbors, we take the weighted sum of ratings of item-neighbors. The prediction is given by:Now we will find the similarity between items.Now, as we have the similarity between each movie and the ratings, predictions are made and based on those predictions, similar movies are recommended. Let us understand it with an example.Here the mean item rating is the average of all the ratings given to a particular item (compare it with the table we saw in user-user filtering). Instead of finding the user-user similarity as we saw earlier, we find the item-item similarity.To do this, first we need to find such users who have rated those items and based on the ratings, similarity between the items is calculated. Let us find the similarity between movies (x1, x4) and (x1, x5). Common users who have rated movies x1 and x4 are A and B while the users who have rated movies x1 and x5 are also A and B.The similarity between movie x1 and x4 is more than the similarity between movie x1 and x5. So based on these similarity values, if any user searches for movie x1, they will be recommended movie x4 and vice versa. Before going further and implementing these concepts, there is a question which we must know the answer to  what will happen if a new user or a new item is added in the dataset? It is called aCold Start. There can be two types of cold start:Visitor Cold Start means that a new user is introduced in the dataset. Since there is no history of that user, the system does not know the preferences of that user. It becomes harder to recommend products to that user. So, how can we solve this problem? One basic approach could be to apply a popularity based strategy, i.e. recommend the most popular products. These can be determined by what has been popular recently overall or regionally. Once we know the preferences of the user, recommending products will be easier.On the other hand, Product Cold Start means that a new product is launched in the market or added to the system. User action is most important to determine the value of any product. More the interaction a product receives, the easier it is for our model to recommend that product to the right user. We can make use of Content based filtering to solve this problem. The system first uses the content of the new product for recommendations and then eventually the user actions on that product.Now lets solidify our understanding of these concepts using a case study in Python. Get your machines ready because this is going to be fun!We will work on the MovieLens dataset and build a model to recommend movies to the end users. This data has been collected by the GroupLens Research Project at the University of Minnesota. The dataset can be downloaded from here. This dataset consists of:First, well import our standard libraries and read the dataset in Python. Here is a live coding window to get you started. You can run the codes and get the output in this window itself:This dataset contains attributes of 1682 movies. There are 24 columns out of which last 19 columns specify the genre of a particular movie. These are binary columns, i.e., a value of 1 denotes that the movie belongs to that genre, and 0 otherwise.The dataset has already been divided into train and test by GroupLens where the test data has 10 ratings for each user, i.e. 9,430 rows in total. We will read both these files into our Python environment.Its finally time to build our recommend engine!We will recommend movies based on user-user similarity and item-item similarity. For that, first we need to calculate the number of unique users and movies.Now, we will create a user-item matrix which can be used to calculate the similarity between users and items.Now, we will calculate the similarity. We can use the pairwise_distance function from sklearn to calculate the cosine similarity.This gives us the item-item and user-user similarity in an array form. The next step is to make predictions based on these similarities. Lets define a function to do just that.Finally, we will make predictions based on user similarity and item similarity.As it turns out, we also have a library which generates all these recommendations automatically. Let us now learn how to create a recommendation engine using turicreate in Python. To get familiar with turicreate and to install it on your machine, refer here.After installing turicreate, first lets import it and read the train and test dataset in our environment. Since we will be using turicreate, we will need to convert the dataset in SFrames.We have user behavior as well as attributes of the users and movies, so we can make content based as well as collaborative filtering algorithms. We will start with a simple popularity model and then build a collaborative filtering model.First well build a model which will recommend movies based on the most popular choices, i.e., a model where all the users receive the same recommendation(s). We will use the turicreate recommender functionpopularity_recommender for this.Various arguments which we have used are:Its prediction time! We will recommend the top 5 items for the first 5 users in our dataset.Note that the recommendations for all users are the same  1467, 1201, 1189, 1122, 814. And theyre all in the same order! This confirms that all the recommended movies have an average rating of 5, i.e. all the users who watched the movie gave it a top rating. Thus our popularity system works as expected.After building a popularity model, we will now build a collaborative filtering model. Lets train the item similarity model and make top 5 recommendations for the first 5 users.Here we can see that the recommendations (movie_id) are different for each user. So personalization exists, i.e. for different users we have a different set of recommendations.In this model, we do not have the ratings for each movie given by each user. We must find a way to predict all these missing ratings. For that, we have to find a set of features which can define how a user rates the movies. These are called latent features. We need to find a way to extract the most important latent features from the the existing features. Matrix factorization, covered in the next section, is one such technique which uses the lower dimension dense matrix and helps in extracting the important latent features.Lets understand matrix factorization with an example. Consider a user-movie ratings matrix (1-5) given by different users to different movies.Here user_id is the unique ID of different users and each movie is also assigned a unique ID. A rating of 0.0 represents that the user has not rated that particular movie (1 is the lowest rating a user can give). We want to predict these missing ratings. Using matrix factorization, we can find some latent features that can determine how a user rates a movie. We decompose the matrix into constituent parts in such a way that the product of these parts generates the original matrix.Let us assume that we have to find k latent features. So we can divide our rating matrix R(MxN) into P(MxK) and Q(NxK) such that P x QT (here QT is the transpose of Q matrix) approximates the R matrix:, where:Choosing the latent features through matrix factorization removes the noise from the data. How? Well, it removes the feature(s) which does not determine how a user rates a movie. Now to get the rating rui for a movie qik rated by a user puk across all the latent features k, we can calculate the dot product of the 2 vectors and add them to get the ratings based on all the latent features.This is how matrix factorization gives us the ratings for the movies which have not been rated by the users. But how can we add new data to our user-movie rating matrix, i.e. if a new user joins and rates a movie, how will we add this data to our pre-existing matrix?Let me make it easier for you through the matrix factorization method. If a new user joins the system, there will be no change in the diagonal feature weight matrix , as well as the item-feature relevance matrix Q. The only change will occur in the user-feature affinity matrix P. We can apply some matrix multiplication methods to do that.We have,Lets multiply with Q on both sides.Now, we haveSo,Simplifying it further, we can get the P matrix:This is the updated user-feature affinity matrix. Similarly, if a new movie is added to the system, we can follow similar steps to get the updated item-feature relevance matrix Q.Remember, we decomposed R matrix into P and Q. But how do we decide which P and Q matrix will approximate the R matrix? We can use the gradient descent algorithm for doing this. The objective here is to minimize the squared error between the actual rating and the one estimated using P and Q. The squared error is given by:Here,Our aim was to decide the p and q value in such a way that this error is minimized. We need to update the p and q values so as to get the optimized values of these matrices which will give the least error. Now we will define an updaterule for puk and qki. The update rule in gradient descent is defined by the gradient of the error to be minimized.As we now have the gradients, we can apply the update rule for puk and qki.Here  is the learning rate which decides the size of each update. The above updates can be repeated until the error is minimized. Once thats done, we get the optimal P and Q matrix which can be used to predict the ratings. Let us quickly recap how this algorithm works and then we will build the recommendation engine to predict the ratings for the unrated movies.Below is how matrix factorization works for predicting ratings:So based on each latent feature, all the missing ratings in the R matrix will be filled using the predicted rui value. Then puk and qki are updated using gradient descent and their optimal value is obtained. It can be visualized as shown below:Now that we have understood the inner workings of this algorithm, well take an example and see how a matrix is factorized into its constituents.Consider a 2 X 3 matrix, A2X3 as shown below:Here we have 2 users and their corresponding ratings for 3 movies. Now, we will decompose this matrix into sub parts, such that:The eigenvalues of AAT will give us the P matrix and the eigenvalues of ATA will give us the Q matrix.  is the square root of the eigenvalues from AAT or ATA.Calculate the eigenvalues for AAT.So, the eigenvalues of AAT are 25, 9. Similarly, we can calculate the eigenvalues of ATA. These values will be 25, 9, 0. Now we have to calculate the corresponding eigenvectors for AAT and ATA.For  = 25, we have:It can be row reduced to:A unit-length vector in the kernel of that matrix is:Similarly, for  = 9 we have:It can be row reduced to:A unit-length vector in the kernel of that matrix is:For the last eigenvector, we could find a unit vector perpendicular to q1 and q2. So,2X3 matrix is the square root of eigenvalues of AAT or ATA, i.e. 25 and 9.Finally, we can compute P2X2 by the formula pi = Aqi, or pi = 1/(Aqi). This gives:So, the decomposed form of A matrix is given by:  Since we have the P and Q matrix, we can use the gradient descent approach to get their optimized versions. Let us build our recommendation engine using matrix factorization.Let us define a function to predict the ratings given by the user to all the movies which are not rated by him/her.Now we have a function that can predict the ratings. The input for this function are:We have to convert the user item ratings to matrix form. It can be done using the pivot function in python.fillna(0) will fill all the missing ratings with 0. Now we have the R matrix. We can initialize the number of latent features, but the number of these features must be less than or equal to the number of original features.Now let us predict all the missing ratings. Lets take K=20, alpha=0.001, beta=0.01 and iterations=100.This will give us the error value corresponding to every 20th iteration and finally the complete user-movie rating matrix. The output looks like this:We have created our recommendation engine. Lets focus on how to evaluate a recommendation engine in the next section.For evaluating recommendation engines, we can use the following metrics 8.1 Recall:  8.2 Precision: 8.3 RMSE (Root Mean Squared Error):The above metrics tell us how accurate our recommendations are but they do not focus on the order of recommendations, i.e. they do not focus on which product to recommend first and what follows after that. We need some metric that also considers the order of the products recommended. So, lets look at some of the ranking metrics: 8.4 Mean Reciprocal Rank: 8.5 MAP at k (Mean Average Precision at cutoff k): 8.6 NDCG (Normalized Discounted Cumulative Gain):Up to this point we have learnt what is a recommendation engine, its different types and their workings. Both content-based filtering and collaborative filtering algorithms have their strengths and weaknesses.In some domains, generating a useful description of the content can be very difficult. A content-based filtering model will not select items if the users previous behavior does not provide evidence for this. Additional techniques have to be used so that the system can make suggestions outside the scope of what the user has already shown an interest in.A collaborative filtering model doesnt have these shortcomings. Because there is no need for a description of the items being recommended, the system can deal with any kind of information. Furthermore, it can recommend products which the user has not shown an interest in previously. But, collaborative filtering cannot provide recommendations for new items if there are no user ratings upon which to base a prediction. Even if users start rating the item, it will take some time before the item has received enough ratings in order to make accurate recommendations.A system that combines content-based filtering and collaborative filtering could potentially take advantage from both the representation of the content as well as the similarities among users. One approach to combine collaborative and content-based filtering is to make predictions based on a weighted average of the content-based recommendations and the collaborative recommendations. Various means of doing so are:Collaborative filteringContent Based Filtering:So, a hybrid recommender engine will combine these ranks and make final recommendations based on the combined rankings. The combined rank will be:The recommendations will be made based on these rankings. So, the final recommendations will look like this: B, A, D, C, E.In this way, two or more techniques can be combined to build a hybrid recommendation engine and to improve their overall recommendation accuracy and power.This was a very comprehensive article on recommendation engines. This tutorial should be good enough to get you started with this topic. We not only covered basic recommendation techniques but also saw how to implement some of the more advanced techniques available in the industry today.We also covered some key facts associated with each technique. As somebody who wants to learn how to make a recommendation engine, Id advise you to learn the techniques discussed in this tutorial and later implement them in your models.Did you find this article useful? Share your opinions / views in the comments section below!",https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/
Hiring the Right Data Scientist  The Needle in a Haystack Problem,Learn everything about Analytics|Introduction|Table of Contents|Types of scientists|Strategy of the talent pool|How to find these super scientists|The Current Process|What can be improved?|End Notes,"Context:|Coding:|Concept:|Start-ups|Enterprises|For R&D|About the Author|Share this:|Like this:|Related Articles|Comprehensive Guide to build a Recommendation Engine from scratch (in Python)|NVIDIA Unveils Amazing Open Source Machine Learning Tools Every Data Scientist Must Check Out|
Guest Blog
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common questions asked these days is what makes a good data scientist. The simple answer  it depends. The long answer  someone who can lead all the phases of a data science project. For an even longer answer, read on.A Data Science project is not just a hackathon competition where a ready-made dataset is provided and the success metric or the error to optimize is clearly laid out.Source: IBMSo whats different? Well, there are various phases in a data science project  Getting the context of the problem, understanding the data, deep diving into it, understanding implementations and coding shortcomings, figuring out the right set of algorithms to use, coding those algos, performance of those algorithms from an engineering and a data science perspective and optimization.As you can imagine, a data science skillset is a mixture of what was traditionally called computer science, and business analytics. Sometimes, given the breadth and depth of the work, you might be unlikely to find a person who knows all these aspects (let alone being good at them). Instead, its better to build a team that has a mix of people who specialize in different areas required for the data science project.In this article, we will look at what types of data scientists are there, how to find them, what the current process is and what can be further improved.Given this prelude, I am going to help us understand and categorize the existing talent pool in the market into different categories of skill sets and knowledge based on three dimensions  Context, Coding and Concept.The context in which the problem is set. Simply put, R/Python or any other open source data science tool with which the person can analyze, create features and build a model. Work with the implementation team to get the codes to a production environment.The depth of understanding the technical solution. Ability to understand the algorithm in detail. Some knowledge of literature in this area. Ability to do a lit survey and differentiate or adopt the solution to the given problem.The size of the bubble in the above chart purely measures knowledge and depth of understanding the algorithms.Given that we now understand how data science talents are, how do you, as a start up or a mid-level company or an enterprise, match the right pool to the available job? Whom do you choose and what weights do you give to each of the 3Cs at what stage of your company?Lets examine this in a bit more detail.If you are starting up or building a data science pool in your organization, chances are that the problem is not well defined and is still very blurry. The need of the hour could be breadth rather than depth. Maybe the balance could be more of geeky business analysts, data scientists and data engineers than the algo Specialists. Depending on the nature of the problem it could be a mix 30  40% Geeky Business Analysts and the rest divided between data scientists and engineersHere I would assume that the problem is well defined. There may be existing data science solutions based either on machine learning or some other technique. The need of the hour may be to upgrade the solutions and get more of the solutions into deployment mode. I would recommend this  40% of data scientists, 20% of data engineers, 20% Algorithm Specialists and another 20% of Geeky Business analysts.For organizations that wants to have a research division, the mix could shift towards algorithm specialists. They can afford to have fewer Data Scientists and Business Analysts. The idea here is that the organization aims to contribute more to research journals and wants to mark its space in certain areas or specializations.But sometimes during this search for talent, we also come across what I like to call Super Scientists. Finding asuperdata scientist is 10 times tougher than afullstackdeveloper. This is why there is no industry tag to them. There is also a fundamental mistake of evaluating data scientists only in terms of knowledge of ML or Python (or any other tool). This yardstick only effectively measures the efficiency of modeling process to model delivery and leaves the other parts to mere chance. Salary is also not a yardstick while finding these super scientists as very few companies realize their potential and hence would have given a premium to them.Before we see how to find them, lets take a look at what a super scientist is capable of doing.As you can see, all 10 steps are important. Currently, most hiring organizations evaluate data scientists only on point 4-5 in the form of an interview discussion. There too the focus ends up being too much on the knowledge and too little on the application itself. How do get your code into production? Can you streamline your pipeline to work with the existing hardware (and even software) that exists in the organization? These are critical questions I feel are not asked enough in interviews.More or less the rest of the 8 steps are left to chance. Its important we start innovating on how we test the usefulness of a person to a job than how much the person knowsCase Studies can be a key instrument in testing all 9 points. Case studies can be presented as real data science problems that would show up the job. For example, instead of interviewing on collaborative filtering, one can give a statement that we want to show or send right items to the right set of users. Then we can evaluate how the candidate arrives at a solution and how does the person think of success metrics, KPIs, etc. Create a scenario where the interviewer plays the role of a business or problem owner and see how the candidate reacts to constraints  be it data or implementations. Then deep dive into programming and algorithms.This is my humble attempt on building a data science team and how to recruit evasive super scientists . Now, time to find the needle in the haystack!If you have ever been in a hiring role, what has your experience been like? On the other hand, folks looking for a data scientist role  what are some of the challenges you have faced in your journey? Use the comments section below to let me know!Mathangi is currently building a Data Science team at PhonePe. She has 13+ years of proven track record in building world-class data sciences solutions and products. She has extensively worked on building chatbots and productizing text mining insights. She has 6 Patent grants and 20+ patents pending in the area of inuitive customer service,indoor positioning and user profiles. She is adept across machine learning , text mining NLP technologies & tools.",https://www.analyticsvidhya.com/blog/2018/06/hiring-the-right-data-scientist-the-needle-in-a-haystack-problem/
NVIDIA Unveils Amazing Open Source Machine Learning Tools Every Data Scientist Must Check Out,Learn everything about Analytics|Overview|Introduction|Our take on this,"TensorRT 4|Apex|Kubernetes on NVIDIA GPUs|NVIDIA DALI and NVIDIA nvJPEG|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Hiring the Right Data Scientist  The Needle in a Haystack Problem|Google is Using Machine Learning to Predict the Likelihood of a Patients Death  with 95% Accuracy!|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"NVIDIA has emerged as one of the leading organizations in the machine learning and deep learning space. We have previously seen some breakthrough software from them in this field  from a robot that can copy and execute human actions to an open source Python library that makes anyone an artist.And now they have announced a slew of machine learning tools at the Computer Vision and Pattern Recognition Conference (CVPR) in Utah. CVPR is an annual machine learning conference which sees the top minds in the ML and DL industry come together to discuss and present the latest tools and research to the community.These latest tools by NVIDIA include TensorRT 4, Apex, NVIDIA DALI (data loading library) and Kubernetes on NVIDIAs GPUs. We have summarized each of these four tools in this article for you!Apex is an open source PyTorch extension that helps data scientists and AI developers maximize the performance of their deep learning training process on NVIDIAs own Volta GPUs. It has been inspired by state-of-the-art techniques like sentiment analysis, translational networks, and image classification.You need to have CUDA 9, PyTorch 0.4 and Python 3 installed before using Apex. Its still in alpha mode, meaning there is a lot more to come before the first official full launch. Since its an open source release, the code is available for everyone to download on GitHub.DALI, short for Data Loading Library, aims to tackle the image recognition and computer vision field. It leverages the power of its own GPUs to decode images at a much greater speed than ever before.nvJPEG on the other hand aims to support decoding of single and batched images, color space conversion, multiple phase decoding, and hybrid decoding. These two are tied together as DALI relies heavily on nvJPEG for that acceleration in speed.DALI is an open source release and you can download the code on GitHub.You can read about each of these tools in more detail on NVIDIAs Developer Blog using the below links:First, I highly recommend everyone check out the CVPR website. As a data scientist (or an aspiring one), this conference is a gold mine for you. There are tons of updates and research papers being presented at this conference by the top minds in the machine learning field. Go through the website, check out the resources and you are sure to learn a lot of new things you did not know about before.Coming to NVIDIA, I can see Apex becoming a hit with deep learning users. Ever since PyTorch hit the open source market, it has seen a rapid adoption rate and combined with Apex, it could be a potential game changer.",https://www.analyticsvidhya.com/blog/2018/06/nvidia-unveils-amazing-open-source-machine-learning-tools-every-data-scientist-must-check-out/
Google is Using Machine Learning to Predict the Likelihood of a Patients Death  with 95% Accuracy!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIA Unveils Amazing Open Source Machine Learning Tools Every Data Scientist Must Check Out|Facebooks Machine Learning Model Manipulates Images to Open Closed Eyes!|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With the amount of data being generated in healthcare, you would think technology would be better able to predict patient outcomes. But so far it has been limited to either the research lab or classroom where small datasets are used for binary classification problems.Googles presence in the healthcare space has been steadily increasing in recent times with studies like this one on cancer detection. Their latest research, led by ateam of data scientists from Stanford, the University of Chicago, and UC San Francisco, tackles an even broader challenge  predicting the likelihood of a patients death. Their AI can even predict how long a patient is likely to be admitted for, and the chances of them being re-admitted.The AI looks at various variables of the patients health records, like gender, age, previous health history, etc. It even manages to incorporate scribbled doctor notes and PDFs into its final predictive model. The AI can then predict the probability of the patient passing away within 24 hours of him/her being admitted  and it does so with almost 95% accuracy!In a research paper published in Nature, Google tested its AI, powered by neural networks, on 216, 221 adult patients from two academic medical centres in the United States. The number of data points used were more than 46 billion (46,864,534,945 to be precise)! The results were staggeringly accurate. The researchers used the AUC-ROC statistic to measure the accuracy of their model. Below is a comparison of the tasks that were predicted by the AI versus traditional results:The logical next step is to move this AI technology into an environment where it can be more widely used  like clinics and hospitals.Only a few organizations have the resources and ability to carry out this kind of research. One of the most intriguing things about this study was that the neural networks can analyze doctors notes and inculcate the findings in its final predictions. This is unprecedented and will be welcomed by the healthcare community since it has the potential to reduce the manual time spent on paperwork.Another highlight of this study is that it eliminates the needs for data preprocessing. You must be well aware that most data science projects require a lot (up to 90%) of time to be dedicated to cleaning up data. Googles approach circumvents that almost entirely. I encourage you to read the research paper to get an idea of how Google approached this challenge and developed the final model.",https://www.analyticsvidhya.com/blog/2018/06/google-using-ai-predict-when-patient-will-die-with-95-accuracy/
Facebooks Machine Learning Model Manipulates Images to Open Closed Eyes!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google is Using Machine Learning to Predict the Likelihood of a Patients Death  with 95% Accuracy!|A Comprehensive Guide to Ensemble Learning (with Python codes)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Weve all taken not-so-great pictures and tried using various photo editing techniques to rehash them and make them presentable. With the advances in technology and hence software applications, most of these tasks are now a piece of cake.But there are still certain things like closed eyes in pictures which have eluded developers. How many times have you had to re-take pictures because someone invariably closed their eyes? Its an all to common occurrence and yet theres no easy solution for it. Sure there are apps that attempt to make things better but so far, there hasnt been a convincing solution. Emphasis on so far, because Facebooks AI research team has used machine learning to build a model that literally opens closed eyes in pictures.These researchers have worked on a technique called in- painting, which basically does the job of filling in spaces in a picture according to what it thinks belongs there. In this instance, Facebooks team used Exemplar Generative Adversarial Network (ExGAN). Basically the training data included faces of the people with the eyes open, so the ExGAN was able to learn which set of eyes belonged to which face. Additionally, the ExGAN also learned about the shape, color and other minute details about the eyes.As explained in the research paper published by the team, below is a high level overview of how a GAN works:When the model was tested and the results were shown to other people, quite a lot of them were unable to tell the difference between the original photo and the manipulated one. There were some obvious issues with the results. For example, if a lock of hair was coming over the eyes, the model struggled to understand how to replicate that and gave out some bizarre results. But it wont take long for Facebook to fix that, thats for sure.This isnt the first time in-painting has been used  Adobe uses it regularly for context-based image painting. But the face that its being used for specific eye-opening purposes definitely makes it a first in that regard.Previously deep neural networks (DNN) have been used for a similar task of replacing closed eyes but the results were not very promising. DNNs are able to learn about closed eyes but they fail to take into account the other features of the face (for instance the shape, or tone) and hence the images produced are easily distinguishable from the original photos.ExGAN on the other hand has presented really impressive results. From a machine learning perspective, this is an awesome use case.",https://www.analyticsvidhya.com/blog/2018/06/facebook-presents-ai-system-that-can-open-closed-eyes-in-pictures/
A Comprehensive Guide to Ensemble Learning (with Python codes),"Learn everything about Analytics|Introduction|Table of Contents|1. Introduction to Ensemble Learning
|2. Simple Ensemble Techniques|3. Advanced Ensemble techniques|4. Algorithms based on Bagging and Boosting|End Notes","2.1 Max Voting|2.2 Averaging|2.3 Weighted Average|3.1 Stacking|3.2 Blending|3.3 Bagging|3.4 Boosting|4.1 Bagging meta-estimator|4.2 Random Forest|4.3 AdaBoost|4.4 Gradient Boosting (GBM)
|4.5 XGBoost|4.6 Light GBM
|4.7 CatBoost
|Learn, train,compete, hackandget hired!|Share this:|Like this:|Related Articles|Facebooks Machine Learning Model Manipulates Images to Open Closed Eyes!|DeepMinds Computer Vision Algorithm Brings the Power of Imagination to Build 3D Scenes from 2D Images|
Aishwarya Singh
|55 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you want to purchase a new car, will you walk up to the first car shop and purchase one based on the advice of the dealer? Its highly unlikely.You would likely browser a few web portals where people have posted their reviews and compare different car models, checking for their features and prices. You will also probably ask your friends and colleagues for their opinion. In short, you wouldnt directly reach a conclusion, but will instead make a decision considering the opinions of other people as well.Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this article.The objective of this article is to introduce the concept of ensemble learning and understand the algorithms which use this technique. To cement your understanding of this diverse topic, we will explain the advanced algorithms in Python using a hands-on case study on a real-life problem.Note:This article assumes a basic understanding of Machine Learning algorithms. I would recommend going through this articleto familiarizeyourself with these concepts.Lets understand the concept of ensemble learning with an example. Suppose you are a movie director and you have created a short movie on a very important and interesting topic. Now, you want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which you can do that?A: You may ask one of your friends to rate the movie for you.
Now its entirely possible that the person you have chosen loves you very much and doesnt want to break your heart by providing a 1-star rating to the horrible work you have created.B: Another way could be by asking 5 colleagues of yours to rate the movie.
This should provide a better idea of the movie. This method may provide honest ratings for your movie. But a problem still exists. These 5 people may not be Subject Matter Experts on the topic of your movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.C: How about asking 50 people to rate the movie?
Some of which can be your friends, some of them can be your colleagues and some may even be total strangers.The responses, in this case, would be more generalized and diversified since now you have people with different sets of skills. And as it turns out  this is a better approach to get honest ratings than the previous cases we saw.With these examples, you can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called Ensemble Learning.Now that you have got a gist of what ensemble learning is  let us look at the various techniques in ensemble learning along with their implementations.
In this section, we will look at a few simple but powerful techniques, namely:The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a vote. The predictions which we get from the majority of the models are used as the final prediction.For example, when you asked 5 of your colleagues to rate your movie (out of 5); well assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.The result of max voting would be something like this:Sample Code:Here x_train consists of independent variables in training data, y_train is the target variable for training data. The validation set is x_test (independent variables) and y_test (target variable) .Alternatively, you can use VotingClassifier module in sklearn as follows:Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.For example, in the below case, the averaging method would take the average of all the values.i.e. (5+4+5+4+4)/5 = 4.4Sample Code:This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41.Sample Code:Now that we have covered the basic ensemble techniques, lets move on to understanding the advanced techniques.Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. Below is a step-wise explanation for a simple stacked ensemble:Sample code:We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model.Now well create two base models  decision tree and knn.Create a third model, logistic regression, on the predictions of the decision tree and knn models.In order to simplify the above explanation, the stacking model we have created has only two levels. The decision tree and knn models are built at level zero, while a logistic regression model is built at level one. Feel free to create multiple levels in a stacking model.
Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process:Sample Code:Well build two models, decision tree and knn, on the train set in order to make predictions on the validation set.Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set.The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Heres a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is bootstrapping.Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.Before we go further, heres another question for you: If a data point is incorrectly predicted by the first model, and then the next (probably all models), will combining the predictions provide better results? Such situations are taken care of by boosting.Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Lets understand the way boosting works in the below steps.Bagging and Boosting are two of the most commonly used techniques in machine learning. In this section, we will look at them in detail. Following are the algorithms we will be focusing on:Bagging algorithms:Boosting algorithms:For all the algorithms discussed in this section, we will follow this procedure:For this article, I have used the Loan Prediction Problem. You can download the dataset from here. Please note that a few code lines (reading the data, splitting into train-test sets, etc.) will be the same for each algorithm. In order to avoid repetition, I have written the code for the same below, and further discussed only the code for the algorithm.Similarly, fill values for all the columns. EDA, missing values and outlier treatment has been skipped for the purposes of this article. To understand these topics, you can go through this article: Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas.Lets jump into the bagging and boosting algorithms!Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:Code:Sample code for regression problem:Parameters used in the algorithms:Random Forest is another ensemble machine learning algorithm that follows the bagging technique.It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.Looking at it step-by-step, this is what a random forest model does:Note: The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) . Code:Parameters Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.Below are the steps for performing the AdaBoost algorithm:Code:Sample code for regression problem:ParametersGradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:Code:Sample code for regression problem:
ParametersXGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known asregularized boosting technique.Let us see how XGBoost is comparatively better than other techniques:Code:Since XGBoost takes care of the missing values itself, you do not have to impute the missing values. You can skip the step for missing value imputation from the code mentioned above. Follow the remaining steps as always and then apply xgboost as below.Sample code for regression problem:ParametersBefore discussing how Light GBM works, lets first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way.Leaf-wise grwth may cause over-fitting on smaller datasets but that can be avoided by using the max_depth parameter for learning. You can read more about Light GBM and its comparison with XGB in this article.Code:Sample code for regression problem:
ParametersHandling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms. Here is an article that explains CatBoost in detail.Code:CatBoost algorithm effectively deals with categorical variables. Thus, you should not perform one-hot encoding for categorical variables. Just load the files, impute missing values, and youre good to go.Sample code for regression problem:
ParametersThis brings us to the end of the ensemble algorithms section. We have covered quite a lot in this article!Ensemble modeling can exponentially boost the performance of your model and can sometimes be the deciding factor between first place and second! In this article, we covered various ensemble learning techniques and saw how these techniques are applied in machine learning algorithms. Further, we implemented the algorithms on our loan prediction dataset.This article will have given you a solid understanding of this topic. If you have any suggestions or questions, do share in the comment section below. Also, I encourage you to implement these algorithms at your end and share your results with us!",https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/
DeepMinds Computer Vision Algorithm Brings the Power of Imagination to Build 3D Scenes from 2D Images,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Comprehensive Guide to Ensemble Learning (with Python codes)|MIT Researchers Built a Neural Network that can See Through Walls!|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As the effort to build more and more complex models and machines continues, it is becoming increasingly expensive to find not only the resources to build such machines, but to collect, annotate and label the training data. Because as you know, without having properly labeled data, the model might as well not exist!Often to train these complex models, we had to manually tag and annotate the images to be used by the algorithm. A group of researchers claim to have found a solution to improvise this time taking process. Googles DeepMind research team has created an algorithm that can create 3D objects based on a few 2D snapshots. In other words, the AI algorithm is able to use the 2D images to understand or imagine how the object looks like from various angles (which are not seen in the image).The team of researchers have published a research paper explaining how the algorithm works. The system, called Generative Query Network or GQN, can think and imagine like humans. It can render 3D objects without having to be trained on what the various angles of the object look like.As mentioned by the team, the AI system has two parts:The GQN has the ability to learn about the shape, size and color of the object independently and can then combine all these features to form an accurate 3D model. Furthermore, the researchers were able to use this algorithm to develop new scenes without having to explicitly train the system as to what object should go where.The tests were conducted in a virtual room and the results have been shared by the team in the above mentioned research paper. The algorithm has been tested only on objects so far and is not yet developed enough to work with human faces.You can read about this algorithm in more detail on DeepMinds blog post.Another computer vision algorithm! Annotating and labeling data is a major time staking and often expensive exercise so this is another welcome addition. While still in its nascent stages (DeepMind admit as much in the blog), the potential applications are HUGE.GQN is not limited to tagging and annotating images, it can further be used by autonomous robots to have a better understanding of their surroundings.What else can you think of where we can apply this algorithm? Let me know your thoughts on this in the comments section below!",https://www.analyticsvidhya.com/blog/2018/06/google-ai-create-3d-objects-using-2d-snapshots/
MIT Researchers Built a Neural Network that can See Through Walls!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DeepMinds Computer Vision Algorithm Brings the Power of Imagination to Build 3D Scenes from 2D Images|Data Scientists built a Random Forest Model to predict the World Cup 2018 Winner|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"X-ray vision has long been limited to science fiction movies and books. We are used to seeing characters break through walls and buildings with just a gaze but could you ever have imagined that turning into reality?A group of researchers from MITs Computer Vision and Artificial Intelligence Lab have found a breakthrough to bring this new reality to life. Their project, called RF-Pose, uses deep learning to see through walls and estimate the posture and movement of people.The AI uses a neural network to train itself and make the required estimations. The neural network has been developed in such a way that it can sense and analyze radio signals which come from a persons body. Then, it creates a digital stick figure (as you can see in the image above) which shows where the person is, and what pose he/she is in (standing, sitting, moving around).Most neural networks require tons of data to be trained properly (supervised learning methods). his was a significant challenge as labeled training data in this study was hard to find. So the researchers had to develop and collect their own data, which they did through wireless devices and a camera. They put together a database of thousands of images which had people doing things like walking, running, standing, sitting, doing a random activity, among other things.The next step involved extracting stick figures from the camera images which were shown to the neural network, along with the radio signal that corresponded to them. After the training process, the neural network was capable enough to estimate the pose and movement without requiring explicit use of the camera.But the uses of this technology dont end there. It can even identify people in a line-up with very decent accuracy. When the researchers tested the system on 100 individuals, it was able to identify 83 out of them correctly. The team will present their research paper (link below) at the Conference on Computer Vision and Pattern Recognition in Utah later this month.I have listed a couple of resources below which provide an in depth understanding of this AI:Watch the below video to get a sense of how the AI works in real time:I can imagine this technology being extremely helpful in the healthcare industry. As the researchers mentioned, it can monitor diseases related to motor functions, allow elderly people to live with a bit more freedom since they can be monitor for falls and injuries. It can also be used in video games (what is it about deep learning and video games?) to estimate movement, and also in search and rescue operations to identify specific people.I have been covering computer vision a lot lately under AVBytes and continue to be amazed at the amount of progress we have made in this field. This field is ripe for data scientists since there is so much research going on. I encourage you to read up on this technology and think of ways how you can implement such a thing if given the chance.",https://www.analyticsvidhya.com/blog/2018/06/mit-researchers-built-neural-network-that-can-see-through-walls/
Data Scientists built a Random Forest Model to predict the World Cup 2018 Winner,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|MIT Researchers Built a Neural Network that can See Through Walls!|Dont miss out on these awesome GitHub Repositories & Reddit Threads for Data Science & Machine Learning (May 2018)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The World Cup begins tomorrow and predictions are in full swing. We have seen Paul the Octopus weaving his magic to predict the 2010 winner (Spain), and we otherwise look at bookmakers odds to gauge who the favorite is. But now that we have the power of machine learning, it wasnt long before data scientists put it to good use.Led by Andreas Groll from the University of Dortmund, the researchers used a random forest model to predict every single game at this years World Cup. Their model is predicting Germany to go all the way. The below chart shows the win probabilities of each match in the knockout stages.The researchers repeatedly simulated the World Cup 100,000 times to arrive at these numbers! Data from the 2002 World Cup through to 2014 was used to train the model. Groll and his team initially used a range of variables to start off the model building phase, including economic factors like the countrys GDP. Other variables included FIFAs ranking, average age of the squad, how many Champions League players are there, is there any home advantage, etc.Interestingly, the final model gave some fascinating insights. As you might be aware, the Random Forest algorithm also has the feature importance functionality up its sleeves. Check out the below bar plot which ranks the features:The highest importance was given to the abilities of individual players, followed by their rank on FIFAs list. Other moderately important variables include the average age of the squad, how many players play in the Champions League and the GDP of the country. Factors like the nationality of the coach and the population of the country turned out to essentially useless.One of the more intriguing aspects of the model was that overall, it predicted Spain to have the likeliest chances of winning. But if Germany (who potentially face stronger opponents in the knockout stage) reach the quarter-finals, they have a higher chance of winning.You can read the full paper describing this research here.Im a huge sports analytics buff so reading the entire research paper was like a goldmine to me. The workings of the model these guys have built is fairly easy to understand and follow. Having said that, sports is a very unpredictable field and anything is possible on the day.Of course, this is not the only machine learning effort to predict the winners. Goldman Sachs have also used a similar approach (though their report doesnt delve into the ML side too much). Their model has predicted a Germany vs. Brazil final with the Samba nation taking the crown.Who are you predicting will lift the trophy this year?",https://www.analyticsvidhya.com/blog/2018/06/data-scientists-used-a-random-forest-model-to-predict-the-world-cup-2018-winner/
Dont miss out on these awesome GitHub Repositories & Reddit Threads for Data Science & Machine Learning (May 2018),Learn everything about Analytics|Introduction|GitHub Repositories|ML.NET|NLP Architect|Amazon Scraper|PIGO  Face Detection in Go|RL-Adventure-2: Policy Gradients|Reddit Discussions|Real-time Multihand Pose Estimation Demo|Which Research paper would you choose to show that Machine Learning is Beautiful?|What do we currently know about Generalization? What should we be asking next about it?|State of Machine Learning in the Healthcare Industry|Potential Career Paths for Data Scientists after 3 Years|End Notes,"Share this:|Like this:|Related Articles|Data Scientists built a Random Forest Model to predict the World Cup 2018 Winner|TensorFlow, GANs and AWS were Combined to Make a Movie and the Results are Stunning|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GitHub and Reddit both serve as interesting discovery platforms for me. I not only learn some of the best applications of data science, but also see how they have been written and will hopefully be contributing to some of these repositories in the near future.GitHub was acquired by Microsoft recently in a multi-billion dollar deal. GitHub has been the ultimate platform for collaboration between developers and we have seen the data science and machine learning community embrace it with equal enthusiasm. We hope this continues under Microsofts umbrella as well.As for Reddit, it continues to be a wonderful source of knowledge and opinion for data scientists. People share links to their code, other peoples codes, general data science news, ask for help and opinions, post research papers, among other things. Its a truly powerful community that continues to provide a solid platform for interacting with fellow data science enthusiasts.We saw a few great Reddit discussions in May, including the role of data scientists in the next 3 years and a collection of the best ML papers ever written. In the GitHub community, Intel open sourced its NLP architect library, Microsoft unveiled ML.NET to enable machine learning for Dot Net developers, etc.Lets dive into the list and look at the top repositories on GitHub and intriguing discussions on Reddit that occurred last month.You can check out the top GitHub repositories and top Reddit discussions (from April onwards) for the last four months below:                                             Source: MSPowerUserML.NET is an open source machine learning framework which aims to make ML accessible to, you guessed it, .NET developers. It enables them to develop their own models in .NET, all without requiring prior experience in building machine learning models. This is currently a preview release and includes basic classification and regression algorithms.ML.NET was originally created by Microsoft and has been used across its wide range of products, like Windows, Excel, Access, Bing, etc. This release also comes bundled with .NET APIs for various model training model tasks.NLP Architect is an open source Python library that enables data scientists to explore state-of-the-art deep learning techniques in the field of natural language processing (NLP) and natural language understandings (NLU). It has been developed and open sourced by researchers at Intel Lab.One of my favorite components of this library is a visualization component that shows the models annotations in a tidy and neat fashion. Check out our coverage of NLP Architect here.This one is for all the reinforcement learning (RL) enthusiasts. Deep learning has propelled RL to program an AI to play Atari games at human expert level skill. This repository covers interesting new extensions to the policy gradient algorithm, one of the favorite default choices for solving RL problems. These extensions have led to an improvement in training time as well as the overall performance of reinforcement learning.This thread took off as soon as the author posted the above concept in video form. It is a fascinating concept and to see it come alive using deep learning is a wonderful thing. It caught the attention of data scientists and ML enthusiasts as you can tell by the amount of questions in the thread. I encourage you to scroll through them all and you will get a very good idea of how this technology was implemented.If youre new to machine learning, or are looking for papers to read or refer, this is magnificent thread. There are some excellent machine learning research papers mentioned in this thread which every data scientist, aspiring or established, will hugely benefit from. The thread contains papers ranging from basic machine learning concepts like Gaussian models, to advanced concepts like neural artistic style transfer, rapid object detection using a boosted cascade of simple features, etc. This is essentially a MUST-READ.Generalization in deep learning has been a topic of constant debate. As the author of this post mentioned, there are still quite a few scenarios where we struggle to achieve any generalization at all. This led to a deep discussion around the current state of generalization and why it has been so hard to understand in deep and reinforcement learning. This discussions includes lengthy posts which can get a little complex if youre new to this field. However I suggest reading through them anyway because these are opinions of some highly experienced and knowledgeable data scientists.This thread delves into the current state of machine learning specifically in the healthcare industry (and not in research areas). Data scientists in this industry have shared their experience and opinions about what they have seen in their work. Refer to this thread whenever anyone asks you anything about ML and DL in the life sciences domain!This is a very pertinent question most people ask before getting into this field. With the rapid adoption of automated machine learning tools, will companies even need data scientists in a few years? This thread is a collection of opinions about where different people in data science see this role expanding or diversifying in the next few years. There is some excellent career advice in here so make sure you check this out.Some of the above Reddit discussions were really insightful, like the healthcare industry and the generalization thread. I personally love curating these GitHub repositories and Reddit discussions threads because it gives me a high level overview of all thats happening in the ML research community.Which repository and/or discissions did you find the most interesting out of these? Get involved and let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/06/top-5-github-reddit-data-science-machine-learning-may-2018/
"TensorFlow, GANs and AWS were Combined to Make a Movie and the Results are Stunning",Learn everything about Analytics|Overview|Introduction|Our take on this,"How the technology works|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Dont miss out on these awesome GitHub Repositories & Reddit Threads for Data Science & Machine Learning (May 2018)|IBM and H2O.ai have Combined Driverless AI and Power9 to Speed Up Machine Learning Tasks|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What happens when you apply artificial intelligence in the movie making business? The AI develops a creative streak of its own and delivers a virtuoso movie.The latest effort to generate a movie using AI is by Oscar Sharp (director) and Ross Goodwin (AI researcher) for the Sci-Fi London 48-Hour Challenge. As the name suggests, the movie had to be produced inside 48 hours and adhere to specific guidelines. But what makes this effort unique is that the ENTIRE production effort was done by the AI.Given the short turnaround time, there are awkward scenes (like a mustache appearing on the womans face, blurry facial expressions) and clunky and gibberish dialogue at times. But the overall effect is sublime.Without further ado, below is the movie in full:The AI put the film together using thousands of hours of old movie footage. The film stars Silicon Valley actor Thomas Middleditch, who the AI portrayed in the movie. Using facial recognition, image processing and voice-generating technologies, the AI handled the entire movie making process.As described in this article, the AI is built using a LSTM recurrent neural network on Amazon Web Services (AWS). The movies that were fed to the neural network were mostly from the 1980s and 90s. The AI, called Benjamin (it named itself!), learned to predict which letters and words followed each other. After a period of time, the AI managed to structure together a screenplay including the dialogues.Face swapping is also prevalent throughout the movie. This technology has really come into the spotlight ever since a flurry of fake videos were published online last year. 11 GANs (Generative Adversarial Networks) were used to assist with the face-swapping feature. TensorFlow was also leveraged throughout the AI making process.Ah, the beauty of combining creativity with artificial intelligence. While the end results are not perfect, they are a microcosm of what can be achieved if this technology was applied on a more diverse dataset and with far more time than just 48 hours.The flip side of this is whether AI will force unemployment in the movie making business. Will we even need actors anymore if the AI can just cobble together a movie from old footage? Time will tell, but this is definitely something to ponder upon.",https://www.analyticsvidhya.com/blog/2018/06/tensorflow-gans-aws-combined-movie-results-stunning/
IBM and H2O.ai have Combined Driverless AI and Power9 to Speed Up Machine Learning Tasks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|TensorFlow, GANs and AWS were Combined to Make a Movie and the Results are Stunning|Launching DataHack Radio  Analytics Vidhyas Exclusive Podcast Series!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"IBMs reach and power is a well known thing in the machine learning community. To enhance their reach into organizations and the wider ML community, they are now partnering up with H2O.ai.In case you havent heard of H2O.ai, they are the developers of the ultra-popular Driverless AI platform. It is one of the best platforms for automated machine learning and has seen a rapid adoption rate since its launch. You dont need to be a hardcore data scientist to enable your organization with ML skills anymore, tools like Driverless AI do the heavy lifting for you.Under this partnership, IBMs Power AI framework will work together with Driverless AI. This combined data science platform will aim to address a wide variety of real-life use cases for machine learning and deep learning in diverse industries. Power AI is basically an enterprise package of open source deep learning frameworks like PyTorch, Keras, TensorFlow, etc. It makes working with these packages easier and more efficient and reduces the training time significantly.The combined platform has so far delivered incredible results. Driverless AI was run on Power9, IBMs incredible fast and powerful processor. According to a blog post by H2O.ai, Driverless AI is built on top of datatable for python for data ingest and feature engineering and H2O4GPU for machine learning. With Power9, the team got almost 2 times the usual speed for data ingesting. The results were 50% faster for automated feature engineering.The below table shows us how quickly analysis was performed for various tasks, using this platform:This is a truly powerful collaboration between 2 industry leaders. I have previously used Driverless AI and can vouch for its incredible accuracy and ease of use. It doesnt require coding, or a deep knowledge of data science (though obviously having that helps). Combining that with the ultra-fast Power9 will enable organizations to scale up their data science departments.Both companies claim that it can be used in various industries which should see a rapid adoption rate once its publicly available.",https://www.analyticsvidhya.com/blog/2018/06/ibm-h2oai-combined-driverless-ai-power9-speed-up-machine-learning-tasks/
Launching DataHack Radio  Analytics Vidhyas Exclusive Podcast Series!,Learn everything about Analytics|Introduction|Sounds good! Where can I subscribe?|Why are we launching DataHack Radio and what can you expect?,"Share this:|Like this:|Related Articles|IBM and H2O.ai have Combined Driverless AI and Power9 to Speed Up Machine Learning Tasks|DataHack Radio Episode #2  Exploring Deep Learning, Open Source Research and More with Tarry Singh|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I am incredibly proud to announce the launch of DataHack Radio! This is Analytics Vidhyas exclusive podcast series which will feature top leaders and practitioners in the data science and machine learning industry.Machine Learning is a vast and diverse field. Keeping up with all the developments and happenings can often feel daunting. This is why podcasts are a great way of catching up with all thats going on in the machine learning world. Listen to them when youre travelling, working on something else, or before you sleep! They save you tons of time and give you invaluable knowledge.At Analytics Vidhya, we saw there was a need in this area and out of this was born DataHack Radio!",https://www.analyticsvidhya.com/blog/2018/06/launching-datahack-radio-analytics-vidhyas-exclusive-podcast-series/
"DataHack Radio Episode #2  Exploring Deep Learning, Open Source Research and More with Tarry Singh",Learn everything about Analytics|Introduction|End Notes,"Share this:|Like this:|Related Articles|Launching DataHack Radio  Analytics Vidhyas Exclusive Podcast Series!|DataHack Radio Episode #1  The World of Machine Learning Competitions with Kaggle CEO Anthony Goldbloom!|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tarry Singh is an AI neuroscience researcher and one of the more popular social media influencers in the data science and machine learning community. He has over 17 years of experience working with data and has advised CXOs of global organizations to setup data-driven organizations from scratch. He speaks regularly at global AI leadership summits worldwide and conducts workshops on a regular basis.In the second episode of DataHack Radio, Kunal chats with him about his background, how he developed an interest in deep learning, his social media popularity, his deep learning research, among various other things.Click the above link to listen to the full podcast! The podcast is also available on our Soundcloud page. Happy listening!Below are the key highlights from the podcast. Happy listening!Tarrys backgroundAfter studying physics and astronomy, Tarry got into the neuroscience field about 26 years ago. In the late 90s he migrated to the Netherlands and eventually got involved in data transformation jobs (managing large data warehouses with Oracle) and other stuff like understanding how data behaves and acts in the infrastructure space.Teaching at the university in Netherlands taught him how to learn neuroscience and also give back to the industry in equal measure at the same time.His Foray into Deep Learning and DeepkaphaInterestingly enough, Tarry had a chance in the 90s to get into this field but decided the time wasnt right. 10 years later, when things were picking up in this domain, the field caught Tarrys interest. A couple of years later, he took his first steps in deep learning and here we are!Deepkapha is Tarrys start-up (started in January this year) which he has grand ambitions from. The company consists of 8 people currently. More on this in the next couple of sections. How do CEOs and C-Suite People React to New Ideas/ServicesKunal mentioned a good point about CEOs taking a high risk high reward strategy when it comes to incorporating new ideas and services. Tarry concurred, saying he has seen all the executives having an openness but not a willingness to go down this path. He expects this to change in the coming months and years otherwise companies risk falling behind the curve.Staying up to date with the Latest ResearchTarrys start-up has its own beta research arm, called Deep Remote Residency Program (DeepRRP). The program pulls in people with a strong ethic, both work and principle wise. Currently the program has 4 residents with over 150 applications pending to be reviewed.The topics are diverse and there is no limit to what can be explored  from US policies to the automotive industry, all ideas are welcomed with equal enthusiasm.Top Tips for CEOs looking to open their AI LabsOpen Sourcing ML and AITarry is a strong believer of open source research. He cited examples of TensorFlow and how various algorithms and technologies are saving lives in healthcare. Its changing the field in ML and makes it a much more involved and developed community.How do you manage Social Media with your day-to-day role?Its almost like living and breathing.Ensuring that his followers questions are answered is one of the most important things for Tarry, in terms of social media. It is important to give back to the community and social media is one way of doing that.About being an Instructor and writing a bookIt all started with a Quora post! Tarry answered a general data science question with his typical passionate self. He got a great response for that post and he started dabbling into some teaching assignment. This led to an invitation from Coursera to be a mentor for their Deep Learning specialization track.He is also writing a book about deep learning. Its going slower than he had anticipated but he expects to be done with it in the coming months. We are looking forward to it!This episode is a great addition to the DataHack Radio series. Tarrys passion for his work and his willingness and commitment to give back to the community is unparalleled.There are a lot more words of wisdom from Tarry in the full podcast. Get listening!",https://www.analyticsvidhya.com/blog/2018/06/datahack-radio-deep-learning-open-source-research-tarry-singh/
DataHack Radio Episode #1  The World of Machine Learning Competitions with Kaggle CEO Anthony Goldbloom!,Learn everything about Analytics|Introduction|End Notes,"Share this:|Like this:|Related Articles|DataHack Radio Episode #2  Exploring Deep Learning, Open Source Research and More with Tarry Singh|Google Open Sources Approach to Visualize Large and High Dimensional Datasets using tSNE|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are incredibly excited to announce the launch of DataHack Radio! This is Analytics Vidhyas exclusive podcast series which will feature Kunal Jain in conversation with top leaders and practitioners in the data science and machine learning industry.In every episode of DataHack Radio, we will bring you discussions with one such thought leader in the community. We will have discussions about their journey, their learnings and plenty of other data science related things. We are sure this will be a lot of fun and full of learning for each of you.Click the below link to listen to the full podcast! The podcast is also available on our Soundcloud page. Happy listening!In the first ever episode of the DataHack Radio podcast, we host Kaggles co-founder and CEO Anthony Goldbloom. He has been featured in Forbes Top 30 under 30 list and has been mentioned in the worlds top journals like the New York Times, Wall Street Journal, among others.Kunal asked him a variety of questions covering diverse fields  from Kaggles history to their future plans, how competitions are planned, current and future trends in machine learning, and much, much more!Below are the key points that were discussed in the podcast. Enjoy!Anthonys Background and how Kaggle was FoundedAnthony was trained as a statistician or econometrician at the Melbourne University in Australia. His first job out of college was at the Australian treasury forecasting GDP, inflation, unemployment, etc. In 2008, he got an internship at the Economist magazine in London.Kaggle started out as competitions (of course, they do more than that now). They used to take problems from companies and put them up on their website to enable high powered solutions from statisticians and data scientists.Kaggles Vision and FutureWe want Kaggle to be the first place data scientists look at when they wake up and the last thing they work on before shutting off their system.Anthony and his team have experimented with different models and approaches since Kaggle was founded. They ultimately want to be the place where data scientists and ML practitioners do all of their work, from projects to learning. The team plans to leverage Kaggle kernels to do this. We will see the addition of TPUs to kernels (hopefully) in the second half of this year to accelerate model processing.For datasets, they are working towards making it a one stop shop for all kinds of datasets. You wont have to Google for specific datasets, head over to Kaggle and find it there.And finally, Kaggle Learn. It has already seen a great response from the community and Kaggles team are working to make it more intuitive by adding more topics.Competitions are about a 1/3rd of the activity on Kaggle. The other three areas are slowly growing and maturing well.Kaggle also has private kernels and private datasets. If you want to work and collaborate within teams without having to publicly expose your work or data, these two features are invaluable additions.They have a lot of organizational accounts but no way to administer them yet. This is something the team might look at in the future. Currently, there are a few other things they dont have on the platform that they expect to add in the foreseeable future.At this point, Kunal asked Anthony if Kaggle had any plans for making a GUI based interface for making things easier for folks who arent so good at programming. As it turns out, Anthony has no such plans for now. Kaggle is focused on people who are good with their platform and there are plenty of growth opportunities for them.Kaggles CompetitionsPost acquisition from Google, Kaggle has been swamped with requests for hosting competitions. So how do they deal with these requests and pick and choose competitions?Turns out they dont pick and choose specific problems. If the problem is well specified, the data is large enough and its a supervised learning problem, they accept it. The only roadblock is the number of requests. The pipeline/queue is getting longer so they are hiring folks to deal with that.Couple of things we might see in the next 12-18 months  Self serving competitions for researchers and internal company employees. The idea is for these people to set the competitions up themselves for internal purposes so they dont rely on Kaggles team to do that.We will see a lot more competitions that will require mandatory Kaggle kernels solutions. Topics like constraint and compute, deep learning and reinforcement learning will be the major focus for these.How do Kaggle competitions relate to the industry?One of the most pertinent questions is how do the solutions generated through Kaggle competitions get used in real-life industry situations?To prevent individuals and teams from making overly complex models (like ensembling 5 different models), competitions are usually limited to a maximum of 3 months. Also, team merging is not allowed after a certain point so you cant merge two completely different approaches at the last minute. Its a subtle point but extremely powerful.When winners present their results to the customer, Kaggle asks them to present a full version of their solution along with a simplified version. This helps the client understand what is going on behind the scenes.Kaggle LearnMost of the data scientists in the next 10 years are not data scientists today.Anthony emphasized on why Kaggle has made inroads into the learning phase of a data scientists journey. With Kaggle Learn and the various webinars they have hosted recently, they want aspiring data scientists to learn from Kaggle and stay within the Kaggle ecosystem to apply what they have learnt.Anthonys Views on Open Source Data ScienceAnthony believes Kaggle has helped various algorithms and packages gain massive adoption in the wider data science community. XGBoost is the perfect example to illustrate this point. It was pioneered on Kaggle and took off when people moved from using Random Forest to XGBoost to win competitions.As far as companies using open datasets is concerned, Kaggle is exploring options that enable companies to join existing open source datasets to their own private ones. What is the future of Data Science and AI?Anthony believes that companies outside the big giants like Google will have resources to enable them to get their codes and models into production.The power of AI will eventually lead to a lot of unemployment (accounting, auditing, etc. Basically jobs that are repetitive). The technology is already there, but the products required to enable it are not yet available. Once they are, we should start seeing a shift in the industry.Rapid Fire Round!KJ: What is one competition you wanted to host but havent been able to yet?AG: Im passionate about weather. I would love to host a weather forecasting competition.KJ: Most satisfying moment in the Kaggle journey?AG: There are two that come to mind. First, weve made machine learning more effective by helping spread which techniques work and which ones dont. Second, its impossible to overfit in Kaggles competitions because they divide the test dataset into public and private.KJ: What is one thing you would want to change in the Kaggle journey?AG: Probably the name of the company (listen to the podcast to find out why!).It was a fascinating talk and we learned a lot about Kaggle, the thought and process that drives the popular platform and what plans Anthony has for its future. I highly encourage you to listen to the entire conversation in this podcast!Did you like this exclusive podcast with Anthony Goldbloom? Leave your suggestions and feedback in the comments below!",https://www.analyticsvidhya.com/blog/2018/06/datahack-radio-1-machine-learning-competitions-with-kaggle-ceo-anthony-goldbloom/
Google Open Sources Approach to Visualize Large and High Dimensional Datasets using tSNE,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DataHack Radio Episode #1  The World of Machine Learning Competitions with Kaggle CEO Anthony Goldbloom!|Eye in the Sky is a Machine Learning Project that Detects Violent People in Crowds|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dealing with high dimensionality data has always been a major challenge for data scientists. You are expected to mine for hidden features by exploring the vast terrain of the (mostly) unknown dataset, which contains hundreds, if not thousands, of variables. How in the world do you even begin?To deal with this, the tSNE algorithm was created. Sure you still have the likes of PCA you can leverage but when you have a MASSIVE dataset and want to search for a pattern in non-linear style (a more advanced method), tSNE has proven to be more effective. It maps multi-dimensional datasets on to two or more dimensions. It also reduces the number of plots you need to generate when dealing with a huge number of dimensions.If youre new to tSNE, or need a refresher, check out this guide that comprehensively explains this wonderful concept.But one of the limitations with tSNE has been that it is computationally very complex. Even though it is meant for large datasets, this restriction has limited it to relatively small datasets. So an intern at Google decided to pioneer a novel approach to maximize the use of tSNE. His method relives heavily on modern graphics hardware.This new approach generates embeddings way faster than any other technique in this space. But where it really stands out is that it can be executed in a web browser! How, you ask? This is where TensorFlow.js comes to the fore. Googles method leverages GPU capabilities through WebGL, a JavaScript API that is used for rendering 2D and 3D graphics within any browser. Check out our coverage of TensorFlow.js here.The team, led by the intern, then tested their approach on the popular MNIST handwritten images dataset. What used to take 15 minutes to calculate can now be done in real-time and in the web browser itself. Talk about breakthroughs!The above image gives an overview of this approach. I have summarized it below, taking an excerpt from their research paper:I have listed a few resources below I encourage you to check out in order to get an in-depth understanding of this approach:If youve ever dealt with high dimensional datasets, you might already be on your way to using this approach! This tSNE approach could be a real game changer for organizations that deal with huge datasets and dont have a lot of time on their hands.One of the (few) limitations currently is its ability to only generate 2D visualizations (doesnt work with 3D yet). But I dont see this as being a problem since most of the visualizations data scientists work with is in the 2D space (unless you work with geospatial data).Google Open Sources has become such an oft-used phrase these days. I love that they consistently open up their research results to benefit the overall ML community. Let me know what you think of this technique in the comments section below.",https://www.analyticsvidhya.com/blog/2018/06/google-open-sources-approach-to-visualize-large-and-high-dimensional-datasets-using-tsne-a-must-know-for-data-scientists/
Eye in the Sky is a Machine Learning Project that Detects Violent People in Crowds,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Open Sources Approach to Visualize Large and High Dimensional Datasets using tSNE|Google has Released an Updated Version of its Open Source YouTube Dataset|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Human pose estimation is a trending topic in machine learning these days. Currently most data scientists work on the basic side of this field, where they attempt to look at a few people to gauge their poses. But a group of researchers have now come up with a wonderful real-life use case for this  a drone that identifies violent individuals in crowds.This project has been developed by researchers from the University of Cambridge, Indias National Institute of Technology and the Indian Institute of Science. They have even released a research paper, called Eye in the Sky: Real-Time Drone Surveillance System (DSS) for Violent Individuals Identification using ScatterNet Hybrid Deep Learning Network, that details the deep learning framework that went into making this system.The concept is pretty straightforward  you take a drone that is equipped with a camera and insert the deep learning model into it. This gives the drone the ability to scan a crowded place and attempt to spot if any individuals are going to turn violent.So how does the technology work behind the scenes? First, the model uses the Feature Pyramid Network to identify humans from aerial shots. Then the region in the image where the human is identified is used by the ScatterNet Hybrid Deep Learning network for human pose estimation. This is where it gets really awesome  according to the paper, the orientations between the limbs of the estimated pose are finally used to identify the violent individuals!The initial results show a lot of promise. When asked to identify violent poses, the model posted an accuracy of 94%. But if there are 10 people in the camera frame at a time, the accuracy drops down to 79%. There is still work to be done and I expect the results to prop up in the next update.The below video shows how this technology works in real-life scenarios:This quite a significant breakthrough in crowd detection and surveillance. Its relatively inexpensive because the technology can be added to a simple drone and its good to go. Its still very much at a nascent stage (as the above accuracy results showed) but the applications are HUGE. I can see it being used in concerts, school and college events, sports stadiums, among other things.From a data science point of view, this is a wonderful example of computer vision and pattern recognition. The algorithm has been very well explained in the research paper and even a newcomer to this field will be able to grasp most of the steps. Take out a few minutes and go through it  youll benefit immensely from the experience!",https://www.analyticsvidhya.com/blog/2018/06/eye-in-the-sky-is-a-machine-learning-project-that-detects-violent-people-in-crowds/
Google has Released an Updated Version of its Open Source YouTube Dataset,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Eye in the Sky is a Machine Learning Project that Detects Violent People in Crowds|Essentials of Deep Learning: Exploring Unsupervised Deep Learning Algorithms for Computer Vision|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This is turning into a week of major open source dataset releases in computer vision! We saw Berkeley unveiling their self-driving dataset recently and now Google has announced an update to its popular YouTube-8M dataset.YouTube-8M is a video dataset that consists of millions of YouTube video IDs. It includes high-quality machine-generated annotations derived from numerous visual entities and audio-visual features from billions of frames and audio segments. In short, it is perfect for anyone learning, or already in the computer vision field.The dataset is designed to fit on a single hard disk which enables training of a baseline model in less than a day on a single GPU! The idea was to create a large-scale dataset that can be used for exploration of complex audio-visual models, which usually take a good number of weeks to train.The major improvements in the new edition include improved quality of annotations and machine-generated labels.These are obtained by combining audio-visual content with title, description and other metadata. The updated version contains 6.1 million URLs, labeled with a vocabulary of 3,862 visual entities. Each video is annotated with one or more labels (an average of 3 labels per video).The team has presented a starter-code on their GitHub page to for this enormous dataset. Along with the code, python scripts for comparison between models using the standard evaluation metrics can also be found.It is recommended by the developers that you download a fraction of the dataset to start with, and then download more as you go along. If you prefer to download the entire dataset in one go, thats also possible but will require a lot of internet bandwidth (not to mention space on your machine). The video-level training set comes up to about 18 GB. The frame-level features will take up approximately 1.3 TB of space so ensure you are all set before you begin downloading!Get the instructions to download this awesome labelled datasethere.From the original version of the dataset that included 8.2M videos with 1.8 labels/video, this updated version has 6.1M videos with 3.0 labels/video. The dataset size may be reduced but dont let that put you off  there is a major improvement in the quality of labels.Also the number of classes has been reduced from 4800 to 3862! Not only this, the latest version has about 2.6B audio-visual features, while previously we saw 1.9B visual-only features. With this update, Google are hoping to help researchers in understanding large-scale videos. If you are into deep learning, or are getting started with this field, I recommend getting your hands on this dataset and practising your newly acquired skills!",https://www.analyticsvidhya.com/blog/2018/06/google-has-released-an-updated-version-of-its-open-source-youtube-dataset/
Essentials of Deep Learning: Exploring Unsupervised Deep Learning Algorithms for Computer Vision,Learn everything about Analytics|Introduction|Table of Contents|Building Blocks of Unsupervised Deep Learning  AutoEncoders|Exploring Unsupervised Deep Learning algorithms on Fashion MNIST dataset|End Notes,"Image Reconstruction using a simple AutoEncoder|Sparse Image Compression using a Sparse AutoEncoder|Image Denoising with Denoising AutoEncoders|Image Generation with Variational AutoEncoders|Share this:|Like this:|Related Articles|Google has Released an Updated Version of its Open Source YouTube Dataset|MLflow  An Open Source Machine Learning Platform that works with any Library, Algorithm and Tool!|
Faizan Shaikh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is often said that in machine learning (and more specifically deep learning)  its not the person with the best algorithm that wins, but the one with the most data. We can always try and collect or generate more labelled data but its an expensive and time consuming task.This is where the promise and potential of unsupervised deep learning algorithms comes into the picture. They are designed to derive insights from the data without any supervision.For example, customers can be segmented into different groups based on their buying behaviour. This information can then be used to serve up better product recommendations.In my previous article Essentials of Deep Learning: Introduction to Unsupervised Deep Learning, I gave you a high level overview of what unsupervised deep learning is, and its potential applications.In this article, we will explore different algorithms, which fall in the category of unsupervised deep learning. We will go through them one-by-one using a computer vision problem to understand how they work and how they can be used in practical applications.Note: This article assumes familiarity with Deep Learning. You can go through the below articles to get an overview:Lets do a quick refresher on the concept of AutoEncoder. There are two important concepts of an AutoEncoder, which makes it a very powerful algorithm for unsupervised learning problems:We will take an example of an AutoEncoder trained on images of cats, each of size 100100. So the input dimension is 10,000, and the AutoEncoder has to represent all this information in a vector of size 10, which makes the model learn only the important parts of the images so that it can re-create the original image just from this vector.An autoencoder can be logically divided into two parts: an encoder part and a decoder part. The task of the encoder is to convert the input to a lower dimensional representation, while the task of the decoder is to recreate the input from this lower dimensional representation.Let us get a more practical perspective on these algorithms by implementing them on a real life problem.Note  This article is heavily influenced byFrancois Chollets tutorial. Special thanks to him for an excellent roundup!Before we dive on to the implementations, let us take a minute to understand our dataset, aka Fashion MNIST, which is a problem of apparel recognition. Fashion is a broad field that is seeming a huge boom thanks in large part to the power of machine learning. Seems like an appropriate challenge to learn a technique!Source: KDDFashionIn this problem, we need to identify the type of apparel in a set of images. We have a total of 70,000 images, out of which 60,000 are a part of train images with the label of the type of apparel (total classes: 10) and the remaining 10,000 images are unlabelled (known as test images).In our experiments below, we will ignore the labels, and only work on the training images in an unsupervised way. A potential use case of applying unsupervised learning on this dataset is suggesting similar fashion items that the person may like.Now that we know what an autoencoder is, we will apply it on a problem to understand how we can leverage it for real life applications. A straight-forward task could be to compress a given image into discrete bits of information, and reconstruct the image back from these discrete bits.A typical use-case could be to transfer images from one location to another, and using it to lower bandwidth.To work on the problem, we will first have to load all the necessary libraries. We will be coding in python, and will build neural network models in keras. So make sure you have set up your system before reading further. Otherwise you can refer to theofficial installation guideto install keras.In[1]:Now that we know how to reconstruct an image, we will see how we can improve our model.For our use case of sending an image from one location to another, we used the output of 10 neurons for compressing the image. An optimization on this we can do is to make this representation sparse, so that we require even less bits than we needed before to transfer the compressed image properties and reconstruct it back to the original image at the other end.This can be done using a modified autoencoder called sparse autoencoder. Technically speaking, to make representations more compact, we add a sparsity constraint on the activity of the hidden representations (called activity regularizer in keras), so that fewer units get activated at a given time to give us an optimal reconstruction.Ready to go on? Now we will see how we can create a sparse autoencoder model. We will redo all the steps again, but with a small change in how we create our model network.We are at the last part of our tutorial, i.e., understanding variational autoencoders and how to implement them.There is a subtle difference between a simple autoencoder and a variational autoencoder. The main idea is that, instead of a compressed bottleneck of information, we can try to model the probability distribution of the training data itself. Statistically speaking, if we know the central tendency along with the spread of the data, we can approximate the properties of the population. In this case, the population represents all the images that can satisfy being in the category of class of training data.Source: Shazam BlogMore specifically, the output of the encoder part is the mean and the variance of the data. The decoder part tries to reconstruct the image back from the output of our encoder.Now lets see a practical implementation of variational autoencoder.In[1]:This can be mathematically defined as:Congratulations! You have come a long way and now you know how to solve unsupervised learning problems using deep learning!In this article, we went through the details of unsupervised deep learning algorithms, and saw how they can be applied to solve real world problems.To summarize, we saw in detail a few unsupervised deep learning algorithms and their applications, more specificallyI hope this article helped you get a good understanding of the topic of unsupervised deep learning. If you have any doubts/ suggestions, reach out to me in the comments section below!",https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/
"MLflow  An Open Source Machine Learning Platform that works with any Library, Algorithm and Tool!",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Essentials of Deep Learning: Exploring Unsupervised Deep Learning Algorithms for Computer Vision|Apple Launches Create ML for Easy Machine Learning Model Training on Macs|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anyone involved in the data science development process knows how difficult it can be to get your model into production. Its all well and good to have achieved a benchmark solution but if you cant get your code into production, it essentially becomes meaningless. There are multiple challenges in machine learning development.Databricks, founded by the creators of Apache Spark, have released a unified solution to all machine learning framework challenges  MLflow. It is an open source machine learning platform that manages the entire ML lifecycle (from start to production) and is designed to work with any ML library.In a blog post announcing the release of MLflow, Databricks have listed down the reasons why they decided to develop this tool. They have seen multiple issues with how companies struggle to manage ML workflows. From data preparation to training the model, data scientists prefer using a myriad of tools to validate how good their system is. This requires productioning a lot of libraries, something that is beyond most organizations. Also, reproducing steps of a workflow is critical but can often by difficult to do without detailed tracking. And of course, getting the model into production is the hardest part. There are potentially multiple tools and environments for deploying and there is no standard way to move models from any library to any of these tools.MLflow can work with any ML library, algorithm, deployment tool or language. Other advantages it offers are:If you have existing code, MLflow can be used with that as well! Since it is open source, you can even share your framework and models across organizations (assuming you also want to open source your code, obviously).The current version of MLflow has three components:The team is working on adding more components like monitoring the progress of your model. You can install MLflow right now using pip:The project is currently in alpha but the developers feel its already good enough to be integrated into an organisations current environment. You can check out and follow their repository on GitHub here.The likes of Facebook, Google and Uber have their own internal framework for machine learning workflows, but even these platforms are limited in their own way. Most of them support only built-in algorithms and are tied to the infrastructure in place at each organization. Not the most flexible way to work.Some of the alternatives to MLflow you can check out are Sagemaker, Sacred and FGLab. I feel MLflow has better options than these but you are free to make up your own mind!I like the concept and am looking forward to them adding the aforementioned components like monitoring the progress of your models. This is another example of the ML community giving back to everyone by making such a breakthrough tool open source. If you try it out, do let us know in the comments below!",https://www.analyticsvidhya.com/blog/2018/06/mlflow-an-open-source-machine-learning-platform-that-works-with-any-library-algorithm-and-tool/
Apple Launches Create ML for Easy Machine Learning Model Training on Macs,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|MLflow  An Open Source Machine Learning Platform that works with any Library, Algorithm and Tool!|Ubers Kepler.gl is a Wonderful Open Source Tool for Analyzing Location Data (No Coding Required)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning has become an integral part of all product launches  from Google I/O to Apples WWDC. All the big organizations are leaning heavily on the power of ML to boost their products and give them a competitive edge. In fact, I cant remember the last big tech conference that did not feature the use of ML!Apple has announced Create ML, a GPU-accelerated tool for training machine learning models on your Mac system. It has been built all in Swift, Apples native programming language. You can train models to perform tasks like recognizing images, analyzing text to extract meaningful insights, or finding relationships between numbers. The general framework of Create ML is shown below:You can even use one-click or drag-and-drop interfaces (like Swift Playground) to train models. According to Adam Federighi, Apples senior Vice President of software engineering, this means you dont need to be a machine learning expert to make use of this tool.Also since the tool is GPU accelerated, it makes things run incredibly fast on Macs. Mr. Federighi used the example of app developer Memrise, which has used Create ML to reduce the training time of its model significantly. Earlier, it used to take them 24 hours to train a model using 20,000 images. Create ML has cut down on that time to 48 minutes on a Macbook Pro and a remarkable 18 minutes on an iMac Pro!The tool also reduces the size of the model hence making things easier of your machine. In the case of Memrise, the model size went from 90 MB right down to 3 MB.Apple also announced the latest version of its Core ML framework, appropriately called Core ML 2. They claim it is 30 times faster than the last iteration thanks to batch prediction. This will also reduce the size of the models by up to 75 percent, thanks to a technique called quantization.You can check out the documentation for Create ML, which includes examples to help you understand the tool, here.Create ML is clearly aimed at Apple developers. It has been built for Macs specifically and can only be used with Apples Swift programming language. So these restrictions will obviously limit its usage for data scientists and developers in general.You might be wondering how Create ML is different from Core ML. Well, Core ML is all about bringing your own models while Create ML is basically an easy and intuitive way to train custom models. For folks who use Swift, Google recently also launched the Swift for Tensorflow package to ease your transition into the ML world.",https://www.analyticsvidhya.com/blog/2018/06/apple-launches-create-ml-for-easy-machine-learning-model-training-on-macs/
Ubers Kepler.gl is a Wonderful Open Source Tool for Analyzing Location Data (No Coding Required),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Apple Launches Create ML for Easy Machine Learning Model Training on Macs|The Most Comprehensive Data Science & Machine Learning Interview Guide Youll Ever Need|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Location data has become an invaluable resource for organizations recently. From ride-sharing to sports and architecture, businesses are riding the geospatial Big Data boom to gain an advantage over the competition. But the current process of visualizing and analyzing data can be a long-drawn one. Add to that the high cost associated with the tools required for this  and it phases out individuals looking to get into this field.Developed and released by Uber, kepler.gl is a powerful web-based geospatial analysis tool that enables visual exploration of large-scale location data. You can make neat looking data-driven maps without having to code and waste time on cumbersome data preprocessing steps.The current process is tedious and resource consuming  it involves data collection, cleaning, visualization using either a library or another tool, and then finally exporting that visualization into JavaScript. Kepler.gl takes care of most of these things allowing you to focus on analyzing and extracting insights for your business.You can get started with doing your own analysis right now! Just go to this link and import your dataset. Currently the tool accepts .csv and GeoJSON files.If you dont have a file handy right now, dont worry! Uber has uploaded a few sample datasets that you can load and get started with straight away.How do you make it work? There is no coding required so even non-programmers can work on this tool. You only need to select your dataset and the tool automatically captures the variables and data points and lays it out on a pretty looking map visualization. You can add filters, apply scales and perform visual aggregations on the fly.The tool has been built on top of Ubers deck.gl  another web.gl data visualization tool. It is built on Redux and React, frameworks that are widely accepted and used in the developer community. Companies like Airbnb and Atkins Global are already making use of this tool to enhance their business processes.I have mentioned a few resources below which will help you get started with this wonderful tool:The below video shows what kepler.gl is capable of:By open sourcing kepler.gl, Uber has made it publicly available for anyone who wants to analyse location specific data. The visualizations are pretty neat, and thanks to GPU support, you can analyze huge datasets as well. It doesnt require you to know any code so both technical and non-technical folks can gain valuable insights from the data.I really liked their Getting Started guide. Its comprehensive and covers each aspect that a data scientist might require to make the geospatial data useful. If you work on analyzing location data, make sure you leverage this awesome free tool.",https://www.analyticsvidhya.com/blog/2018/06/ubers-kepler-gl-open-source-tool-analyzing-location-data-no-coding-required/
The Most Comprehensive Data Science & Machine Learning Interview Guide Youll Ever Need,Learn everything about Analytics|Introduction|Table of Contents|1. Data Science and Statistics Questions|2. Machine Learning Questions|3. Deep Learning Questions|4. Case Studies|5. Puzzles and Guess Estimates|6. Tool/Language Specific Questions|7. Tips and Tricks for Freshers|8. Other (Really) Helpful Resources for All Aspiring Data Scientists|9. Inspiring Stories|End Notes,"Share this:|Like this:|Related Articles|Ubers Kepler.gl is a Wonderful Open Source Tool for Analyzing Location Data (No Coding Required)|Berkeley Open Sources Largest Self-Driving Dataset Every Data Scientist Should Download NOW|
Pranav Dar
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",1.1 40 Interview Questions asked at Startups in Machine Learning/Data Science|1.2 40 Questions on Probability for Data Science|1.3 7 Most Commonly Asked Questions on Correlation|1.4 41 Questions on Statistics for Data Scientists and Analysts|1.5 30 Questions to Test a Data Scientist on Linear Regression|1.6 30 Questions to test your Understanding of Logistic Regression|2.1 40 Questions to test a Data Scientist on Machine Learning|2.2 30 Questions to test a Data Scientist on Natural Language Processing|2.3 30 Questions to test a Data Scientist on Tree Based Models|2.4 25 Questions to test a Data Scientist on Support Vector Machines|2.5 40 Questions to test a Data Scientist on Dimensionality Reduction Techniques|2.6 40 Questions to test a Data Scientist on Clustering Techniques|3.1 45 Questions to Test a Data Scientist on the Basics of Deep Learning|3.2 30 Questions to test a Data Scientist on Deep Learning|3.3 40 Questions to Test a Data Scientist on Deep Learning|3.4 25 Questions to test a Data Scientist on Image Processing|3.5 12 Frequently Asked Questions on Deep Learning|4.1 Solve Interview Case Studies 10x Faster using Dynamic Programming|4.2 Case Study for Analytics Interviews  Dawn of Taxi Aggregators|4.3 An Analytics Interview Case Study|4.4 Case Study for Freshers: Call Center Optimization (Level: Medium)|4.5 Case Study: Optimize the Price of Products for an Online Vendor (Level: Hard)|5.1 Tips to Crack a Guess Estimate Problem|5.2 20 Challenging Job Interview Puzzles which every analyst should solve at least once|5.3 3 Tricky Puzzles which most people get Wrong in Job Interviews|5.4 Commonly asked Puzzles in Analytics Interview (Part 1)|5.5 Commonly asked Puzzles in Analytics Interview (Part 2)|6.1 40 Questions to Test your Skill on R for Data Science|6.2 4 Tricky R Interview Questions|6.3 4 Tricky SAS Questions Commonly Asked in Interviews|6.4 Tricky Base SAS Interview Questions|6.5 40 Questions to Test your Skill in Python for Data Science|6.6 42 Questions on SQL for all Aspiring Data Scientists|7.1 Tips for Freshers to Crack Campus Interviews for Analytics/Data Science Companies|7.2 How Freshers can Ace Interviews for Business Analytics Roles|8.2 Definitive Guide to prepare for an analytics interview|8.3 8 Essential Tips for People Starting a Career in Data Science|8.4 10 Things you Should Know about Analytics Related Career|8.5 Moving into Analytics After a Break in Career? Dont Expect a Rosy Land!|8.6 The lack of Analytics Work Experience and how to Overcome it|8.7 Planning a Late Career Shift to Analytics/Big Data? Better be Prepared!|8.8 How to Train your Mind for Analytical Thinking?|8.9 Taking a New Job in Analytics? Ask these 5 Questions First|9.1 How I became a Data Scientist after 8 Years Working as a Software Test Engineer|9.2 How I became a Data Scientist after Working for 10 Years in the IT Industry,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you aspiring to become a data scientist, but struggling to crack the interviews? Well  youre not alone! Getting a break in the data science field can be difficult. Doubly so, if youre coming from a non-data science background (which in all likelihood you are).The stories you hear from other aspiring data scientists can make interviews feel more intimidating and daunting. So you better be prepared before facing the interviews.What kind of questions can be asked? How can you prepare and what are the resources you should refer to? What is the structure of a typical data science interview? How should your body language be? These are just some of the questions youll have in mind.Dont worry  youre at the right place!I have been there as well. Believe me  it will take discipline, hard work, and an understanding of data science interview process (and questions!) to get your break.Thats why we combined our experience of conducting hundreds of data science interviews in theAce Data Science Interviews course.The course is structured around a comprehensive 7-step process, detailing the kind of questions and things you might face in your data science interview.In this article, we provide you with a comprehensive list of questions, case studies and guesstimates asked in data science and machine learning interviews. We have also listed additional resources including handy tips and tricks to guide you through your interview process and come out on the other side successfully.This is the ultimate resource guide you can find for FREE. You should bookmark this page for every time you have to prepare for an interview.Happy learning and all the best! And heres a sneak peek into our Ace Data Science Interviews course, and why you should take it TODAY:Not ready for Interview? Up Level your Data Science ResumeThis section is meant to test, enhance and improve your data science and statistics concepts. From probability to correlation, linear and regression to logistic regression, your concepts will be set in stone by the time you reach the last question!This is a list of 40 plausible & tricky questions whichare likely to come across your way in interviews. If you can answer and understand these questions, rest assured, you willgive a tough fight in your job interview. Thekey to answering these questions is to have a concretepracticalunderstanding of ML and related statistical concepts.Probability is considered the backbone of quite a few data science concepts and techniques. You will need to have a good grasp on this subject in order to have a chance to land a data science role. These questions will test how well you know probability.Correlation is one of the core concepts in data science. It seems easy from the outside but it has its own tricky features.If you are learning statistical concepts, you are bound to facethese questions which mostly people try to avoid. For folks who are well versed with statistics, this will be a good refresher.Your statistical concepts should be rock solid before you go for an interview in this field.To help you improve and test your knowledge on statistics, we have put together this list of questions. The article covers both descriptive and inferential statistics along with explanations for each question.Linear Regression is still one of the most prominently used statistical techniques in the data science industry and in academia to explain relationships between features. It is a technique you absolutely MUST KNOW inside out if you want to become a data scientist.Logistic Regression is likely the most commonly used algorithm for solving all classification problems.The questions in this article are especially designed for you to test your knowledge on logistic regression and its nuances.Machine learning has become central to a lot of organizations strategies. If you want to carve a career for yourself in this field, you should be prepared to face the hard questions. This section will definitely test your ML techniques to the limit.If you are a data scientist (or an aspiring one), then you need to be good at Machine Learning  no two ways about it.These questions have been designed to test your conceptual knowledge of machine learning and make you industry ready. Get ready to test yourself!Natural Language Processing (NLP) is the science of teaching machines how to understand the language we humans speak and write. It is a very upcoming field in machine learning. Organizations are waking up to the power of how ML can be used to gain actionable insights from text. Go through these questions and see how well versed you are with NLP.Decision Trees are one of the most respected algorithm in machine learning and data science. They are transparent, easy to understand, robust in nature and widely applicable. You can actually see what the algorithm is doing and what steps does it perform to get to a solution. This trait is particularly important in business context when it comes to explaining a decision to stakeholders, which makes an integral part of the interview process as well.You can think of machine learning algorithms as an armoury full of axes, sword and blades. You have various tools, but you ought to learn to use them at the right time. Support Vector Machines is like a sharp knife  it works on smaller datasets, but on them, it can be much more stronger and powerful in building models. Test yourself with these 25 questions to enhance your knowledge of this wonderfully adept technique.One of the most common questions in interviews is based on how you will deal with a massive dataset that consists of millions of rows and thousands of columns. Knowing dimensionality reduction techniques and when to use them comes in handy in these cases.Clustering plays an important role to draw insights from unlabeled data. Itclassifies the data in similar groups which improves various business decisions by providing a meta understanding. It is used in industries like marketing, finance and many others. Its another must-know concept you should have a good grasp on.Deep learning is the hottest research field in the industry right now.It has led to amazing innovations, incredible breakthroughs, and we are only just getting started! But jobs in this field are few and far between. If you manage to land an interview, you need to be completely prepared for the hard questions  there is no easy way out when you work in the deep learning domain. This section will tell you how prepared (or not) you are to apply and sit for these interviews.This is a relatively easier set of questions that are MUST-KNOW if you wish to work in deep learning. Before you go further down this section, take this quiz first and then see where you stand. If you dont understand a concept, the article has links to resources where you can learn them. Get going!This is as good a place to start as any to test your deep learning knowledge. This contains basic as well as advanced questions. When we released this quiz, most people clearly took it without having an inherent knowledge of the subject. Can you do better? Go for it!This article carries on from the above one. It will test your conceptual knowledge of deep learning.When it comes to deep learning, image processing is the leading domain right now. With big players like Google and IBM launching automated platforms to build image classification models, the interest in this field is pretty high.The questions in this article are especially designed for you to test your knowledge on how to handle image data, with an emphasis on image processing.While these are not specifically interview based, you should have a comprehensive answer for each of these 12 questions. These are some of the most basic questions around deep learning and should be on your fingertips.Case studies are an integral part of the data science interview process. The hiring manager will be sure to check how you structure your thinking when faced with a case study. Ensure you go through the below case studies in detail. Before you see the solutions, first solve the problem yourself and then check your answers.Dynamic Programming isnt a trick or amathematical formula whichdelivers correct answer just by providing the inputs. Rather, its a combination of structured thinking & analytical mindset which does the job. The concept is an old one, yet used by just few of us. Learn this unique approach and your interviewer will be bowled over!Taxi aggregators have become a MASSIVE deal in certain parts of the country.In this article, well solve a case study of taxi aggregators. Along side this, we will also focus on the essentials required for solving a case study like a pro. Consulting firms like Bain, BCG and McKinsey prefer candidates who think like a pro when given any case study. Lets make you one.This is a classic route optimization problem. You are given data about alternate roads and have to figure out possible routes that minimize the time taken to travel. As you answer each question, you are provided more and more data to dive deeper into the case study. This is exactly how it happens in the interview room so strap in!In this article, we will look at a real life case in the form of a call center optimization problem.This case study will give you a good feel of how to simulate an entire environment in such an operation intensive function. The codes mentioned here are in R but even if you dont know the tool, you can work out the problem in Excel.This case study is a classic because of its applications in the real world. The objective of this case study is to optimize the price level of products for an online vendor. The calculations which youll need to perform are ones which often take place in real life. Therefore, its not just mathematical, but practical too. For experienced job roles, similarcase studiesoftenappears in job interviews. So, give your best attempt!If you aspire to become a data scientist, your out-of-the-box thinking and ability to quickly calculate and structure your thoughts is critical. One of the first things the interviewer will test you on is exactly this. You will be given a puzzle or a guess estimate questions (sometimes both) to see how quickly and logically you solve a challenging problem. This section will help you be prepared to crack such challenges!Guess estimate questions are very common in analytics and management consulting interviews. If you wish to crack the data science interview, this article will be very useful in going past the first step. In this article you will walk through some tried and tested techniques to crack guess estimates.In this article, the author has covered some of the trickiest and most challenging puzzles he was given while interviewing for data science roles.These questions have been asked at companies like Goldman Sachs, Amazon, Google, JP Morgan, etc.This article contains 3 of the most challenging puzzles which most people get wrong in interviews. Since these questions are tricky to understand at first, its perfectly fine evenif you do not figure out the answer in your first attempt. Dont give up though! Sometimes the most tricky questions can have the simplest of solutions.These cover some of the most common puzzle questions asked in interviews. These are some of the easier puzzles so you should not have too hard a time in solving them. In case you were not able to crack two of the puzzles within the given time limits, you might need to solve different variety of puzzles to get a hang of these types of questions.Part 2 of this article continues in the same vein as the above one  puzzles from easy to high level of difficulty. The puzzles are divided into 3 stages and you have not been given solutions to the first stage, If you dont get those answers yourself, you might need to go through puzzle solving from scratch!Every aspiring data scientist must have mastery over at least one tool in order to produce quality analysis. But the more tools you know, the diverse your skillset becomes, hence increasing your chances of landing your preferred role. Questions on tools are a mandatory part of every data science interview and you should have certain things already in your mind before you face the panel. This section takes care of the questions related to Python, R, SQL and SAS.This article is a comprehensive test of your R skills. From coding questions to conceptual ones, you will need to be quick on your feet to give rapid answers. I would suggest timing yourself with each question so you dont hesitate when it comes to facing the interview panel.R is one of the most popular languages in use today, thanks to its open source nature and an excellent user community. These 4 questions are some of the trickiest you might have to handle in pressure situations. Better be prepared!What distinguishes SAS from other such languages is its simplicity to code.There are some very tricky SAS questions and handling them can become overwhelming for some candidates. This article covers 4 such questions with detailed examples to help you get started.This is essentially a continuation of the above article. These questions are tougher and lengthier than those covered in the first part of this article series. These questions are widely asked in companies that have a broad analytics base and deal with big data on a daily basis.Python has well and truly taken the lead in the data science tools debate. This is a must-read list of questions about this awesome programming language. Before you go for any data science interview, ensure you test yourself with these questions so your base is rock solid.You can here access Python for Data science for FreeIrrespective of which language you use to build your models, SQL is a mandatory addition to your CV. Without it, your chances of landing a data scientist role are little to none. This is a comprehensive list of questions that will ensure your SQL skills are polished and ready to go.Getting that first break in analytics is critical for students coming out of college. Some get lucky when theyre picked up by organizations and then placed in analytics. But you cant reply on luck alone! This section is especially for freshers to better prepare you for acing your interview process.The author of this article has analyzed the essential patterns for cracking campusinterviews. These patterns can help you in clearing any type of analytics interview. Here he has shared these insights along with some useful interview tips. A lot of candidates often take these tips for granted, and end up getting disappointed when the offer letter fails to materialize.Campus interviews can be very competitive, especially so, if you want to secure a job with the best companies. Further, if you are a fresher, the experience of giving interviews can be unnerving at times. However, you can train yourself to make sure that you present your best when it matters the most.This article provides some tips using which you can blaze through any analytics interview.So far we have covered the question and answer part of the interview process. But even having that knowledge might not be enough if you dont follow the tips and behavioural guidelines covered in this section! Things like body language, the way you structure your thoughts, your awareness of the industry, domain knowledge and how caught up you are with all the latest developments in machine learning  these all matter a great deal.8.1 Beware  Interviewer for the Analytics Job is Observing you Closely!As an analyst, getting into details and studying them carefully, almost becomes second nature to you.In an interview, you will likely be interviewed by someone who has been an analyst for a longer duration that you have been.Hence, you should expect a thorough and close examination of minute details. The tips mentioned here will prove to be very handy.This article lays down the general structure of an analytics interview. It covers aspects like the different points the employer judges you on, the different stages of an interview, how a technical interview is conducted, etc.This guide is meant to help you ace the next analytics interview you sit for!Which tool to learn  R or Python? What techniques to focus on? How much statistics to learn? Do I need to learn coding? These are some of the many questions you need to answer as part of your data science journey. This was the idea behind writing this simple and not very long article. It sets a framework which can help you learn data science through your initial stages.This is an awesome resource  its a guide within a guide! Its a curated list of articles based on career related suggestions and knowledge. These articles will help you get acquainted with thesteps that you must take if you are planning to enter the analytics industry.Can you relate to the heading? A lot of people take a 1-3 year break for various reasons during their career. How can a person without past experience get a break in analytics? In this article, Kunal draws on his wealth of experience and gives his own perspective on this question.Like the above article, this one also aims to help folks with no prior experience in this field get a break in analytics. There are very valid points that apply to both freshers as well as people with experience. Kunal has written this from the perspective of both employer and prospective candidate which makes this a must-read!This is one of the most common questions floating around in the last 4 years and this article gives the low down on what to expect. It doesnt pull any punches and tells the situation like it is  the challenge is going to be tough but it can be overcome with a lot of hard work and dedication. The tips mentioned in here are invaluable.As you make calculations on a daily basis, they become more reflexive and accurate. An average working person in weekday spends 25-30% of his time sleeping, 40-60% of his time working , 10% of time eating and 15-25% idle. In this busy world more than 50% of our idle time is spent on road. You can use this particular time to develop sharper reflexes on numbers. This article illustrates some engaging methods that you can use in this idle time to sharpen your brains reflexes.This is a list of questions that you should ask your prospective employer before taking up a job in Analytics.The aim of these questions is to make sure you know what you are getting into. Using these questions will not only help you make the right choice, it will also tell the employer that you are dead serious about the role and this industry!Looking for inspiration? Look no further! The below stories will inspire you to work even harder to get your coveted data science role.This is an awesome story ofBindhya Rajendranwho is an Electronics and Communications Engineer. After 8 years of working in the Quality Assurance field, she managed to carve out a career in the data science field through hard work, application and some luck.In this article, Karthe tells his story of how he transitioned into data science after working in IT for 10 years. He has also given some nifty tips and a heavy dose of inspiration and experience which everyone in his position can lean on to get their first break.To stay a step ahead Ace your Data Science Interview here.This is as comprehensive a list as youll find anywhere. You will be ready and gunning for that data science role if you go through this end-to-end. Even if you know most of these topics, this guide will act as a refresher for you.Whats your story? Did this guide help you better prepare for your next interview? Let us know in the comments below!",https://www.analyticsvidhya.com/blog/2018/06/comprehensive-data-science-machine-learning-interview-guide/
Berkeley Open Sources Largest Self-Driving Dataset Every Data Scientist Should Download NOW,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Most Comprehensive Data Science & Machine Learning Interview Guide Youll Ever Need|A Chinese School is Using Facial Recognition to Analyze Students Behavior|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Self-driving cars are on the verge of transforming the way we travel. However, there have been hiccups along the way which have derailed the initial hype around this field. But with the Andrew Ng backed Drive.ai initiative, and now with Berkeleys latest release, the perception that autonomous vehicles are unsafe is giving way to positive developments.UC Berkeley has open sourced the largest and most diverse self-driving dataset for the general public. It is being called BDD100K and comes added with rich annotations. You can download it right now here.As the name suggests, the dataset contains 100,000 video sequences. Each video sequence is about 40 seconds long and is in moderately high definition (720p and 30 frames per second). GPS information, recorded from mobile phones, is also available in these videos to illustrate the rough driving trajectories. These videos were collected from various locations in the United States.What makes the dataset even more unique and rich to work with is the different weather conditions it has covered, like sunny, overcast, rainy and haze. There is also a good balance between daytime and nighttime scenarios. The annotated images have been divided into two types of lane markings to make them easily distinguishable.The uses of this dataset extend beyond just building self-driving cars  you can use the data for detecting pedestrians on the roads/pavements. The dataset has over 85,000 instances of pedestrians which make it ideal for this exercise.As you can see in the image below, their claims of this being the largest ever self-driving dataset are not exaggerated in the slightest. Back in March, we saw Baidu release the largest dataset (at that time) in this domain. Berkeleys release is 800 times larger than that. Its 4,800 times bigger than Mapillarys dataset and an incredible 8,000 times bigger than KITTI (lets not even compare it to the Cityscapes size!).I personally think open sourcing datasets like these will massively help the autonomous driving field. At Analytics Vidhya, we have seen a few requests coming in from people asking for self-driving data so this release, coupled with Baidus ApolloScape, will go a long way in helping those data scientists.You can even take part in three challenges set up by Berkeley for this data  Road Object Detection, Drivable Area Segmentation and Domain Adaptation of Semantic Segmentation. So not only do you have enough data to start working on building your own autonomous vehicle, you can even compare your progress with the best data scientists in this domain! What are you waiting for? Download the dataset now and get started!",https://www.analyticsvidhya.com/blog/2018/06/berkeley-open-sources-largest-self-driving-dataset-every-data-scientist-should-download-now/
A Chinese School is Using Facial Recognition to Analyze Students Behavior,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Berkeley Open Sources Largest Self-Driving Dataset Every Data Scientist Should Download NOW|IBMs Cloud Private Platform Combines Data Science and Data Engineering into One Powerful Package|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The perils of AI are well documented. You must have seen Elon Musk preaching that it has a lot of downsides as well as upsides. But when it comes to using it to monitor the behavior of people, which side of the line do you stand on  do you think this is beneficial or intrusive? China has been emphatic in using AI to transform its way of living and they are now using it in education as well.Source: AP Photo/Mark SchiefelbeinA school in Hangzhou, China, is using machine learning in the form of facial recognition to monitor and analyze how students are behaving and responding to the teacher. The technology is built into cameras and is called smart eyes. It aims to provide real-time data to the teachers (and supervisors) on the emotions their students are displaying in the class. The underlying concept behind this system analyses the facial expression of the students and classifies them into the below categories:The technology has also been programmed to record various actions students perform  like reading, writing, raising their hands, and even if a student falls asleep. This not only helps understand which students are slacking, it is also helping teachers alter and tailor their teaching style to get a more positive response.Another use case of this system is to monitor the attendance of students by checking students faces with the records it has in the school database. The system also manages to detect whether a student is not feeling well and flags it to the teacher.Unfortunately the results of most actions have not yet been published. How does the school react when a student is flagged as sad or angry? What if a student falls asleep? These are just some of the questions that are yet to be answered. But one thing is for certain, Chine is miles ahead of the rest when it comes to using facial recognition technology to help and monitor its citizens.Watch the video below to see the technology in action:Using face recognition for scanning behaviour of students in an interesting approach but using the systems in schools has raised obvious privacy concerns. There are upsides to this as well though  it helps the school tailor its approach better to keep the students involved.China has been using facial recognition technologyfor various other applications as well, likepredicting a crime before it happens.Other than China, countries like the US have also used facial recognition technology in schools to protect children. From the perspective of machine learning, this is a step forward towards AI being all pervasive. What is your take on this latest development? Does it excite or scare you? Let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/06/china-school-facial-recognition-analyse-students/
IBMs Cloud Private Platform Combines Data Science and Data Engineering into One Powerful Package,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Chinese School is Using Facial Recognition to Analyze Students Behavior|24 Ultimate Data Science Projects To Boost Your Knowledge and Skills (& can be accessed freely)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"IBMs latest offering in the data science field brings analytics to your data, and not the other way around. Organizations looking to leverage AI to gain a competitive edge need to ensure that their data needs to have the proper security around it in order to be extract useful insights from it. IBM saw this gap and developed the Cloud Private for Data to fulfil that demand.IBM Cloud Private for Data had been announced a couple of months ago at the companys Think conference and they have now officially rolled out the first version to the general public. This platform aims to accelerate the AI journey of its users by simplifying data management, data governance and business analytics, all packaged into a single interface. In other words, this tool goes beyond simply data warehousing  it combines the power of data science and data engineering.The platform can access all of the data you have on the cloud, whether its in Teradata, Oracle, MongoDB, PostgreSQL, Hadoop or an IBM repository itself. It is able to run natively on Red Hats container orchestration platform called OpenShift. According toIBMs GM of Analytics Rob Thomas, the platform can intake 250 billion events a day. Remarkable!The Cloud Private for Data platform brings the below four features to the data scientist or company using it:Check out the below video which showcases a real-life scenario of this tool:The end goal of using a tool for data scientists is to spend more time with the data in order to build your models (and less time collecting and cleaning it). This is where IBM Cloud Private for Data fills the gap. There are a few tools out there that aim to leverage cloud solutions in AI (Oracle Cloud, Microsoft Azure Stack), but with clout IBM has in the ML community, I expect this tool to be picked up quickly by organizations.It reduces multiple iterations of combining your data from various platforms. It really speeds up your data science pipeline processes and reduces manual efforts. Data engineers, let us know your take on this platform!",https://www.analyticsvidhya.com/blog/2018/05/ibm-platform-combines-data-science-data-engineering-one-powerful-package/
24 Ultimate Data Science Projects To Boost Your Knowledge and Skills (& can be accessed freely),Learn everything about Analytics|Introduction|Useful Information|Table of Contents|Beginner Level|Intermediate Level|Advanced Level|End Notes,"1. Iris Data Set|2. Loan Prediction Dataset|3. Bigmart Sales Data Set|4. Boston Housing Data Set|5. Time Series Analysis Dataset|6. Wine Quality Dataset|7. Turkiye Student Evaluation Dataset|8. Heights and Weights Dataset|1. Black Friday Dataset|2. Human Activity Recognition Dataset|3. Text MiningDataset|4. Trip History Dataset|5. Million Song Dataset|6. Census Income Dataset|7.Movie Lens Dataset|8.Twitter Classification Dataset|1. Identify your Digits Dataset|2. Urban Sound Classification|3. Vox Celebrity Dataset|4.ImageNet Dataset|5. Chicago Crime Dataset|6.Age Detection of Indian Actors Dataset|7.Recommendation Engine Dataset|8.VisualQA Dataset|Participate in ourHackathonsand compete with the bestData Scientists from all over the world!|Share this:|Related Articles|IBMs Cloud Private Platform Combines Data Science and Data Engineering into One Powerful Package|Check Out this Entirely Different Approach to Understand Machine Learning by IBM|
Analytics Vidhya Content Team
|49 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published on October 26, 2016 and updated with new projects on 30th May, 2018.Data science projects offer you a promising way to kick-start your career in this field. Not only do you get to learn data science by applying it but you also get projects to showcase on your CV! Nowadays, recruiters evaluate a candidates potential by his/her work and dont put a lot of emphasis on certifications. It wouldnt matter if you just tell them how much you know if you have nothing to show them! Thats where most people struggle and miss out.You might have worked on several problems before, but if you cant make it presentable & easy-to-explain, how on earth would someone know what you are capable of? Thats where these projects will help you. Think of the time youll spend on these projectslikeyour training sessions. The more time you spend practicing, the better youll become!Weve made sure to provide you with a taste of a variety of problems from different domains. We believe everyone must learnto smartly work with huge amounts of data, hence large datasets are included. Also, weve made sure all the datasets are open and free to access.24 Data Science ProjectsTo help youdecide where to begin, weve divided this list into 3 levels, namely:This is probably the most versatile, easy and resourceful dataset in pattern recognition literature. Nothing could be simpler than the Iris dataset to learn classification techniques.If you are totally new to data science, this is your start line. The data has only 150 rows & 4 columns.Problem: Predict the class of the flower based on available attributes.Start: Get Data| Tutorial: Get HereLets have a look at the Iris data and build a Logistic Regression Model in the Live Coding window below.Among all industries, the insurance domain has one of the largest uses of analytics & data science methods. This dataset provides you a taste of working on data sets from insurance companies  what challenges are faced there, what strategies are used, which variables influence the outcome, etc. This is a classification problem. The data has 615 rows and 13 columns.Problem: Predict if a loan will get approved or not.Start: Get Data| Tutorial: Get HereLets have a look at the Loan data and build a Logistic Regression Model in the Live Coding window below.Retail is another industry which extensively uses analytics to optimize business processes. Tasks like product placement, inventory management, customized offers, product bundling, etc. are being smartly handled using data science techniques. As the name suggests, this data comprises of transaction records of a sales store. This is a regression problem. The data has8523 rowsof 12 variables.Problem: Predict the sales of a store.Start: Get Data| Tutorial: Get HereLets have a look at the Big Mart Sales data and build a Linear Regression Model in the Live Coding window below.This is another popular dataset used in pattern recognition literature. The data set comes from the real estate industry in Boston (US). This is a regression problem. The data has 506 rows and 14 columns. Thus, its a fairly small data set where you can attempt any technique without worrying about your laptops memory being overused.Problem: Predict the median value of owner occupied homes.Start: Get Data| Tutorial:Get HereTime Series is one of the most commonly used techniques in data science. It has wide ranging applications  weather forecasting, predicting sales, analyzing year on year trends, etc. This dataset is specific to time series and the challenge here is to forecast traffic on a mode of transportation. The data has ** rows and ** columns.Problem: Predict the traffic on a new mode of transport.Start: Get Data | Tutorial: Get HereThis is one of the most popular datasets along data science beginners. It is divided into 2 datasets. You can perform both regression and classification tasks on this data. It will test your understanding in different fields  outlier detection, feature selection, and unbalanced data. There are 4898 rows and 12 columns in this dataset.Problem: Predict the quality of the wine.Start: Get Data| Tutorial: Get HereThis dataset is based on an evaluation form filled out by students for different courses. It has different attributes including attendance, difficulty, score for each evaluation question, among others. This is an unsupervised learning problem. The dataset has 5820 rows and 33 columns.Problem: Use classification and clustering techniques to deal with the data.Start: Get Data| Tutorial: Get HereThis is a fairly straightforward problem and is ideal for people starting off with data science. It is a regression problem. The dataset has 25,000 rows and 3 columns (index, height and weight).Problem: Predict the height or weight of a person.Start: Get Data| Tutorial: Get HereIf youre new to the world of data science, Analytics Vidhya has curated a comprehensive course  Introduction to Data Science, aimed for beginners!We will cover the basics of Python, before moving to Statistics and finally going through various Modelling techniques.This dataset comprises of sales transactions captured at a retail store. Its a classic datasetto explore and expand your feature engineering skills and day to day understanding from multiple shopping experiences. This is a regression problem. The dataset has 550,069 rows and 12 columns.Problem:Predictpurchase amount.Start: Get Data| Tutorial: Get HereThis data set is collected from recordings of 30 human subjects captured viasmartphones enabled with embedded inertial sensors. Many machine learning courses use this data for teaching purposes. Its your turn now.This is a multi-classification problem. The data set has 10,299 rows and 561 columns.Problem: Predict the activity category of a human.Start: Get Data| Tutorial: Get HereThis dataset is originally from the Siam Text Mining Competition held in 2007. The data comprises of aviation safety reports describing problem(s) which occurred in certain flights. It is a multi-classification and high dimensional problem. It has 21,519 rows and 30,438 columns.Problem: Classify the documents according to their labels.Start: Get Data| Tutorial: Get HereThis dataset comes from a bike sharing service in the United States.This dataset requires you to exercise your pro data munging skills. The data is provided quarter-wise from 2010 (Q4) onwards. Each file has 7 columns. It is a classification problem.Problem: Predict the class of user.Start: Get Data| Tutorial: Get HereDid you know data science can be used in the entertainment industry also? Do it yourself now. This data set puts forward a regression task. It consists of 5,15,345 observations and 90 variables. However, this is just a tinysubset of theoriginal database of data about a million songs.Problem: Predict release year of the song.Start: Get Data| Tutorial: Get HereIts an imbalanced classification and a classic machine learning problem. You know, machine learning is being extensively used to solve imbalanced problems such as cancer detection, fraud detection etc. Its time to get your hands dirty. The data set has 48,842 rows and 14 columns. For guidance, you can check thisimbalanced data project.Problem: Predict the income class of US population.Start: Get Data| Tutorial: Get HereHave you built a recommendation system yet? Heres your chance! This dataset is one of the most popular & quoted datasets in the data science industry. It is available in various dimensions. Here Ive used a fairly small size. It has 1 million ratings from 6,000 users on 4,000 movies.Problem: Recommend new movies to users.Start: Get Data| Tutorial: Get HereWorking with Twitter data has become an integral part of sentiment analysis problems. If you want to carve a niche for yourself in this area, you will have fun working on the challenge this dataset poses. The dataset is 3MB in size and has 31,962 tweets.Problem: Identify the tweets which are hate tweets and which are not.Start: Get Data| Tutorial: Get HereThis dataset allows you to study, analyze and recognize elements in the images. Thats exactly how your camera detects your face, using image recognition! Its your turnto build and test that technique. Its a digit recognitionproblem.This data set has 7,000 images of 28 X 28 size, totalling 31MB.Problem: Identify digits from an image.Start: Get Data| Tutorial: Get HereWhen you start your machine learning journey, you go with simple machine learning problems like titanic survival prediction. But you still dont have enough practice when it comes to real life problems. Hence, this practice problem is meant to introduce you to audio processing in the usual classification scenario. This dataset consists of 8,732 sound excerpts of urban sounds from 10 classes.Problem: Classify the type of sound from the audio.Start: Get Data| Tutorial: Get HereAudio processing is rapidly becoming an important field in deep learning hence heres another challenging problem. This dataset is for large-scale speaker identification and contains words spoken by celebrities, extracted from YouTube videos.Its an intriguing use case for isolating and identifying speech recognition. The data contains100,000 utterances spoken by 1,251 celebrities.Problem: Figure out which celebrity the voice belongs to.Start: Get Data| Tutorial: Get HereImageNet offers variety of problems which encompasses object detection, localization, classification and screen parsing. All the images are freely available. You can search for any type of image and build your project around it. As of now, this image engine has more than 15 million images of multiple shapes sizing up to140GB.Problem: Problem to solve is subjected to the image type you download.Start: Get Data| Tutorial: Get HereThe ability to handle large datasets is expected of every data scientist these days. Companies no longer prefer to work on samples when they the computational power to work on the full dataset. This dataset provides you a much needed hands-on experience of handling large data sets on your local machines. The problem is easy, but data management is the key! This dataset has 6M observations. Its a multi-classification problem.Problem: Predict the type of crime.Start: Get Data| Tutorial: Get HereThis is a fascinating challenge for any deep learning enthusiast. The dataset contains thousands of images of Indian actors and your task is to identify their age.All the images aremanuallyselected and cropped from the video frames resulting in a high degree of variability interms of scale, pose, expression, illumination, age, resolution, occlusion, and makeup. There are 19,906 images in the training set and 6,636 in the test set.Problem: Predict the age of the actors.Start: Get Data| Tutorial: Get HereThis is an advanced recommendation system challenge.In this practice problem, you are given the data of programmers and questions that they have previously solved, along with the time that they took to solve that particular question. As a data scientist, the model you build will help online judges to decide the next level of questions to recommend to a user.Problem: Predict the time taken to solve a problem given the current status of the user.Start: Get DataVisualQA is a dataset containing open-ended questions about images. These questions require an understanding of computer vision and language. There is an automatic evaluation metric for this problem. The dataset has 265,016 images, 3 questions per image and 10 ground truth answers per question.Problem: Use deep learning technique to answer open-ended questions about images.Start: Get Data| Tutorial: Get HereOut of the 24 datasets listed above, you should start by finding the one that matches your skillset. Say, if you are a beginner in machine learning, avoid taking up advanced level data sets from the get go. Dont bite more than you can chew and dont feel overwhelmed with how much you still have to do. Instead, focus on making step-wise progress.Once you complete 2  3 projects, showcase them on your resume and your GitHub profile (very important!). Lots of recruiters these days hire candidates by checking their GitHub profiles.Your motive shouldnt be to do all the projects, but to pick out selected ones based on the problem to be solved, domain and the dataset size. If you want to look at complete project solution, take a look at this article.Did you find this article useful? Have you already built any projects on these datasets? Do share your experience, learnings and suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
Check Out this Entirely Different Approach to Understand Machine Learning by IBM,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|24 Ultimate Data Science Projects To Boost Your Knowledge and Skills (& can be accessed freely)|Facebook Unveils a Stunning ML Model that Transforms Music into Different Styles|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So a team of researchers from IBM decided to explore this part of human nature and attempted to integrate that into the world of machine learning. They have used this idea to justify how a machine learning model performs the task of classification. The objective of their study was to use the missing results and explain the inner working of machine learning models, and to strip away the black box reputation surrounding them.Taking another example, if a model was trained to identify a car, the model might use information such as  does it have wheels? How about headlights? The object does not have legs. The researchers claim that the features that are missing also play an important part in perceiving how a model performs and arrives at its final conclusion.Based on this idea, the team performed their experiments on three different datasets, namely:The team has also presented a paper (link below), where they have explained deep neural network classification based on the characteristics present (wheel, headlights) and absent (hands). They have created a system for contrastive explanations that specifically looks for missing information in the data. The contrastive explanation method has two parts:Each experiment was evaluated with the help of domain experts and performed fairly well.You can read the research paper in full here to deep dive into the various experiments they conduced and how they arrived at their final conclusion.This is a pretty fascinating approach to understanding models. Once of the most common issues with models today is how complex they can become (especially deep neural networks). Explaining them to the client or end user is a mammoth task and often ends in failure. This approach, while certainly nascent right now, should help strip away some of the misunderstandings around machine learning.If you knew why you are being recommended something, there is a higher chance that you might buy it (as opposed to something that you perceived as a random recommendation). This approach is ideal for those studies where you need to make a binary classification  like a rejected loan. Not only will this approach explain what was there in the application (like a previous default), but also what wasnt there (lack of a college degree).As a data scientist, does this approach appeal to you? Do you see any upside in this? Let us know in the comments below!",https://www.analyticsvidhya.com/blog/2018/05/check-out-this-entirely-different-approach-to-understand-machine-learning-by-ibm/
Facebook Unveils a Stunning ML Model that Transforms Music into Different Styles,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Check Out this Entirely Different Approach to Understand Machine Learning by IBM|This Machine Learning Algorithm Identifies you by your Walking Style|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The music industry is in the middle of a machine learning revolution. From Googles NSynth to projects aimed at transforming every aspect of the digital age of music, machine learning has changed the landscape of how music is created. And here comes another breakthrough  an AI that can translate one song into a completely different one. And you wont be able to tell which one has been generated by a machine.Researchers at Facebooks AI research lab (FAIR) have developed an algorithm that can translate one style, instrument or genre of music into a completely different one. In a scintillating video demonstration (at the end of this article), youll see how Bachs symphony was turned into Beethovens orchestra, without a glitch! The system even accepts whistles as input and converts them into music!The developers built their model by leveraging two technologies that have recently become available:At the core of this system is, as you might have guessed, a deep neural network. In the architecture of this model, the researchers employed a single, universal encoder and applied it to all the input sounds. This had the distinct advantage of training fewer networks, and also enabled the team to translate from music domains that were not heard/used during the training phase. The below image shows the framework of this model. That dashed line you see? Its employed only during the training period.FAIR has become the first AI research arm to have created an unsupervised learning process for completely translating one form of music into another, using a neural network. But to allow the system to translate music in an unsupervised manner, the researchers distorted the input music intentionally. What this did was it forced the system to ignore what made the music unique  like the style, genre and instrument used  and concentrate specifically on the core structure of the song.According to the researchers, The system was implemented in the PyTorch framework, and trained on eight Tesla V100 GPUs for a total of 6 days. We used the ADAM optimization algorithm with a learning rate of 103 and a decay factor of 0.98 every 10,000 samples. We weighted the confusion loss with  = 102.The final output and evaluation of the system has been promising so far. In fact, the system is so good at translating music, that many humans were unable to tell the difference what the original input music was, and the AI generated output.As is the norm with FAIRs research, they have published an official paper on this study and you can read it in full here. Also be sure to check out the below video which shows this AI in action:This is yet another example of how far AI has penetrated the music industry. We have seen previous efforts in this field but they have not been able to fully turn an input into something this spectacular. This system has the potential to make even amateurs into musicians. Or you might be developing a video on a small budget  use this research to generate music for yourself without having to pay a high cost for it.If you have an interest in deep learning, GANs and music, go through the paper I have linked above. This is an amazing time to a data scientist as more and more breakthroughs are happening.",https://www.analyticsvidhya.com/blog/2018/05/facebook-unveils-a-stunning-ml-model-that-transforms-music-into-different-styles/
This Machine Learning Algorithm Identifies you by your Walking Style,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Facebook Unveils a Stunning ML Model that Transforms Music into Different Styles|Career paths in Business Analytics  Plan your Next Best Role in the Data Science World|
Aishwarya Singh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The only ways of practically identifying people was either by fingerprint scanning or using CCTV camera to directly identify them. Recently with the advancements in machine learning, we have seen facial recognition software take a giant leap forward. But these have known issues which crop up from time to time.So here comes a system, powered by Artificial Intelligence, that can pick out an individual based on the way they walk!This artificial intelligence system is developed by a group of researchers from the University of Manchester and the University of Madrid. The main idea behind the study was to train their model to distinguish between various walking speeds and styles. According to one of the researchers, each individual has 24 unique factors and movements when they are walking, and these can be used as a benchmark for identifying them. This is essentially the equivalent of scanning fingerprints or the retina.In order to build an the AI, the team first needed to collect a lot of data that would eventually help them distinguish walking styles. So the team collected a footstep database consisting of approximately 20,000 footstep signals from 127 people. Using floor-only sensors and high-resolution cameras, they compiled the samples and the dataset. This dataset, called SfootBD, was used to develop the advanced computational models needed for automatic footprint biometric verification.To validate the final model, the researchers put it to the test in three distinct security scenarios. According to the scientists, their AI system correctly identified a person almost 100 percent of the time, with an error rate of just 0.7 percent.According to the Financial Express, footprint recognition is an ideal process because it is ultimately non-intrusive to the person being verified by the AI. The individual doesnt even need to bother removing them footwear  the algorithm uses their gait to identify them.The most obvious application of this technology is for security purposes. Since the accuracy of the AI system is very close to 100%, it definitely bears testing out in practical scenarios to see how it holds up. Although this is a creative idea and will save a lot of time for an individual (since its non intrusive), I personally have one concern  will this system accurately identify an individual when he/she changes his pace, for instance while running to catch a flight?Regardless, this is an excellent use of data and machine learning. As a data scientist, you need to think outside the box and come up with use cases never seen before, even on scenarios that most people feel have no scope of innovation.",https://www.analyticsvidhya.com/blog/2018/05/this-machine-learning-algorithm-identifies-you-by-your-walking-style/
Career paths in Business Analytics  Plan your Next Best Role in the Data Science World,Learn everything about Analytics|Introduction|Table of Contents|About the analytics market|What does a business analytics professional do?|Reporting Roles|Intermediate Analytics Roles|Strategy Roles|Data Scientist Roles|End Notes,"Share this:|Like this:|Related Articles|This Machine Learning Algorithm Identifies you by your Walking Style|Data Science in the Indian Agriculture Industry|
Tavish Srivastava
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Scientist: The Sexiest Job of the 21st Century is one of the most popular Harvard Business Review (HBR) articles and has inspired tons of people to pursue their careers in the field of analytics. One of the main themes of this article published in HBR was the trend of growing jobs in the analytics industry.The exact same inference was predicted by IBM recently saying that the number of US data professionals will increase from 364,000 to 2.72 million by 2020!Unanimously, across the industry we are seeing a surge in Business Analytics job openings, but do all these jobs need the exact same skill set? I have received a number of queries focused on what are the possible career trajectories in the analytics industry. These queries usually come from people seeking a break in the analytics domain or people already working in the industry and are looking for a deeper role.In this article, we will look at the major roles available in the analytics industry. I will also propose a framework to think about your career in the space of business analytics.Tip: Up Level your Data Science ResumeLet me start with a few lines/data points that were published in a McKinsey report onBig Data(May 2011) :The United States alone faces a shortage of 140,000 to 190,000 people with analytical expertise and 1.5 million managers and analysts with the skills to understand and make decisions based on the analysis of big data.Pay attention to the words with the skills to understand and make decisions based on the analysis of big data. The industry will require a lot of big data and machine learning experts, and needs even more (about 10x) people who can make decisions based on the analysis, even though they might not be experts on Big Data or Machine Learning.These roles will primarily be strategy roles and product management roles that can define new challenges for their analytics specialists to solve. We will contrast these strategic roles with data scientist roles later in this article. First, lets try to understand how diverse this industry really is.If you plot a word cloud of all the articles related to analytics (sample shown in the image below), you will see all kind of words popping up, including statistics, computer programming, strategy, planning, reporting, etc.The field of business analytics is extremely diverse and people with both analytical skills and business acumen are sought after in all industries in numerous and diverse roles. Thinking about your career with so many possible options can be overwhelming, and you might feel like youre losing sense of whether you are making progress in your career or not.The word business analytics perfectly summarizes every type of job we categorize under business analytics.Business emphasizes on the importance of business understanding, and Analytics refers to the importance of statistics, computer engineering and operation research in this type of role.An analytics professional can ultimately work in a very strategy oriented role or can work as a very specialized deep learning scientist. The former role has a stronger component of business, while the latter role has a much stronger component of analytics. Obviously, your role generally has a trade-off between these two components and you can switch between roles that have different proportions of the two components. The value which you create for yourself is a positively correlated function of business understanding and analytics. Mathematically speaking,With this understanding, I have plotted various roles in our industry in a cross-tab graph below:Obviously, the above graph is my personal understanding of the industry and the position of each role in this graph can certainly be debated. The main idea which I want you to focus on is the diversity of roles you can take in the business analytics industry and the variation in path you can take from your current role. Let us first try to understand each of the 5 highlighted boxes above regarding the category of roles.Between 2000 and 2012, this was the major category of roles for business analytics professionals. The role was mainly concerned with What (event) happened rather than Why did it (the event) happen. However, most of these roles have evolved in recent times after companies automated a lot of these processes and machine learning became popular. However, there are still a lot of roles that will have more than 50% work on reporting and the rest of the role on answering the question  Why did the event happen?.This is a good role for starting your career in the analytics industry. But in the long run, you should take initiative and move into a role focused on either Whats happening now? i.e., business Intelligence/dashboard, or focused on Whats going to happen next? i.e., predictive analytics.This is the type of role I started my career with. The majority of Economics/Statistics/Computer science graduates will begin their journey with these roles. This is an optimum combination of business and analytics. Its great way to understand the best of both the worlds.The roles in the intermediate analytics field are also quite diverse. One extreme role in this category will be focused on Business Intelligence trying to solve What is happening now?. The other extreme in this category will be highly business focused roles like Product Pricing, where you are required to create a lot of business scenarios and finding the optimum price for the products your company is selling.Majority of roles strike a more optimal balance between knowing the business and working with cutting edge tools like Deep learning in Decision Management/Risk analytics/ Fraud analytics. Most of these roles are involved in automated decision making. For instance, you might be tasked with creating an algorithm that can accept or reject credit card applications based on customer risk profile, or that can select customers that have a high propensity to opt in for a cross sell offer of an insurance product. All these business problems require you to create predictive models on bulk customer profiles and rank them based on some business metric.If you are in this group, almost all your options will be open. You can now choose to move to a more strategic role or you can choose to become a data scientist. In case you dont know where to go next, a good way to find your fit is to take a role on the border of the two boxes. For instance, if you want to take a strategic role in the future, you can test your fit by taking a role in a P&L based intermediate analytics role like Product Pricing. There are a few more roles like portfolio analytics that you can choose in order to get a hang of strategic roles. Note that you might have to live without data science techniques like deep learning if you choose to move ahead on the path of strategy roles.On the other hand if you want to test your fit as a data scientist, you can take up business embedded data scientist roles rather than pure data scientist roles. This way you dont need to lose your grip on the business before you move onto the path of research oriented roles.Apart from the above two paths, you have one more way to find a good trade-off between business and analytics  Tech Product Manager roles. But such roles are not easily available in the industry. Data science is primarily used by companies to find the competitive advantage over other firms by building data backed strategies.Tech firms like Google and Facebook use analytics not only to build strategy, but also to create products. For instance, Google Instant search is a tech product that uses machine learning to give search results. These tech firms look for people with a skill set in both business P&L and machine learning to design such products. If you choose to move ahead on this path, you should not only apply to the big tech giants but should also look for product manager roles in niche skill companies like NICE, Aspect or Interactions.You might have heard about an important economy principle  no economic profits in competitive market:The existence of economicprofitsattracts entry, economic losses lead to exit, and in long-run equilibrium, firms in aperfectly competitiveindustry will earn zero economicprofit.If all the businesses are in a perfect competitive market, how do they make any money? If you are an economics student, you will know the answer well. All the successful businesses are built on inefficiencies in the market, hence theres no perfect competition. The role of a strategist is to identify these imperfections and nurture them to run a successful business. For big firms, we have strategists on both corporate level as well as the business level.Corporate strategy is when you work on a corporate level answering questions like What is the right business portfolio for your company?, To reach this portfolio, what new businesses do you need to acquire/invest/grow/shut down?, What is the right organization structure for your business that will foster synergies in operations and other domains?. For instance, if you work for Wells Fargos corporate strategy, you will build a strategy to acquire or close businesses like investments/retail banking/credit cards; you will also work on creating global operations to eliminate operational costs of individual businesses, etc.Business strategy is linked more to a particular line of business. While corporate strategy might be more focused on the expense side of things at the corporate level, business strategy is a lot more focused on maximizing the net revenue. For instance, Wells Fargo Credit Card strategists might be focused on maximizing the revenue coming from their cards customers. A lot of operations might be a shared asset across all lines of businesses of Well Fargo, like call centers, chat centers, branch premises, etc. Hence, these expense heads are better optimized on the corporate level rather than the business level. The distribution of responsibilities might differ across companies, but mostly both business and corporate strategists work hand in hand.Both these roles will require you to estimate benefits from changes in a products features, process change and technology investments by creating various business scenarios and computing Net Present Value of different investments. Analytics professionals are well suited for such roles because of their grasp of numbers and deep understanding of the latest technology that will be used to create a competitive advantage. Analytics professionals who started their career before 2010 currently make up a big proportion of population in strategy roles.Coming to the most fascinating role for most people looking to get into data science. The data scientist role is a position for specialists. You can specialize in different types of skills like speech analytics , text analytics (NLP) , image processing, video processing, medicine simulations, material simulation, etc. Each of these specialist roles are very limited in number and hence the value of such a specialist is immense. This is why we are seeing such a high demand for data scientists these days.For you to excel in these roles, you need to keep yourself updated with the latest tools and technologies. You should also invest on training yourself in relevant languages and have the skill to explain your complex models in simple terms to clients and businesses. You can always move back to the strategy side in case you feel the need to grasp business concepts.The career paths mentioned in this article are based on my personal experience and a number of discussions I had with successful professionals in various analytics fields. With all the resources available online for FREE, you can easily migrate to any desired role with the right strategy. I hope this article helped you in defining your career trajectory.If you have any ideas or suggestions regarding the topic, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/05/career-paths-business-analytics-role-data-science/
Data Science in the Indian Agriculture Industry,Learn everything about Analytics|Introduction|Table of Contents|Big Data To The Rescue|What is Smart Farming?|Components of Smart Farming|What is Precision Agriculture?|Career as a Data Scientist in Agriculture|Challenges in the Indian Agriculture Scene|About the Author,"Management Information Systems|Share this:|Like this:|Related Articles|Career paths in Business Analytics  Plan your Next Best Role in the Data Science World|Yellowbrick  A set of Visualization Tools to Accelerate your Model Selection Process|
Guest Blog
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Devices,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Agriculture is the backbone of the Indian economy, but the industry currently needs more support than any other. India is a country of over a billion people in population, out of which, over 70% of the population lives in the rural areas. With 40% of the countrys workforce, agriculture is a major industry and an influencer of the Indian economy. Despite this, its contribution to the $2.3 trillion economy is just a meager 16% of the entire GDP.Agriculture in India lacks institutional attention, support from banks in terms of loans and farmer welfare schemes, and suffer from a myriad of disasters like depleting groundwater levels in rural areas, climate change, unpredictable monsoon or lack of it, droughts, floods, unfair price fixing policies of produce, migration of farmers towards the cities in search of better paying jobs, and more.Agriculture is one sector responsible for feeding every individual, but the people involved in it are the last to be taken care of. After failing institutions, time has indeed come for technology to take over the change. With newer problems cropping up every day in the most inevitable indigenous sectors, it is high time we resort to emerging technologies for solutions.The revolutionary technology that goes by the name Big Data has already made waves in other Indian industries from IT to healthcare. And now, investors and market players are planning to leverage the potential of Big Data for the benefit of agriculture in India. Apart from major companies, it is the vision of several youth of the country that has attracted the use of Big Data for farming. For instance, SatSure, founded by the 33-year-old Abhishek Raju works on using Big Data and its allied technologies like data science and IoT to better the lives of farmers. Abhishek shares the fact that he was deeply moved by the rate of farmer suicides and the lack of application of science and newer technologies in the oldest Indian industry  agriculture. His solution to this is SatSure. According to him, The parameters associated with soil health and crop growth have had a very restricted scope for research and his technology immensely uses Big Data and Machine Learning technologies to solve the restriction and bring about insights on crop phenology.When we got on a call with Mr. Abhishek Raju, he shared that, Indian agriculture sector is very disorganized and heavily cash oriented. Electronic transactions are almost non-existent, and that is why most of the transactions are unrecorded. We help them provide insights about farm productivity, when to irrigate, sow, harvest, and the patch of land that can be used by farmers. We help banking and insurance companies in settlement of risk assessment, crop loss, and offer insights by analyzing current and historical satellite images.The satellite images are not only in a single visual spectrum but have multiple data layers which contain images merged into one to gather as much information as possible. This is what we at SatSure mine. However, data is one thing, and what you infer from data is another thing. We analyze data to make action oriented conclusion-able intelligence. He adds.Facilities like satellite-based filed monitoring, embedded sensors on crops and fields, predictions on wind direction, fertilizer requirement notifications, pest infestations, GPS-enabled tractors, water cycles, and more are acting as points of rich data sources that could be used for better agriculture practices. Besides, Big Data and analytics now also enable monitoring and supervision for growth rate and nutrient requirements on a plant-by-plant basis. Moreover, analytics is enabling farmers to make data-based decisions like which crops to plant for their next harvest. The rich information on soil health, water availability, and predictions on rainfall and precipitation make this data source. Welcome to the world of Smart Farming.Satsure enables insurance companies, banks, traders, pesticide and seed manufacturing companies, and farmers the ability to take informed farming decisions by leveraging the combined potentials of technologies like  Raman Singh Saluja, founder of Gramco Infratech says, Agriculture is a very physical business, requiring physicality in terms of handling/warehousing/value addition / etc, which will continue to be the bedrock. When it comes to the use of technologies, he shares that data analytics offers tremendous potential for improving cost to output ratio, reduce/optimize Input usage, increase yields, offer timely actionable information and do more.He further reveals , At Gramco, there are two initiative underway which will be brought to market by the 3-4th quarter of 2018. One has been piloted with very encouraging results with on ground support of a leading insurance company.Smart Farming is the breakthrough application of science and technology in the field of agriculture. Smart farming is the application of technologies like IoT, Big Data and analytics on an agricultural field. It makes use of technologies like the Internet of Things, cloud computing, Machine Learning, and Big Data to enable farmers to have more insights on the consequences of their actions and take a much better and informed decision on farming practices.The power of smart farming lies in the fact that it goes beyond solving the shortcomings and pitfalls of agriculture. The application of Big Data is leaving significant impact on the entire realm of supply-chain, giving predictive insights on farming practices and operations, help redesign business models, deliver realtime decisions on operations and more.Jyoti Vaddi of Cropin shares, The world population is estimated to cross the 10 billion mark by the middle of this century. This population growth combined with urbanization will require the agricultural production to double. To succeed, Jyoti recommends the need for smart solutions for fairly produced, sustainable food, feed, and fibre, which is one of the mainstay principles of CropIn.During their market opportunity study, Cropin gathered that agribusinesses had minimal and outdated technological/digital resources, and were not able to make informed data-driven decisions. She reveals that with consumers keen to know the origin of their food and how it was produced and processed, there was a need for transparency along the end-to-end agribusiness supply chain.The technologies that power Cropin in delivering efficient farming solutions to stakeholders in its network include:-These technologies foster an environment for production forecast, risk management and coverage, output predictability, quality maximization, and increased farm sustainability to agriculture input companies, banks and financial institutions, insurance companies, farming enterprises, seed manufacturing companies and government bodies respectively.Smart farming is a network of interdisciplinary and complementing technologies and facilities. The components of smart farming are best if they comprise of the following:This is generally the database where all chunks of data from multiple sensors and resources are gathered, stored, analyzed, and retrieved for actions. An optimized management information system should offer information on:Crops:SoilClimateTechnology is what puts the smart in smart farming and the following make up the network:Technologies like Machine Learning, Data Analytics, and Big Data for the entire process and setup to make senseAlso referred to as Site-specific Crop Management System or Satellite Farming, this is a concept in farming that relies on observation, measurement, and response to various inbound and outbound requirements in agricultural fields. The primary vision of precision farming is to optimize RoI and preserve resources by allowing farmers and landlords to take optimized and informed decisions from the available field data. Precision agriculture fosters an environment where farmers can zero-down precise locations in their fields for the spatial availability of several resources like water availability, topography, soil fertility, organic matter, nitrogen levels, moisture content, the presence of magnesium, potassium, and more.Complemented by services and features like GPS devices, sensors that are even capable of measuring chlorophyll levels, drones, and satellite imagery, precision agriculture offers a treasure chest of information for farmers.The science of agriculture is a very complex field and is interdisciplinary. It uses the fundamentals of chemistry, physics, math, statistics, biology and economics and business management. The scope of the agriculture scene in India is still in its developing stage and requires niche experts with versatile skill sets to bring about changes. The role of a data scientist in agriculture is very similar to that of in the roles and responsibilities in other industry. However, exposure to wings like plant science, plant biotechnology, soil science and animal science will help aspiring data scientists to create an impact in the field and allow them to make more sense out of the clusters of unstructured data from multiple resources. As far as the technologies deployed in agriculture analytics is concerned, you can divide the requirements of them into the following:Though the technologies are efficient, proven to work, and revolutionary, one of the major challenges lies in their application in the Indian agriculture sector. Problem is not when there are no technological solutions to farming concerns, but not having a proper application of them is a bigger concern. Mr. Hemendra Mathur, Managing Director, SEAF India Investment Advisor, sharedthat his interactions with farmers from Himachal, Madhya Pradesh, and Rajasthan made him come to a conclusion that farmers today are ready to embrace the new technologies for better farm economics. However, there is the need to educate them on risk mitigation and potential upsides probable with the use of data. He shares a real-time incident wherein a pilot project conducted by an ag-data company made farmers realize that their estimation of their farm areas were completely different from the estimation arrived at after geo-tagging and that they were able to work on input application better after knowing information to the points.This also makes us realize the amount of hours of training that has to be given to the farmers in the implementation of these technologies in everyday farming. This involves training on the use of the devices, basic troubleshooting, use of data, use of smartphones and app, and more. The concerns dont end there. Problems like infrastructure, need for uninterrupted power and internet connectivity, and finance to deploy the technology are always a concern.Though opportunities for data analytics in the field of agriculture in India are aplenty, the best use case of it is yet to happen. With the visions of those like Abhishek Raju, Jyoti Vaddi, Raman Singh Saluja, and others gradually taking shape, we can be sure that in the coming years, farmers will see better days in the farm and their harvest.Shweta Gupta, is currently VP, Digital Vidya, contributing towards building technology equipped youth for solving problems in the era of data as part of Digital Vidyas mission in building skills and reskilling the existing workforce in the technology fields and specifically Data Science.She has 19+ years of Technology Leadership experience, hold a patent and number of publications in ACM, IEEE and IBM journals like Redbook and developerWorks. She has been speaker at technology events like IBM Commerce Global conference (Amplify), Regional Technical Leadership Exchange, Society of Women Engineer (SWE).",https://www.analyticsvidhya.com/blog/2018/05/data-analytics-in-the-indian-agriculture-industry/
Yellowbrick  A set of Visualization Tools to Accelerate your Model Selection Process,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Data Science in the Indian Agriculture Industry|NLP Architect  An Awesome Open Source NLP Python Library from Intel AI Lab (with GitHub link)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So youve built a bunch of models for your project (or hackathon). A few of them are even giving you a really good AUC-ROC score. But how do you go about choosing one? Sure you can get into the nitty gritty of it but wouldnt it help if you could just visualize them and pick the best one from there?Yellowbrick is a suite of visualization tools that extend the scikit-learn API to enable acceleration and ease of model selection for data scientists. In simple words, this Python package combines the power of scikit-learn with the capabilities of matplotlib to generate intuitive visualizations of your models.This set of tools are called Visualizers, which are basically objects that learn from the data and then create visualizations so you can gain a deeper insight into the model selection process. Some of the most popular visualizers are listed below:Yellowbrick is compatible with Python 2.7 as well but the developers recommend using Python 3.5 to utilise this package to its maximum potential. Of course you will also need to install scikit-learn and matplotlib as well. To install Yellowbrick, use the pip method:To see example of Yellowbrick in action and to replicate what the developers have demonstrated, head over to the GitHub page here.I love this package! Adding visualization to any part of the data science process always helps in my opinion. We have previously covered the ANN visualiser which illustrates the artificial neural network you build and we have also seen the deep learning process being visualized.Yellowbrick will really help you, as a data scientist, dig deeper into your model understanding and also save you time by portraying the process in the form of a neat diagram. I urge you to try out this library and let us know your experience using it.",https://www.analyticsvidhya.com/blog/2018/05/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process/
NLP Architect  An Awesome Open Source NLP Python Library from Intel AI Lab (with GitHub link),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Yellowbrick  A set of Visualization Tools to Accelerate your Model Selection Process|Rivalling Google Duplex, Microsofts XiaoIce is an AI that has already made Millions of Phone Calls|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you noticed how common chatbots have become lately? Almost all business websites seem to have a chatbot tucked away on their home page. This, along with other multiple and diverse examples are applications of Natural language processing. Its potential is seemingly unlimited and the general perception is that we are only just scratching the surface!So it comes as no surprise that the big tech giants like Google, Facebook and Intel (among others) have opened research divisions to explore this field. The latest NLP offering, called NLP Architect comes from the Intel AI Lab.NLP Architect is an open source Python library that enables data scientists and developers to explore state-of-the-art deep learning techniques in the field of natural language processing (NLP) and natural language understandings (NLU). According to Intel, this library includes their past and currently ongoing research and development efforts.The existing version of NLP Architect includes features which aim to provide support for both research and practical applications. These features are:Below is the framework of the library:The library comes with the NLP Architect Server, which has been designed with the aim of making predictions across different models in NLP Architect. This server includes a visualizer that shows you your models annotations in a pretty neat way. Check out an example below:Various popular open source frameworks have been leveraged in the NLP Architect repository:You can check out their GitHub library here to get a taste of what you can do with Intels resources.Last month we saw the folks at Intel AI Lab open sourcing nGraph, a framework neutral tool that allows data scientists to focus on their data science work, rather than worrying about hardware related limitations. This research arm of Intel is proving to be a truly incredible boon for the machine learning community as a whole.This library will help both beginners and advanced machine learning developers. For beginners, it helps by providing ready-made examples you can replicate and for advanced users, you can incorporate it into your existing frameworks (if possible) or build your next project using it.In future releases, the researchers are planning to implement and show the use cases of this library on real-life applications of their customers. Im looking forward to data scientists from the Analytics Vidhya community utilizing this library and building applications of their own!",https://www.analyticsvidhya.com/blog/2018/05/nlp-architect-an-awesome-open-source-nlp-python-library-from-intel-ai-lab-with-github-link/
"Rivalling Google Duplex, Microsofts XiaoIce is an AI that has already made Millions of Phone Calls",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NLP Architect  An Awesome Open Source NLP Python Library from Intel AI Lab (with GitHub link)|Comprehensive Beginners Guide to Jupyter Notebooks for Data Science & Machine Learning|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"WhenGoogle Duplex was unveiled a couple of weeks back, the world was left stunned at how far machine learning and AI had come. It was a truly transcendent moment and everyone was left wondering and debating whether only Google had the keys to this type of technology.As it turns out, Google wasnt alone. At Microsofts AI Innovate event this week, Microsoft presented a similar AI chat bot, Xiaoice, that can talk and sound exactly like a human. They were just 2 weeks late to the party.Microsofts project, called XiaoIce was initially launched in August 2017. By April, XiaoIce could speak and listen at the same time, similar to what humans do. The company has been testing this social chat bot with millions of users in China. At the conference, the team announced some exciting new developments and features for this technology. Below is a summary of the important points that were covered at the event:According to a blog post by Harry Shum, Microsofts Executive Vice President for Artificial Intelligence and Research, the AI chatbot had over 30 billion conversations with 100 million friends and has talked with over 600,000 people on the phone,since last August.XiaoIce was also demonstrated live during the conference. Microsofts AI called a user and had a very natural sounding conversation. Check out the below video (the conversation is in Chinese with English translation), which will give you an essence of the capabilities of this AI.The chatbot is only available in China at this time and has become incredibly popular with more than 500 million users already!Another week in machine learning, and another flagship product comes along by a tech giant. The race to be the biggest and leading name in the ML and AI world is heating up.Microsoft mentioned that they are working with a non-profit organization to create the largest audio-book library with Xiaoice for children who are blind or visually impaired. This is a wonderful use case of machine learning being used for good.From the ethical standpoint, questions still remain. Google Duplex has been slammed in recent days by people who are concerned that it will be used for creating fake voices. Keeping this in mind, Micrsosoft specifically mentioned that when it uses this AI to place calls, it ensures that the person on the other end of the line knows that he/she is talking to an AI.Whats your take on this latest Microsoft product? Does it excite or scare you? Let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/05/microsoft-unveils-ai-bot-challenges-google-duplex/
Comprehensive Beginners Guide to Jupyter Notebooks for Data Science & Machine Learning,"Learn everything about Analytics|Introduction|Table of Contents|What is a Jupyter Notebook?|How to install Jupyter Notebook|Getting Started!|Using Jupyter Notebooks Magic Functions|Not just limited to Python  Use R, Julia and JavaScript within Notebooks|Interactive Dashboards in Jupyter Notebooks  Why not?|Keyboard Shortcuts Save time and become even more productive!|Useful Jupyter Notebook Extensions|Saving and Sharing your Notebook|JupyterLab  The evolution of Jupyter Notebooks|Best Practices|End Notes","Anaconda|The pip method|Learn, train,compete, hackandget hired!|Share this:|Related Articles|Rivalling Google Duplex, Microsofts XiaoIce is an AI that has already made Millions of Phone Calls|Mode is a Free and Intuitive Collaboration Platform for Every Data Scientist (Supports SQL, Python and R!)|
Pranav Dar
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common question people ask is which IDE / environment / tool to use, while working on your data science projects. As you would expect, there is no dearth of options available  from language specific IDEs like R Studio, PyCharm to editors like Sublime Text or Atom  the choice can be intimidating for a beginner.If there is one tool which every data scientist should use or must be comfortable with, it is Jupyter Notebooks (previously known as iPython notebooks). Jupyter Notebooks are powerful, versatile, shareable and provide the ability to perform data visualization in the same environment.Jupyter Notebooks allow data scientists to create and share their documents, from codes to full blown reports.They help data scientists streamline their work and enable more productivity and easy collaboration. Due to these and several other reasons you will see below, Jupyter Notebooks are one of the most popular tools among data scientists.In this article, we will introduce you to Jupyter notebooks and deep dive into its features and advantages.By the time you reach the end of the article, you will have a good idea as to why you should leverage it for your machine learning projects and why Jupyter Notebooks are considered better than other standard tools in this domain!Are you ready to learn? Lets begin!Jupyter Notebook is an open-source web application that allows us to create and share codes and documents.It provides an environment, where you can document your code, run it, look at the outcome, visualize data and see the results without leaving the environment. This makes it a handy tool for performing end to end data science workflows data cleaning, statistical modeling, building and training machine learning models, visualizing data, and many, many other uses.Jupyter Notebooks really shine when you are still in the prototyping phase. This is because your code is written in indepedent cells, which are executed individually. This allows the user to test a specific block of code in a project without having to execute the code from the start of the script. Many other IDE enviornments (like RStudio) also do this in several ways, but I have personally found Jupyters individual cells structure to be the best of the lot.As you will see in this article, these Notebooks are incredibly flexible, interactive and powerful tools in the hands of a data scientist. They even allow you to run other languages besides Python, like R, SQL, etc. Since they are more interactive than an IDE platform, they are widely used to display codes in a more pedagogical manner.As you might have guessed by now, you need to have Python installed on your machine first. Either Python 2.7 or Python 3.3 (or greater) will do.For new users, the general consensus is that you should use the Anaconda distribution to install both Python and the Jupyter notebook.Anaconda installs both these tools and includes quite a lot of packages commonly used in the data science and machine learning community. You can download the latest version of Anaconda from here.If, for some reason, you decide not to use Anaconda, then you need to ensure that your machine is running the latest pip version. How do you do that? If you have Python already installed, pip will already be there. To upgrade to the latest pip version, follow the below code:Once pip is ready, you can go ahead and install Jupyter:You can view the official Jupyter installation documentation here.Weve now learned all about what these notebooks are and how to go about setting them up on our own machines. Time to get the party started!To run your Jupyter notebook, simply type the below command and youre good to go!Once you do this, the Jupyter notebook will open up in your default web browser with the below URL:http://localhost:8888/treeIn some cases, it might not open up automatically. A URL will be generated in the terminal/command prompt with the token key. You will need to copy paste this entire URL, including the token key, into your browser when you are opening a Notebook.Once the Notebook is opened, youll see three tabs at the top: Files, Running and Clusters. Files basically lists all the files, Running shows you the terminals and notebooks you currently have open, and Clusters is provided by IPython parallel.To open a new Jupyter notebook, click on the New option on the right-hand side of the page. Here, you get four options to choose from:In a Text File, you are given a blank slate. Add whatever alphabets, words and numbers you wish. It basically works as a text editor (similar to the application on Ubuntu). You also get the option to choose a language (there are a plethora of them given to you) so you can write a script in that. You also have the ability to find and replace words in the file.In the Folder option, it does what the name suggests. You can create a new folder to put your documents in, rename it and delete it, whatever your requirement.The Terminal works exactly like the terminal on your Mac or Linux machine (cmd on Windows). It does a job of supporting terminal sessions within your web browser. Type python in this terminal and voila! Your python script is ready to be written.But in this article, we are going to focus on the notebook so we will select the Python 3 option from the New option. You will get the below screen:You can then start things off by importing the most common Python libraries: pandas and numpy. In the menu just above the code, you have options to play around with the cells: add, edit, cut, move cells up and down, run the code in the cell, stop the code, save your work and restart the kernel.In the drop-down menu (shown above), you even have four options:The developers have inserted pre-defined magic functions that make your life easier and your work far more interactive. You can run the below command to see a list of these functions (note: the % is not needed usually because Automagic is usually turned on):Youll see a lot of options listed and you might even recognise a few! Functions like %clear, %autosave, %debug and %mkdir are some you must have seen previously. Now, magic commands run in two ways:As the name suggests, line-wise is when you want to execute a single command line while cell-wise is when you want to execute not just a line, but the entire block of code in the entire cell.In line-wise, all given commands must started with the % character while in cell-wise, all commands must begin with %%. Lets look at the below example to get a better understanding:Line-wise:Cell-wise:I suggest you run these commands and see the difference for yourself!And the magic doesnt stop there. You can even use other languages in your Notebook, like R, Julia, JavaScript, etc. I personally love the ggplot2 package in R so using this for exploratory data analysis is a huge, huge bonus.To enable R in Jupyter, you will need the IRKernel (dedicated kernel for R) which is available on GitHub. Its a 8 step process and has been explained in detail, along with screenshots to guide you, here.If you are a Julia user, you can use that within Jupyter Notebooks too! Check out this comprehensive article which is focused on learning data science for a Julia user and includes a section on how to leverage it within the Jupyter environment.If you prefer working on JavaScript, I recommend using the IJavascript kernel. Check out this GitHub repository which walks you through the steps required for installing this kernel on different OS. Note that you will need to have Node.js and npm installed before being able to use this.Before you go about adding widgets, you need to import the widgets package:The basic type of widgets are your typical text input, input-based, and buttons. See the below example, taken from Dominodatalab, on how an interactive widget looks like:You can check out a comprehensive guide to widgets here.Shortcuts are one of the best things about Jupyter Notebooks. When you want to run any code block, all you need to do is press Ctrl+Enter. There are a lot more keyboard shortcuts that Jupyter notebooks offer that save us a bunch of time.Below are a few shortcuts we hand picked that will be of immense use to you, when starting out. I highly recommend trying these out as you read them one by one. You wont know how you lived without them!A Jupyter Notebook offers two different keyboard input modes  Command and Edit. Command mode binds the keyboard to notebook level commands and is indicated by a grey cell border with a blue left margin. Edit mode allows you to type text (or code) into the active cell and is indicated by a green cell border.Jump between command and edit mode using Esc and Enter, respectively. Try it out right now!Once you are in command mode (that is, you dont have an active cell), you can try out the below shortcuts:When in edit mode (press Enter when in command mode to get into Edit mode), you will find the below shortcuts handy:To see the entire list of keyboard shortcuts, press H in command mode or go to Help > Keyboard shortcuts. Keep checking this regularly as new shortcuts are added frequently.Extensions are a very productive way of enhancing your productivity on Jupyter Notebooks. One of the best tools to install and use extensions I have found is Nbextensions. It takes two simple steps to install it on your machine (there are other methods as well but I found this the most convenient):Step 1: Install it from pip:Step 2: Install the associated JavaScript and CSS files:Once youre done with this, youll see a Nbextensions tab on the top of your Jupyter Notebook home. And voila! There are a collection of awesome extensions you can use for your projects.To enable an extension, just click on it to activate it. I have mentioned 4 extensions below that I have found most useful:These are just some of the extensions you have at your disposal. I highly recommend checking out their entire list and experimenting with them.This is one of the most important and awesome features of a Jupyter Notebook. When I have to do a blog post and my code and comments are in a Jupyter file, I need to first convert them into another format. Remember these notebooks are in json format and that isnt really helpful when it comes to sharing it. I cant go about posting the different cells blocks in an email or on the blog, right?Go to the Files menu and youll see a Download As option there:You can save your Notebook in any of the 7 options provided. The most commonly used is either a .ipynb file so the other person can replicate your code on their machine or the .html one which opens as a web page (this comes in handy when you want to save the images embedded in the Notebook).You can also use the nbconvert option to manually convert your notebook into a different format like HTML or PDF. You can also use jupyterhub, which lets you host notebooks on its server and share it with multiple users. A lot of top notch research projects use this for collaboration.JupyterLab was launched in February this year and is considered the evolution of Jupyter Notebooks. It allows a more flexible and powerful way of working on projects, but with the same components that Jupyter notebooks have. The JupyterLab environment is exactly the same as a Jupyter Notebook, but with a more productive experience.JupyterLab enables you to arrange your work area with notebooks, terminals, text files and outputs  all in one window! You just have to drag and drop the cells where you want them. You can also edit popular file formats like Markdown, CSV and JSON with a live preview to see the changes happening in real time in the actual file.You can see the installation instructions here if you want to try it out on your machine. The long term aim of the developers is for JupyterLab to eventually replace Jupyter notebooks. But that point is still a bit further away right now.While working alone on projects can be fun, most of the time youll find yourself working within a team. And in that situation, its very important to follow guidelines and best practices to ensure your code and Jupyter Notebooks are annotated properly so as to be consistent with your team members. Here I have listed down a few best practices pointers you should definitely follow while working on a Jupyter Notebook:Another bonus tip! When you think of creating a presentation, the first tools to come to mind are PowerPoint and Google Slides. Nut your Jupyter Notebooks can create slides too! Remember when I said its super flexible? I wasnt exaggerating.To convert your Notebook into slides, go to View -> Cell Toolbar and click on Slideshow. Boom! Each block of code now displays a Slide Type drop-down option on the right. You will get the below 5 options:
Play around with each option to understand it better. It will change the way you present your code!Do note that this is not an exhaustive list of things you can do with your Jupyter notebook. There is so much more to it and you pick these things up the more you use it. The key, as with so many things, is experimenting with practice.Check out this GitHub repository which contains a collection of fascinating Jupyter Notebooks.This guide is just the starting point in your data science journey and Im glad you are taking it with me! Let me know your take on Jupyter Notebooks and how they have helped you in the comments section below. Also, if you have any questions  let me know!",https://www.analyticsvidhya.com/blog/2018/05/starters-guide-jupyter-notebook/
"Mode is a Free and Intuitive Collaboration Platform for Every Data Scientist (Supports SQL, Python and R!)",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Comprehensive Beginners Guide to Jupyter Notebooks for Data Science & Machine Learning|Big Data and no Processing Power? Leverage Googles Cloud TPU to Accelerate ML Tasks|
Pranav Dar
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Collaboration is key in data science. Its a team game and being able to share your model with your team or clients is imperative in any data science project. GitHub is of the most popular collaboration platforms leveraged by data scientists these days but it can be a little intimidating to learn for a newcomer. Mode, a browser-based data science collaboration platform by Mode Analytics, has been designed to work in such a way that it streamlines with the way a data scientists thinks. It allows you to connect to almost any type of database, analyse the dataset with the tool of your choice, and makes sharing a breeze.Talking about databases, Mode has a remarkably diverse set of options that allow you to connect and get your data from anywhere. When you sign up on the site (which is free, by the way), the first option you will see is a list of databases which you can connect to, in order to import your data. Microsoft Azure SQL, Amazon Redshift, Hive, Oracle, MySQL and SQL Server are just some of the many options available. Check out the below image to see the list in full:When it comes to actually getting into the data science tasks, like data cleaning and preparation, visualization, building your model and validation, Mode offers a variety of languages to do this. SQL is ideal for initial data collection, while R and Python are of course the go-to model building tools for most data scientists these days.Mode also gives you the option to build charts and dashboards in a matter of seconds. Apart from the usual popular data visualization libraries like Rs ggplot2 and pythons matplotlib, Mode has its own drag-and-drop tool that you can use if coding is not to your taste.One of the things that makes Mode unique is that every project you create comes with its own URL. This makes sharing with your team, or a client, a whole lot easier. The analysis that you perform will stay on the same URL regardless of updates and modifications done over time.If youre new to data science, or dont know a specific language or tool, the platform also gives you plenty of free tutorials to learn from. The team has also posted a few examples that you can clone so you dont need to build or start from scratch. And as we mentioned, its free!Check out the below video from the company exploring Mode:I took Mode for a spin and was pretty impressed with the overall platform. Its really lightweight since its a browser-based platform and its pretty intuitive. The diverse database options and switching between tools is obviously a unique selling point for Mode. I especially liked the SQL and python learning resources.They have recently added R support as well last month so we are expecting to see them launch a learning platform for this language as well. Do let us know in the comments below if you are planning to use this awesome tool.",https://www.analyticsvidhya.com/blog/2018/05/mode-analytics-data-science-platform-sql-r-python/
Big Data and no Processing Power? Leverage Googles Cloud TPU to Accelerate ML Tasks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Mode is a Free and Intuitive Collaboration Platform for Every Data Scientist (Supports SQL, Python and R!)|Exciting updates from Analytics Vidhya  Launch of Trainings and DataHack platform update!|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the biggest challenges in machine learning is finding enough computational resources to deal with the data at hand. When you take part in ML competitions, you must have faced the same problem when the data set size went into gigabytes. Standard machines do not have the capability to handle such large amounts of data. This is where GPUs and TPUs have come into play recently, but they can be expensive as well.So to eliminate this issue for data scientists and smaller organizations, Google has announced that it will be offering TPU support for its Cloud ML platform. This will accelerate training machine learning and deep learning models, without having to sacrifice on precision and accuracy. The Cloud TPU quota is available for all current Google Cloud Platform customers to use right now.Google first launched its Cloud ML catalogue back in early 2017 as a managed TensorFlow service and has seen its demand and popularity grow exponentially. Since that release, the team behind this technology has released many other features, like support for NVIDIAs V100 GPUs, improvements to the hyperparameters feature, among other things.These Google Cloud TPUs are still in beta mode but are available for training virtually any machine learning model. According to Googles blog post, these TPUs enable you to train a variety of high-performance, open-source reference models with differentiated performance per dollar. Or, you can choose to accelerate your own models written with high-level TensorFlow APIs.To add to the jewels already in Cloud TPUs crown, it recently came top inImageNets Training Cost category of Stanfords DAWNBench competition. In second place? Also Googles Cloud TPU. At the current pricing, this is one of the cheapest options available in the market compared to all competitors at its level (like AWS).This release by Google should help small to medium sized organizations take better advantage of machine learning capabilities. One of biggest positives of Googles Cloud ML platform is that it handles all the hardware part  like the infrastructure, computation resources, scheduling, etc. on your behalf.As a data scientist, this is a dream scenario where you can narrow down your focus on dealing with data and developing your models. If you are still wondering if Googles platform is worth it for ML tasks, check out these 2 real-life examples of how it has produced better results than any other tool currently around. Incredible, isnt it?",https://www.analyticsvidhya.com/blog/2018/05/google-adds-tpu-cloud-machine-learning-platform/
Exciting updates from Analytics Vidhya  Launch of Trainings and DataHack platform update!,Learn everything about Analytics|Introduction||Launch of Analytics Vidhya training marketplace|Current Courses in Trainings Marketplace:|Updates to the DataHack platform|Get Started!,"Why create this marketplace?|What can you expect from our trainings marketplace?|Free vs. Paid Courses:|Share this:|Like this:|Related Articles|Big Data and no Processing Power? Leverage Googles Cloud TPU to Accelerate ML Tasks|NVIDIAs Deep Learning AI Trains Robots to Copy and Execute Human Actions|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A lot has changed at Analytics Vidhya over last few years.We transformed from a blog into a community portal. Our small office is buzzing with activities throughout the day (and night). We delivered Indias largest data science conference  DataHack Summit 2017 and are making it bigger this year.But, one thing has stayed same throughout our journey. This is our commitment to provide world class learning resources to our community. If there is one thing which defines Analytics Vidhya  it is the amount of learning we enable for our community and the impact we have created in their professional careers.Today, I am excited to announce a couple of updates which will take this learning to the next level.Given our focus on enabling learning and our ability to create / differentiate high quality content  building a trainings platform was only a matter of time.There is no dearth of trainings available today. But, when it comes to high quality trainings  there are only a few of them available.This is why we decided to create a marketplace where each course is vetted by our standard of creating content. We understand how to enable learning and we will use our knowledge and experience to make sure that every course in this marketplace is vetted for quality.We plan to offer a mix of free and paid training courses. Some of these courses will be created by Analytics Vidhya, others by our community members and partners. Every course in our market place will be vetted for quality by Analytics Vidhya.Each course will cover multiple real life industry problems and would enable you to become better in data science.If any course falls below our quality threshold  we will not launch it.There are a few differences between our Free and Paid courses. Here is a brief summary of them:There are a range of training courses available on the marketplace today  some free, some paid. All our courses today have an average rating of 4.5 or above on a scale of 1  5.Here is the current spread of courses:Free Courses:Paid Courses:Invite only courses:All the paid courses are running under introductory pricing for the next 10 days. So, if you plan to buy one  make sure you buy them before end of this month.In addition to the launch of trainings, we have made a few changes in our DataHack platform. The idea is to help you learn more from each of the competitions we host.Here are the changes in our DataHack platform:Excited? We are excited as well. Looking forward to hear from you on what you think about these changes. Also, if you have any suggestions / feedback  let us know.Till then  Happy Learning!",https://www.analyticsvidhya.com/blog/2018/05/analytics-vidhya-trainings-and-datahack-platform-update/
NVIDIAs Deep Learning AI Trains Robots to Copy and Execute Human Actions,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Exciting updates from Analytics Vidhya  Launch of Trainings and DataHack platform update!|12 Frequently Asked Questions on Deep Learning (with their answers)!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Hollywood, meet deep learning. With each passing week, we see AI reach closer to our level of intelligence. No longer are those Terminator movies looking like a pipe dream or a hallucination. Robots are getting smarter, thanks to the breakthroughs achieved by deep learning.NVIDIA has developed a deep learning system that enables robots to learn and teach themselves, simply by observing human actions. In the initial demonstration, we were shown how robots detected objects (coloured boxes and a toy car in this case), picked them up and moved them.The deep learning framework consists of a series of neural networks that have been built to perform perception of objects, generation of programs and their execution as well. In other words, the system observes a humans action, learns from them, and then replicates it by itself. Its like a sci-fi movie come to life!These neural networks were trained on NVIDIAs Titan X GPUs. The below flow chart illustrates how the system learns and executes the actions it sees:The researchers have also made their research paper available to the public and you can view it here. The paper will be presented this week by the researchers at a robotic and automation conference in Brisbane, Australia.Watch the below video, released by NVIDIA, that demonstrates this remarkable system in action:I feel we are seeing a real cornerstone point in AI right now. Thanks to GPUs and TPUs, organizations (at least the big ones), can essentially use tons of data to train and test deep learning algorithms. The gaming community has been key in training many popular algos recently and this system might see a use there as well.This latest study has applications not just for house tasks (like picking and moving stuff around the house), but can also be used in elderly homes to assist people, in the manufacturing industry and even to do environmental friendly tasks. As a data scientist (or an aspiring one), you should go through the above mentioned research paper to understand the structure and logic of how NVIDIA built this model.",https://www.analyticsvidhya.com/blog/2018/05/nvidias-deep-learning-ai-trains-robots-to-copy-and-execute-human-actions/
12 Frequently Asked Questions on Deep Learning (with their answers)!,Learn everything about Analytics|Introduction|Table of Contents|What is Deep Learning and why is it so popular these days?|Is Deep Learning just a hype or does it have real-life applications?|What is the difference between Deep Learning and Machine Learning?|What are the prerequisites for starting out in Deep Learning?|Do I need to do a PhD to make a career in Deep Learning?|Which Tools/Languages should I prefer to build Deep learning models?|Why are GPUs necessary for building Deep Learning models?|When (and where) to apply Neural Networks ?|Do we need a lot of data to train deep learning models?|Where can I find basic project ideas in order to practice deep learning?|What are some of the free learning resources for Deep Learning?|What are some Deep Learning interview questions?|What is the future of Deep learning?|End Notes,"Learn,compete, hackandget hired!|Share this:|Like this:|Related Articles|NVIDIAs Deep Learning AI Trains Robots to Copy and Execute Human Actions|Meet the first Machine Learning Algorithm that Completely Controls your Facial Expressions|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"From Facebooks research to DeepMinds legendary algorithms, deep learning has climbed its way to the top of the data science world. It has led to amazing innovations, incredible breakthroughs, and we are only just getting started!However if you are a newcomer to this field, the word deep might throw you into doubt. Deep learning is one of the hottest topics of this industry today, but it is unfortunately foreign and cryptic to most people. A lot of people carry an impression that deep learning involves a lot ofmathematics and statistical knowledge.If you had similar questions about deep learning, but were not sure how, when and where to ask them  you are at the right place. This article should answer most of what you would want to know.By end of this article, we will dispel a few myths about deep learning and answer some widely asked questions about this field. We have also included plenty of resources to get you started.Here is the exciting part  It isnt as difficult as most people make it out to be. Read on to find more!Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in the recent years. This is because of the fact that Deep Learning shows great analogy with the functioning of the human brain. The superiority of the human brain is an evident fact, and it is considered to be the most versatile and efficient self-learning model that has ever been created.Let us understand the functioning of a deep learning model with an example:What do you see in the above image?The most obvious answer would be a car, right? Despite the fact, that there is sand, greenery, clouds and a lot of other things, our brain tags this image as one of a car. This is because our brain has learnt to identify the primary subject of an image. This ability of deriving useful information from a lot of extraneous data is what makes deep learning special. With the amount of data that is being generated these days, we want our models to be better with more of this data being fed into it. While deep learning models get better with the increase in the amount of data.Now although Deep Learning has been around for many years, the major breakthroughs from these techniques came just in the recent years. This is because of two main reasons  the first and foremost, as we saw before, is the increase of data generated through various sources. The infographic below succinctly visualizes this trend. The second is the growth in hardware resources required to run these models. GPUs, which are becoming a requirement to run deep learning models, are multiple times faster and they help us build bigger and deeper deep learning models in comparatively less time than we required previously.This is the reason that Deep Learning has become a major buzz word in the data science industry.Deep Learning has found many practical applications in the recent past. From Netflixs famous movie recommendation system to Googles self-driving cars, deep learning is already transforming a lot of businesses and is expected to bring about a revolution in almost all industries. Deep learning models are being used from diagnosing cancer to winning presidential elections, from creating art and writing literature to making real life money. Thus it would be wrong to say that it is just a hyped topic anymore.Some major applications of deep learning that are being employed by technology companies are:However, some people develop a thinking that deep learning is overhyped because of the fact that labeled data required for training deep learning models is not readily available. Even if the data is available, the computational power required to train such models does not come cheap. Hence, due to these barriers, people are not able to experience the power of deep learning and term it as just hype.Go through the following blog to build some real life deep learning applications yourself:This is one of the most important questions that most of us need to understand. The comparison can be done mainly on the below three verticals:Data dependenciesThe most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms dont perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly. On the other hand, traditional machine learning algorithms with their handcrafted rules prevail in this scenario. Below image summarizes this fact.Feature engineeringFeature engineering is a process of putting domain knowledge into the creation of feature extractors to reduce the complexity of the data and make patterns more visible to learning algorithms to work. This process is difficult and expensive in terms of time and expertise.In Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.For example, features can be pixel values, shape, textures, position and orientation. The performance of most of the Machine Learning algorithm depends on how accurately the features are identified and extracted.Deep learning algorithms try to learn high-level features from data. This is a very distinctive part of Deep Learning and a major step ahead of traditional Machine Learning. Therefore, deep learning reduces the task of developing new feature extractor for every problem. Like, Convolutional NN will try to learn low-level features such as edges and lines in early layers then parts of faces of people and then high-level representation of a face.InterpretabilityLast but not the least, we have interpretability as a factor for comparison of machine learning and deep learning. This factor is the main reason deep learning is still thought 10 times before its use in industry.Lets take an example. Suppose we use deep learning to give automated scoring to essays. The performance it gives in scoring is quite excellent and is near human performance. But theres is an issue. It does not reveal why it has given that score. Indeed mathematically you can find out which nodes of a deep neural network were activated, but we dont know what there neurons were supposed to model and what these layers of neurons were doing collectively. So we fail to interpret the results.On the other hand, machine learning algorithms like decision trees give us crisp rules as to why it chose what it chose, so it is particularly easy to interpret the reasoning behind it. Therefore, algorithms like decision trees and linear/logistic regression are primarily used in industry for interpretability.If you would like to learn about a more in-depth comparison between machine learning and deep learning, I recommend you go through the following blog:Starting out in deep learning is not as difficult as people might make you believe. There are a few elementary basics that you should cover before diving into deep learning. Deep learning requires knowledge of the following topics:For a more detailed understanding about the prerequisites please follow:No, a PhD is not a mandatory requirement to make a career in deep learning. You can learn, experiment, and build up your work experience portfolio without going to university. The emphasis for any job or role is usually on demonstrating your competence, and not on your degree, per se.Having said that, a PhD in a specific field (like linguistics for NLP) will definitely accelerate your path if you choose to combine that with deep learning.I would recommend you use Python, because of its robust ecosystem for machine learning. The python ecosystem comprises of developers and coders who are providing open source libraries and support for the community of python users. This makes the task of writing complex codes for various algorithms much easier and the techniques easier to implement and experiment with. Also, Python being a more generalized programming language, can be used for both the development and implementation. This greatly simplifies the transition from development to operations. That is, a deep learning product that can predict the price of flight tickets, can not only be developed in python but can also be attached with your website in the same form. This is what makes Python a universal language.Besides this, I would suggest that beginners use high level libraries like Keras. This makes experimentation easier by providing abstraction to the unnecessary information that is hidden under the algorithms. And giving access to the parameters that can be tweaked to enhance the performance of such models. Let us understand this with an example:When you press the buttons on a television remote, do you need to care about the background processes that are happening inside the remote? Do you need to know about what signal is being sent out for that key, or how is it being amplified? No, right? Because maybe an understanding of these processes is required for a physicist but for a lame man sitting in his bedroom, it is just an information overload.There are also other contenders apart from Python in the deep learning space such as R, Julia, C++, and Java. For alternatives of libraries, you can check out TensorFlow, Pytorch, Caffe2, DL4J, etc. We should stay updated with their developments as well.If you are not well versed with programming, there are also a few GUI based softwares, that require no coding, to build deep learning models, such as Lobeor Googles AutoML, among others.When you train a deep learning model, two main operations are performed:In forward pass, input is passed through the neural network and after processing the input, an output is generated. Whereas in backward pass, we update the weights of neural network on the basis of error we get in forward pass.Both of these operations are essentially matrix multiplications. A simple matrix multiplication can be represented by the image belowHere, we can see that each element in one row of first array is multiplied with one column of second array. So in a neural network, we can consider first array as input to the neural network, and the second array can be considered as weights of the network.This seems to be a simple task. Now just to give you a sense of what kind of scale deep learning  VGG16 (a convolutional neural network of 16 hidden layers which is frequently used in deep learning applications) has ~140 million parameters; aka weights and biases. Now think of all the matrix multiplications you would have to do to pass just one input to this network! It would take years to train this kind of systems if we take traditional approaches.We saw that the computationally intensive part of neural network is made up of multiple matrix multiplications. So how can we make it faster?We can simply do this by performing all the operations at the same time instead of doing it one after the other. This, in a nutshell, is why we use GPU (graphics processing units) instead of a CPU (central processing unit) for training a neural network.Deep Learning have been in the spotlight for quite some time now.Its deeper versions are making tremendous breakthroughs in many fields such as image recognition, speech and natural language processing etc.Now that we know it is so impactful; the main question that arises is when to and when not to apply neural networks? This field is like a gold mine right now, with many discoveries uncovered everyday. And to be a part of this gold rush, you have to keep a few things in mind:It is true that we need a large amount of data to train a typical deep learning model. But we can generally overcome this by using something called transfer learning. Let me explain thoroughly.One of the barrier for using deep learning models for industry applications is where the data is not in huge amount. A few examples of data needed to train some of the popular deep learning models are:However, a deep learning model trained on a specific task can be reused for different problem in the same domain even if the amount of data is not that huge. This technique is known as Transfer Learning. For instance, we have a set of 1000 images of cats and dogs labeled as 1 and 0 (1 for cat and 0 for dog) and we have another set of 500 test images that we need to classify. So, instead of training a deep learning model on the data of 1000 images, we can use a pre-trained VGGNet model and retrain it on our data and use it to classify the unlabeled set of images. A pre-trained model may not be 100% accurate in your application, but it saves huge efforts required to reinvent the wheel.You may have a look at this article to get a better intuition of using a pre-trained model.
To practice deep learning, ideas alone will not help. We also need labeled data to test our ideas using deep learning.You can also refer this list of exciting deep learning datasets and problems.Being a comparatively newer technology, there is not enough content and tutorials available for the beginners. However, free-quality content and resources related to deep learning are steadily increasing. The learning resources can be classified on the different applications of deep learning.Besides this, you can also go through the following blogs for a more extensive list of resources:Some common questions that may be asked on deep learning are:Do note that this is not an exhaustive list that will make you completely ready for an interview.You can go through the following skill test to test yourself on important questions on deep learning.Deep learning has come a long way in recent years, but still has a lot of untapped potential. We are still in the nascent stages of this field, with new breakthroughs happening seemingly every day. One of the use-cases that we can definitely see in the suture is of automobile industry, where Deep Learning can revolutionize it by making self-driving cars a reality. While we dont have a crystal ball to predict the future, we can see deep learning models requiring less and less involvement from human data scientists and researchers.In the immediate future, we can definitely see a trend where the knowledge of deep learning will be a skill required by every Data Science practitioner. In fact, you must have caught sight of a job position spurn out recently, called a Deep Learning Engineer. This person is responsible to deploy and maintain Deep Learning models used by various departments of that company. Needless to say, there will be a huge demand of such people in the industry.Currently, one of the limitations of DL is that it does what a human asks of it. It requires tons of data to learn its target objective, and replicates that. This has induced bias in certain applications. We can see this improving over time such that the bias is eliminated in the training process.We might even stop differentiating deep learning from the other types of learning, with time. It is primed to become a popular and commonly used field and will not require special branding efforts to market or sell.There are a lot of cases still where researchers, after training a DL model, are unable to explain the why behind it. Its producing great results but why did you tune a hyperparameter a certain way? Hopefully with the rapid advancement in DL, we will see this black box concept becoming history, and we can explain the intuition behind the decision it takes.These are broadly the answers to the most frequently asked questions on our portal or elsewhere by the people who want to jump onto this Deep Learning bandwagon.Do you have any other questions on deep learning that you need clarification on? Use the comments section below to ask; or hop onto our Discussion portal, and we will help you out!",https://www.analyticsvidhya.com/blog/2018/05/deep-learning-faq/
Meet the first Machine Learning Algorithm that Completely Controls your Facial Expressions,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|12 Frequently Asked Questions on Deep Learning (with their answers)!|Announcing DataHack Summit 2018  Bengaluru, 22  25 November 2018|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence is a wonderful thing, if applied correctly. It has diverse applications and it is well and truly transforming our lives in a positive way (like healthcare). But there can be certain applications, like the one you will read about below, that are a mix between genius and scary. They have the potential to be game changing, and only time will tell if itll be a good or bad thing.These researchers are the first to have successfully transferred the full 3 dimensional head pose, expressions, eye motions, etc. of a face, into the face of a different actor.. The results are simply mind blowing.How does this approach work? When a video is given as as the input, the algorithm first tracks the source and target actor(s) using a facial reconstruction approach. The resulting output represents dimensions such as the pose of the head, the facial expressions and the motion of the eyes. The level of detail is simply staggering.At the core of the algorithm is a generative neural network with a space-time architecture. The realism in the end result has been achieved through adversarial training. This approach only requires a few minutes of training on the input video source.With the ability to freely recombine source and target parameters, the developers have been able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background.The researchers then compared their algorithm to previous approaches and showed how well their algorithm works. Previous efforts in this regard have involved recreating facial expressions but they pale in comparison to this study. No one yet had successfully been able to blend the background, and reconstruct head poses along with minute details like eye blinking, etc.Additionally, the algorithm allows you to interactively edit the input video as well. Change the shape of the face, expand the facial expressions, remove the hair, close one eye, among other things.To visually see how this algorithm works, check out the below video released by the developers:There are two sides to consider here  one from the machine learning perspective and one from the ethical point of view. From the ML view, this is a truly awesome and novel approach. The fact that machines can now fully reconstruct 3D facial expressions with such minute details is truly awesome. From film making to medical imaging, this can potentially be a very useful tool.From the ethics side, this is a scary prospect. With the amount of fake news and fake videos doing the rounds recently, this could make things worse. Time will tell how this approach is received and applied in practical real-life cases. What is your take on this? Are you excited or scared by its implications? Use the comments section to tell us your thoughts.",https://www.analyticsvidhya.com/blog/2018/05/meet-first-ai-controls-peoples-facial-expressions-live-videos/
"Announcing DataHack Summit 2018  Bengaluru, 22  25 November 2018","Learn everything about Analytics|Imagine a world, where|DataHack Summit 2018  The Next-Gen AI, ML & IOT Conference|DataHack Summit 2018 by numbers|What can you expect in DataHack Summit 2018|Mind Zones at DataHack Summit 2018|Trending Topics for DataHack Summit 2018","What do you need to do to attend DataHack Summit 2018?|Share this:|Like this:|Related Articles|Meet the first Machine Learning Algorithm that Completely Controls your Facial Expressions|DeepMinds Recurrent Neural Network Explores the role of Dopamine for Machine Learning|
Kunal Jain
|13 Comments",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"the next gen data scientists are building intelligent bots, droids, humanoids to unleash a new era in human civilisation.A world where we are doing cutting-edge research in latest technologies including Artificial Intelligence, Machine Learning, Deep Learning and Reinforcement learning. That research is reaching the industry and is being applied to create products which were never seen before.A world where technological innovations and their applications will enable us to solve problems impacting millions of people in a manner people could not even imagine a few years ago.So, what do we need to build this world?We need to Build The Next-Gen Data Science EcosystemThis is the world we dream at Analytics Vidhya. This is the world which we are creating. We are building this step by step and we will not rest till we reach there.DataHack Summit 2017 was the first time we saw this ecosystem coming together in India. We saw Dr. Kirk Borne laying out latest industry trends in his Keynote. We saw practitioners presenting the impact of Artificial Intelligence and Machine Learning in Cyber Security, Bio genomics, Automotives, Financial Services and several other industries. We saw multiple hands-on workshops teaching the latest tools and techniques to our community members.This year at DataHack Summit 2018  we are not only dreaming bigger, but we are also planning to showcase you the world where Humans meet Artificial Intelligence.We will showcase the latest industry case studies, the latest research in AI and live hack sessions building Artificial Intelligence in front of you. You will see machines becoming smarter in front of your eyes and some even doing things better than humans in specific tasks.Book your tickets NOW!DataHack Summit 2018 is happening on 22nd  25th November 2018 in Bengaluru. It will be bigger, better and more impactful than DataHack Summit 2017 by every means and aims to take the ecosystem to a different level. Make sure you block these dates now!DataHack.AI: From witnessing human-like capabilities in machines to building deep learning predictive models for an intelligent future, this zone will surely amaze you with solutions that exhibit human-like behaviour and intelligence.DataHack.DL: Take a deeper dive intomost significant innovations ofcoming generations with this zone  self-driving cars, self-organizing drone swarms, computer vision, conversational interfaces, gene editing, emotion recognition, and more.DataHack.ML: Ground-breakingresearch and applications of algorithms, tools, and platforms related to analyzing massive data sets, to be discussed in this zone.DataHack.IOT: Migrate to the zone to learn about IoT ecosystem, including cloud and edge computing, data analytics, sensor networks, mobile devices, Internet architecture, middleware, and IoT applications.For now, block your calendar and make sure that you subscribe to the updates here. We will share more details about the Summit in the coming weeks.What do you feel about DataHack Summit 2018  make sure you tell us through comments below or by using #DHS2018 on any social media platform. Looking forward to hearing from you.Tickets are available and you can book them NOW!",https://www.analyticsvidhya.com/blog/2018/05/announcing-datahack-summit-2018-bengaluru/
DeepMinds Recurrent Neural Network Explores the role of Dopamine for Machine Learning,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Announcing DataHack Summit 2018  Bengaluru, 22  25 November 2018|Google Champions NLP by using Neural Networks to Help you Write Emails|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machines have already started outperforming humans in some tasks, like classifying images, reading lips, forecasting sales, curating content, among other things. But there is a caveat attached to it  they require tons and tons of data to learn and train the model. Some of the best algorithms, like DeepMinds AlphaGo, take a lot of data and hundreds of hours to understand the rules of a video game and master it. Humans can usually do this in one sitting.DeepMinds latest research aims to figure out how it can get machines to learn something in a few hours, replicating human behavior. The researchers behind this study believe that it might have something to do with dopamine, the brains pleasure signal. Dopamine has been associated with the reward prediction error signal used in AI reinforcement learning algorithms. These systems learn to act by trial and error, guided by the reward.The researchers propose that dopamines role includes helping us to learn efficiently and rapidly, all the while allowing flexibility in case the context changes. They tested their theory by recreating six meta-learning experiments from the field of neuroscience. Each experiment required the model to perform tasks that use the same underlying skills. They did this by training a recurrent neural network (RNN) using standard reinforcement learning methods. Finally, the team compared their results to real world data collected from the neuroscience experiments mentioned above.So what did the team find out? Upon running an experiment known as the Harlow Experiment, they found out that their model learned to pick up things remarkably quickly, in a manner resembling animals (2 monkeys, in this case). They figured out that the model was able to adapt to previously unseen situations really quickly.The majority of the learning for the model took place within the RNN, which confirmed their initial theory  that dopamine plays a far more pivotal role in the meta-learning process than we previously imagined.You can read more about the experiments on DeepMinds blog hereand read the entire research paper here.This isnt the first effort in this field but its the first to produce a solid theory towards our understanding of how machines can replicate a human brain. If a machine stops requiring massive amounts of data to learn, it will be quite an achievement  imagine the time and money itll save organizations and data scientists worldwide. Plus the fact that the time to learn will come down significantly, from days to a mere few hours.This study also goes to show how ripe the field of medicine can be for deep learning to be applied. What are your thoughts on this research? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/deepminds-rnn-aims-learn-human-understanding-works/
Google Champions NLP by using Neural Networks to Help you Write Emails,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DeepMinds Recurrent Neural Network Explores the role of Dopamine for Machine Learning|19 Data Science and Machine Learning Tools for people who Dont Know Programming|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A future where machines book appointments, write emails, and do other tasks for us  it is no more the talk of the future. The future is here and Google is leading the way with a slew of new products and services, backed by the awesome power of machine learning.Google debuted its latest NLP development  Smart Compose  at last weeks Google I/O conference. Its a Gmail feature that uses machine learning to predict the next words you are going to write and offers sentence completion suggestions accordingly. The aim is to help users write emails faster so they can focus on their daily work, rather than be stuck in the black hole of their inbox.To develop such a technology, Googles AI team faced three key challenges:Typical NLP techniques like n-grams, Bag of Words (BoW), and RNN (recurrent neural networks) were considered in order to learn and predict the next word that the user might type, based on the previous word. But this does not really add context to the sentence and the chance of getting it wrong is pretty high. So the team included the subject of the email as well as the email trail to understand the full context of the message.The team also tried a sequence-to-sequence model but it failed to meet their strict latency constraints even though it did well with predictions. Speed of prediction So they combined two models  the bag of words and the RNN-LM, which was a significant improvement on the sequence-to-sequence model in terms of speed. Below is the structure of the final RNN model:Once the final approach to model building was decided, the team then had to tune the requisite hyperparameters and do the most crucial part of any system  train the model. Google has a distinct advantage over most other companies in this regard  they have tons and tons of data to experiment with and train their models on, especially when it comes to NLP. They used billions of examples and to accelerate the training phase, they turned to the TPUv2 Podwhich managed to finish the task in less than a day!They had initially tested on a standard CPU but it gave them an average latency of hundreds of milliseconds, a time they deemed too long and costly for this task. By moving the majority of the computations power to TPUs, the average latency time came down to tens of milliseconds.Next time you use Smart Compose, you might appreciate the work that went into building those sequence of words!I really appreciate that Google reveals the technology behind its products. It gives data scientists and others aspiring to get into this field a good idea of how a leading company structures a problem statement and goes about building solutions for it.Time will tell how well Google has done with the bias problem. It has been inherent in a lot of technologies recently and they cannot afford to let it ruin a perfectly good product. Their advantage lies in the amount of data they have collected over the years, which should definitely help in recognizing and eliminating bias.",https://www.analyticsvidhya.com/blog/2018/05/google-using-neural-networks-help-write-emails-smart-compose/
19 Data Science and Machine Learning Tools for people who Dont Know Programming,Learn everything about Analytics|Introduction|List of Tools|RapidMiner|DataRobot|BigML|Google Cloud AutoML|Paxata|Trifacta|MLBase|Auto-WEKA|Microsoft Azure ML Studio|MLJar|Amazon Lex|IBM Watson Studio|Automatic Statistician|More Tools|End Notes,"Share this:|Like this:|Related Articles|Google Champions NLP by using Neural Networks to Help you Write Emails|This Artificial Intelligence Model Trains Itself based on its own Dreams|
Aarshay Jain
|48 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published on 5 May, 2016 and updated with the latest tools on May 16, 2018.Programming is an integral part of data science. Among other things, it is acknowledged that a person who understands programming logic, loops and functions has a higher chance of becoming a successful data scientist. But, what about those folks who never studied programming in their school or college days?Is there no way for them to become a data scientist then?With the recent boom in data science, a lot of people are interested in getting into this domain. but dont have the slightestidea about coding. In fact, I too was a member of your non-programming league until I joined my first job. Therefore, I understand how terrible it feels when something you have never learned haunts you at every step.The good news is that there is a way for you to become a data scientist, regardless of your programming skills! There are tools that typically obviate the programming aspect and provide user-friendly GUI (Graphical User Interface) so that anyone with minimal knowledge of algorithms can simply use them to build high quality machine learning models.Many companies (especially startups) have recently launched GUI driven data science tools. I have tried to cover a few important ones in this article and provided videos as well, wherever possible.Note:All the information provided is gather from open-source information sources.We are just presentingsome facts and not opinions. In no manner do we intent to promote/advertise any of the products/services.RapidMiner(RM) wasoriginally started in 2006as an open-source stand-alone software named Rapid-I. Over the years, they havegiven it the name of RapidMiner and also attained ~35Mn USD infunding. The toolis open-source for oldversion (below v6) butthe latest versions come ina 14-day trialperiod and licensed after that.RM covers the entire life-cycle of prediction modeling, starting from data preparation to model building and finally validation and deployment. The GUIis based on ablock-diagram approach, something very similar to Matlab Simulink. There are predefined blocks which act as plug and play devices. You just have to connect themin the right manner and a large variety of algorithms can be runwithout a single line of code. On top of this, they allow custom R and Python scripts to be integrated into the system.There current product offerings include the following:RM is currently being used in various industries includingautomotive,banking,insurance, life Sciences, manufacturing, oil and gas, retail, telecommunicationand utilities.DataRobot (DR) is a highly automated machine learning platform built by all time bestKagglers including Jeremy Achin, Thoman DeGodoyandOwen Zhang. Their platformclaims to have obviated the need for data scientists. This is evident froma phrase from their website  Data science requires math and stats aptitude, programmingskills, and business knowledge. With DataRobot, you bring the business knowledge and data, and ourcutting-edge automationtakes care of the rest.DR proclaims to have the following benefits:BigMLprovides a good GUI which takes the user through 6 steps as following:These processes will obviously iterate in different orders.The BigML platform provides nice visualizations of results and has algorithms for solvingclassification,regression,clustering,anomaly detectionandassociation discovery problems. They offer several packages bundled together in monthly, quarterly and yearly subscriptions. They even offer a free package but the size of the dataset you can upload is limited to 16MB.You can get a feel of how their interface works using their YouTube channel.Cloud AutoML is part of Googles Machine Learning suite offerings that enables people with limited ML expertise to build high quality models.The first product, as part of the Cloud AutoML portfolio, isCloud AutoML Vision. This service makes it simpler to train image recognition models.It has a drag-and-drop interface that lets the user upload images, train the model, and then deploy those models directly on Google Cloud.Cloud AutoML Vision is built on Googlestransfer learningandneural architecture searchtechnologies(among others). This tool is already being used by a lot of organizations. Check out this article to see two amazing real-life examples of AutoML in action, and how its producing better results than any other tool.Paxatais one of the feworganizations which focus on data cleaning and preparation, and not the machine learning or statistical modeling part. It is an MS Excel-like application that is easy to use. It also provides visual guidance making it easy to bring together data, find and fix dirty or missing data, and share and re-use data projects across teams. Like the other tools mentioned in this article, Paxata eliminates codingorscripting, hence overcoming technical barriers involved in handling data.Paxata platform follows the following process:Praxata has set its foot in financial services, consumer goods and networking domains. It might be a good tool touse if your work requires extensive data cleaning.Trifacta is another startup with a heavy focus on data preparation. It has 3 product offerings:Trifacta offers a very intuitive GUI for performing data cleaning. It takes data as input and provides a summary with various statistics by column. Also, for each column it automatically recommends some transformations which can be selected using a single click.Various transformations can be performed on the data using some pre-definedfunctions which can be called easily in the interface.Trifacta platform usesthe following steps of data preparation:Trifacta is primarily used in the financial, life sciences and telecommunication industries.MLBase is an open-source project developed by AMP (Algorithms Machines People) Lab at the University of California, Berkeley. The core idea behind this is to provide an easy solution for applying machine learning to large scale problems.It has 3 offerings:Auto-WEKAis a data mining software written in Java, developed by the Machine Learning Group at the University of Waikato, New Zealand. It is a GUI based toolwhich is very good for beginners in data science. The best part about it is that it is open-source and the developers have provided tutorials and papers to help you get started. You can learn more about it in AVs article.It is primarily used for educational and academic purposes for now.Driverless AIDriverless AI is a magical platform for enterprises from h2o.ai that supports automatic machine learning. A 1 month trial version is available as a docker image at this link. All you have to do is using simple dropdowns select the files for train, test and mention the metric using which you want to track model performance. Sit back and watch as the platform with an intuitive interface trains on your dataset to give excellent results at par with a good solution an experienced data scientist can come up with.These are some mindblowing features of Driverless AIWhen there are so many big name players in this field, how could Microsoft lag behind? The Azure ML Studio is a simple yet powerful browser based ML platform. It has a visual drag-and-drop environment where there is no requirement of coding. They have published comprehensive tutorials and sample experiments for newcomers to get the hang of the tool quickly. It employs a simple five step process:MLJar is a browser based platform for quickly building and deploying machine learning models. It has an intuitive interface and allows you to train models in parallel. It comes with built-in hyper-parameters search and makes deploying your model easier. MLJar offers integration with NVIDIAs CUDA, python, TensorFlow, among others.You only need to perform three steps to build a decent model:Currently the tool works on a subscription plan. It has a free plan as well with a 0.25GB dataset limit. Its definitely worth checking out.Amazon Lex provides an easy-to-use console for building your own chatbot in a matter of minutes. You can build conversational interfaces in your applications or website using Lex. All you need to do is supply a few phrases and Amazon Lex does the rest! It builds a complete Natural Language model using which a customer can interact with your app, using both voice and text.It also comes with built-in integration with the Amazon Web Services (AWS) platform.Amazon Lex is a fully managed service so as your user engagement increases, you dont need to worry about provisioning hardware and managing infrastructure to improve your bot experience.How could we leave out IBM Watson from this list? It is one of the most recognizable brands in the world. IBM Watson Studio provides a beautiful platform for building and deploying your machine learning and deep learning models. You can interactively discover, clean and transform your data, use familiar open source tools with Jupyter notebooks and RStudio, access the most popular libraries, train deep neural networks, among a a vast array of other things.For people just starting out in this field, they have provided a bunch of videos to ease the introductory phase. You can choose to take a free trial and check out this awesome tool by yourself. The above video guides you through how to create a project in Watson Studio.Automatic Statistician is not a product per se but a research organization which is creating a data exploration and analysis tool. It can take in various kinds of data and uses natural language processing at its core to generate a detailed report. It is beingdeveloped by researcherswho have worked in Cambridge and MIT and also won Googles Focussed Research Award with a price of $750,000.It is still under active development but its one to keep an eye on in the near future. You can check out a few examples of how the final reports pan out here.If youre hearing a lot of these names for the first time, you wont be the only one! The market for automated machine learning is expanding as more and more data is collected. Will they flood the market in the next few years? Time will tell. But these are excellent tools to assist organizations that are looking to start out with machine learning or are looking for alternate options to add to their existing catalogue.In this article, we have discussed various initiativesworking towards automatingvarious aspects of solving a data science problem. Some of them are in a nascent research stage, some are open-source and others are already being used in the industry with millions in funding. All of these pose a potential threat to the job of a data scientist, which is expected to grow in the near future. These tools are best suited for people who are not familiar with programming & coding.Do you know any other startups or initiatives working in this domain? Please feel free to drop a comment below and enlighten us!",https://www.analyticsvidhya.com/blog/2018/05/19-data-science-tools-for-people-dont-understand-coding/
This Artificial Intelligence Model Trains Itself based on its own Dreams,Learn everything about Analytics|Overview|Introduction|Our take on this,"How does the algorithm work?|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|19 Data Science and Machine Learning Tools for people who Dont Know Programming|Launching Student DataFest 2018  The Largest Student Machine Learning Festival|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A tennis player, on the receiving end of a booming 150km/hr serve, has milliseconds to decide which way the ball is coming, how high itll bounce, and how he/she wants to swing the racket so as to make it go where he/she wants. The player predicts all these things subconsciously, based on the images the brain generates.We have a tendency of creating a mental image of the world around us, based on events that are perceived by our limited senses. The decisions we make and the actions we take are built around these mental models. There is a VAST amount of information that we intake every single day; we observe something and proceed to remember an abstract version of it. Think about this for a minute  it is true for all of us.Two researchers, David Ha and Jurgen Schmidhuber, have developed an AI model that not only plays video games with awesome accuracy, but also has the ability to conjure up new scenarios (or dreams), learn from them, and then apply them on the game itself. The model can be trained in an unsupervised manner to learn the spatial and temporal representation of the environment. The model was trained to play a car racing game and the VizDoom game.The researchers trained a deep neural network (RNN) to deal with Reinforcement Learning tasks, by dividing the agent into two parts: a large world model and a small controller model. To start with, they trained a large neural network so it could learn a model of the AIs world in an unsupervised manner, and then trained the smaller controller model to perform tasks using the previously built model. Below is the structure of the final model:In the racing game, on a selection of 100 randomly selected tracks, the average score of the model was almost three times higher than that of DeepMinds initial Deep Q-Learning algorithm!You can view the official research paper here and check out the GitHub library here.This concept is wonderfully explained in the below video:A truly jaw dropping application of AI. We are getting closer and closer towards machines imitating humans. Its quite similar to DeepMinds AlphaZero (which has basically come the benchmark for similar algorithms) but this AI does its training in an unsupervised manner. It is ideal for games where the rules are complex, and not straightforward.Most deep learning models need gigantic data sets to be trained on and an algorithm like this really brings the data size down, consequently saving a ton of money for the researchers/organizations. And obviously, since it learn by itself, it takes a lot of the human effort out of the equation as well.I highly recommend checking out all the resources we have provided in this article.",https://www.analyticsvidhya.com/blog/2018/05/ai-dreams-scenarios-trains-itself/
Launching Student DataFest 2018  The Largest Student Machine Learning Festival,Learn everything about Analytics|Introduction|How can I participate in Student DataFest 2018?|What will I get to learn in Student DataFest 2018?|The Ultimate Student Hackathons|Prizes and Career Opportunities|Are there any pre-requisites?,"Share this:|Like this:|Related Articles|This Artificial Intelligence Model Trains Itself based on its own Dreams|This Neural Network can Replicate our Brains Navigation System, with Human Level Accuracy!|
Kunal Jain
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"All of a sudden, data is everywhere and growing exponentially. Data Scientist has become THE MOST sought after job of the 21st century. The demand is so high that Gartner predicts there will be a shortfall of 100,000 data scientists in the job market by 2020.But there are just not enough data literate people around. The gap between what the industry demands of data scientists and the skills & knowledge that students out of college currently have is huge. This is where Analytics Vidhya is making a big difference.We are happy to announce the launch of Student DataFest 2018, the ultimate learning guide for students looking to step into the world of Data Science. The aim of the Student DataFest 2018 is to help students bridge the gap between their college education and industry expectations. Student DataFest 2018 will enable students to learn, engage, compete and get hired in this super competitive world of data science.Are you ready to step up your game and carve a niche for yourself in the demanding yet utterly fulfilling world of data science? Enrol in the Student DataFest today and take your first step towards becoming a data scientist!You need to be a student* to participate in the Student DataFest 2018. Additionally, students who get their profiles verified will be able to avail our comprehensive Introduction to Data Science course and get listed on the hackathon leaderboard (more on these in the next sections). So, what are you waiting for? Get your profile verified here!*This includes undergraduates, students currently enrolled in a Masters course and research studentsAfter your student profile is verified, you will get online access to the Introduction to Data Science course, specially curated for students and taught by experienced instructors from Analytics Vidhya. This course is being offered to all verified students free of cost. Dont miss out on this golden opportunity to learn one of the hottest subjects in todays global job market!The online course consists of three modules  Python, Statistics and Modelling. These modules will be taught over a span of 8 weeks. You will also be tested on your knowledge after each module, from MCQs to challenging skilltests.During the Student DataFest 2018, you will also get the chance to hear from industry leaders. We will be hosting a series of webinars where you will understand the data science landscape from the industrys perspective, as well as gain an understanding of how to do well in machine learning competitions. Also, you will get to learn how to crack data science interviews with top companies.The buck doesnt just stop at learning; for the resolute and strong-willed individuals, the ultimate test comes in the form of 2 hackathons  The Data Identity and The Data Supremacy!The Data Identity will test your data science learning to the limit and enable you to innovate and find new solutions. Its the perfect case of learning through practice. This challenge will help you understand how far youve come and how much you still have to learn!The Data Supremacy will be the ultimate battleground for aspiring data scientists! The aim of this hackathon is to let your imagination guide you to push the boundaries of your knowledge and skills in this field. Use your data science skillset to hack and innovate and climb your way to the top of the student leaderboard rankings on Analytics Vidhya. By the end of this competition, your appetite for data science will have grown exponentially!Remember, the private leaderboard will only be available for verified students so dont forget to enrol yourself here.There are plenty of prizes on offer adding to a total amount of Rs 1,50,000 just in cash prizes! The winners of the learning hackathon, The Data Identity, will take home Rs. 50,000, while the gladiators of the ultimate student hackathon, The Data Supremacy, stand to win Rs. 1 lakh!The most promising students will also be given internship interview opportunities with Analytics Vidhya and some of the other top companies working in the field of data science.Perform well on the MCQs and skilltests of the Introduction to Data Science course, and you will also receive a certificate from Analytics Vidhya!None! Just ensure you have a working computer and an internet connection  we will guide you during the rest of this amazing journey. If you have any questions regarding this event, please use the comments section below to let us know. Happy learning and happy winning!",https://www.analyticsvidhya.com/blog/2018/05/launching-student-datafest-2018-largest-student-machine-learning-festival/
"This Neural Network can Replicate our Brains Navigation System, with Human Level Accuracy!",Learn everything about Analytics|Overview|Introduction,"|Our take on this|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Launching Student DataFest 2018  The Largest Student Machine Learning Festival|Qure.ai uses Machine Learning to Detect Brain Anomalies in less than 10 seconds|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Finding a shortcut to anything has always fascinated human beings. From saving time to cutting down on budgets, finding the shortest path to the end goal is quite often the ultimate aim. But what if machines could do that for us, with the same level of accuracy and precision that we manage?It might be a very real possibility now.According to a report presented in Nature last week by researchers at DeepMind, a navigational artificial intelligence system can explore complex simulated environments (like a maze) and find the shortest route to an end destination, like humans and animals! It works like a GPS system, and the AI manages to carve a path around the environment (or maze) with remarkable skill and accuracy.The AI consists of an artificial neural network that uses the concept of grid cells. A grid cell is a type of neuron in the brains of many species that allows them to understand their position in space. While learning how to navigate, the neural net spontaneously develops the equivalent of grid cells.For investigating the role of grid cells in navigational functions, the researchers attempted to use deep-learning neural networks. They have also released the approach they followed, which we have summarized below:The researchers have admitted they came upon this algorithm unexpectedly but its ended up being a truly valuable research for the community.AI researchers can use this for improving existing automated navigation systems (maybe self-driving cars, or helping robots). This approach is not only restricted to navigation though. It can also be used for testing theories related to brain functioning, a field which has seen tons of coverage but little breakthrough.",https://www.analyticsvidhya.com/blog/2018/05/neural-network-can-replicate-brains-navigation-system/
Qure.ai uses Machine Learning to Detect Brain Anomalies in less than 10 seconds,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|This Neural Network can Replicate our Brains Navigation System, with Human Level Accuracy!|An Alternative to Deep Learning? Guide to Hierarchical Temporal Memory (HTM) for Unsupervised Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning has made a significant impact in healthcare but the general perception is that there is still a long way to go before we can fully trust machines to make life and death decisions. Its an understandable line of thought, but a few researchers are breaking new ground in their attempts to improve on the existing healthcare technology.One of them is Qure.ai, a company aiming to revolutionize healthcare with the power and assistance of deep learning. When a patient comes in with a head injury, time is of the utmost importance. A few seconds here or there can make all the difference. Qure.ai has developed their algorithms such that they can analyse CT scans of the brain in less than 10 seconds.But these algorithms, as you can imagine, require a lot of data to be built upon. The research team at Qure.ai collected a dataset of 313,318 images of head CT scans plus their medical reports from multiple centres. Out of this data, 21,095 scans were held out to validate the final model. The remaining data points were used to develop the algorithms. The final model has shown a 95% accuracy on the validation set.Qure.ai can also develop automated reports. Their algorithms can localize and quantify haemorrhages and fractures. Put together withbrain anatomy segmentationalgorithms, their machine learning powered system can automatically generate reports, an example of which you can see below:Thefindings from this studywere publishedin a research paper called Development and Validation of Deep Learning Algorithms for Detection of Critical Findings in Head CT Scans. You can also check out their GitHub library here to follow their progress.We have recently seen Googles heart disease andcancer detection algorithm and IBMs psychosis detection algorithms. Qure.ai has put forward a worthy addition to that list. This is quite a useful tool for radiologists. With the high level of true positives, this can certainly be used to assist decisions (rather than replace doctors and assistants, as is the general fear). Anyone in the field of deep learning, or interested in this field, should download their dataset and work on it. It will not only enhance your data science skills but might end up helping the world of healthcare, and consequently, the community as a whole.I highly recommend reading their research paper and then trying out their techniques on this dataset. Let us know your findings and thoughts in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/qure-ai-uses-machine-learning-detect-brain-anomalies-less-10-seconds/
An Alternative to Deep Learning? Guide to Hierarchical Temporal Memory (HTM) for Unsupervised Learning,Learn everything about Analytics|Introduction|Table of Contents|Progress areas of deep learning|Is deep learning really brain-like learning?|Crash course on Neocortex|How is HTM different from deep learning?|Applications of HTM implemented & commercially tested|Working of Hierarchical Temporal Memory (HTM)|Simple python implementation of HTM|So whats next for Numenta?|End Notes,"Trying out API implementation of HTM|Share this:|Like this:|Related Articles|Qure.ai uses Machine Learning to Detect Brain Anomalies in less than 10 seconds|Move Over Photoshop  This Python Script Works like Magic on Low Light Photos (GitHub link included)|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Concept 1 : Sparse Distributed Representation|Concept 2 : Semantic Encoding|How are encoders developed?|Concept 3: Spatial Pooling|Concept 4: Hebbian Learning|Concept 5 : Boosting|Concept 6 : Temporal Memory,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep learning has proved its supremacy in the world of supervised learning, where we clearly define the tasks that need to be accomplished. But, when it comes to unsupervised learning, research using deep learning has either stalled or not even gotten off the ground!There are a few areas of intelligence which our brain executes flawlessly, but we still do not understand how it does so. Because we dont have an answer to the how, we have not made a lot of progress in these areas.If you likedmy previous article on the functioning of the human brain to create machine learning algorithms that solve complex real world problems, you will enjoy this introductory article onHierarchical Temporal Memory (HTM). I believe this is the closest we have reached to replicating the underlying principles of the human brain.In this article, we will first look at the areas where deep learning is yet to penetrate. Then we will look at the difference between deep learning and HTM before deep diving into the concept of HTM, its workings and applications.Lets get into it.Following is a list of the few areas where Deep Learning has a long way to go yet:Deep learning has made some progress in each of the above six directions, but we are far from the end state. Following is what we have achieved through deep learning in each of the six fields:You might have already realized that an Artificial Neural Network does not provide a single solution for all the above skills. And yet our brain uses a common learning algorithm that can take care of all the above attributes. Which brings me to a fundamental question.The human brain has always been the ultimate motivation in the field of deep learning. But we have created a very mathematical formulation to replicate brain functions in form of ANNs and it has not changed over a period of decades. Are we saying that our brain works on such complex mathematical functions without even realizing it?We are pretty sure that our brain does not learn through backpropagation of gradients (which is the basic fundamental principle of deep learning/ANN).Recurrent Neural Network (RNN) architectures are the closest we have reached to our brain in the deep learning space. You can read one of my previous articles on RNNs where I compare them to the human brain. But RNNs are supervised learning models, unlike the brain. So if deep learning is far from replicating the human brain structure, is there anyone trying to replicate brain structure and have they found any success yet?The answer is yes! Numenta, a company founded in 2005, is solely dedicated to replicating the functioning of the human brain and using it in the space of artificial intelligence. Numenta was founded by Jeff Hawking, the man behind Palm Pilot. Nementa has created a framework called Hierarchical Temporal Memory (HTM) that replicates the functioning of the Neocortex, the component of our brain responsible for the real intelligence in humans. I will talk about HTM and its practical applications in this article, butfirst lets do a crash course on Neocortex.Our brain primarily has three parts  Neocortex, Limbic system and Reptilian complex.Simplistic view of a brainThe limbic system supports most of the emotions linked functions, including behavior, motivation and emotional state. Reptilian complex is for all the survival instincts like eating, sleeping etc. Neocortex is the that part of the brain which gives us power to reason and other higher order brain functions like perception, cognition, spatial reasoning, language and generation of motor command.Neocortex is a mammalian development and is almost like a dinner napkin squeezed in our skull. In general, whenever we talk about brain or intelligence in colloquial terms, we are almost always referring to the Neocortex. If we look at the detailed structure of the Neocortex (below diagram; reference  HTM school videos on Youtube), you will see many sections responsible for different tasks.An interesting fact about Neocortex is that the cellular structure throughout all these regions is almost the same, whether it be from the visual processing region or the audio processing region. This finding is extremely important as this means that the brain is trying to solve similar problems to process any kind of sensory data  visual, audio etc. These regions are logically related to each other in a hierarchical structure. We will refer to this hierarchical structure later when we cover HTM.The sensory data is represented as simple ideas in the lower level and the idea gets more abstract in the higher level. A parallel to this process in the deep learning space  the initial layers in neural networks detect simple ideas like edges, intermediate layers detect shapes, and final layers identify objects.Enough of biology, lets now get down to business and talk about HTM models. The best way to initialize your brain with what you are about to learn is by contrasting against a known concept  Deep Learning.As you can see in the image above, the differences between these two approaches are significant. If you have used deep/machine learning before, you will know how hard it is to imagine how a model can work without finding gradients. Hebbian learning is one of the oldest learning algorithms and works on an extremely simple principle  synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs.Before diving into how HTM works, I will give you a flavor of where we can use HTM to solve real world problems. This will give you the motivation to learn more about this novel technique.First, lets try to nail down a few pointers on when can we expect HTM to outperform other learning techniques?:If the answer to all the above questions is yes, HTM is the way to go. Anomaly detection is one such task as it needs action in real time and it is an unsupervised model. Here is the general framework for anomaly detection:Below are few of the use cases that have already been commercially tested:HTM works as follows (dont get scared):Input temporal data generated from various data sources is semanticallyencodedas a sparse array called as sparse distributed representation (SDR). This encoded array goes through a processing called spatial pooling to normalize/standardize the input data from various sources into a sparse output vector or mini-columns (column of pyramidal neurons) of definitive size and fixed sparsity. The learning of this spatial pooling is done through Hebbian learning with boosting of prolonged inactive cells. The spatial pooling retains the context of the input data by an algorithm called temporal memory.For people who did not understand the above language at all, dont worry! I will break it down. The key words have been highlighted in bold and need to be understood first to completely grasp HTM.SDR is simply an array of 0s and 1s. If you take a snapshot of neurons in the brain, it is highly likely that you will only see less than 2% neurons in an active state. SDR is a mathematical representation of these sparse signals which will likely have less than 2% ones. We represent SDR as follows:SDR has a few important properties :We use an encoding engine to take input from an input source and create an SDR. We need to make sure that the encoding algorithm gives us similar SDR for similar objects. This concept is very similar to embedding in the deep learning space. A lot of pre-built encoders are already available online that include numeric encoding, datetime encoding, English word encoding, etc.Lets say we have a simple sequence  1,2,1,2,1,1. The sixth element breaks the sequence, i.e., it should be 2 but the actual value is 1. We will try to understand how HTM pinpoints this anomaly. The first step is semantic encoding. For the purpose of this article, I will use a dense vector as encoded SDR. In real world scenarios, these encoded vectors are extremely sparse.Even though we have a lot of built-in encoders, you might need to create your own encoder for specific problems. I will try to give you a brief introduction of how word encoders are developed.Spatial pooling is the process of converting the encoded SDR into a sparse array complying with two basic principles:So the overlap of both input and output SDR of two similar objects need to be high. Lets try to understand this with our example.The input vector had a sparsity varying from 33% to 67%, but the spatial pooling made sure the sparsity of the output array is 33%. Also the semantics of the two possible inputs in the series are completely different from each other, and the same was maintained in the output vector. How do we use this framework to pinpoint anomalies? We will come back to this question once we cover temporal memory.Learning in HTM is based on a very simple principle. The synapse between the active column in the spatially pooled output array, and active cells in encoded sequence, is strengthened.The synapses between the active column in the spatially pooled output array, and inactive cells in encoded input, is weakened. This process is repeated again and again to learn patterns.Most of the spatial pooling processes will create exceptionally strong columns in the output array which will suppress many columns from contributing at all. In such cases, we can multiply the strength of these weak columns to encoded sequence by a boosting factor. This process of boosting makes sure that we are using a high capacity of the spatially pooled output.Spatial pooling maintains the context of the input sequence by a method called temporal memory. The concept of temporal memory is based on the fact that each neuron not only gets information from lower level neurons, but also gets contextual information from neurons at the same level. In the spatial pooling section, we had shown each column in the output vector by a single number. However, each column in the output column is comprised of multiple cells that can individually be in active, inactive, or predictive state.This mechanism might be a bit complex, so lets go back to our example. Instead of a single number per column in the spatial pooling step, I will now show all cells in the columns of the output vector.Now let me break down the above figure for you.At step 1, our HTM model gets an input 1 for the first time which activates the first column of the output sequence. Because none of the cells in the first column were in predictive mode, we say column 1 goes burst and we assign an active value to each of the cells in column 1. We will come back on how a cell is placed to a predictive state.At step 2, our HTM model gets an input 2 again for the first time in the context of 1, and hence, none of its cells are in predictive state so column 2 goes burst.Same thing happens at step 3, as the model is seeing 1 in context of 2 for the first time. Note that our model has seen 1 before, but it has never seen 1 in context of 2.At step 4, something interesting happens. Our HTM model has seen 2 in context of 1 before, so it tries to make a prediction. (Here I have ignored the cascading context complexity to keep this article simple. Cascading context means 2 in context of 1 in context of 2 and so on. For now, just assume our model has a 2 degree memory that it is able to remember one last step).The method it uses to make this prediction is as follows: It checks with all the cells that are currently active, i.e., column 1, to tell which of the 9 cells do they predict will turn active in the next time step. Say, the synapse between (2,2) cell is stronger with column 1 among (2,1),(2,2) and (2,3), so column 1 unanimously replies (2,2). Now (2,2) is put into a predictive state before consuming our next element of the sequence. Once our next element arrives, which is actually a 2, the prediction goes right and none of the columns burst this time.At step 5, again none of the columns burst and only (1,1) is put in active state as (1,1) had a strong synapse with (2,2).At step 6, the HTM model is expecting a value of 2 but it gets 1. Hence, our first column goes burst and our anomaly is detected in this sequence.The entire algorithm can be overwhelming without visual simulations. So I strongly recommend that you check out the freeonline videos published by Numenta that have some very cool simulations of the process I mentioned above.Numenta Platform for Intelligent Computing (NuPIC) is a machine intelligence platform that implements the HTM learning algorithms. We have NuPIC as one of the importable libraries in Python. The library is not supported by Anaconda yet. A simple implementation of HTM can be found on this link. This is a very well documented code by Numenta . The code starts with Encoding, where you can see how numbers/date/categories can be encoded in HTMs. It then gives examples of spatial pooling and temporal memory with a working example of a predictive model. The code is self-explanatory so I will skip this part to avoid replication of content.One cool way to experience what HTM is capable of doing is to use an API provided by cortical.io . To use this API, go to this link. Here, I will show you a simple example of how can you use the API. When you go to the link, you will see the following screen:You can try any of the tabs as each implementation gives very clear instructions of what kind of input it is expecting. I will show you one tab to help you get going  Term. Once you click on the Term tab, you will see the instructions of using this tab and the output format:All we need to enter is a term, and the API will return terms that are synonyms or associated (you can choose either) to the term. You can also choose to get the fingerprints of all these words. Here are my inputs to get synonyms of cricket:Here is a sample of the response output I get:The words are sorted by the similarity score. The top 5 words that were found similar to cricket were cricket, wickets, cricketers, bowling, wicket. We can also choose to get fingerprints of each of these words. Lets pull the fingerprints of wicket and wickets and see if they are more similar to each other or to the word cricket.In the above table, column 2 and 3 are fingerprints (indices) of the word wicket and wickets. The last column is when the active index of wickets is also found in wicket. The overlap score comes out to be 96, which is far better than the best match of any word with the word cricket (obviously except the word itself). Hence, this API does a good job of mapping these words semantically as SDR.Here is a snapshot of the slide from Jeff Hawkins, showing the pipeline of research:The layers are showing the hierarchy of the cortical tissue. Most of the current research efforts have been focused on the high-order inference layer. Everything covered in this article was related to the high-order inference layer. The second layer in the diagram (labeled as 4) mainly works on sensory-motor inference. This is an important function of the brain where it collaborates between the signals from sensory organs and motor cells to create concepts.For instance, if you move your eyes, the image they capture changes rapidly. If the brain doesnt know what was the cause of this drastic change (which only motor cells can tell), it will fail to simplify the environment around us. However, if we combine the signals from sensory organs and motor cells, the brain can map a stable understanding of the surroundings. If we can master this skill of the brain, we can apply this skill on complex problems like image classification where we have to move our eyes across the picture to understand it in its entirety. This task is similar to what we do in Convolutional Neural Networks.The third layer in the diagram is the capability of the brain that makes it goal oriented, which is something similar to reinforcement learning. With this new skill you can work on complex robotics problems. The last layer is the most complex part where we are talking about putting the entire hierarchy of concept understanding in a place that can be used for multi-sensory modalities that can combine, say, a visual data with an audio data.If I want to put the above paragraph in simple deep learning terms,So who wins between ANN/deep learning and HTM? As of now they are solving very different problems. Deep learning is very specialized for classification problems and HTM are specialized for real time anomaly detection problems. HTM still needs a lot of research to solve problems like image classification etc. that deep learning can solve pretty easily. However, the underlying theory behind HTM looks promising and you should keep this field of research in your radar.If you have any ideas, suggestions or feedback regarding the article, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/05/alternative-deep-learning-hierarchical-temporal-memory-htm-unsupervised-learning/
Move Over Photoshop  This Python Script Works like Magic on Low Light Photos (GitHub link included),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An Alternative to Deep Learning? Guide to Hierarchical Temporal Memory (HTM) for Unsupervised Learning|D-Wave Launches Quadrant  Build High Accuracy Machine Learning Models with Noisy and Unlabelled Data!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have all taken photos from our smartphone cameras in low light  of people, places or food. And then inevitably we turn to filters in order to increase the brightness and add context to the picture. How often has it turned out the way we wanted it to?Thanks to a group of researchers from Intel and the University of Illinois Urbana-Champaign, we now have another approach to turn up the brightness in a low light photo, with stunning accuracy. As you might have guessed by now, deep learning, and more specifically computer vision and pattern recognition, is at the core of this approach.The above example was shared by the researchers on their GitHub page. The results are truly mesmerizing.The algorithm behind this technique has trained the model on how an image, taken in awfully poor light, should be brightened and coloured. In order to build the said model however, the researchers first curated a datasetof 5,094 raw short-exposure low-light images, with the same number of corresponding long-exposure reference images as well.At the core of the algorithm is a convolutional neural network thatoperates directly on raw sensor data and replaces much of the traditional image processing pipelines, which have previously tended to perform poorly on such data. The results have been promising so far.You can read their official research paper here and access the GitHub library here. Check out the below video illustrating this technique:The advantages of this deep learning algorithm are obvious. This will not only help professional photographers in their work but will make data scientists into photographers as well! Instead of spending big bucks on expensive software and working on minute details, you now have the option to just leverage this model and get to work on low level images.Its the perfect algorithm for data scientists getting started in the image processing, computer vision and patter recognition fields.",https://www.analyticsvidhya.com/blog/2018/05/move-over-photoshop-this-python-library-works-like-magic-on-low-light-photos-github-link-included/
D-Wave Launches Quadrant  Build High Accuracy Machine Learning Models with Noisy and Unlabelled Data!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Move Over Photoshop  This Python Script Works like Magic on Low Light Photos (GitHub link included)|Top 6 Artificial Intelligence announcements from Google I/O 2018|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine building a deep learning model, from scratch, that requires minimum data points. Its almost unthinkable. So far, whenever we think of deep learning, we know well require millions of data points to build a good enough model. Im sure most of you who are familiar with DL must have done the cats and dogs classification problem. That model needed tons and tons of images to be trained properly.D-Wave, one of the worlds leading companies in quantum computing software and systems, this week launched a business unit called Quadrant that provides machine learning services to developers for building state-of-the-art deep learning models. Whats unique about this framework is that it can produce high accuracy results on deep neural networks, without requiring much data.This is done by building generative models which combine the flexibility of deep neural nets with probabilistic graphical models. If you do have this data, then thats not an issue. But the real practical challenge is getting this amount of data AND labelling it as well, so the algorithm understands what it has to classify (also called supervised learning). These tasks are time consuming and expensive and usually out of scope for most organizations. For instance, in healthcare to detect a rare disease, you will need a lot of data to generate a high accuracy model. But you wont find many images of this disease. Thats a real problem.Quadrant aims to solve this conundrum. Below is the comparison they have shown of their algorithms against the others currently in use, or now defunct. The difference is stark.The team at D-Wave has developed semi-supervised algorithms that make use of images with noisy labels. These include images found on social media, search engines, etc. and consequently are less expensive to obtain. The algorithms also make use of unlabelled data to train the model and produce amazingly accurate results.Quadrant solutions run on standard GPU-based systems using D-waves quantum technology. As mentioned by the team, D-Wave plans to integrate the Quadrant solutions in hybrid quantum/classical platforms for use with its next generation quantum system, which is currently in development with prototypes being tested.This heralds a breathtakingly refreshing new breakthrough in deep learning. Imagine the uses of such a framework  medical imaging, finance, sports, telecommunications, marketing, advertising, among others. It has the potential to integrate smaller sized organization into the deep learning scene as well.D-Wave has already announced it has partnered with SIemensHealthineers to win first place at theCATARACTS medical imaging grand challenge. The team used Quadrants machine learning algorithms to identify surgical instruments in the given videos, with high accuracy. The possibilities are endless.",https://www.analyticsvidhya.com/blog/2018/05/d-wave-launches-quadrant-ml-service-trains-high-end-models-minimum-data/
Top 6 Artificial Intelligence announcements from Google I/O 2018,Learn everything about Analytics|Overview|Introduction|Smart compose for Gmail|ML Kit|Google Duplex|Suggested Actions for photos|Updates to Google Maps|Google Lens |Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|D-Wave Launches Quadrant  Build High Accuracy Machine Learning Models with Noisy and Unlabelled Data!|Microsoft Interns used AI to Transform the way you use Screenshots on Windows 10|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google is one of the leaders in the Artificial Intelligence field and it has cemented its place at the top of ladder with its mind blowing products at the recently held annual I/O conference. New products like Google Duplex and Google Lens was showcased which left the audience gaping with awe. From updates in Google Maps to improvements for Android, Google has made every effort to make the user experience better for its existing line of products as well.In the article, we have covered the top 6 announcements from the conference.Googles smart reply has made answering emails extremely convenient. The team has added an extension to this amazing feature. Googles G Suite team introduced Smart compose, a new feature powered by AI, which can auto-complete your sentences! As you are typing, the tool can make suggestions for the rest of the sentence. All you need to do is press the tab button and it will complete the sentences (and the entire email) for you. Awesome!Google launched ML Kit, which is a software development kit for app developers on iOS and Android. ML Kit can help developers add artificial intelligence-powered features to their mobile apps, irrespective of the OS. It also allows them to integrate a number of pre-built Google-provided machine learning models into their apps. The models support text recognition, face detection, barcode scanning, image labeling and landmark recognition.
Google Duplex can conduct natural conversations and perform practical and realistic tasks over the phone!At Google I/O 2018, the team presented a demo of the AImaking a call to a real hair salon and having a remarkably human-like conversation to book an appointment. This was one of the most fascinating announcements and we have covered it here. Check out the below video to see the AI in action:The team launched a new feature for Google Photos, called Suggested Actions. This includes AI-driven suggestions in Google Photos which offer you features like brighten, colorize, fix, or share your photos, based on what Google Photos recognizes. For instance, it can spot friends in your photos and lets you to share these photos with them through a one-click option.Google Maps has been updated to provide an even better and intuitive experience to the users. It can alert you with interesting updates near your area, like a festival or a restaurant shutting down, give you a foodie list , or generate a list of popular brunch spots. Not only this, with the help of your phone camera, Google Maps can help you navigate around town, using Street View. The updated Google Maps will be out later this year and will be available for both iOS and Android. Check out the new features in the below video:Google Lens has integrated AI to give users a big pictureabout their environments. It will alsoprovide contextual suggestions for objects in their surrounding environment. For example, scanning a restaurant can show you the menu, pricing, reservations, and timings. It can also help you copy and save text from a book to your phones.Another very interesting feature introduced in Google Lens is Style Match. Ithas been built usingobject-recognition and machine-learning techniques. Once you point your phones camera at an outfit, or accessories, or furniture, the technology will help you buy that item online, and even show similar styles you might like.The Google team has announced some breathtaking updates and we are eagerly waiting for these to be unveiled to the public. They are expected to be out in the next few months. The features offered by Google Lens are beyond imagination. Personally I am really excited to try the new features on Google Maps as well. In the case of Google Duplex, they have also listed down the technology used  at the core is a recurrent neural network.Also, Google has revamped its old Google Research division as Google AI which shows their focus on developing AI products in the future. All Google Research projects and studies on artificial intelligence and machine learning will now be posted here. I encourage you to go through their posts as they have listed downWhat app or service are you most looking forward to using? Get involved in the discussion and let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/top-6-ai-announcements-at-google-io-2018/
Microsoft Interns used AI to Transform the way you use Screenshots on Windows 10,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Top 6 Artificial Intelligence announcements from Google I/O 2018|Generate Quick and Accurate Time Series Forecasts using Facebooks Prophet (with Python & R codes)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine this  your manager asks you to take any product in your organization and add the power of AI to it to make it a commercial release. Thats just what a group of Microsoft interns have done. Challenged by their manager to integrate AI into a widely used tool, these interns took up the challenge and transformed the way you take screenshots on Windows 10.The name of this application is Snip Insights and has been developed under the Microsoft Garage project umbrella. It is an open-source desktop application that enables Windows users to get intelligent insights from a simple screenshot! It uses Cloud AI services to perform many tasks, including converting images to translated text, automatically detecting and tagging your image content, etc.Snip Insights has the capability of taking a scanned image of a document, read it, and convert it into text. Additionally, it can also recognise celebrities, other famous personalities, landmarks and places that are captured within a screenshot. Imagine you see a cool pair of sunglasses online. Instead of wondering where you buy them and searching e-commerce sites, you can take a screenshot and Snip Insights will list down places where you can buy them. How cool is that?When the manager laid down the challenge, the interns were given tons of resources to figure out how to use AI. It was daunting challenge from a technical viewpoint but through trial and error, they managed to bring down their options to 2 tools  Windows Snipping Tool and Snip. They decided to go with the latter as it offered an easy route to building Snip Insights as an independent app. Snip is built on the WPF platform using C#.You can read the official Microsoft blog post about it hereand access their GitHub repositoryhere.While some of these functionalities are offered by Pinterest, Snip Insights offers other features packed into one release. This will save a lot of time and effort for people looking to quickly edit text in an image (or even a PDF!), search for products online, etc. Its a very interesting example of how AI can change the way you use everyday applications.They have also open sourced the project on GitHub in the hope that developers and even students around the world can take inspiration and improve on these efforts.",https://www.analyticsvidhya.com/blog/2018/05/microsoft-added-ai-screenshot-tool-results-mind-blowing/
Generate Quick and Accurate Time Series Forecasts using Facebooks Prophet (with Python & R codes),Learn everything about Analytics|Introduction|Table of Contents|Whats new in Prophet?|The Prophet Forecasting Model|Prophet in action (using Python)|End Notes,"Trend|Seasonality|Holidays and events|R Code|Share this:|Like this:|Related Articles|Microsoft Interns used AI to Transform the way you use Screenshots on Windows 10|10 Must watch videos illustrating Amazing Applications of Artificial Intelligence (AI)|
Ankit Choudhary
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Understanding time based patterns is critical for any business. Questions like how much inventory to maintain, how much footfall do you expect in your store to how many people will travel by an airline  all of these are important time series problems to solve.This is whytime series forecasting is one of the must-know techniques for any data scientist. From predicting the weather to the sales of a product, it is integrated into the data science ecosystem and that makes it a mandatory addition to a data scientists skillset.If you are a beginner, time series also provides a good way to start working on real life projects. You can relate to time series very easily and they help you enter the larger world of machine learning.Prophet is an open source library published by Facebook that is based on decomposable (trend+seasonality+holidays) models. It provides us with the ability to make time series predictions with good accuracy using simple intuitive parameters and has support for including impact of custom seasonality and holidays!In this article, we shall cover some background on how Prophet fills the existing gaps in generating fast reliable forecasts followed by a demonstration using Python. The final results will surprise you!When a forecasting model doesnt run as planned, we want to be able to tune the parameters of the method with regards to the specific problem at hand. Tuning these methods requires a thorough understanding of howthe underlying time series models work. The first input parameters to automated ARIMA,for instance, are the maximum orders of the differencing, the auto-regressive components,and the moving average components. A typical analyst will not know how to adjust theseorders to avoid the behaviour and this is the type of expertise that is hard to acquire and scale.The Prophet package provides intuitive parameters which are easy to tune. Even someone who lacks expertise in forecasting models can use this to make meaningful predictions for a variety of problems in a business scenario.We use a decomposable time series model with three mainmodel components: trend, seasonality, and holidays. They are combined in the followingequation:Using time as a regressor, Prophet is trying to fit several linear and non linear functions of time as components.Modeling seasonality as an additive component is thesame approach taken by exponential smoothing in Holt-Winters technique .We are, in effect, framing the forecasting problem as a curve-fitting exercise rather than looking explicitly at the time based dependence of each observation within a time series.Trend is modelled by fitting a piece wise linear curve over the trend or the non-periodic part of the time series. The linear fitting exercise ensures that it is least affected by spikes/missing data.Saturating growthAn important question to ask here is  Do we expect the target to keep growing/falling for the entire forecast interval?More often than not, there are cases with non-linear growth with a running maximum capacity. I will illustrate this with an example below.Lets say we are trying to forecast number of downloads of an app in a region for the next 12 months. The maximum downloads is always capped by the total number of smartphone users in the region. The number of smartphone users will also, however, increase with time.With domain knowledge at his/her disposal, an analyst can then define a varying capacity C(t) for the time series forecasts he/she is trying to make.ChangepointsAnother question to answer is whether my time series encounters any underlying changes in the phenomena e.g. a new product launch, unforeseen calamity etc. At such points, the growth rate is allowed to change. These changepoints are automatically selected. However, a user can also feed the changepoints manually if it is required. In the below plot, the dotted lines represent the changepoints for the given time series.As the number of changepoints allowed is increased the fit becomes more flexible. There are basically 2 problems an analyst might face while working with the trend component:A parameter called changepoint_prior_scale could be used to adjust the trend flexibility and tackle the above 2 problems. Higher value will fit a more flexible curve to the time series.To fit and forecast the effects of seasonality, prophet relies on fourier series to provide a flexible model. Seasonal effects s(t) are approximated by the following function:P is the period (365.25 for yearly data and 7 for weekly data)Parameters [a1, b1, .., aN, bN] need to be estimated for a given N to model seasonality.The fourier order N that defines whether high frequency changes are allowed to be modelled is an important parameter to set here. For a time series, if the user believes the high frequency components are just noise and should not be considered for modelling, he/she could set the values of N from to a lower value. If not, N can be tuned to a higher value and set using the forecast accuracy.Holidays and events incur predictable shocks to a time series. For instance, Diwali in India occurs on a different day each year and a large portion of the population buy a lot of new items during this period.Prophet allows the analyst to provide a custom list of past and future events. A window around such days are considered separately and additional parameters are fitted to model the effect of holidays and events.Currently implementations of Prophet are available in both Python and R. They have exactly the same features.Prophet() function is used do define a Prophet forecasting model in Python. Let us look at the most important parameters:3.1 Trend parameters3.2 Seasonality & Holiday Parametersyearly_seasonality,weekly_seasonality & daily_seasonality can take values as True, False and no of fourier terms which was discussed in the last section. If the value is True, default number of fourier terms (10) are taken. Prior scales are defined to tell the model how strongly it needs to consider the seasonal/holiday components while fitting and forecasting.Predicting passsenger traffic using ProphetNow that we are well versed with nuts and bolts of this amazing tool. Lets dive into a real dataset to see its potential. Here I have used Prophet in python for one of the practice problems available on datahack platform at this link.The dataset is a univariate time series that contains hourly passenger traffic for a new public transport service. We are trying to forecast the traffic for next 7 months given historical traffic data of last 25 months. Basic EDA for this can be accessed from this course.Import necessary packages and reading datasetWe see that this time series has a lot of noise. We could re-sample it day wise and sum to get a new series with reduced and noise and thereby easier to model.Prophet requires the variable names in the time series to be:So, the next step is to convert the dataframe according to the above specificationsFitting the prophet model:We can look at the various components using the following command:Using the mean hourly fraction for each hour from 0 to 23, we could then convert the daily forecasts into hourly forecasts make submission. This is how our forecasts over the daily data looks like.This gets a score of 206 on the public leaderboard and does produce a stable model. Readers can go ahead and tweak the hyperparameters (fourier order for seasonality/changeover) to get a better score. Reader could also try and use a different technique to convert the daily predictions to hourly data for submission and may get a better score.Implementation in R for the same problem statement is given below.Prophet certainly is a good choice for producing quick accurate forecasts. It has intuitive parameters that can be tweaked by someone who has good domain knowledge but lacks technical skills in forecasting models. Readers can also try and fit Prophet directly over the hourly data and discuss in the comments if they are able to get a better result.",https://www.analyticsvidhya.com/blog/2018/05/generate-accurate-forecasts-facebook-prophet-python-r/
10 Must watch videos illustrating Amazing Applications of Artificial Intelligence (AI),"Learn everything about Analytics|Introduction|Boston Dynamics Helping Robot|Amazons Warehouse Robots|An Autonomous Bike Driving Robot|Googles DeepMind AI Taught itself How to Walk, Run, Jump..|An Interview with Sophia, the Robot|Whats new, Atlas?|Flippy, the Burger Cook|Google Duplex|Amazon Go|End Notes","Share this:|Like this:|Related Articles|Generate Quick and Accurate Time Series Forecasts using Facebooks Prophet (with Python & R codes)|Google Duplex is a Jaw Dropping Application of Natural Language and Audio Processing|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"5 years ago, who would have imagined AI getting this far? Self-driving cars are no longer a figment of our imagination, they are here! As Google Duplex showed recently, machines can now book appointments for us, all the while sounding completely like a human! Hmmhmm!Drones have the ability to deliver orders to our doorstep and robots are taking over manufacturing and shipping functions.The world is well and truly being transformed by the far reaching potential and applications of Artificial Intelligence. From Boston Dynamics robots to Amazons AI solutions, machines are getting smarter, more intuitive and capable of multi-tasking without dropping their efficiency levels.In this article, we will take a look at a list of videos that show the level of intelligence, todays machines are capable of. Keep an open mind because some of these videos are truly incredible.Strap in  this is going to be awesome!This video from Boston Dynamics went viral a few weeks ago. Here we see a robot dog unsure how to get past the door obstacle in front of it. Cue the entry of Boston Dynamics helping robot. It walks in, opens the door, and steps aside to let the other robot inside. Amazing!And recently, the robots robustness was tested. In the below video, the determination of the robot dog to complete its task is simply stunning.Have you always wondered how Amazon stocks its inventories and readies your orders? Wonder no longer. In this video, take a look at how Amazon uses AI robots to pick out the orders and make the e-commerce giant even more ruthlessly efficient.Valentino Rossi is a multiple time MotoGP champion. In this video, a robot called Motobot attempted to beat Rossis lap time. It didnt succeed but the below video is extremely impressive nonetheless.The most remarkable part about the Motobot is that the bike it used is completely unmodified. The entire lap was driven by the robot itself using its six actuators that are able to accelerate and maneuver the bike using all sorts of data points.DeepMind has developed an AI that taught itself all the things about motion  walking, running, jumping and climbing..without any prior guidance! The below video is crazy and amazing in equal measure.Sophia is the most recognizable humanoid robot out there but the way her intelligence has increased since her creation has been an eye-opening experience. In this video, Sophia is interviewed by a Saudi Arabian channel and her responses are intelligent and well crafted sentences.Another Boston Dynamics entry in this list. This robot can perform human movements..probably a tad bit better than actual humans! It understands obstacles and can jump on or over them with ease.Using thermal vision, 3D and computer vision, and machine learning algorithms, Flippy was created by Miso Robotics and has been trained on how to cook and handle the burgers. The deep learning model was built using data about kitchen equipment, the temperatures the grill can heat up to and what the ideal temperature is for cooking the burgers.When the kitchen works put the patties on the grill, Flippy is able to detect where they are. We covered this AI here.How could we leave Google Duplex out of this list? Launched yesterday at the Google IO conference, this AI system makes machines sound incredible human-like. Listen to the the two conversations in the below video to see what Im talking about. It will blow your mind. To understand the technology behind this AI, read our article here.Not quite a robot, but this AI technology is changing the retail and shopping landscape. It is a cashier-less store where customers scan their app at the front of the store, walk in, pick up what they want, and walk out. They get charged once they have left the store. No more wasting time in check-out lines! Its a concept thats been coming for some years and has finally been launched this year.You can read more about it here.AI is becoming more and more human like  basically the vision that was set out when this term was coined. Its a little scary how good machines are getting but its exciting in equal measure. The potential of helping mankind is limitless and with the amount of research going on at the big tech giants like Google, AI will only keeping getting better.What did you think of this list? Are there any other videos or AI applications you know of that the community should see? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/10-videos-machine-intelligence/
Google Duplex is a Jaw Dropping Application of Natural Language and Audio Processing,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|10 Must watch videos illustrating Amazing Applications of Artificial Intelligence (AI)|Andrew Ng Supported Drive.ai Launches its First Self-Driving Car|
Pranav Dar
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",How does this technology work?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you get a call from a digital machine, you can tell it right away. A lot of marketing efforts and service calls are being routed through machines in this way. Im sure you must have had a lot of experience getting these calls (think calling up your bank and taking ages to get through!).But what if you couldnt tell the difference between a humans voice on the phone and a robots? We have seen a lot of improvements in recent years in natural language processing thanks to advancements in deep learning. But it can still be a frustrating experience when the voice on the other end of the line is unable to decipher what youre trying to tell it. We have to adjust for the machine, instead of the machine adjusting for us.                                       Source: AppleinsiderGoogle Duplex is an AI machine intelligence system that bridges this gap. Announced at the Google IO conference yesterday in a stunning demo, it can conduct natural conversations and perform practical and realistic tasks over the phone!The brains behind Google Duplex unveiled this technology with 2 pre-recorded examples  both of around a minute. In the first example, a woman has a conversation with the machine to set up an appointment at a hair salon. It is a truly mind-blowing back and forth conversation  you wont be able to tell the difference between the human and the machine. In the second example, Google Duplex calls up a restaurant to reserve a table. Its incredible technology, it really is.At the heart of Google Duplex is a RNN, or a recurrent neural network. It has been built using TensorFlow Extended. To make the voice behind Duplex sound human-like, the developers used a combination of a text-to-speech engine and asynthesis TTS engine to vary the tone of the machine.Speech disfluencies (um, hmm, etc.) have been added to the AI to make it sound even more human like. The machine can even understand when to give slow responses and when to respond quickly using low-confidence models or faster approximations.The developers have used real-time supervised training to train the system whenever in new domains. This is akin to a teacher instructing a student on a subject with various examples.Google Duplex will be integrated into Google Assistant and rolled out to the public in July. Check out the below video to see the two examples I mentioned above:We have come such a long way in the field of NLP. The days of just analysing sentiments from Tweets feels like ages ago. Audio processing combined with NLP is a truly powerful thing, and Google has tapped into that potential with all its might. The demo at the IO conference floored the audience and it has inspired us as well.Its both scary and inspiring how awesome deep learning married with real life applications can be. What are your thoughts on this mind blowing AI by Google? Use the comments section below to let us know your thoughts!",https://www.analyticsvidhya.com/blog/2018/05/google-duplex-natural-language-audio-processing/
Andrew Ng Supported Drive.ai Launches its First Self-Driving Car,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Duplex is a Jaw Dropping Application of Natural Language and Audio Processing|Python or R? Hadley Wickham and Wes McKinney are Building Platform Independent Libraries!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The concept of driverless cars has been floating around for a number of years but it has remained a distant and elusive dream for many organizations trying to turn it into reality. Ubers recent tragic story only hindered the efforts to turn the prototype of a self-driving car into a real-life case.But Andrew Ng, the super professor and pioneer of so many things in the machine learning, deep learning and AI community, announced yesterday that Drive.ai would be launching the first self-driving car in Texas, USA. If you have been following Andrew Ngs achievements in recent years, youll know how close to him this concept of autonomous vehicles has been.Drive.ai was founded in 2015 by Andrew Ngs graduate students out from Stanfords AI lab. Drive.ai leverages deep learning to creare self-driving systems that are adaptable and scalable.Initially, this car will not be available throughout the city. Instead, it will run on a six month testing phase  driving people on public roads between areas that are too far to walk, but have been deemed a waste for human drivers to drive (due to the relatively short distance). People who want to avail this service can schedule their rides through Drive.ais phone application.With the recent scrutiny over self-driving cars and accidents, this particular testing phase will include a human driver ready to take over, in case anything goes wrong. The ultimate goal, and the plan for the next year, is of course to make the service available throughout the city to everyone on every route. This will mean phasing out the concept of the human drive entirely.So how does this AI work?The company has released the below video to promote their launch:The negative media coverage following Ubers incident has soured the mood around driverless cars recently. But when it comes to Andrew Ng, theres a certain degree of confidence that this launch will go smoothly. He has been championing the case for autonomous vehicles since years so its great to see his dream turn into a practical use case.As a data scientist, this is the kind of technology you want to be working on! You can check out Baidus open source self-driving dataset (the largest of its kind) and work on it to understand how this AI works.",https://www.analyticsvidhya.com/blog/2018/05/andrew-ng-drive-ai-first-driverless-car/
Python or R? Hadley Wickham and Wes McKinney are Building Platform Independent Libraries!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Andrew Ng Supported Drive.ai Launches its First Self-Driving Car|Essentials of Deep Learning: Introduction to Unsupervised Deep Learning (with Python codes)|
Pranav Dar
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The debate of R v Python was just getting started when Analytics Vidhya was founded 5 years back. R had the slightly upper hand at that time with its awesome community support and Hadley Wickhams RStudio IDE. Over the years, Python has become the overwhelming leader. But the problem of collaborating between different tools has plagued data scientists since forever!Ursa Labs, founded by pandas creator Wes McKinney, aims to end the debate once and for all. Hadley Wickham, the chief data scientist at RStudio and creator of some of the most popular R libraries (like ggplot2, the tidyverse suite, etc.), is the technical advisor on this project. Ursa Labs has one specific goal in mind  improve the state-of-the-art open source software for data cleaning, preparation, feature engineering, model building, and other tasks involved in a typical data science project.But heres the best part about Ursa Labs  their libraries will be platform independent. Rather than build libraries and packages for individual tools, they are in the process of developing these libraries that will work in multiple programming languages. They languages they are currently working on are shown in the below image:Its often been a nightmare for data scientists and organizations when it comes to collaborating on projects that use different tools. Code portability becomes a nuisance. There is the additional problem of code redundancy. Ursa Labs will eliminate these issues. And who knows? It might just end the thriving debate about which language is better for data scientists  Python or R.Wow! This is one of the most ambitious projects taken up in the machine learning community and who better to do it than Wes and Hadley? This will make the process of sharing projects and collaborating on them so much easier for data scientists. No longer do you have worry about how to run a certain function in R if it was already performed in python, and vice versa.How excited are you about this release? Use the comments section below to share your thoughts!",https://www.analyticsvidhya.com/blog/2018/05/python-and-r-are-joining-hands-to-eliminate-platform-dependency/
Essentials of Deep Learning: Introduction to Unsupervised Deep Learning (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Why Unsupervised Learning?|Case Study of Unsupervised Deep Learning|Code Walkthrough of Unsupervised Deep Learning on MNIST data|End Notes,"Defining our Problem  How to Organize a Photo Gallery?|Approach 1  Arrange on the basis of time|Approach 2  Arrange on the basis of location|Approach 3  Extract Semantic meaning from the image and use it to define my collection|Learn,compete, hackandget hired!|Share this:|Like this:|Related Articles|Python or R? Hadley Wickham and Wes McKinney are Building Platform Independent Libraries!|Inspired by DeepMind, Facebook Open Sources its own Go Beating Algorithm|
Faizan Shaikh
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In one of the early projects, I was working with the Marketing Department of a bank. The Marketing Director called me for a meeting. The subject said  Data Science Project. I was excited, completely charged and raring to go. I was hoping to get a specific problem, where I could apply my data science wizardry and benefit my customer.The meeting started on time. The Director said Please use all the data we have about our customers and tell us the insights about our customers, which we dont know. We really want to use data science to improve our business.I was left thinking What insights do I present to the business?Data scientists use a variety of machine learning algorithms to extract actionable insights from the data theyre provided. The majority of them are supervised learning problems, because you already know what you are required to predict. The data you are given comes with a lot of details to help you reach your end goal.Source: danielmiesslerOn the other hand, unsupervised learning is a complex challenge. But its advantages are numerous. It has the potential to unlock previously unsolvable problems and has gained a lot of traction in the machine learning and deep learning community.I am planning to write a series of articles focused on Unsupervised Deep Learning applications. This article specifically aims to give you an intuitive introduction to what the topic entails, along with an application of a real life problem. In the next few articles, I will focus more on the internal workings of the techniques involved in deep learning.Note  This article assumes a basic knowledge of Deep Learning and Machine learning concepts. If you want to brush up on them, you can go through these resources:So lets get started!A typical workflow in a machine learning project is designed in a supervised manner. We tell the algorithm what to do and what not to do. This generally gives a structure for solving a problem, but it limits the potential of that algorithm in two ways:To solve this issue in an intelligent way, we can use unsupervised learning algorithms. These algorithms derive insights directly from the data itself, and work as summarizing the data or grouping it, so that we can use these insights to make data driven decisions.Lets take an example to better understand this concept. Lets say a bank wants to divide its customers so that they can recommend the right products to them. They can do this in a data-driven way  by segmenting the customers on the basis of their ages and then deriving insights from these segments. This would help the bank give better product recommendations to their customers, thus increasing customer satisfaction.In this article, we will take a look at a case study of unsupervised learning on unstructured data. As you might be aware, Deep Learning techniques are usually most impactful where a lot of unstructured data is present. So we will take an example of Deep Learning being applied to the Image Processing domainto understand this concept.I have 2000+ photos in my smartphone right now. If I had been a selfie freak, the photo count would easily be 10 times more. Sifting through these photos is a nightmare, because every third photo turns out to be unnecessary and useless for me. Im sure most of you will be able to relate to my plight!Ideally, what I would want is an app which organizes the photos in such a manner that I can go through most of the photos and have a peek at it if I want. This would actually give me context as such of the different kinds of photos I have right now.To get a clearer perspective of the problem, I went through my mobile and tried to identify the categories of the images by myself. Here are the insights I gathered:sudo add-apt-repository ppa:pinta-maintainers/pinta-stableNow that you know the scenerio, can you think of the different ways to better organize my photos through an automated algorithm? You can discuss your thoughts on this discussion thread.In the below sections, we will discuss a few approaches I have come up with to solve this problem.The simplest way is to arrange the photos on the basis of time. Each day could have a different folder for itself.Typically, most of the photo viewing apps use this approach (eg. Google Photos app).The upside of this will be that all the events thathappened on that day will be stored together. The downside of this approach is that it is too generic. Each day, I could have photos that are from an outing, or a motivational quote, etc. Both of them will be mixed together  which defeats the purpose altogether.A comparatively better approach would be to arrange the photos based on where they were taken. So, for example, with each camera click we would capture where the image was taken. Then we can make folders on the basis of these locations  either country/city wise or locality wise, depending on the granularity we want. This approach is also being used by most photo apps.The downside of this approach is the simplistic idea on which it was created. How can we define the location of a meme, or a cartoon  which takes a fair share of my image gallery? So this approach lacks ingenuity as well.The approaches we have seen so far were mostly dependent on the metadata that is captured along with the image. A better way to organize the photos would be to extract semantic information from the image itself and use that information intelligently.Lets break this idea down into parts. Suppose we have a similar variety of photos (as mentioned above). What trends should our algorithm capture?So our algorithm should ideally capture this information without explicitly tagging what is present and what is not, and use it to organize and segment our photos. Ideally, our final organized app could look like this:This approach is what we call an unsupervised way to solve problems. We did not directly define the outcome that we want. Instead, we trained an algorithm to find those outcomes for us! Our algorithm summarizes the data in an intelligent manner, and then tries to solve the problem on the basis of these inferences. Pretty cool, right?Now you may be wondering  how can we leverage Deep Learning for unsupervised learning problems?As we saw in the case study above, by extracting semantic information from the image, we can get a better view of the similarity of images. Thus, our problem can be formulated as  how can we reduce the dimensions of an image so that we can reconstruct the imageback from these encoded representations?Here, we can use a deep learning architecture called Auto Encoders.Let me give you a high level overview of Auto Encoders. The idea behind using this algorithm is that you are training it to recreate what it just learnt. But the catch is that it has to use a much smaller representation phase to recreate it.For example, an Auto Encoder with encoding set to 10 is trained on images of cats, each of size 100100. So the input dimension is 10,000, and the Auto Encoder has to represent all this information in a vector of size 10 (as seen in the image below).An auto encoder can be logically divided into two parts: an encoder and a decoder. The task of the encoder is to convert the input to a lower dimensional representation, while the task of the decoder is to recreate the input from this lower dimensional representation.This was a very high level overview of auto encoders. In the next article  we will look at them in more detail.Note  This is more of a forewarning; but the current state-of-the-art methods still arent mature enough to handle industry level problems with ease. Although research in this field is booming, it would take a few more years for our algorithms to become industrially accepted.Now that you have an intuition of solving unsupervised learning problems using deep learning  we will apply our knowledge on a real life problem. Here, we will take an example of the MNIST dataset  which is considered as the go-to dataset when trying our hand on deep learning problems. Let us understand the problem statement before jumping into the code.The original problem statement is to identify individual digits from an image. You are given the labels of the digit that the image contains. But for our case study, we will try to figure out which of the images are similar to each other and cluster them into groups. As a proxy, we will check the purity of these groups by inspecting their labels. You can find the data on AVs DataHack platform  theIdentify the Digits practice problem.We will perform three Unsupervised Learning techniques and check their performance, namely:We will look into the details of these algorithms in another article. For the purposes of this post, lets see how we can attempt to solve this problem.Before starting this experiment, make sure you have Keras installed in your system. Refer to the official installation guide. We will use TensorFlow for the backend, so make sure you have this in your config file. If not, follow the steps given here.We will then use an open source implementation of DEC algorithm by Xifeng Guo. To set it up on your system, type the below commands in the command prompt:You can then fire up a Jupyter notebook and follow along with the code below.First we will import all the necessary modules for our code walkthrough.Read the train and test files.Note that in this dataset, you have also been given the labels for each image. This is generally not seen in an unsupervised learning scenario. Here, we will use these labels to evaluate how our unsupervised learning models perform.Now let us plot an image to view what our data looks like.Mutual information is a symmetric measure for the degree of dependency between the clustering and the manual classification. It is based on the notion of cluster purity pi, which measures the quality of a single cluster Ci, the largest number of objects in cluster Ci which Ci has in common with a manual class Mj, having compared Ci to all manual classes in M. Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters.The formula for NMI is:Source: SlideshareIn this article, we saw an overview of the concept of unsupervised deep learning with an intuitive case study. In the next series of articles, we will get into the details of how we can use these techniques to solve more real life problems.If you have any comments / suggestions, feel free to reach out to me below!",https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/
"Inspired by DeepMind, Facebook Open Sources its own Go Beating Algorithm",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Essentials of Deep Learning: Introduction to Unsupervised Deep Learning (with Python codes)|The Best of ICLR 2018  The Leading Machine Learning and Deep Learning Global Conference|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The game Go has become quite the hit lately in the machine learning community ever since DeepMind unveiled its AlphaGo algorithm. It was the first machine led effort that beat a Go world champion. Since then, a lot of data scientists and researchers have dedicated themselves to understanding how to better DeepMinds algorithm.The latest effort was by Facebooks Research team. At the recently concludedF8 conference, Facebook CTO Mike Schroepfer revealed that the team had been building a Go bot of their own, called ELF OpenGo. It is built on the ELF (Extensive, Lightweight, and Flexible) platform.The exclamation point in this is that unlike DeepMind, Facebook has open sourced its algorithm. The team has released both the trained model and the code used to create it. All the details and resources are availablehereand on their GitHub page here.ELF OpenGo was trained on 2,000 GPUs over a two week period, and has reached an amazingly high performance level. In its training phase, the bot was made to play against other open source bots and human Go players. The results? Overwhelmingly positive.The bot started off by winning a series of games (198 wins, 2 losses) against LeelaZero, the strongest publicly available bot (using its default settings and no pondering.)It also set a record of 14 wins and 0 losses against four of the top 30 world-ranked human Go players! Incredible stuff.These games were all played using a single GPU, making moves every 50 seconds. The games were played on Chinese rules with 7.5 komi, and human players were given unlimited time to consider their moves.As an additional note, the team has also upgraded the ELF framework which was first released in June 2016. The latest version has higher efficiency, more friendly APIs, and support of distributed computation against thousands of machines.As DeepMind continues to keep shut about its AlphaGo algorithm, Facebook has taken the open source route, which is an immensely welcoming step.This will certainly encourage research and enable the community to experiment with new strategies. The aim of releasing the code and models is to inspire others to think about new applications and research directions for this technology.We encourage you to explore the code they have provided and try to understand how it works. The possibilities are endless and we hope to see someone in the AV community crack the next algorithm!Under the AVBytes umbrella, we have also previously seenMinigo, an open source python implementation inspired by AlphaGo. Check it out as well.",https://www.analyticsvidhya.com/blog/2018/05/facebook-open-sources-elf-opengo/
The Best of ICLR 2018  The Leading Machine Learning and Deep Learning Global Conference,Learn everything about Analytics|Overview|Introduction|On the convergence of ADAM and Beyond|Spherical CNNs|Continuous adaptation via meta-learning in nonstationary and competitive environments|Other Resources,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Inspired by DeepMind, Facebook Open Sources its own Go Beating Algorithm|Lobe is an Automated Deep Learning Tool for People who dont know Programming|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As a data scientist, its very important to keep yourself updated with the latest developments in the community. Reading research papers from the top researchers should be of the highest priority! And I can assure you there is no better conference that brings all the top minds under one roof than theThe International Conference on Learning Representations (ICLR).ICLR 2018 concluded yesterday in Vancouver, Canada, and was suitably represented by a whole host of top research papers, including entries by DeepMind and Facebook. This year, the ICLR community received 935 papers for review (double that of last year) and 337 papers were accepted into the final conference.In this article, we have listed the best three papers, as chosen by the ICLR committee, and provided you the link for other resources which you will find handy.Lets look at the 3 best papers!This paper was submitted by Google New York. In this, the researchers have proposed a new take on popular optimization algorithms likeAdam and RMSProp. The team claims that gradient boosting, while being an effective neural network technique, fails to converge to an optimal solution in non-convex settings. This paper investigates the likely cause of failures and goes on to provide an example of where Adam does not converge to the optimal solution.The teams analysis suggests ways on how the convergence issues can be fixed, and proposes new variants of the Adam algorithm. These variants not only fix the convergence issues but have been proven to improve the performance as well!You can view the research paper here.This paper was published by researchers from the University of Amsterdam. In this paper, they have introduced the building blocks for constructing spherical Convolutional Neural Networks (CNNs). What pushed this research? Well CNNs were already the algorithm of choice for problems involving 2D images. But those previous methods used to fail when 3D objects came into the picture.Examples of 3D spherical problems include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling.The researchers have demonstrated the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs when applied to 3D model recognition.You can view the research paper here.This paper was a join effort between researchers from UC Berkeley, OpenAI, UMass Amherst and CMU. In our quest towards true artificial intelligence, the ability to continuously learn and adapt from limited experience in non-stationary environments is considered a massive milestone. The researchers have developed a gradient-based algorithm that adapts in complex dynamically changing scenarios.Not only that, theyve also developed a new multi-agent competitive environment that theyre called RoboSumo.You can view the research paper here.This leaves 334 more papers that were covered at the conference! You can find the full session list here. Each session includes the corresponding research paper.If you like consuming your content in the form of videos, the recordingsare available on ICLRs Facebook page here.Let us know your thoughts on this conference in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/best-iclr-2018-leading-machine-learning-deep-learning-global-conference/
Lobe is an Automated Deep Learning Tool for People who dont know Programming,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Best of ICLR 2018  The Leading Machine Learning and Deep Learning Global Conference|Supported by Google and Intel, MLPerf Compares the Speed of Machine Learning Tools|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep learning can be hard to grasp for most people  especially those who are from a non-programming background. Most people who are unfamiliar with its working seem to be intimidated by the connotations of the word deep. Can a person with no programming experience ever succeed in deep learning?As it turns out, you can! Lobe is a drag-and-drop tool that makes automates deep learning for folks with no programming expertise. It does not require any coding. Its a visual tool that lets you build custom deep learning models, train them in double-quick time, and deploy them in your application.Lobe works in three steps:Another fascinating aspect of Lobe is that you can see your entire dataset visually. You can select any icon and see how that specific example is performing in your custom model. You dont need to worry about the train-test split either! Lobe does that automatically as well.The below 10 second video shows how the drag-and-drop interface works. You connect building blocks together (called lobes) to create the deep learning model. Watch how it plays out below:The product is currently available in beta. You can visit their site to check out examples of how this tool can be used and for any tutorials in case you are planning to use it. Also below is another video (slightly longer at 13.5 minutes) where the developers explain in details how Lobe works from scratch:This makes deep learning accessible for anyone! No longer do you need to worry about which tool to pick out and what language to learn. The initial results this has produced have yielded impressive accuracies in models. I can already the potential this has to revolutionize industries. Plus this will get a lot more aspiring data scientists into the world of deep learning, which is an added bonus.The team behind Lobe is also working on getting text processing into the tool so this will only get better in the coming months. Definitely one we are keeping an eye out for!",https://www.analyticsvidhya.com/blog/2018/05/lobe-automated-deep-learning-tool-people-dont-know-programming/
"Supported by Google and Intel, MLPerf Compares the Speed of Machine Learning Tools",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Lobe is an Automated Deep Learning Tool for People who dont know Programming|Facebook Announces PyTorch 1.0  A Major Release for Data Scientists and AI Researchers|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What tools make up a data scientists skillset? How do you figure out which tool is the best and is there a benchmark? So far, these were questions with a variety of answers, but none of them conclusive. We saw companies launching new tools and libraries and touting them as faster than the previous iteration which belonged to their competition. Its all a matter of perspective and how you play it.Until now.A number of tech giants, including Google, Baidu and Intel, have collaborated to create a benchmarking tool, called MLPerf, which lets users understand and improve the performance of machine learning tools and techniques.AI pioneer and leader Andrew Ng has previously said, AI is transforming multiple industries, but for it to reach its full potential, we still need faster hardware and software. MLPerf aims to bridge this gap by aiming to be the benchmark tool for measuring speed and performance of existing ML tools.The main goal of introducing MLPerf is to accelerate progress in machine learning via a fair and useful measurement system. The approach behind MLPerf is to select a set of problems, each already defined by a dataset and a set quality target. Then, it measure the time taken to train the model for each problem separately.A few examples of where MLPerf has been tested are mentioned in the table below:As the team mentioned, the first release of MLPerf, (MLPerf version 0.5, as they are calling it for now) will focus on training jobs on a range of systems from workstations to large data centers. Later releases will expand to include inference jobs, eventually extended to include ones run on embedded client systems. An initial version will be ready for use in August.To view the complete specifications, along with reference codes and examples, you can visit the official siteof MLPerf. MLPerf is available on GitHub.This can be used as a constructive development, for comparing software and hardware systems, creating a benchmark and encouraging innovation & improvement. How easier would life be for a data scientist if they could easily compare tools and techniques from different domains in one place? Sounds awesome.MLPerf combines the best practices from previous benchmarks, including SPECs use of a suite of programs. After SPECs release in 1988, CPU performance improved 1.6x per year for the next 15 years, and we are hoping to witness a similar improvement with the release of MLPerf!This is a tool to keep your eye on, if only because of the names backing this project.",https://www.analyticsvidhya.com/blog/2018/05/mlperf-compare-speed-machine-learning-tools-google-intel/
Facebook Announces PyTorch 1.0  A Major Release for Data Scientists and AI Researchers,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Supported by Google and Intel, MLPerf Compares the Speed of Machine Learning Tools|Improve Your Model Performance using Cross Validation (in Python and R)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Moving your machine learning project from research to production has historically been a challenge  both budget wise as well as framework wise. It can also be time consuming along with being complicated to test when new approaches need to be deployed. Doesnt sound promising, does it?A research team at Facebook is aiming to smoothen this process for the community.PyTorch 1.0, announced by Facebook at the F8 event yesterday, aims to provide developers a seamless path to take their project from research to production in a single framework! With this tool, data scientists and AI developers can experiment and optimize performance through a hybrid front end. It might not surprise you to know that this technology is already in use at Facebook where it is performing close to 6 billion language translations per day (this translation tool is called, appropriately, Translate).So how is PyTorch 1.0 different from their latest release  v0.4.0? v1.0 takes the existing super flexible framework and combines it with the production-oriented capabilities from Caffe2 and ONNX. The releases so far have had one challenge  their performance at production. Developers often need to first translate their code to a graph mode representation in Caffe2 in order to make it production ready. This issue has been eliminated in PyTorch 1.0.Microsoft and Amazon Web Services have already indicated that they will be using PyTorch 1.0 in their flagship offerings.PyTorch 1.0 will be available in beta version in the next few months. It is expected to include tools, libraries, datasets and pre-trained models for each stage of development. You can read more about this latest release by Facebook on its blog post here.Facebook, from time to time, pioneers new tools and techniques that are state-of-the-art. I love that they oen source their research so that the entire ML community can benefit from it. Deploying your code to production from research has commonly been a major challenge so this release should get rid of that headache soon.Time will tell how many organizations decide to integrate PyTorch into their existing services but all things considered, this is a major announcement in the wider perspective. Soon, you wont be able to say that TensorFlow is better for production than PyTorch!",https://www.analyticsvidhya.com/blog/2018/05/facebook-announces-pytorch-1-0-open-source/
Improve Your Model Performance using Cross Validation (in Python and R),Learn everything about Analytics|Introduction|Table of Contents|Why do models lose stability?|What is Cross Validation?|A few common methods used forCross Validation|How to measure the models bias-variance?|End Notes,"The validation set approach|Leave one out cross validation (LOOCV)|k-fold cross validation|4. Stratified k-fold cross validation|5. Adversarial Validation|6. Cross Validation for time series|7. Custom Cross Validation Techniques|Share this:|Related Articles|Facebook Announces PyTorch 1.0  A Major Release for Data Scientists and AI Researchers|Top 5 GitHub Repositories and Reddit Discussions for Data Science & Machine Learning (April 2018)|
Sunil Ray
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published on November 18, 2015, and updated on April 30, 2018.One of the most interesting and challenging things about data science hackathons is getting a high score on both public and private leaderboards.I have closely monitored the series of data science hackathonsand found an interesting trend. This trend is based onparticipant rankings on the public and private leaderboards.One thing that stood out was that participants who rank higher on the public leaderboard lose their position after their ranks gets validated on the private leaderboard. Some even failed to secure rank in the top 20s on the private leaderboard (image below).Eventually, I discoveredthe phenomenon which brings such ripples on the leaderboard.Take a guess! What could be the possible reason for high variation in these ranks? In other words, why does their model lose stabilitywhen evaluated on the private leaderboard?In this article, we will look at possible reasons for this. We will also look at the concept of cross validation and a few common methods to perform it.Note: This article is meant for every aspiring data scientist keen to improve his/her performance in data science competitions. Each technique is followed by code snippets from both R and Python.Lets understand this using the below snapshot illustrating the fit of various models:Here, we are trying tofind the relationship between size and price. To achieve this, we have taken the following steps:A commonpractice indata science competitions is toiterate over various models to find abetter performing model. However, it becomes difficult to distinguish whether this improvement in score is coming because we are capturing the relationship better, or we are just over-fitting the data. To find the right answer for this question, we use validation techniques. This method helps us in achieving more generalized relationships.Cross Validation is a technique which involvesreserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it.Here are the steps involved in cross validation:There are various methods available for performing cross validation. Ive discussed a few of them in this section.In this approach, we reserve 50% of the dataset for validation and the remaining 50% for model training. However, amajor disadvantage of this approach is that since we are training a model on only 50% of the dataset, there is a huge possibility that we might miss out on some interesting information about the data which will lead to a higher bias.Python Code:R Code:In this approach, we reserve only one data point from the available dataset, and train the model on the rest of the data. This process iterates for each data point. This also has its own advantages and disadvantages. Lets look at them:Python Code:R Code:LOOCV leaves one data point out. Similarly, you could leave p training examples out to have validation set of size p for each iteration. This is called LPOCV (Leave P Out Cross Validation)From the above two validation methods, weve learnt:Dowe have a method which takes care of all these3 requirements?Yes! That method is known ask-fold cross validation. Its easy to follow and implement.Below are the steps for it:Below is the visualization of a k-fold validation when k=10.Now, one of most commonly asked questions is,How to choose the right value of k?.Always remember, a lower value of k is more biased, and hence undesirable. On the other hand, a higher value ofK is less biased, but can suffer from large variability. It is important to know that a smaller value of k alwaystakes us towards validation set approach, whereas a higher value of k leads to LOOCV approach.Precisely, LOOCV is equivalent to n-fold cross validation where n is the number of training examples.Python Code:R code:Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the data such that in every fold, each class comprises of about half the instances.It is generally a better approach when dealing with both bias and variance. A randomly selected fold might not adequately represent the minor class, particularly in cases where there is a huge class imbalance.Python code snippet for stratified k-fold cross validation:R Code:Having said that, if the train set does not adequately represent the entire population, then using a stratified k-fold might not be the best idea. In such cases, one should use a simple k-fold cross validation with repetition.In repeated cross-validation, the cross-validation procedure is repeated n times, yielding n random partitions of the original sample. The n results are again averaged (or otherwise combined) to produce a single estimation.Python code for repeated k-fold cross validation:When dealing with real datasets, there are often cases where the test and train sets are very different. As a result, the internal cross-validation techniques might give scores that are not even in the ballpark of the test score. In such cases, adversarial validation offers an interesting solution.The general idea is to check the degree of similarity between training and tests in terms of feature distribution. If It does not seem to be the case, we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0  train, 1-test) and evaluating a binary classification task.Let us understand, how this can be accomplished in the below steps:val_set_ids will get you the ids from the train set that would constitute the validation set which is most similar to the test set. This will make your validation strategy more robust for cases where the train and test sets are highly dissimilar.However, you must be careful while using this type of validation technique. Once the distribution of the test set changes, the validation set might no longer be a good subset to evaluate your model on.Splitting a time-series dataset randomly does not work because the time section of your data will be messed up. For a time series forecasting problem, we perform cross validation in the following manner.We progressively select a new train and test set. We start with a train set which has a minimum number of observations needed for fitting the model. Progressively, we change our train and test sets with each fold. In most cases, 1 step forecasts might not be very important. In such instances, the forecast origin can be shifted to allow for multi-step errors to be used. For example, in a regression problem, the following code could be used for performing cross validation.Python Code:R Code:h = 1 implies that we are taking the error only for 1 step ahead forecasts.(h =4) 4-step ahead error is depicted in the below diagram. This could be used if you want to evaluate your model for multi-step forecast.Unfortunately, there is no single method that works best for all kinds of problem statements. Often, a custom cross validation technique based on a feature, or combination of features, could be created if that gives the user stable cross validation scores while making submissions in hackathons.For example, in the recently finished contest Lord of the Machines by Analytics Vidhya, the most stable validation technique used by the top finishers was using the campaignid variable.Please have a look at the problem statement and a few approaches discussed by the participants at this thread.After k-fold cross validation, well get k different model estimation errors (e1, e2 ..ek).In an ideal scenario, these error values should sum up to zero. To return the models bias, we take the average of all the errors. Lower the average value, better the model.Similarly for calculating the model variance, we take standard deviation of all the errors. A low value of standard deviationsuggestsourmodel does not vary a lot with different subsets of training data.We should focus on achieving a balance between bias and variance. This can be done by reducing the variance and controllingbias to an extent. Itll result in a better predictive model. This trade-off usually leads to building less complex predictive models as well. For understanding bias-variance trade-off in more depth, please refer to section 9 of this article.In this article, we discussed about overfitting and methods like cross-validation to avoid overfitting. We also looked at different cross-validation methods like validation set approach, LOOCV, k-fold cross validation, stratified k-fold and so on, followed by each approachs implementation in Python and R performed on the Iris dataset.Did you find this article helpful? Please share your opinions/thoughts in the comments section below. And dont forget to test these techniques in AVs hackathons.",https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/
Top 5 GitHub Repositories and Reddit Discussions for Data Science & Machine Learning (April 2018),Learn everything about Analytics|Introduction|GitHub Repositories|Deep Painterly Harmonization|Swift for TensorFlow|MUNIT:Multimodal UNsupervised Image-to-image Translation|GluonNLP|PyTorch GAN|Reddit Discussions|Helping Users in Understanding Research Papers|Statement on Nature Machine Intelligence|Michael Jordans Lecture on the Current State of AI|Scientists Planning Huge European AI Hub to Compete with US|Measuring the Intrinsic Dimension of Objective Landscapes,"Share this:|Like this:|Related Articles|Improve Your Model Performance using Cross Validation (in Python and R)|Google has Released the Latest Open Images Dataset! Every Data Scientist should Work with this|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GitHub and Reddit are two of the most popular platforms when it comes to data science and machine learning. The former is an awesome tool for sharing and collaborating on codes and projects while the latter is the best platform out there for engaging with data science enthusiasts from around the world.This year, we have covered the top GitHub repositories each month and from this month onwards,we will be including the top Reddit threads as well that generated the most interesting and intriguing discussions in the machine learning space.April saw some amazing python libraries being open sourced. From Deep Painterly Harmonization, a library that makes manipulated images look ultra realistic, to Swift for TensorFlow, this article covers the best from last month.
Lets look at Aprils top repositories and most interesting Reddit discussions.You can check out the top GitHub repositories for the last three months below:The task of manipulating images and still making them look realistic has been around for ages. But with deep learning, this is becoming far more efficient and remarkably life-like.A developer has come up with an algorithm that takes a painting, adds an external element to it, and harmonizes it to make it look almost undistinguishable from the original painting.Just look at the above image  the third frame is the final output and if we didnt have the preceding two images, we would never be able to tell the balloon is an external object!This algorithm produces far more precise results than photo compositing or global stylization techniques and it achieves levels of edits that have so far been very difficult to achieve.You can read more about this library on AVByteshere.Swift for TensorFlow was demod at theTensorFlow Developer Summitlast month and the team behind the technology has now open sourced the code on GitHub for the entire community. Their aim is to provide a new interface to TensorFlow that will build on its already awesome capabilities, while taking its usability to a whole new level.This is still in its very nascent stages so it isnt ready to be written into deep learning models yet. The team admits that the goals it has in mind while launching this are still a while away from being achieved. But there is a lot of potential here that is as yet untapped.We have covered Swift for TensorFlow here for your reference.A team of researchers from Cornell University have proposed a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework for translating images from one domain to another. The aim is to take an image and generate a new image from it that is from a new category (for instance, transforming an image of a dog to a cat).The previously existing approaches are able to perform only one-to-one mapping of the given an image and thus fail to produce diverse outputs of the same.MUNIT, on the other hand is able to provide more than one output.Exciting times!We covered this on AVBytes and you can read about how it works here.Deep Learning in the field of Natural Language Processing has taken off in a big way recently. There is a plethora of text available on the internet, dating back to centuries! GluonNLP is a toolkit that aims to make NLP tasks easier for a data scientist. It makes text preprocessing easier, along with loading the dataset(s) and building the deep learning neural models. This enables you to to do your NLP research faster and in a more efficient manner.This repository has a nice documentation, along with a detailed example of how to use the library. They even have a nicely packaged 60-minute crash course for folks who are new to Gluon.This repository is a goldmine. Its a collection of PyTorch implementations of GANs (or Generative Adversarial Networks) that have been presented in research papers. Currently the repository lists 24 different implementations, each adding to your knowledge in its unique way. The list contains implementations like Adversarial Autoencoders, CycleGAN, Least Squares GAN, Pix2Pix, etc.If youre having trouble trying to understand any research paper, the Reddit machine learning community is willing to help you out. This is an awesome idea that has already helped a bunch of people in extracting valuable information where before they used to give up and move on.But when you post there, ensure you provide as much detail as you can, like a summary of the paper, where you are stuck, what research have you done to find out by yourself, etc. This line sums it up well  Think of each paper as an invite to an open study group for that paper, not just a queue for an expert to come along and answer it.The debate about whether research should be open sourced or closed has been raging on for decades. Recently, the popular Nature magazine announced itll be publishing a closed-access journal. This has led to a major campaign against them, with a lot of big names (Jeff Dean, Ian Goodfellow, among others) adding their signatures to a petition stating they will not write for such a publication.This discussion thread has diverse and knowledgeable opinions about whether research should have open or closed access. Its a fascinating read and I highly recommend going through the entire thread to see what the ML community thinks about this topic.Michael Jordan is a celebrated professor from Berkeley and in a recent talk he spoke at length about how we are miles away from reaching true intelligence in machines. Its a sobering presentation and really makes one think about the topic.This thread has generated more than 100 comments, with users weighing in with their opinions about where they perceive AI to be. What makes this a fascinating read is the depth of comments which some users have gone into. Go ahead, read it and participate in the still active discussion.This looks like a reasonably straightforward topic right? Wait till you dive into the thread. Data scientists and machine learning researchers from all over Europe and the USA are involved in an intense discussion about how the structure of ML is shaping in both continents, and what the salary figures look like. You will gain a lot of perspective about the architecture of ML projects and prospective salaries.This thread was launched from Ubers video on developing intrinsic dimension as a fundamental property of neural networks. If you have any doubts regarding the content presented in the video, the community has answered those questions in detail. The biggest poisitve seems to be that people love that a research paper was turned into a video, which makes it easier to understand the research.Have you used any of the GitHub libraries before? And whats your take on the Reddit discussions? If you have any feedback or suggestions, or need clarification on anything, get involved in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/top-5-github-reddit-data-science-machine-learning-april-2018/
Google has Released the Latest Open Images Dataset! Every Data Scientist should Work with this,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Top 5 GitHub Repositories and Reddit Discussions for Data Science & Machine Learning (April 2018)|4 Data Science Music Projects Aiming to Transform the Music Industry|
Pranav Dar
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As a data scientist, finding large datasets to work with is a challenge. Most organizations treasure their data and prefer not releasing it to the community. But Google has been one of the few who has consistently open sourced a lot of their research in order to speed up studies and also help budding data scientists.This week, they have released version 4 of their popular Open Images dataset  free and available for anyone to download and work with.Open Images is a massive dataset of images which was released by Google back in 2016. The dataset consists of 9 million images that have already been labelled by the team. According to their site, The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it thelargest existing dataset with object location annotations.These annotations have been drawn manually by professional annotators in order to ensure accuracy and consistency. The subject matter in the images is diverse in nature. There are 8.4 objects per image on average in this dataset. To add the icing on the cake, the data is annotated with image-level labels that span thousands of classes!The Open Images dataset is pre-split into the training, validation and test sets. The training set contains 9,011,219 images, the validation set has 41,260 images and the test set has 125,436 images. All of these images come with proper labels to help you get down to building a model as quickly as possible.Along with this dataset release, Google has announced the Open Images Challenge 2018. This is scheduled to be held at the European Conference on Computer Vision and will be an object detection challenge. This latest competition is offering a far more broader range of object classes than any previous challenge. It will have two tracks:The deadline for submission of results is 1st September, 2018. The evaluation metric for this challenge will be mean Average Precision (mAP) over the given 500 classes.This is the fourth update the team has released in the last 2 years. You can download the dataset from Googles page here.This is a treasure trove for data scientists! Anyone interested in deep learning and image classification can download and work on this dataset. The fact that Google has worked on labelling the images is a testament to their team and to the power of their resources. The training set, with its massive size, is expected to stimulate research on more complex detection models. The hope is that this release will help in improving current state-of-the-art models.Their open challenge is already generating a huge buzz in the ML community and we are expecting to see some serious competition. We will be sure to cover any major projects that come up in this challenge.If youre a newcomer to image processing, or have been working in this field for a while, this dataset is perfect for you. Use the comments section below to tell us how you plan on using this!",https://www.analyticsvidhya.com/blog/2018/05/google-released-latest-open-images-dataset/
4 Data Science Music Projects Aiming to Transform the Music Industry,Learn everything about Analytics|Overview|Introduction|Our take on this,"Understanding and Mining Patterns of Audience Engagement and Creative Collaboration in Large-Scale Crowdsourced Music Performances|Understanding How the Brain Processes Music Through the Bach Trio Sonatas|The Sound of Text|A Computational Study of Patterned Melodic Structures Across Musical Cultures|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google has Released the Latest Open Images Dataset! Every Data Scientist should Work with this|Visualize and Perform Dimensionality Reduction in Python using Hypertools|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"From digitally enhancing beats to creating completely new songs, machine learning is well and truly transforming the music industry. A lot of artists these days are using ML to enhance their songs and add elements to their albums that were unthinkable before.Researchers at the University of Michigan are also using machine learning to leave their own imprint on the digital era of music. They are changing the way we understand, create and interact with music.Four research teams, that utilize machine learning and deep learning tools and techniques for the study of music theory, performance, social media-based music making, and the connection between words and music, will be given support by experts. They will be funded and this funding will be provided under theData Science for Music Challenge Initiative through the Michigan Institute for Data Science (MIDAS).The major focus of these projects will be to use ML techniques to automate musical accompaniment of text and data-driven analysis of music performance. Each project will be given $75,000 over a period of a year. Below are the projects:The researchers who are selected for this project will be tasked with developing a platform for crowdsourced music making and performance. They will need to use data mining techniques to discover patterns in audience engagement.This is probably the most fascinating project of the lot. Researchers will attempt to develop and analyse digitized performances of Bachs Trio Sonatas. They will be required to produce algorithms that study the music structure from the perspective of data science. The end goal is to understand what makes performers so good artistically as well as figure out common mistakes they make.The aim of this project is to develop a data science framework that will connect music with language. The researchers will need to develop tools that produce musical interpretations of texts, based completely on emotion and content. As the name suggests, the end goal is to create a tool that can transform any piece of text into music.This project aims to combine computational analysis and music theory. This will be done to compare music across six cultures, including Indian songs, in order to identify common ground in how music is generated and structured in different cultures.You can read more about the MIDAS challenge herefor further details.This goes to show how far machine learning has penetrated the music industry and how far it still has to go. These projects are just the beginning, or the tip of the iceberg, that have the potential to start a revolution.Assuming these projects are successful, they will broaden and deepen the current horizon in the digital music world.The results can be applied to other interactive settings as well, including developing new educational tools. What are the use cases you can think of for these projects? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/05/data-science-music-projects-transform-industry/
Visualize and Perform Dimensionality Reduction in Python using Hypertools,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|4 Data Science Music Projects Aiming to Transform the Music Industry|An Introductory Guide to Understand how ANNs Conceptualize New Ideas (using Embedding)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data is getting more and more complex these days as the number of data sources increases. A data scientists job is to extract actionable insights from this data, but as more and dimensions are added to it, this is no easy task. Humans perceive the world in 3 dimensions so recognizing patterns from thousands, if not millions, of variables is a task we rely heavily on machines for.But even machines can struggle with this. This is where the awesome technique of dimensionality reduction comes into the picture. In case you havent come across this term yet, you can check out AVs article about it here. As the name suggests, it basically reduces the number of dimensions in a dataset to make it more easy to work with. There are a few different techniques to achieve this, and one of the most common ones is called PCA, or Principal Component Analysis.Hypertools was designed with PCA and data visualization at the core. Its a python library designed to implement dimensionality reduction-based visual explorations of datasets (or a series of datasets) with high dimensions.How does it work? As input, you feed in the dataset with high dimensions. In a single function command, Hypertools reduces the dimensionality of the data and visualizes it in the form of a plot. The library has been developed on top of a few popular python libraries, like scikit-learn, seaborn and of course, matplotlib.As mentioned by the developers, below are a few mainfeatures which HyperTools provides for data scientists:To install the latest stable version of Hypertools from pip, run the below command:You can check out the GitHub repository for HyperTools hereand also read their research paper here. Also be sure to check out the short video below which introduces this library:I love this library! Anyone who has handled a dataset with a lot of variables knows what a headache it can be. While performing PCA is considered necessary, Hypertools makes it so much more easier for a data scientist to deal with thousands and millions of variables.Im a huge advocate of visualizing data so this is quickly becoming one of my favourite libraries. The way it allows you to look at your dimensions, in hyperspace and from all angles, its truly awesome. Its no wonder the library has received almost a 1000 stars so quickly and has become popular in the data science community.Try out this library and let us know how it worked out for you.",https://www.analyticsvidhya.com/blog/2018/04/visualize-manipulate-high-dimensional-data-using-hypertools/
An Introductory Guide to Understand how ANNs Conceptualize New Ideas (using Embedding),Learn everything about Analytics|Introduction|Table of Contents|A simple thought experiment|Embedding in the human brain|Embedding in Artificial Neural Networks (ANN)|Applications of semantics embedding|Learning Embedding from data|Formulation of Text Embedding|Popular word embedding algorithms|Image & Speech Embedding|One shot learning|Siamese Network|End Notes,"Share this:|Like this:|Related Articles|Visualize and Perform Dimensionality Reduction in Python using Hypertools|AVBytes: AI & ML Developments this week  a Major R Update, Nvidia DL model autocompletes pictures, Windows Support for PyTorch, etc.|
Tavish Srivastava
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Heres something you dont hear everyday  everything we perceive is just a best case probabilistic prediction by our brain, based on our past encounters and knowledge gained through other mediums. This might sound extremely counter intuitive because we have always imagined that our brain mostly gives us deterministic answers.Well do a small experiment to showcase this logic. Take a look at the below image:Q1. Do you see a human ?Q2. Can you identify the person?Q3. Can you identify the color of the clothes this person is wearing?Keep these questions in mind. We will get back to them after seeing what we will be covering in this article.Understanding Artificial Neural Networks becomes easy if you draw parallels with the working of the human brain. Today, we will explore the concept of embedding. We will first understand what embedding means in our human brain space, and then look at its applications and use cases.Lets get back to our experiment. Recall the blurred image and our three questions. All of you must have guessed the answer to the first question, most of you must have guessed the second one, and a few of you might have guessed the third one. Give a score on your confidence level (out of 100) for each of the three questions and average them.Why do you think you did not score an absolute 100? The reason is simple, our brain does not have enough data to identify this image with a probability ~ 1.0 . Now look at the image below:Try to answer the three questions again. Did you get a 100 this time? You might think that our brain is returning a deterministic value for the three questions this time. But it turns out that your brain is making predictions that are very, very close to 1.0 probability because this time our brain got enough information.Lets make this experiment more interesting. Look at the first image again and compute your score. Did you score higher than before? If yes, the reason is simple. Your brain has used its past memory to add new information which is not even there in the picture and increased the returned confidence value. This new information is that the brain has seen an almost similar picture before.The above experiment demonstrates that our brain tries to predict everything around us and works with the best guess. This statement is true to such an extent that our brain even has to predict the location of parts of our own body through experience. The brain cannot even be deterministic of the facts like where your hand or legs or chest are located. Google body transfer illusion to read about experiments that will prove this fact.Coming to the big question  if our brain predicts everything with past experience or data, how do we work so well in situations where we have no prior experience or knowledge? For instance, if you go to a grocery store and get a new fruit called Alphaberry, what will you do with this item?Probably keep it in the refrigerator, wash it and eat it. How do you know so much about what to do with this new item even when you have no experience with the subject? It turns out that our brain tries to create a semantic understanding of everything and refines this understanding with experience. This initial semantic understanding gives it a head start.For instance, the brain knows that Alphaberry is a fruit, hence it will share a lot of property with other fruits, i.e., we can eat it without any danger. Once we have eaten this fruit, our brain adds the new information to refine the semantic representation of Alphaberry.These semantic representations can be used by the brain to find similarities between concept/objects or draw analogies or make reasoning. The name Alphaberry is just the address of this semantic representation in our memory. The moment I ask you  Have you tried alphaberry?, your brain will look for semantic representations of alphaberry and retrieve all the experiences/information about the fruit. It then evaluates probabilistic answers to the question and gives you the appropriate response  Yes. It was very sweet.Lets take another example. What is 20 * 10 ? Obviously 200. Numbers, unlike words and pictures, have their semantics encoded in itself. Hence, our brain does not see 10 and 20 as addresses of semantics but as semantics itself. Our brain can directly do operations on these numbers unlike alphaberry, where it had to retrieve semantic representations before it could answer anything about the subject. This is an important concept and we will refer this concept later in this article.The above description, as you can imagine, is an over simplification of what really goes in our complex brain. In the next section, we will talk about a concept used in Artificial Neural Networks (ANN), that is parallel to semantic representation in humans.In the last decade, we have made computers highly efficient with numbers (even better than humans). Computers can only work with numbers as they are the only entities that have semantic encoded on itself. How can we make computers understand concept like words, images, audio or videos? The answer lies in our previous section.We need a semantic representation of all these concepts in numbers or array of numbers, as numbers are all computers can understand. It is important that the array of numbers do a good job of denoting the semantics of the entity, else we will have a garbage in garbage out kind of model.We will contrast two models to denote the semantics in order to get a deeper understanding of this concept. Here we will talk about representation of 6 concepts  Lion, Cub, Cat,Kitten,Apple, Alphaberry. You have 6 pictures of each and need to represent them in numeric form.Our first model is one-hot encoding vector representation of words. The representation looks as follows:Our second model is based on attributes of each picture. This is more like the semantic/meaning of the picture. Following is an example:Now lets try to contrast. Try answering the following questions with both the representations individually.I am sure you already know that the second representation did well on all the above 3 questions. Not only does the semantic representation give us a head start on new concepts like Alphaberry, it also helps us infer logically. For instance if you do the following task mathematically, you will find the answer to our second question :How awesome is that! Now we can do mathematical operations on abstract concepts like words/pictures/audio. We also know semantic representation in form of numeric vectors for abstract concepts can be very helpful for our ANN models. But how can we create so many semantic representations for all the words/images/audios in this world? The answer is simple  by learning from lots and lots of data. The features from such a learning will be a lot more abstract unlike the one we used in our demonstrative example above.Like our brain uses semantics in all the cognitive tasks, Artificial Neural Networks use semantic embedding for numerous tasks. We will categorize these applications under 3 main types of embedding they use.Let us now try to understand a generic concept of learning embedding.The underlying methodology for developing any type of embedding is almost the same. We prepare a training data with one hot encoded entity (this can be a word or image or audio), define some kind of target function and develop a neural network. We then throw out the last layer of the neural net and use the weights of the intermediate layer as the embedding. Following is the generic process flow:Most of the algorithms that are used to train text embedding work on a very simple framework. Lets try to understand this with an example. We will use the following sentence to illustrate the logic:Word2vec  Word2vec is the most popular embedding algorithm. It works on very simplistic scenarios of the generic algorithm. We randomly choose a target word and then choose one word from the generic context vector as the final context. For instance, we can choose playing as the target vector and cricket as the context. Now we run the generic model that we discussed in the last section.The only limitation of this methodology is that the calculation of the softmax function at the end of the framework is extremely expensive. This is because the size of the output node is equal to the vocabulary size (which is generally more than 10k). You can always use pre-trained word2vec matrix for your business case to avoid this computational cost.Negative Sampling  This is another powerful concept which gets rid of the one challenge in word2vec. Instead of multiple output nodes, we convert the problem into a binary classification. The target word is chosen at random, as before. For the context word, we initially choose one of the correct proximity words, and then choose random words from the dictionary. Each of these pairs are seen as separate observations.In the above table, we use the field correct as the output node and treat this model as a binary classifier. Hence, we can avoid the expensive calculation of softmax function in the word2vec algorithm.Other algorithms  Many other algorithms such as GloVe word vectors, etc. have been used in the industry. All the models work on the same generic architecture with minor changes. If you have a small dataset, it is generally recommended to use pre-trained embedding. These embeddings have been trained on millions of documents and hence have very accurate semantic information.Even though word embedding is the most popular application of embedding, image & speech embedding is no less when it comes to practical applications. The main use of both image and speech embedding is authentication. We authenticate customers in every industry before we share any private information. There is a good chance you have encountered the use of embedding without even knowing it. Consider the following examples:Hopefully you can relate to some of the above use cases. Each of these is primarily based on image or speech embedding. Both speech and image are analyzed with the same objective, i.e., to find similarity between multiple voice/images, with almost the same architecture. The only difference is that speech is first converted to image using filter banks/MFCC in order to visualize how humans perceive sound. Then it follows the same process as the formation of image embedding. In both speech and image, we see two broad use cases:Why do we need embedding for the verification/recognition tasks? Why cant we train a model for each face/voice separately? We already know neural networks need a lot of data to be precise and accurate. However, most of our verification/recognition tasks are one shot learning. One shot learning is learning made on one, or very few examples. For instance, Baidus system will probably have 1 or 2 pictures of every employee. How can we create a model when we have just a few data-points per class? This is the reason we create embedding for each image and then try to find similarities between the embeddings. This concept will become clearer once we are done with the neural network architecture of training image/voice embedding.The neural network architecture for training image embedding is commonly known as Siamese Network. I have included only one out of the many algorithms that are used to create image embedding. In this method, we randomly choose two images from our population and send them through shared CNN stacked layers. The vector we get as output is the image embedding. Then we take a distance/difference between the two embeddings. This difference is finally passed through an activation function to check if the image is of the same person.Note that the embedding matrix we get in this process is not for any unique person, but to find features that can tell us How similar the two faces look.I hope this article has given you a strong foundation in the concept of embedding and helped you understand how important it is when it comes to analyzing unstructured data. In simple terms, we are trying to create a structured data out of unstructured underlying data using these embeddings. This structured data has the meaning of underlying data embedded in form of a vector and hence the name embedding.If you have any ideas or suggestions regarding the topic, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/introductory-guide-understand-how-anns-conceptualize-new-ideas/
"AVBytes: AI & ML Developments this week  a Major R Update, Nvidia DL model autocompletes pictures, Windows Support for PyTorch, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|An Introductory Guide to Understand how ANNs Conceptualize New Ideas (using Embedding)|Google has Released TensorFlow 1.8.0!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This past week saw updates on major libraries and tools that heavily focused on saving time for data scientists. Tableau Prep was launched which makes data cleaning a breeze, the latest version of R focuses on speed improvements and PyTorch has added Windows support.. We covered each of these in AVBytes.Apart from these updates, DeepCode was released to the general public  a tool that scans and finds bugs and improvements in your code, NVIDIAs deep learning model was open sourced that completes the missing parts in images, and other significant happenings took place in the data science and machine learning community.Scroll down to view all the articles from last week. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.The above AVBytes were published from 23rd to 29th April, 2018.I love the Tableau Prep product! I have recently used it and it is a breeze how easy data cleaning has become. And the ease with which it integrates with Tableau makes it a complete MUST HAVE for data visualization tasks.There were so many new updates and releases last week!What excited you the most?",https://www.analyticsvidhya.com/blog/2018/04/avbytes-ai-ml-developments-this-week-300418/
Google has Released TensorFlow 1.8.0!,Learn everything about Analytics|Overview:|Introduction|Our take on this:,"Other important features and changes|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  a Major R Update, Nvidia DL model autocompletes pictures, Windows Support for PyTorch, etc.|Swift for TensorFlow is now Open Sourced on GitHub|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The TensorFlow updates keep on rolling!Less than a month ago, the team behind this ultra-popular library had released TensorFlow 1.7 for the general public, with the TensorFlowRT and TensorFlow Debugger plugin features.Now, they have unveiled the full version of TensorFlow 1.8.0, just a week after the Release Candidate release. It contains modifications and improvements on previously launched features like Eager Execution and tf.keras.In this article, well take a look at the main features that come packaged in this release.Let us have a look at the major features and improvements in TensorFlow 1.8.0:Very recently (and in the last couple of updates) tf.data, tk.keras, Eager execution were released and demonstratedin the TensorFlow Dev summit! Here are a major features and improvements in the same:tf.data:Eager Execution:tf.keras:Accelerated Linear Algebra (XLA):TensorFlow Debugger (tfdbg) CLI:tf.contrib:There are a few other changes made which you can see on the github page.Within less than a months time, TensorFlow team has provided updates and bug fixes to their latest release. TensorFlow has also provided a guide to install r1.8to your machines as well. Looking at the number of features they have added in such less time they have got us excited about whats coming up.But a quick glance on Reddit shows us that the ML community is divided on the number of updates TensorFlow seems to be getting lately. It seems a new update is rolled out at a never-before-seen frequency and that has turned into a source of some agitation among data scientists.What do you think about this latest release? Any feature you are particularly looking forward to? Use the comments section below to let us know.",https://www.analyticsvidhya.com/blog/2018/04/tensorflow-1-8-0/
Swift for TensorFlow is now Open Sourced on GitHub,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google has Released TensorFlow 1.8.0!|DeepCode Analyzes and Cleans your Code withthe Help of Machine Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Swift is an open source programming language which has really taken off in the last couple of years. It has a large, and ever expanding user base. And TensorFlow, as you will no doubt be aware, is one of the most popular open source libraries used in machine learning. So combining the two together was a no-brainer for the folks at TensorFlow.Swift for TensorFlow was demod at the TensorFlow Conference last month and the team behind the technology has now open sourced the code on GitHub for the entire community. Their aim is to provide a new interface to TensorFlow that will build on its already awesome capabilities, while taking its usability to a whole new level.According to the official blog post by the TensorFlow team, Swift for TensorFlow provides a new programming model that combines the performance of graphs with the flexibility and expressivity of Eager execution, with a strong focus on improved usability at every level of the stack. Note that this isnt just a TensorFlow API wrapper written in the Swift language. The team has added compiler and language enhancements to Swift with the aim of providing a top notch user experience for data scientists and machine learning developers.You can access the GitHub repository hereand watch the TensorFlow conference launch in the below video:This is still in its very nascent stages so isnt yet ready to be written into deep learning models. The team admits that the goals it has in mind while launching this are still a while away from being achieved. But there is a lot of potential here that is as yet untapped.What I liked about this release is that the team has documented each step in extreme detail with the assumption that most of the users will not be familiar with Swift, or wouldt have used it before.",https://www.analyticsvidhya.com/blog/2018/04/swift-tensorflow-now-open-sourced-github/
DeepCode Analyzes and Cleans your Code withthe Help of Machine Learning,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Swift for TensorFlow is now Open Sourced on GitHub|Tableau Prep  Redefine Data Cleaning with Tableau|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When we write code, we follow certain inherent guidelines since we started programming. There are things we inevitably miss, even when reviewing the script again. This is where machines are proving to be an unprecedented success. Once they are trained to perform a task, they do so with incredible time-saving speed.DeepCode, a Swiss based organization, has tapped into this space and developed a tool that assists programmers with improving their code, and finding hidden bugs. To perform this, it reads your public and private GitHub repositories. Its a lot like the super-popular Grammarly tool, but for programmers. It connects to multiple data sources and learns all the information about the code.As with all machine learning applications, the more data it comes across, the better it gets. It is built to continuously and automatically improve its accuracy and precision with each new knowledge it encounters.The developers behind this system trained their machine learning model on a corpus of almost 250,000 rules. The model has the capability of reading your public and private GitHub repositories and then suggests fixes to the problems in your script. The suggestions are pretty accurate, they make the code remain compatible, and end up improving your program.We ran the code on a GitHub repository and got the below suggestion to fix the code:The tool currently supports Python, JavaScript and Java. They are working on adding other languages as well to their toolset.You can try it out on their website.This is a concept which will be welcomed by data scientists, programmers, and organizations overall. It scans the code and comes up with suggestions really quickly. And the advantage for DeepCode here is that the more people run their code on their tool, the more data they will collect to improve the machine learning model behind this system. And as you can see in the above section, it gave an intuitive suggestion for us!Are you planning to use this tool? Use the comments below to get involved in the discussion!",https://www.analyticsvidhya.com/blog/2018/04/deepcode-analyzes-and-cleans-your-code-with-the-help-of-machine-learning/
Tableau Prep  Redefine Data Cleaning with Tableau,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|DeepCode Analyzes and Cleans your Code withthe Help of Machine Learning|A Guide to Sequence Prediction using Compact Prediction Tree (with codes in Python)|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve been in data science long enough, you will have heard the popular line  people spend 80% of their time prepping data and only 20% of their time analyzing it. And as more and more data sources become available, dealing with dirty data becomes more time consuming and challenging. It is one of the most tedious and repetitive processes in a data scientists job.But what does Tableau have to do with it?It has long been touted as one of the best (if not the best) data visualization tool out there but the ability to clean data has always been a gaping hole in its repertoire. With Tableau Prep, the company has integrated this long missing feature into its offerings. Tableau is popular for its neat interface and deep features  and with this latest product, that visual element gives you a deeper understanding and ability to deal with the data you collect.Tableau Prep reduces the struggle of common tasks, like joins, pivots, unions, etc. And of course, you dont need to perform any coding to do this. Isnt that one of the major attractions of Tableau? This product is built on three coordinated views:The algorithms behind Tableau Prep turn repetitive tasks into one-click or drag-and-drop operations. It is fully integrated with the Desktop version of Tableau so you can move smoothly from data preparation to data exploration.Try it out by downloading Tableau Prep from here.As a long-term Tableau user, I am loving this product! The release has been really well received in the data visualization community. Its the ideal support tool for Tableau Desktop and its easy integration with the main software has been well developed. The amount of time this will save the user is one of the biggest unique selling points of this tool.Use the comments section below to tell us your experience using it!",https://www.analyticsvidhya.com/blog/2018/04/tableau-prep-brand-new-tableau-product-data-cleaning/
A Guide to Sequence Prediction using Compact Prediction Tree (with codes in Python),Learn everything about Analytics|Introduction|Table of Contents|Primer about Sequence Prediction|Enter CPT (Compact Prediction Tree)|Understanding the Data Structures in CPT|Understanding how Training and Prediction works in CPT|Creating Model and Making Predictions|End Notes,"Current landscape of solutions|1. Prediction Tree|2. Inverted Index (II)|3. LookUp Table (LT)|Training Phase|Prediction Phase|Share this:|Like this:|Related Articles|Tableau Prep  Redefine Data Cleaning with Tableau|Winning Solutions & Codes from AVs Signature Hackathon  Lord of the Machines|
NSS
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Sequence prediction is one of the hottest application of Deep Learning these days. From building recommendation systems to speech recognition and natural language processing, its potential is seemingly endless. This is enabling never-thought-before solutions to emerge in the industry and is driving innovation.There are many different ways to perform sequence prediction such as using Markov models, Directed Graphs etc. from the Machine Learning domain and RNNs/LSTMs from the Deep Learning domain.In this article, we will see how we can perform sequence prediction using a relatively unknown algorithm called Compact Prediction Tree (CPT). Youll see how this is a surprisingly simple technique, yet its more powerful than some very well known methods, such as Markov Methods, Directed Graphs, etc.I recommend reading this article before going further A Must-Read Introduction to Sequence Modelling(with use cases). In this, Tavishintroduced us to an entirely new class of problems called Sequence Modelling, along with some very good examples of their use cases and applications.Sequence prediction is required whenever we can predict that a particular event is likely to be followed by another event and we need to predict that.Sequence prediction is an important class of problems which finds application in various industries. For example:There are numerous additional areas where Sequence Prediction can be useful.To see different kinds of solutions available for solving problems in this field, we had launched the Sequence Prediction Hackathon. The participants came up with different approaches and the most popular of them was LSTMs/RNNs which was used by most of the people in the top 10 on the private leaderboard.LSTMs/RNNs have become a popular choice for modelling sequential data, be it text, audio, etc. However, they suffer from two fundamental problems:Compact Prediction Tree (CPT) is one such algorithm which I found to be more accurate than traditional Machine Learning models, like Markov Models, and Deep Learning models like Auto-Encoders.The USP of CPT algorithm is its fast training and prediction time. I was able to train and make predictions within 4 minutes on the Sequence Prediction Hackathon dataset mentioned earlier.Unfortunately, only a Java implementation of the algorithm exists and therefore is not as popular among Data Scientists in general (especially those who use Python).So, I have created a Python version of the library using the documentation developed by the algorithm creator. The Java code certainly helped in understanding certain sections of the research paper.The library for public usage is present hereand you are most welcome to make contributions to it. The library is still incomplete in the sense that it does not have all recommendations of the author of the algorithm, but is good enough to get a decent score of 0.185 on the hackathon leaderboard, all within a few minutes. Upon completion, I believe the library should be able to match the performance of RNNs/LSTMs for this task.In the next section, we will go through the inner workings of the CPT algorithm, and how it manages to perform better than some of the popular traditional machine learning models like Markov Chains, DG, etc.As a prerequisite, it is good to have an understanding of the format of the data accepted by the Python Library CPT. CPT accepts two .csv files  Train and Test. Train contains the training sequences while the test file contains the sequences whose next 3 items need to be predicted for each sequence. For the purpose of clarity, the sequences in both Train and Test files are defined as below:Note that the sequences could be of varying length. Also, One-hot encoded sequences will not give appropriate results.The CPT algorithm makes use of three basic data structures, which we will talk about briefly below.A prediction tree is a tree of nodes, where each node has three elements:A Prediction Tree is basically a trie data structure which compresses the entire training data into the form of a tree. For readers who are not aware of how a trie structure works, the trie structure diagram for the below two sequences will clarify things.Sequence 1: A, B, C
Sequence 2: A, B, DThe Trie data structure starts with the first element A of the sequence A,B,C and adds it to the root node. Then B gets added to A and C to B. The Trie again starts at the root node for every new sequence and if an element is already added to the structure, then it skips adding it again.The resulting structure is shown above. So this is how a Prediction Tree compresses the training data effectively.Inverted Index is a dictionary where the key is the item in the training set, and value is the set of the sequences in which this item has appeared. For example,Sequence 1: A,B,C,D
Sequence 2: B,C
Sequence 3: A,BThe Inverted Index for the above sequence will look like the below:II = {
 A:{Seq1,Seq3},
 B:{Seq1,Seq2,Seq3},
 C:{Seq1,Seq2},
 D:{Seq1}
 }A LookUp Table is a dictionary with a key as the Sequence ID and value as the terminal node of the sequence in the Prediction Tree. For example:Sequence 1: A, B, C
Sequence 2: A, B, DLT = {
 Seq1 : node(C),
 Seq2 : node(D)
 }We will go through an example to solidify our understanding of the Training and Prediction processes in the CPT algorithm. Below is the training set for this example:As you can see, the above training set has 3 sequences. Let us denote the sequences with ids: seq1, seq2 and seq3. A, B, C, and D are the different unique items in the training dataset.The training phase consists of building the Prediction Tree, Inverted Index (II), and the LookUp Table (LT) simultaneously. We will now look at the entire training process phase.Step 1: Insertion of A,B,C.We already have a root node and a current node variable set to root node initially.We start with A, and check if A exists as the child of the root node. If it does not, we add A to the child list of the root node, add an entry of A in Inverted Index with value seq1, and then move the current node to A.We look at the next item, i.e B, and see if B exists as the child of the current node, i.e, A. If not, we will add B to the child list of A, add an entry of B in the Inverted Index with value seq1 andthen move the current node to B.We repeat the above procedure till we are done adding the last element of seq1. Finally, we will add the last node of seq1, C, to the LookUp table with key = seq1 and value = node(C).Step 2: Insertion of A,B.Step 3: Insertion of A,B,D,C.Step 4: Insertion of B,C.)We do keep doing this till we exhaust every row in the training dataset (remember, a single row represents a single sequence). We now have all the required data structures in place to start making predictions on the test dataset. Lets have a look at the prediction phase now.The Prediction Phase involves making predictions for each sequence of the data in the test set in an iterative manner. For a single row, we find sequences similar to that row using the Inverted Index(II). Then, we find the consequent of the similar sequences and add the items in the consequent in a Counttable dictionary with their scores. Finally, the Counttable is used to return the item with the highest score as the final prediction. We will see each of these steps in detail to get an in-depth understanding.Target Sequence  A, BStep 1: Find sequences similar to the Target Sequence.Sequences similar to the Target Sequences are found by making use of the Inverted Index. These are identified by:For example:Sequences in which A is present = {Seq1,Seq2,Seq3}
Sequences in which B is present = {Seq1,Seq2,Seq3,Seq4}Similar sequences to Target Sequence = intersection of set A and set B = {Seq1,Seq2,Seq3}Step 2: Finding the consequent of each similar sequence to the Target SequenceFor each similar sequence, consequent is defined as its longest sub-sequence after the last occurrence of the last item of the Target Sequence in the similar sequence minus the items present in the Target Sequence.** Note this is different from what the developers have mentioned in their research paper, but this has worked for me better than their implementation.Lets understand this with the help of an example:Target Sequence = [A,B,C]
Similar Sequence = [X,A,Y,B,C,E,A,F]
Last item in Target Sequence = C
Longest Sub-Sequence after last occurrence of C in Similar Sub-Sequence = [E,A,F]
Consequent = [E,F]
Step 3: Adding elements of the Consequent to the Counttable dictionary along with their score.The elements of consequent of each similar sequence is added to the dictionary along with a score. Lets continue with the above example for instance. The score for the items in the Consequent [E,F] is calculated as below:current state of counttable = {}, an empty dictionaryIf the item is not present in the dictionary, then,
score = 1 + (1/number of similar sequences) +(1/number of items currently in the countable dictionary+1)*0.001otherwise,
score = (1 + (1/number of similar sequences) +(1/number of items currently in the countable dictionary+1)*0.001) * oldscoreSo for element E, i.e. the first item in the consequent, the score will bescore[E] = 1 + (1/1) + 1/(0+1)*0.001 = 2.001
score[F] 1 + (1/1) + 1/(1+1)*0.001 = 2.0005After the above calculations, counttable looks like,counttable = {'E' : 2.001, 'F': 2.0005}Step 4: Making Prediction using CounttableFinally, the key is returned with the greatest value in the Counttable as the prediction. In the case of the above example, E is returned as a prediction.Step 1: Clone the GitHub repository from here.git clone https://github.com/NeerajSarwan/CPT.git
Step 2: Use the below code to read the .csv files, train your model and make the predictions.#Importing everything from the CPT file
 from CPT import *#Importing everything from the CPT file
 from CPT import *#Creating an object of the CPT Class
 model = CPT()#Reading train and test file and converting them to data and target.
 data, target = model.load_files(./data/train.csv,./data/test.csv)#Training the model
 model.train(data)#Making predictions on the test dataset
 predictions = model.predict(data,target,5,1)In this article, we covered a highly effective and accurate algorithm for sequence predictions  Compact Prediction Tree. I encourage you to try it out yourself on the Sequence Prediction Hackathon dataset and climb higher on the private leaderboard!If you want to contribute to the CPT library, feel free to fork the repository or raise issues. If you know of any other methods to perform Sequence Predictions, write them in the comments section below. And do not forget to star the CPT library. ",https://www.analyticsvidhya.com/blog/2018/04/guide-sequence-prediction-using-compact-prediction-tree-python/
Winning Solutions & Codes from AVs Signature Hackathon  Lord of the Machines,Learn everything about Analytics|Introduction|Table of Contents|About the Competition|The Problem Statement|Winners!|Winners Solutions|Key learnings from the Competition|End Notes,"Rank 3: Aditya and Akash|Rank 2: SRK and Mark|Rank 1: Kunal Chakraborty|Participate in ourHackathonsand compete with the bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|A Guide to Sequence Prediction using Compact Prediction Tree (with codes in Python)|R 3.5.0 has been Released! This is a Major Update You Should Not Miss|
Pranav Dar
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Adityas Approach|Cross Validation|Hyper Parameter Tuning|Akashs Approach|Ensemble Model,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lord of the Machines, Analytics Vidhyas recently concluded signature hackathon, was one of the most intriguing and challenging competitions we have hosted. It featured a real-world dataset and some really awesome innovative solutions from data scientists around the world.Initially planned for a duration of 2 days, the hackathon was extended to span 9 days in total to give participants more time to fine tune and improve their models. An incredible 3500+ participants registered for the signature hackathon!In this article, the top three winners have shared their approach through which they climbed to the top of the leaderboard. We have also provided the GitHub code link for each approach. All the three winning solutions were executed in python.For all those who were not able to participate, you missed out on a cracking competition! Nevertheless, theres always a next time. Stay tuned for upcoming hackathons.The journey to become a data scientist is often a long, difficult and obstacle-filled path. There are problems to solve, models to be built and conclusions to be drawn.Analytics Vidhya hosted Lord of the Machines, a data science / machine learning hackathon designed to discover the best data scientists in the community. In this hackathon, you  the participants  were given the opportunity to come up with innovative and exciting data science solutions to claim your supremacy.Email Marketing is still the most successful marketing channel and the essential element of any digital marketing strategy. Marketers spend a lot of time in writing that perfect email, laboring over each word, catchy layouts on multiple devices to get them best in-industry open rates & click rates.How can I build my campaign to increase the click-through rates of email?  a question that is often heard when marketers are creating their email marketing plans. Can we optimize our email marketing campaigns with Data Science? Its time to unlock marketing potential and build some exceptional data-science products for email marketing.Analytics Vidhya sends out marketing emailers for various events such as conferences, hackathons, etc. For this hackathon, we provided a sample of user-email interaction data from July 2017 to December 2017. Participants wererequired to predict the click probability of links inside a mailer for email campaigns from January 2018 to March 2018.The evaluation metric used was AUC ROC.And the wait is over! Below are the final top 3 winners of the Lord of the Machines:Rank 1: Kunal ChakrabortyRank 2: SRK and MarkRank 3: Aditya and AkashHere are the final rankings of all the participants in theLord of the Machineshackathon.The top 3 winners have shared their approach with us. We have listed them below in their own words, along with the code, for your perusal.Aditya and Akash worked on different models and then teamed up. Below are both their approaches.Multiple classification model has been create to predict the click probability of links inside mailer for email campaign. Following derived features has been created, for training different modelCampaign_id has been used to split data into train and validation.Manual tuning has been performed based on public leaderboard and cross validation score.ModellingI posed it as a problem of sequence prediction where we want to find whether a user will click on an email, given his past interactions on platform. The first thing that comes to mind when we think of sequence prediction problems is RNN or more specifically LSTM.FeaturesI formed sequences of users actions (in form of clicked and opened). 4 sequences were formed:These sequences acted as 4 features for sequential input.Network Architecture(s)The output of these models gave me a probability whether these users will click the next email or not. I used this probability across all emails sent to that user (I did not want to add prediction to sequence and predict again because that can cause errors to propagate further).This allowed me to make predictions for the users for whom we have some data (previous behavior), but in the test set, 20% of the entries were for users for whom we do not have any data (aka cold start).To deal with Cold Start, I grouped by campaign_id and sent_weekday and sent_quarter_of_day and filled the missing values by 90% quantile across each group.All the model prediction has been rank averaged to reach the final submission.Link to Code.Most of our time was spent on creating new features. We did validation split based on campaign ids. Our best single model is a light GBM that scored 0.7051 in the leaderboard. The list of important features we used are:Link to Code.I created several features based on textual information and user behavior to arrive at my final solution. The features created were:This became my general frame work for data preparation before feeding it into any model. An xgboost model with these set of features gave me a score of 0.695+ on the public leaderboard. What followed after this was sheer pragmatism. I created several models based on approximately the same framework and differentiated them by adding variability. Some of the important variations were:These are just some of the features. I created many notebooks and added/dropped/modified many features and performed many experiments which most of the time gave me a public leaderboard score in the vicinity of (0.685  0.69). Even though the performance of all the models was similar, their predictions were not highly correlated. This gave me the opportunity to take advantage of weighted ensembles to arrive at a higher score.I took the most similar scoring prediction files with the least correlation and took their weighted average. I continued this process in an uphill fashion. This led to my four best performing predictions with scores (0.699  0.7011). I again followed the same heuristic to arrive at my final score which gave me a public leader board score of 0.704. This entire process is very similar to model stacking where diverse base classifiers prediction is fed to a meta classifier to arrive at better predictions. Only in my case, it was me manually adjusting the weights assigned to different models by validating them against the public leader board.Link to Code.Below are the key learnings from this competition:This was one of the most interesting and challenging competitions we have hosted so far on Analytics Vidhya. It saw great participation, some really good solutions. I highly recommend going through the approaches and code links mentioned above to gain a deeper understanding of how these competition winners structure their thinking.For those who missed out this time, dont worry! We will regularly host hackathons so be sure to head over to our DataHack platform and get cracking on the practice problems we have for you on various domains.",https://www.analyticsvidhya.com/blog/2018/04/winning-solutions-codes-from-avs-signature-hackathon-lord-of-the-machines/
R 3.5.0 has been Released! This is a Major Update You Should Not Miss,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Winning Solutions & Codes from AVs Signature Hackathon  Lord of the Machines|PyTorch 0.4.0 Released with Windows Support!|
Pranav Dar
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Heres some exciting news for R users! R 3.5.0 has been released this week and it comes packed with new features, major improvements and bug fixes. The most exciting thing packaged into the entire update? Awesome speed improvements!Despite the recent rise of the Python programming language, R continues to maintain a massive and loyal user base. It is still the tool of choice for many organizations, data scientists and researchers. I personally love using R and its my go-to tool any time I want to perform exploratory data analysis on a dataset.The biggest update in this release is under the hood  speed and performance improvement! ALTREP, an alternative representation for R objects, has been added to the R engine. How does this improve performance? ALTREP uses more efficient representations of vectors. This leads to less memory usage and faster computation.Another new feature, which will be more visible to users, is that ALL packages will now be byte-compiled on installation. Rs base packages and those uploaded on CRAN already had this feature but this move will benefit packages installed from GitHub. Also, you will notice R performing better when a lot of packages are loaded in the same time frame.Apart from these, we have summarised a few other additions and improvements below which we felt are important:But take note  since this classifies as a major release, you will have to re-install any packages you use in R. Any old scripts you have should still continue to work without need for modifications. But its best to be safe before you upgrade so ensure you perform thorough checks and take appropriate backups.Download the latest version from here. You can also read the official documentation for this release herewhich includes the full list of additions, improvements and bug fixes.Given the significant changes in this release, it would be prudent to wait for the maintenance release if you are using it for production. However, if youre a data scientist, update away! There are a lot of benefits of doing this. We tried out a few high-level operations on dataframes and can report that we got the output 3 times faster than the previous version.Let us know how much quicker your code runs on updating!",https://www.analyticsvidhya.com/blog/2018/04/r-3-5-0-released/
PyTorch 0.4.0 Released with Windows Support!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|R 3.5.0 has been Released! This is a Major Update You Should Not Miss|NVIDIAs DL Model can Complete the Missing Parts in a Photo with Incredible Results!|
Aishwarya Singh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",New Features Launched|Major Core Changes and improvements,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"PyTorch users have been waiting a long time for the package to be officially launched on Windows and that wait is finally over! The latest release, PyTorch 1.4.0, has added Windows support among a slew of other additions and major improvements (and, needless to say, bug fixes).For people who have not used this before, PyTorch is a Python package that developers usually use either as a replacement for NumPy (to utilise the power of GPUs), or as a deep learning platform that provides amazing flexibility and speed.
In this article, we give you an overview of the most important features and significant changes in the PyTorch 0.4.0 version.This was just a glimpse of the latest PyTorch version. There are many other interesting features, bug fixes and improvements made which you can view on theirGitHub page.You can also refer to this migration guide to transition your code to the newest version.As you might have guessed by now, we are most excited about PyTorch finally makes its debut on Windows (officially). There have been previous workarounds provided in the community by users but making this official is a welcome feature indeed. I really liked the addition of the probability distributions as well.The Tensor-Variable merge and Zero Dimension (scalar) Tensor improvements should make your code tidier and easier to read. Which feature are you most looking forward to using? Let us know your opinion in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/pytorch-0-4-0-windows-support-released/
NVIDIAs DL Model can Complete the Missing Parts in a Photo with Incredible Results!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|PyTorch 0.4.0 Released with Windows Support!|Swiftapply  A Python Package for Efficient and Superfast use of Pandas apply Function|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine youre given half a photo and asked to fill in the other half. Even with the variety of softwares in the market, producing realistic results would be a tall order.Researchers at NVIDIA have unveiled a state-of-the-art deep learning model that can edit images and also reconstruct incomplete ones. It can understand the image and fill in missing pixels.The method behind this algorithm is being called image inpainting.In order to train their deep neural network, the researchers generated over 55,000 masks and holes of different shapes and sizes. No model is built without a testing dataset so they generated 25,000 such masks and holes for test purposes. In order to improve the accuracy of the reconstructed photos, these holes were divided into six different categories based on the input images.You might be wondering at this point about the underlying algorithm. According to NVIDIAs blog post, using NVIDIA Tesla V100 GPUs and the cuDNN-accelerated PyTorch deep learning framework, the team trained their neural network by applying the generated masks to images from the ImageNet, Places2 and CelebA-HQ datasets.In the training phase, missing parts (or holes as mentioned above) are shown to the model with the complete images in order for it learn how to perform the reconstruction. While in the testing phase, those missing parts which were held out from the training phase are introduced. This leads to unbiased accuracy results.The team claims this is the best model out there in the industry and their results prove that.NVIDIA have also published their research paper which you can read here. Check out the two minute video below where the researchers showcase this algorithm:Deep learning never ceases to amaze me. Except perhaps the eyes in the above video, everything else looks remarkably life-life. We have previously covered NVIDIAs FastPhotoStyle librarybut this is quite a breakthrough in the image processing field.Existing studies in this area have previouslyused a standard convolutional network over the corrupted image but this model uses the concept of partial convolutions. I suggest you go through their research paper to gain a better understanding of this concept.",https://www.analyticsvidhya.com/blog/2018/04/nvidias-deep-learning-model-reconstruct-incomplete-photos-incredible-results/
Swiftapply  A Python Package for Efficient and Superfast use of Pandas apply Function,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIAs DL Model can Complete the Missing Parts in a Photo with Incredible Results!|A Comprehensive Guide to Understand and Implement Text Classification in Python|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is of the absolute essence when youre working on a machine learning project. You dont want to be waiting around for pandas to finish its loop before you can proceed with your model building, do you? But this is exactly what happens in most cases.When you use the apply function in python, it lets you apply any function you specify on the pandas dataframe. But as pointed out so well in this article, the apply function is basically a for loop and it takes its own sweet time to run. How can we go about combating this?Well, this is what led to the creation of the Swiftapply package. Developed by Jason Carpenter, this package aims to make the execution of the apply function more efficient, and less time consuming. As the developer has explained in this blog post, swiftapply tries to run your operation in a vectorized fashion. Failing that, it automatically decides whether it is faster to perform dask parallel processing or use a simple pandas apply.How to go about installing it? You can do so using pip. Follow the below command:You can view the GitHub respository for Swiftapply here. To read more abouthow this apply function works, we recommend reading this documentation.Pandas is an amazing flexible library and most of the operations you perform using it are still done fairly quickly. If you have a Linux or Mac machine, you can also try Pandas on Ray to boost your codes performance. You can also try compiling with cython or numba. All in all, there are quite a lot of options available in the market now for accelerating your operations.Use this package and let us know your experience in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/swiftapply-python-package-makes-apply/
A Comprehensive Guide to Understand and Implement Text Classification in Python,Learn everything about Analytics|Introduction|Project to apply Text Classification|Problem Statement|Table of Contents|Getting your machine ready|1. Dataset preparation|2. Feature Engineering|3. Model Building|Improving Text Classification Models|Projects||End Notes,"2.1 Count Vectors as features|2.2 TF-IDF Vectors as features|2.3 Word Embeddings|2.4 Text / NLP based features|2.5 Topic Models as features|3.1 Naive Bayes|3.2 Linear Classifier|3.3 Implementing a SVM Model|3.4 Bagging Model|3.5 Boosting Model|3.6 Shallow Neural Networks|3.7 Deep Neural Networks|Learn, compete, hack and get hired!|Share this:|Related Articles|Swiftapply  A Python Package for Efficient and Superfast use of Pandas apply Function|AVBytes: AI & ML Developments this week  Pandas to end Python 2 Support, Intels Framework-Neutral Library, Googles Cancer Detection Algo, etc.|
Shivam Bansal
|40 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",3.7.1 Convolutional Neural Network|3.7.2 Recurrent Neural Network  LSTM|3.7.3 Recurrent Neural Network  GRU|3.7.4 Bidirectional RNN|3.7.5 Recurrent Convolutional Neural Network,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the widely used natural language processing task in different business problems is Text Classification. The goal of text classification is to automatically classify the text documents into one or more defined categories. Some examples of text classification are:If youre a beginner in NLP, then youve come to the right place! We have designed a comprehensive NLP course just for you. It is one of our most popular courses and is just the right guide to kick start your NLP journey.The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.Formally, given a training sample of tweets and labels, where label 1 denotes the tweet is racist/sexist and label 0 denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.Practice NowIn this article, I will explain about the text classification and the step by step process to implement it in python.Text Classification is an example of supervised machine learning task since a labelled dataset containing text documents and their labels is used for train a classifier. An end-to-end text classification pipeline is composed of three main components:1. Dataset Preparation:The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.
2. Feature Engineering:The next step is the Feature Engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.
3. Model Training:The final step is the Model Building step in which a machine learning model is trained on a labelled dataset.4. Improve Performance of Text Classifier: In this article, we will also look at the different ways to improve the performance of text classifiers.Note : This article does not narrate NLP tasks in depth. If you want to revise the basics and come back here, you can always go through this article.Lets implement basic components in a step by step manner in order to create a text classification framework in python. To start with, import all the required libraries.You would need requisite libraries to run this code  you can install them at their individual official linksFor the purpose of this article, I am the using dataset of amazon reviews which can be downloaded at thislink. The dataset consists of 3.6M text reviews and their labels, we will use only a small fraction of data. To prepare the dataset, load the downloaded data into a pandas dataframe containing two columns  text and label. (Source)Next, we will split the dataset into training and validation sets so that we can train and test classifier. Also, we will encode our target column so that it can be used in machine learning models.The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.2.1 Count Vectors as features
2.2 TF-IDF Vectors as features2.3 Word Embeddings as features
2.4 Text / NLP based features
2.5 Topic Models as featuresLets look at the implementation of these ideas in detail.Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document.TF-IDF score represents the relative importance of a term in the document and the entire corpus. TF-IDF score is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)
IDF(t) = log_e(Total number of documents / Number of documents with term t in it)TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)a. Word Level TF-IDF :Matrix representing tf-idf scores of every term in different documents
b. N-gram Level TF-IDF :N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams
c. Character Level TF-IDF :Matrix representing tf-idf scores of character level n-grams in the corpusA word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec. Any one of them can be downloaded and used as transfer learning. One can read more about word embeddingshere.Following snnipet shows how to use pre-trained word embeddings in the model. There are four essential steps:You can download the pre-trained word embeddings fromhereA number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:These features are highly experimental ones and should be used according to the problem statement only.Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that contains best information in the collection. I have used Latent Dirichlet Allocation for generating Topic Modelling Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents. One can read more about topic modellinghereLets see its implementation:The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:Lets implement these models and understand their details. The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed.Implementing a naive bayes model using sklearn implementation with different featuresNaive Bayes is a classification technique based on Bayes Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other featurehere .Implementing a Linear Classifier (Logistic Regression)Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. One can read more about logistic regressionhereSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes. One can read more about ithereImplementing a Random Forest ModelRandom Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family. One can read more about Bagging and random forestshereImplementing Xtereme Gradient Boosting ModelBoosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). Read more about these modelshereA neural network is a mathematical model that is designed to behave similar to biological neurons and nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled data. A shallow neural network contains mainly three types of layers  input layer, hidden layer, and output layer. Read more about neural networkshereDeep Neural Networks are more complex neural networks in which the hidden layers performs much more complex operations than simple sigmoid or relu activations. Different types of deep learning models can be applied in text classification problems.In Convolutional neural networks, convolutions over the input layer are used to compute the output. This results in local connections, where each region of the input is connected to a neuron in the output. Each layer applies different filters and combines their results.Read more about Convolutional Neural NetworkshereUnlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a memory state of the neurons. This state allows the neurons an ability to remember what have been learned so far.The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.Read more about LSTMshereGated Recurrent Units are another form of recurrent neural networks. Lets add a layer of GRU instead of LSTM in our network.RNN layers can be wrapped in Bidirectional layers as well. Lets wrap our GRU layer in bidirectional layer.Once the essential architectures have been tried out, one can try different variants of these layers such as recurrent convolutional neural network. Another variants can be:While the above framework can be applied to a number of text classification problems, but to achieve a good accuracy some improvements can be done in the overall framework. For example, following are some tips to improve the performance of text classification models and this framework.1. Text Cleaning :text cleaning can help to reducue the noise present in text data in the form of stopwords, punctuations marks, suffix variations etc. Thisarticlecan help to understand how to implement text classification in detail.2. Hstacking Text / NLP features with text feature vectors :In the feature engineering section, we generated a number of different feature vectros, combining them together can help to improve the accuracy of the classifier.3. Hyperparamter Tuning in modelling :Tuning the paramters is an important step, a number of parameters such as tree length, leafs, network paramters etc can be fine tuned to get a best fit model.4. Ensemble Models :Stacking different models and blending their outputs can help to further improve the results. Read more about ensemble modelshereNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:In this article, we discussed about how to prepare a text dataset like cleaning/creating training and validation dataset, perform different types of feature engineering like Count Vector/TF-IDF/ Word Embedding/ Topic Modelling and basic text features, and finally trained a variety of classifiers like Naive Bayes/ Logistic regression/ SVM/ MLP/ LSTM and GRU. At the end, discussed about different approach to improve the performance of text classifiers.Note: There is a video course, Natural Language Processing using Python, with 3 real life projects, two of them involve text classification.Did you find this article useful ? Share your views and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/
"AVBytes: AI & ML Developments this week  Pandas to end Python 2 Support, Intels Framework-Neutral Library, Googles Cancer Detection Algo, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|A Comprehensive Guide to Understand and Implement Text Classification in Python|The Rise of AI Continues  Robots have Mastered the Task of Assembling Furniture|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Portability of code and environment is one of the challenge every data scientist faces. The code can be framework dependent or it can be machine dependent. The end result  A model that works like a charm on one machine might not do so on another.Last week we saw a couple of significant developments in this regard  first, it was announced that Pandas would no longer be supported on Python 2 from next year. Then, we saw Intel open source nGraph, a framework-neutral compiler, library and runtime that allows you to run models on various devices.Apart from these, this past week saw other huge developments in the machine learning industry. Google has created a deep neural network that can detect cancer in real-time, Cars.com is using machine learning to predict sales and to enrich the buyer experience, and other fascinating ongoings were covered in AVBytes last week.Scroll down to view all the articles from last week. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.Source: ZDNetThe above AVBytes were published from 16th to 22nd April, 2018.I really liked the generating cartoons from just text concept. Granted its still in its infancy and has a while to go, but the building blocks are there. What excited you the most among the above developments? Use the comments section below to get involved!",https://www.analyticsvidhya.com/blog/2018/04/avbytes-ai-ml-developments-this-week-230418/
The Rise of AI Continues  Robots have Mastered the Task of Assembling Furniture,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Pandas to end Python 2 Support, Intels Framework-Neutral Library, Googles Cancer Detection Algo, etc.|Intel open sources nGraph  Now Focus on Data Science and Stop Worrying about Frameworks and Hardware|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youre one of the folks worried about the how AI will replace humans in the future, this latest development might add fuel to that fire.Researchers from the Nanyang Technological University in Singapore have built 2 robot arms, powered by deep learning and 3D sensors, that can assemble a workable IKEA chair.The 2 robotic arms, working together, completed the job of assembling the chair in 20 minutes and 19 seconds. All the parts required to build it were first placed in front of them. The AI took almost ten minutes deciding how to go about building the chair and then spent the next nine minutes doing the task. According to an Ikea representative, it takes a person 10-15 minutes on average to do this task.If youre a regular follower of AVBytes, you wouldve guessed by now that this task did not happen at the first attempt. It took a lot of training data and multiple failed iterations to finally get the AI right. The training data included things like images of how chairs looked, how different parts are glued together and where, etc.The research team is hoping this technology will be more mainstream in the next 5-10 years.Their research was published in the Sciencemag here. Check out the video below showing this AI technology:These are scary and exciting times in the AI world. We have previously seen AI being able to cook burgers and another AI able to sort through our stuff in a human-like fashion. Studies likes these are paving the way for AI to penetrate through in the industry for tasks like building computers, vehicles, and even complete houses.But that is a long, long way away from now. AI, despite its recent rise, is still very much in its nascent stages. Machines still struggle to differentiate between moving parts and require tons of data to be trained on.What do you think about this technology? Does it scare or excite you?",https://www.analyticsvidhya.com/blog/2018/04/ai-powered-robots-mastered-task-assembling-furniture/
Intel open sources nGraph  Now Focus on Data Science and Stop Worrying about Frameworks and Hardware,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|The Rise of AI Continues  Robots have Mastered the Task of Assembling Furniture|5 years of building Analytics Vidhya  our journey and learnings|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Code portability is one of the more challenging features of a data scientists work. How awesome would it be if you didnt have to worry about what library or framework youre using and will it be available on another framework? Will your code run on a machine with significantly different configuration? These are the current gaps in the industry.Intel has stepped into this space and open sourced nGraph to reduce these complexities and tedious processes of adjusting models to different frameworks. nGraph isa framework-neutral deep neural network (DNN) model compiler that has the potential of targeting a wide variety of devices.This allows data scientists to focus on the data science aspect of their projects (for example developers working on TensorFlow or PyTorch), rather than spending (or wasting) time on wondering how in the world their deep neural network will train and run if ported to a different device.So how does it work exactly? You start by installing the nGraph library. Then, you write a framework with this library in order to train your deep learning model. The next step is crucial  you need to specify nGraph as the backend framework from the command line on any supported system.Then Intel takes over. Its Intermediate Representation (IR) layer takes care of all the device details and lets data scientists focus on their algorithms, approaches and models. Sounds perfect, doesnt it?The below image, taken from Intels blog post, describes the nGraph ecosystem:As of today, nGraph supports three deep learning compute devices and six deep learning frameworks. Refer to the below table for clarity:The below graph shows the comparison between the different frameworks. nGraph is able to easily better previously optimized frameworks.Intel has said itll keep adding other frameworks and devices to this list in the coming months. You can read in detail about this technology in Intels research paper here and access the open source code on GitHub here.In summary, nGraph has the below advantages:Why is this important? Because it allows data scientists freedom of choice in frameworks and hardware. If youve worked with limited computation resources, or trouble with code portability between machines, you will appreciate this latest release from Intel.Its ability to let framework owners add features without much effort works for it. It also lets cloud service providers to manage the demand in the market.What do you think of this latest research? I encourage you to go through the paper and the code to gain a deeper appreciation for Intels latest effort.",https://www.analyticsvidhya.com/blog/2018/04/intel-open-sources-ngraph-now-focus-data-science-stop-worrying-hardware/
5 years of building Analytics Vidhya  our journey and learnings,Learn everything about Analytics|The journey to become one of the largest data science community|What is coming next?|Learnings from the journey|A big thanks to the entire AV family and community,"Your time to speak|Share this:|Like this:|Related Articles|Intel open sources nGraph  Now Focus on Data Science and Stop Worrying about Frameworks and Hardware|Cars.com is using Machine Learning to Predict the Sales of Cars|
Kunal Jain
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Here is a bit of wisdom from Elon Musk:When something is important enough, you do it even if the odds are not in your favour Elon MuskI will actually extend it a bit:When you feel passionately about something, so much that the work would impact the generations to come  you do it without calculating the odds.By no means I am comparing my wisdom to the great man here, but when I look back  the only reason to start Analytics Vidhya was the passion and the need to create a knowledge based community portal for data science professionals.If I hadcalculated the odds, in all likelihood I wouldnt have started Analytics Vidhya.Little did I know that it will turn out to be the most exciting and impactful decision in the years to come.It all started 5 years back. I booked the domain to launch a (personal) blog on analytics and data science learning. The idea was to share what I had learnt during my industry experience with a larger audience.After a few articles on site, things started taking its own shape and destiny  blog found its own followers and life, to the extent that I could see a much larger impact coming through Analytics Vidhya rather than continuing in my job. So I started doing Analytics Vidhya full time.Since then, it has been a dream come true. It has been a journey where I have cherished every moment  every up and down, every problem, every triumph. I have had immense support from friends, family, mentors, the Analytics Vidhya team and their family members and reaching here would not have been possible without their efforts and support (more on this later).Check out the journey in brief to relive some of the moments you might have seen:
If you see the video above, you would see that the journey has just begun. The effort to build and strengthen the ecosystem only doubles down from here. This means more interactions and activities for our users, more resources to enhance your knowledge, more competitions, meetups, webinars and career opportunities for our community members.Here are a few things you can expect to see in coming days:If you have any suggestions on what you would want AV to do this year  let us know. We will be more than happy to hear from you and your need.This is something I get asked quite often. I usually dont talk about this because I want Analytics Vidhya to stay as the place and platform to learn data science. This being a slightly different post, I will take the liberty to share some of my learnings very briefly. If you want more on this  let me know and I will write a detailed post on a more appropriate portal.So, here are some learnings I had in building Analytics Vidhya. I personally think these would be very useful and relevant for people building communities or bootstrapping startups:This journey would not be possible without the efforts and the support of the larger ecosystem I rely on. This is where I draw my strength for. Let me thank each of them:This has been a very different post  one which has no data science knowledge explicitly, but one which shares immense learnings we had through the years building Analytics Vidhya. If you have any suggestions / feedback / experience how we have helped you or how we can help you better  I am all ears to listen it. Looking forward to your comments.",https://www.analyticsvidhya.com/blog/2018/04/5-years-building-analytics-vidhya-journey-learnings/
Cars.com is using Machine Learning to Predict the Sales of Cars,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|5 years of building Analytics Vidhya  our journey and learnings|An Overview of Regularization Techniques in Deep Learning (with Python code)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most essential and common tasks of any business is forecasting sales. Before, it used to be done manually by pouring over sheets of numbers. Now, machine learning has taken over and performs the calculations in a matter of minutes (if not seconds).                                              Source: adage.comSo Cars.com, one of the most popular websites for cars, has announced that it is using machine learning to predict the sales of cars, and to help buyers determine when and how to act on a purchase. This technology is being called Hot Car by the company and is already live on their pricing page.Hot Car is being used to predict which vehicles are most likely to sell quickly, based on several variables including car make and model, geographic demand, time on lot, pricing and consumer shopping behaviors. According to Tony Zolla, chief product officer, said that the developers had taken data from the past 20 years and built the model on over 50 factors.Used vehicles that have a 70% chance of being sold within seven days from their addition to Cars.com will feature a Hot Car badge. Brand new vehicles will earn this badge when they had a 70% chance of being sold within 20 days of their addition to the site.The initial testing phase of using this model resulted is double digit increase in the conversion! It performed particularly strong on mobile.This should be one of the obvious uses of machine learning. It benefits both the user as well as the organization. It crunches the numbers give the buyer insights using which they can make data driven decision on purchasing a car.This machine learning model will drive up sales, and benefit the dealers by increasing views on car pages, lead conversion and inventory turnover rate. Have you ever used something similar in your projects? Use the comments section below to let us know your views!",https://www.analyticsvidhya.com/blog/2018/04/cars-com-is-using-machine-learning-to-predict-the-sales-of-cars/
An Overview of Regularization Techniques in Deep Learning (with Python code),Learn everything about Analytics|Introduction|Table of Contents||What is Regularization?|How does Regularization help reduce Overfitting?|Different Regularization Techniques in Deep Learning|A case study on MNIST data with keras|End Notes,"L2 & L1 regularization|Dropout||Data Augmentation||Early stopping|Learn,engage, competeandget hired!|Share this:|Like this:|Related Articles|Cars.com is using Machine Learning to Predict the Sales of Cars|An Introduction to Graph Theory and Network Analysis (with Python codes)|
Shubham Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common problem data science professionals face is to avoid overfitting. Have you come across a situation where your model performed exceptionally well on train data, but was not able to predict test data. Or you were on the top of a competition in public leaderboard, only to fall hundreds of places in the final rankings? Well  this is the article for you!Avoiding overfittingcan single-handedly improve our models performance.In this article, we will understand the concept of overfitting and how regularization helps in overcoming the same problem. We will then look at a few different regularization techniques and take a case study in python to further solidify these concepts.Note: This article assumes that you have basic knowledge of neural networks and its implementation in keras. If not, you can refer to the below articles first:Fundamentals of Deep Learning  Starting with Artificial Neural NetworkTutorial: Optimizing Neural Networks using Keras (with Image recognition case study)Before we deep dive into the topic, take a look at this image:
Have you seen this image before? As we move towards the right in this image, our model tries to learn too well the details and the noise from the training data, which ultimately results in poor performance on the unseen data. In other words, while going towards the right, the complexity of the model increases such that the training error reduces but the testing error doesnt. This is shown in the image below.Source: SlideplayerIf youve built a neural network before, you know how complex they are. This makes them more prone to overfitting.Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the models performance on the unseen data as well.Lets consider a neural network which is overfitting on the training data as shown in the image below.If you have studied the concept of regularization in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.Assume that our regularization coefficient is so high that some of the weight matrices are nearly equal to zero.
This will result in a much simpler linear network and slight underfitting of the training data.Such a large value of the regularization coefficient is not that useful. We need to optimize the value of regularization coefficient in order to obtain a well-fitted model as shown in the image below.Now that we have an understanding of how regularization helps in reducing overfitting, well learn a few different techniques in order to apply regularization in deep learning.L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.Cost function = Loss (say, binary cross entropy) + Regularization termDue to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.However, this regularization term differs in L1 and L2.In L2, we have:Here, lambda is the regularization parameter. It is the hyperparameter whose value is optimized for better results. L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).In L1, we have:In this, we penalize the absolute value of the weights. Unlike L2, the weights may be reduced to zero here.Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it.In keras, we can directly apply regularization to any layer using the regularizers. Below is the sample code to apply L2 regularization to a Dense layer.Note: Here the value 0.01 is the value of regularization parameter, i.e., lambda, which we need to optimize further. We can optimize it using the grid-search method.Similarly, we can also apply L1 regularization. We will look at this in more detail in a case study later in this article.This is the one of the most interesting types of regularization techniques. It also produces very good results and is consequently the most frequently used regularization technique in the field of deep learning.To understand dropout, lets say our neural network structure is akin to the one shown below:
So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below.So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning. Ensemble models usually perform better than a single model as they capture more randomness. Similarly, dropout also performs better than a normal neural network model.This probability of choosing how many nodes should be dropped is the hyperparameter of the dropout function. As seen in the image above, dropout can be applied to both the hidden layers as well as the input layers.Source: chatbotslifeDue to these reasons, dropout is usually preferred when we have a large neural network structure in order to introduce more randomness.In keras, we can implement dropout using the keras core layer. Below is the python code for it:As you can see, we have defined 0.25 as the probability of dropping. We can tune it further for better results using the grid search method.The simplest way to reduce overfitting is to increase the size of the training data. In machine learning, we were not able to increase the size of training data as the labeled data was too costly. But, now lets consider we are dealing with images. In this case, there are a few ways of increasing the size of the training data  rotating the image, flipping, scaling, shifting, etc.In the below image, some transformation has been done on the handwritten digits dataset.This technique is known as data augmentation. This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.In keras, we can perform all of these transformations using ImageDataGenerator. It has a big list of arguments which you you can use to pre-process your training data.Below is the sample code to implement it.Early stopping is a kind of cross-validation strategy where we keep one part of the training set as the validation set. When we see that the performance on the validation set is getting worse, we immediately stop the training on the model. This is known as early stopping.
In the above image, we will stop training at the dotted line since after that our model will start overfitting on the training data.In keras, we can apply early stopping using the callbacks function. Below is the sample code for it.Here, monitor denotes the quantity that needs to be monitored and val_err denotes the validation error.Patience denotes the number of epochs with no further improvement after which the training will be stopped. For better understanding, lets take a look at the above image again. After the dotted line, each epoch will result in a higher value of validation error. Therefore, 5 epochs after the dotted line (since our patience is equal to 5), our model will stop because no further improvement is seen.Note: It may be possible that after 5 epochs (this is the value defined for patience in general), the model starts improving again and the validation error starts decreasing as well. Therefore, we need to take extra care while tuning this hyperparameter.By this point, you should have a theoretical understanding of the different techniques we have gone through. We will now apply this knowledge to our deep learning practice problem  Identify the digits. Once you have downloaded the dataset, start following the below code! First, well import some of the basic libraries.Now, lets load the dataset.Take a look at some of our images now.img_name = rng.choice(train.filename)Create a validation dataset, in order to optimize our model for better scores. We will go with a 70:30 train and validation dataset ratio.First, lets start with building a simple neural network with 5 hidden layers, each having 500 nodes.Note that we are just running it for 10 epochs. Lets quickly check the performance of our model.Now, lets try the L2 regularizer over it and check whether it gives better results than a simple neural network model.Note that the value of lambda is equal to 0.0001. Great! We just obtained an accuracy which is greater than our previous NN model.Now, lets try the L1 regularization technique.This doesnt show any improvement over the previous model. Lets jump to the dropout technique.Not bad! Dropout also gives us a little improvement over our simple NN model.Now, lets try data augmentation.Now, fit the training data in order to augment.Here, I have used zca_whitening as the argument, which highlights the outline of each digit as shown in the image below.Wow! We got a big leap in the accuracy score. And the good thing is that it works every time. We just need to select a proper argument depending upon the images we have in our dataset.Now, lets try our final technique  early stopping.You can see that our model stops after only 5 iterations as the validation accuracy was not improving. It gives good results in cases where we run it for a larger value of epochs. You can say that its a technique to optimize the value of the number of epochs.I hope that now you have an understanding of regularization and the different techniques required to implement it in deep learning models. I highly recommend applying it whenever you are dealing with a deep learning task. It will help you expand your horizons and gain a better understanding of the topic.Did you find this article helpful? Please share your opinions/thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/
An Introduction to Graph Theory and Network Analysis (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Graphs and their applications|History and Why Graphs?|Terminology you should know|Graph Theory concepts|Getting Familiar with Graphs in python|Analysis on a Dataset|Conclusion|Bibiliography and References,"History of Graphs|Why Graphs?|About the Author|Share this:|Related Articles|An Overview of Regularization Techniques in Deep Learning (with Python code)|Heres a Deep Learning Algorithm that Transforms an Image into a Completely Different Category|
Guest Blog
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Average Path Length|BFS and DFS|Centrality|Network Density|Graph Randomizations|Graph Creation|Accessing edges and nodes|Graph Visualization,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A picture speaks a thousand words is one of the most commonly used phrases. But a graph speaks so much more than that. A visual representation of data, in the form of graphs, helps us gain actionable insights and make better data driven decisions based on them.But to truly understand what graphs are and why they are used, we will need to understand a concept known as Graph Theory. Understanding this concept makes us better programmers (and better data science professionals!).                                 Source: QuantdareBut if you have tried to understand this concept before, youll have come across tons of formulae and dry theoretical concepts. That is why we decided to write this blog post. We have explained the concepts and then provided illustrations so you can follow along and intuitively understand how the functions are performing. This is a detailed post, because we believe that providing a proper explanation of this concept is a much preferred option over succinct definitions.In this article, we will look at what graphs are, their applications and a bit of history about them. Well also cover some Graph Theory concepts and then take up a case study using python to cement our understanding.Ready? Lets dive into it.Let us look at a simple graph to understand the concept. Look at the image below Consider that this graph represents the places in a city that people generally visit, and the path that was followed by a visitor of that city. Let us consider V as the places and E as the path to travel from one place to another.The edge (u,v) is the same as the edge (v,u)  They are unordered pairs.Concretely  Graphs are mathematical structures used to study pairwise relationships between objects and entities. It is a branch of Discrete Mathematics and has found multiple applications in Computer Science, Chemistry, Linguistics, Operations Research, Sociology etc.The Data Science and Analytics field has also used Graphs to model various structures and problems. As a Data Scientist, you should be able to solve problems in an efficient manner and Graphs provide a mechanism to do that in cases where the data is arranged in a specific way.Formally,In the case of digraphs, there is a distinction between `(u,v)` and `(v,u)`. Usually the edges are called arcs in such cases to indicate a notion of direction.There are packages that exist in R and Python to analyze data using Graph theory concepts. In this article we will be briefly looking at some of the concepts and analyze a dataset using Networkx Python package.From the above examples it is clear that the applications of Graphs in Data Analytics are numerous and vast. Let us look at a few use cases:If you want to know more on how the ideas from graph has been formlated  read on!The origin of the theory can be traced back to the Konigsberg bridge problem (circa 1730s). The problem asks if the seven bridges in the city of Konigsberg can be traversed under the following constraintsThis is the same as asking if the multigraph of 4 nodes and 7 edges has an Eulerian cycle (An Eulerian cycle is an Eulerian path that starts and ends on the same Vertex. And an Eulerian path is a path in a Graph that traverses each edge exactly once. More Terminology is given below). This problem led to the concept of Eulerian Graph. In the case of the Konigsberg bridge problem the answer is no and it was first answered by (you guessed it) Euler.In 1840, A.F Mobius gave the idea of complete graph and bipartite graph and Kuratowski proved that they are planar by means of recreational problems. The concept of tree, (a connected graph without cycles) was implemented by Gustav Kirchhoff in 1845, and he employed graph theoretical ideas in the calculation of currents in electrical networks or circuits.In 1852, Thomas Gutherie found the famous four color problem. Then in 1856, Thomas. P. Kirkman and William R.Hamilton studied cycles on polyhydra and invented the concept called Hamiltonian graph by studying trips that visited certain sites exactly once. In 1913, H.Dudeney mentioned a puzzle problem. Eventhough the four color problem was invented it was solved only after a century by Kenneth Appel and Wolfgang Haken. This time is considered as the birth of Graph Theory.Caley studied particular analytical forms from differential calculus to study the trees. This had many implications in theoretical chemistry. This lead to the invention of enumerative graph theory. Any how the term Graph was introduced by Sylvester in 1878 where he drew an analogy between Quantic invariants and covariants of algebra and molecular diagrams.In 1941, Ramsey worked on colorations which lead to the identification of another branch of graph theory called extremel graph theory. In 1969, the four color problem was solved using computers by Heinrich. The study of asymptotic graph connectivity gave rise to random graph theory. The histories of Graph Theory and Topology are also closely related. They share many common concepts and theorems.Here are a few points that help you motivate to use graphs in your day-to-day data science problems Before you go any further into the article, it is recommended that you should get familiar with these terminologies.In this section, well look at some of the concepts useful for Data Analysis (in no particular order). Please note that there are a lot more concepts that require a depth which is out of scope of this article. So lets get into it.The average of the shortest path lengths for all possible node pairs. Gives a measure of tightness of the Graph and can be used to understand how quickly/easily something flows in this Network.Breadth first searchandDepth first searchare two different algorithms used to search for Nodes in a Graph. They are typically used to figure out if we can reach a Node from a given Node. This is also known asGraph TraversalThe aim of the BFS is to traverse the Graph as close as possible to the root Node, while the DFS algorithm aims to move as far as possible away from the root node.One of the most widely used and important conceptual tools for analysing networks. Centrality aims to find the most important nodes in a network. There may be different notions of important and hence there are many centrality measures. Centrality measures themselves have a form of classification (or Types of centrality measures). There are measures that are characterized by flow along the edges and those that are characterized by Walk Structure.Some of the most commonly used ones are:These centrality measures have variants and the definitions can be implemented using various algorithms. All in all, this means a large number of definitions and algorithms.A measure of how many edges a Graph has. The actual definition will vary depending on type of Graph and the context in which the question is asked. For a complete undirected Graph the Density is 1, while it is 0 for an empty Graph. Graph Density can be greater than 1 in some situations (involving loops).While the definitions of some Graph metrics maybe easy to calculate, it is not easy to understand their relative importance. We use Network/Graph Randomizations in such cases. We calculate the metric for the Graph at hand and for anothersimilarGraph that is randomly generated. This similarity can for example be the same number of density and nodes. Typically we generate a 1000 similar random graphs and calculate the Graph metric for each of them and then compare it with the same metric for the Graph at hand to arrive at some notion of a benchmark.In Data Science when trying to make a claim about a Graph it helps if it is contrasted with some randomly generated Graphs.We will be using thenetworkxpackage in Python. It can be installed in the Root environment of Anaconda (if you are using the Anaconda distribution of Python). You can alsopip installit.Let us look at some common things that can be done with the Networkx package. These include importing and creating a Graph and ways to visualize it.Node and Edge attributes can be added along with the creation of Nodes and Edges by passing a tuple containing node and attribute dict.In addition to constructing graphs node-by-node or edge-by-edge, they can also be generated by applying classic graph operations, such as:Separate classes exist for different types of Graphs. For example thenx.DiGraph()class allows you to create a Directed Graph. Specific graphs containing paths can be created directly using a single method. For a full list of Graph creation methods please refer to the full documentation. Link is given at the end of the article.Nodes and Edges can be accessed together using theG.nodes()andG.edges()methods. Individual nodes and edges can be accessed using the bracket/subscript notation.NodeView((1, 2, 3))EdgeView([(1, 2), (1, 3), (2, 3)])AtlasView({2: {}, 3: {}}){}{}Networkx provides basic functionality for visualizing graphs, but its main goal is to enable graph analysis rather than perform graph visualization. Graph visualization is hard and we will have to use specific tools dedicated for this task.Matplotliboffers some convenience functions. ButGraphVizis probably the best tool for us as it offers a Python interface in the form ofPyGraphViz(link to documentation below).You will first have to Install Graphviz from the website (link below). And thenpip install pygraphviz --install-option="" <>. In the install options you will have to provide the path to the Graphvizlibandincludefolders.PyGraphviz provides great control over the individual attributes of the edges and nodes. We can get very beautiful visualizations using it.Usually, visualization is thought of as a separate task from Graph analysis. A graph once analyzed is exported as a Dotfile. This Dotfile is then visualized separately to illustrate a specific point we are trying to make.We will be looking to take a generic dataset (not one that is specifically intended to be used for Graphs) and do some manipulation (in pandas) so that it can be ingested into a Graph in the form of a edgelist. And edgelist is a list of tuples that contain the vertices defining every edgeThe dataset we will be looking at comes from the Airlines Industry. It has some basic information on the Airline routes. There is a Source of a journey and a destination. There are also a few columns indicating arrival and departure times for each journey. As you can imagine this dataset lends itself beautifully to be analysed as a Graph. Imagine a few cities (nodes) connected by airline routes (edges). If you are an airline carrier, you can then proceed to ask a few questions likeWe now have time columns in the format we wanted. Finally we may want to combine theyear,monthanddaycolumns into a date column. This is not an absolutely necessary step. But we can easily obtain the year, month and day (and other) information once it is converted intodatetimeformat.Now import the dataset using the networkx function that ingests a pandas dataframe directly. Just like Graph creation there are multiple ways Data can be ingested into a Graph from multiple formats.Output:Output:Output:Output:Output:As is obvious from looking at the Graph visualization (way above)  There are multiple paths from some airports to others. Let us say we want to calculate the shortest possible route between 2 such airports. Right off the bat we can think of a couple of ways of doing itWhat we can do is to calculate the shortest path algorithm by weighing the paths with either the distance or airtime. Please note that this is an approximate solution  The actual problem to solve is to calculate the shortest path factoring in the availability of a flight when you reach your transfer airport + wait time for the transfer. This is a more complete approach and this is how humans normally plan their travel. For the purposes of this article we will just assume that is flight is readily available when you reach an airport and calculate the shortest path using the airtime as the weightLet us take the example ofJAXandDFWairports:Output:Output:This article has at best only managed a superficial introduction to the very interesting field of Graph Theory and Network analysis. Knowledge of the theory and the Python packages will add a valuable toolset to any Data Scientists arsenal. For the dataset used above, a series of other questions can be asked like:If you do solve them, let us know in the comments below!Network Analysis will help in solving some common data science problems and visualizing them at a much grander scale and abstraction. Please leave a comment if you would like to know more about anything else in particular.Srivatsa currently works for TheMathCompany and has over 7.5 years of experience in Decision Sciences and Analytics.He has grown, led & scaled global teams across functions, industries & geographies. He has led India Delivery for a cross industry portfolio totalling $10M in revenues. He has also conducted several client workshops and training sessions to help level up technical and business domain knowledge.During his career span, he has led premium client engagements with Industry leaders in Technology, e-commerce and retail. He helped set up the Analytics Center of Excellence for one of the worlds largest Insurance companies.",https://www.analyticsvidhya.com/blog/2018/04/introduction-to-graph-theory-network-analysis-python-codes/
Heres a Deep Learning Algorithm that Transforms an Image into a Completely Different Category,Learn everything about Analytics|Overview|Introduction||Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An Introduction to Graph Theory and Network Analysis (with Python codes)|Googles Machine Learning Model can Detect Cancer in Real-Time|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning keeps producing remarkably realistic results. 10 years ago, could you imagine taking an image of a dog, running an algorithm, and seeing it being completely transformed into a cat image, without any loss of quality or realism? It was the stuff of movies and dreams!A team of researchers from Cornell University have proposed a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework for translating images from one domain to another. The aim is to take an image and generate a new image from it that is from a new category (for instance, transforming an image of a dog to a cat). According to the researchers, for a given image in the source domain, the proposed model learns the conditional distribution of corresponding images in the target domain.
The previously existing approaches are able to perform only one-to-one mapping of the given an image and thus fail to produce diverse outputs of the same. MUNIT, on the other hand is able to provide more than one output. It decomposes the image representation into a domain invariant content code and a domain specific style code, and later recombine the content code with random style code sampled from the style space.The team has presented a research paper covering the algorithm and techniques behind the approach, along with the results of the research. You can also check out the GitHub repository here.Below is a video that shows this very technique  image-to-image translation for various images: This is not the first attempt in image-to-image translations. UNIT and CycleGAN were used for translation previously and could perform one-to-one mapping, that is, it could translate one dog image to one cat image. MUNIT overcomes this limitation as it can perform many-to-many mappings, in other words,one dog image to many kinds of cat images!This is undoubtedly an amazing algorithm but there are a few drawbacks or limitations which still need to be corrected. For example, a scene in winter could have a different appearance during summer. The algorithm will not consider different appearances as the existing technique assumes a unimodal approach. Try it out and let us know your experience in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/framework-for-unsupervised-image-to-image-translation/
Googles Machine Learning Model can Detect Cancer in Real-Time,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Heres a Deep Learning Algorithm that Transforms an Image into a Completely Different Category|Pandas will No Longer Support Python 2 Next Year|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","This is all very fascinating, but how does this AI actually work?",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Detecting cancer at an early stage has long been a focus area in healthcare. From IBM Watson to other major players, a lot of money has been spent trying to make headway in this field but with little success.Now, Google has entered the crowded field and made a splash with their promising results. They have developed a deep learning model that has been incorporated in a microscope that doctors can use to detect cancer.As with all deep learning studies, a deep neural network was trained to detect cancer cells by analysing images of human tissues. Then, a slide with human tissue is placed under the lens of the modified microscope. The image that you see in the microscope is sent to a computer and the deep learning model does to work detecting cancer in the tissue.This is all done in real-time and is fast enough that when the slide is moved, the results are still populated in the computer.The hardware setup is made up of a modified light microscope that enables real-time image analysis and presentation of the outcomes of the machine learning algorithms. The best part about this is that the microscope can be retrofitted in the existing setup in hospitals and clinics around the world, at a relatively low cost.The initial results have shown a lot of promise. The researchers tested the model on two different cancer types  breast cancer and prostate cancer. A test on the former showed an AUC (area under the curve) as 0.98 and for the latter, the AUC was 0.96.The study has also been jotted down in a research paper that you can access here and also view Googles official blog post here. This is still pending review and the researchers have said they need to perform a more in-depth study to make the model far more accurate and to overcome the current limitations.Check out Googles video below depicting this AI:Google has previously published results on how the team used a trained convolutional neural network to detect breast cancer. But this latest release, pending review, will be a boost in the arm for the healthcare community. It can potentially be used to detect other illnesses like tuberculosis and malaria.These AR microscopes can be used beyond healthcare  in life sciences, manufacturing, and material research as well.",https://www.analyticsvidhya.com/blog/2018/04/googles-machine-learning-model-can-detect-cancer-real-time/
Pandas will No Longer Support Python 2 Next Year,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Googles Machine Learning Model can Detect Cancer in Real-Time|Thanks to AI, you can now create Cartoons from Text Based Descriptions|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Pandas is one of the most common and popular python libraries out there. It provides fast and incredibly flexible data structures that make working with data really easy and straightforward. Its essentially the first library you learn about when you start with Python.Starting from 1st January 2019, pandas will no longer be supporting Python 2. This means that the final release before December 31st, 2018 will the last ever to support Python 2 and all releases post that will be for Python 3 only.This has been announced in line with python core teams decision to drop support for Python 2.7 from 1st January, 2020 onwards.Note here that pandas will NOT stop working  instead it will stop pushing out bug fixes, security enhancements and new features to the older version of Python. This will only be possible if someone outside the developer team volunteers to do it.A recently conducted survey revealed that a massive 47% of users are still using Python 2 so they will have to switch over to the latest version very soon. A lot of other packages have also decided to drop their support for Python 2 recently, so this will just expedite the process.You can read their official announcement on GitHub here.The reaction to this announcement has been mixed on social media. While a few people argue that they shouldnt be forced to upgrade their version, most folks understand the need for this change.By dropping support for Python 2, it becomes easier and more streamlined for developers and people maintaining the latest version. Another case to be made is that if the support isnt removed, then users will not have a lot of incentive to upgrade.But moving your scripts from Python 2 to 3 has seen the code break for a few people so be wary when you make the move. You can check out this link to learn more about porting your code from Python 2 to 3.What are your thoughts on this? Will you be affected by this move?",https://www.analyticsvidhya.com/blog/2018/04/pandas-will-drop-all-support-for-python-2-next-year/
"Thanks to AI, you can now create Cartoons from Text Based Descriptions",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Pandas will No Longer Support Python 2 Next Year|A Must-Read Introduction to Sequence Modelling (with use cases)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Creating animated videos is a long and arduous process. Even with the introduction of computers and software in the animation industry, the task still takes quite a lot of time to accomplish. But with the advancements in AI, this time problem might just have been solved.Researchers from the University of Illinois and the Allen Institute for Artificial Intelligence have developed an AI model, called CRAFT (Composition, Retrieval, and Fusion Network), that takes text descriptions (or captions) from the user and generates scenes from The Flintstones cartoon series. And unlike pixel generation approaches, this model is based on text to entity segment retrieval from a video database.The final model was developed by training it on set of more than 25,000 video clips, each three seconds and 75 frames long. As you can imagine, each video had to be labelled and annotated with which character(s) was in the scene and what the scene was about.The AI matches videos to the words descriptions and builds a set of parameters. CRAFT can convert the provided text descriptions into video clips of this animated series, featuring characters, props and location, as it learned from the videos. It can not only put the characters into place, but also parse objects, retrieve the background, etc.The results produced are still raw in nature. As youll see in the video posted below, the AI does get things right most of the time but when it gets it wrong, the video looks like a mess. Safe to way this is a work in progress, albeit with a massive amount of potential.Below is a video that gives you a glimpse of how CRAFT works:I would recommend reading their research paper here to gain a deeper understanding of the algorithm.The Flintstones comes from an old school animation line, with relatively static backgrounds. Animation has since come a long way  the style of generating videos is far more dynamic in nature. And this will be a challenge for researchers in the video generation field going forward.One thought is that by providing the mode more complex video frames, it could be made to adapt to the dynamic parts of the video as well. What do you think is required to improve this algorithm? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/this-ai-create-cartoons-text-description/
A Must-Read Introduction to Sequence Modelling (with use cases),"Learn everything about Analytics|Introduction|Table of Contents|Two Main Advances in the Field of ANN|Thought Experiment|Practical Applications of Sequence Modelling|First, lets talk about the easiest of the lot  Sequence Generators|Next, lets talk about the favorites  Sequence to sequence NLP Models|Finally, we will talk about a few more sequence to sequence models that go beyond text|End Notes","Here goes the list:|Learn,engage, competeandget hired!|Share this:|Like this:|Related Articles|Thanks to AI, you can now create Cartoons from Text Based Descriptions|AVBytes: AI & ML Developments this week  Stanfords NLP Course Projects, R Package for Anomaly Detection, Create Deep Learning Dataset, etc.|
Tavish Srivastava
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Neural Networks (ANN) were supposed to replicate the architecture of the human brain, yet till about a decade ago, the only common feature between ANN and our brain was the nomenclature of their entities (for instance  neuron). These neural networks were almost useless as they had very low predictive power and less number of practical applications.But thanks to the rapid advancement in technology in the last decade, we have seen the gap being bridged to the extent that these ANN architectures have become extremely useful across industries.In this article, we will look at the two main advances in the field of artificial neural networks that have made these ANNs more like the human brain,Can we introduce this concept of Thought in an ANN? The answer is yes, and we will explore more about the idea in this article.Sequence models have garnered a lot of attention because most of the data in the current world is in the form of sequences  it can be a number sequence, image pixel sequence, a video frame sequence or an audio sequence.Over the last 10 years, we have stored 1000s of Petabytes (or more than 10 ^ 9 GBs) of unstructured sequence data for absolutely no reason as we had no way to fetch information out of such data formats. Luckily, we now have this new family of neural network architectures called sequence models that can turn this data dump into GOLD MINES.The scope of this article is not to talk about all the complex mathematics that goes behind the scene in Sequence Modelling or give you some sample codes to run on sequence modelling (I will park that for some later articles), but to give you practical examples of sequence modelling implementations in the industry. These will enable you to identify business problems in your industry that might need this special tool.To get a better understanding of what this article is about, below is a scenario which I want you to imagine. Put your analytical thinking hats on!
Walmrt has appointed you as the head of its new vertical  WalKiosk. The company wants you to lead the development of a self servicing (human-less) store where a customer will only interact with Walmrts Kiosk, which is very similar to a vending machine. They want to install this Kiosk in various locations across the United States.A key difference between this Kiosk and a normal vending machine is that the Kiosks display does not show the list of items, but simply an audio enabled Google-like search tab. The customer can literally walk up to these Kiosks, and say or type anything after the keyword OK Walmrt, xxxxxx. Here is a sample interaction (try to evaluate if a human can do a better job than this Kiosk):Customer says  OK Walmrt,I want the shoes which Leonardo DiCaprio wore in the 1st scene of the 1st movie he did with Nolan in any possible spoken language.The idea is for the Kiosk to do a quick search and if it finds a convincing answer, it should reply, in the same language as the customers query, something like  Leonardo DiCaprio wore black colored Nike shoes of model xxxxx. Click the link on the kiosk to watch a video cut of the scene you asked me to look at. Great news  we currently have the exact same shoe with the same size as you are wearing, and its cost is $200. As you are a loyal customer of Walmrt, I have found a steal deal for you! The new price of the shoe, if you buy it immediately, is $150 for you.If the customer says I want to buy it, the Kiosk dispenses the shoe once the customer makes the payment.Kiosk finally replies  Thanks Mr. XYZ for shopping with us today. Please give your valuable feedback for us to improve our service further. Customer writes or says the feedback of this transaction and leaves.This simple transaction, that will probably take a good chunk of your time in todays world, will be resolved in less than 2 minutes (if everything works, that is).Sounds futuristic? Heres a spoiler  all the fancy next gen functional skills you need to build in this Kiosk will be done mainly by a single architecture  sequence modelling. Here is a small list of tasks the Kiosk needs to do:The skills required to create WalKiosk are not limited to these nine steps, but they are good enough to bring out the core idea. Each of these nine skills can be modeled by a single architecture  Sequence Modelling (but you already knew this).You can imagine sequence modelling as a black box which stays almost the same; all you need to change is the input and target data for each of the nine skill sets. Leveraging the idea that all the model architectures in each step is the same, we can take this a step further and create a single model that takes input in any language and completes the self service process/reporting process/inventory management process all together.If this was not enough to make you Google all about sequence modelling, lets look at an exhaustive list of all functions sequence modelling is capable of.
To make sure we cover most of the possible applications of sequence modelling, we will categorize them based on the type of input and output sequences. Inputs and outputs can be one of the following: Scalar, Trend, Text, Image, Audio or Video. If each of these six can be both input and output, we have 36 categories in total. However, not each of these pairs has been explored in depth yet.Before moving to the list, pause for a moment and create your own list of applications (you can use our thought experiment as a reference).Reading the table is fairly straight forward:We will review a few of these use cases in order to get a grasp of the superpowers that our sequence model possess.
These generators generally take scalar inputs. The scalar input can be any random seed/number. Following are a few examples of generators:Note that we can train our model on any specific type of data. For instance, if we train our text generator on a Harry Potter book, it is highly likely that you will get a text which is full of imagination/magic with the main character as Harry Potter. If you were lucky, you might get a chapter that makes sense and you can enjoy this privileged chapter that no one has access to!Another example  if you train the model on Jazz music, you can create new songs in the same genre using this model. Yet another example  if you train the model on images of animals, you might see how cross breeds might look like.
Machine Language Translation has reached new heights and is now competing strongly with human translators. Today, you can find real-time translating machines which are based on the core concept of sequence to sequence models.Text summarization is another important use case of sequence models. Text summarization can significantly reduce the task of manually reading lengthy customer complaints, monitoring compliance based call/chat monitoring, and reviewing customer feedback on product etc.Chatbot is yet another important application and is now being widely used in Operations/Call Centers/Chat Centers/Personal assistants like Siri/Google Home/Alexa.
Speech recognition is currently the category which has absorbed the maximum investment in terms of money. Speech recognition is extremely important in tools like personal AI assistants (Alexa, Google Home, etc.) and call center speech recording tools.Currently we have billion dollar companies whose sole competency is speech recognition. Speech recognition also uses sequence to sequence models extensively. Image Captioning is one of the hottest research fields which has a wide application in the social media industry. Subtitle generation has not reached the stage of production yet, but is being actively researched.A lot of the data science talent today focuses its effort on solving problems that already exist. An equally important task, for any successful data scientist or analyst, is to identify and create new tasks that can be solved analytically. The latter is a very different exercise and does not need a lot of coding experience or mathematically background. All you need to know is what is possible and what is not, using a given tool.Problem identification is a skill set that is a must for any senior analytics professional. I hope this introductory article on sequence learning gave you strong motivation to start searching for new problems in your industry that can be solved using this method.If you have any ideas or suggestions regarding the topic, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/
"AVBytes: AI & ML Developments this week  Stanfords NLP Course Projects, R Package for Anomaly Detection, Create Deep Learning Dataset, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|A Must-Read Introduction to Sequence Modelling (with use cases)|This AI Can Create Your 3D Avatar using just your Smartphone Camera!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Developments in AI & ML are happening at break neck speed. Hardly a day goes without hearing about a new development in these field. It is difficult to stay on top of these developments.This was one of the main reasons behind launching AVBytes and the response from community has been phenomenal.The past week saw some intriguing developments in machine learning and deep learning. Stanford released a list of all its NLP course projects for 2018 (its a goldmine of knowledge), the Google Research team unveiled its deep neural network to extract audio by looking at a persons face, a R package was released to deal with anomalies in time series, and many other developments happened this week which we have covered under the AVBytes umbrella.Scroll down to view all the articles from last week. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.Source: MIT Technology ReviewThe above AVBytes were published from 9th to 15th April, 2018.Since I am a R user, the anomalize package is a must-have for dealing with time series data. It makes life so much more easier! Stanfords NLP course projects were also an eye opener  the quality of research papers released by the students is mind-boggling.What excited you the most about this week?",https://www.analyticsvidhya.com/blog/2018/04/avbytes-ai-ml-developments-this-week-160418/
This AI Can Create Your 3D Avatar using just your Smartphone Camera!,Learn everything about Analytics|Overview:|Introduction:|Our take on this:,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Stanfords NLP Course Projects, R Package for Anomaly Detection, Create Deep Learning Dataset, etc.|Add Objects to Paintings and Images seamlessly with this Amazing Python Script|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve ever played the ultra popular video game The Sims, you will love what the latest AI has come up with. That concept of building your own character in front of your eyes is no longer a fantasy.This AI has the ability of creating 3D versions of peoples bodieswhich can be used for a variety of purposes, like animated movies, retail shopping, virtual gaming, telepresence, etc. It requires one a smartphone camera or a computer webcam to identify the persons features and replicate them in their digital avatar.The algorithm is built in different stages as we have shown below:The team then tested their model on various body shapes, clothing, backgrounds and environments. The results were mind boggling  the model had an accuracy within 5 millimetres! They will be presenting their findings at theComputer Vision and Pattern Recognition conference in Salt Lake City in June this year.For a better understanding of this algorithm, take a look at the below video:The researchers have published their research paper here.Why this research is significant is because so far, computer vision and pattern recognition tasks have required multiple camera angles and expensive equipment to gather the data before it ready for collection. With this AI, it reduces the cost and can be used in a vast number of fields  from virtual reality avatars, movie making, video games, online retail, fashion design, and even surveillance.",https://www.analyticsvidhya.com/blog/2018/04/ai-can-create-3d-model-analyzing-seconds-long-video/
Add Objects to Paintings and Images seamlessly with this Amazing Python Script,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|This AI Can Create Your 3D Avatar using just your Smartphone Camera!|Googles Neural Network Extracts the Audio Source by Looking at the Persons Face|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have ever tried inserting images of objects into a photo, you would know that the most difficult task is to make the boundaries merge with the original photo. Even when you do manage it, the resulting photo clearly looks like its been photoshopped. But with deep learning, this is slowly being overcome.We covered NVIDIAs FastPhotoStyle library that makes styling photos simpler and now a developer has come up with an algorithm that takes a painting, adds an external element to it, and harmonizes it to make it look almost undistinguishable from the original painting.The developer trained a VGG convolutional neural network (CNN) on a dataset of 80,000 paintings which he collected from wikiart.org. The CNN estimates the stylization level of a painting and then adjusts the weights accordingly. Since they have not yet released any pre-trained model for this, you will have to adjust the weights manually if you use this on an image.It is a two-pass algorithm, an improvisaatoin on the previously used single-pass approach.In order to make the object merge with the painting,VGG neural network is used.Below is the breakdown of the algorithm behind this:This algorithm produces far more precise results than photo compositing or global stylization techniques and it achieves levels of edits that have so far been very difficult to achieve.Check out some of the images below that utilized this model. The original painting is on the left, the naive composite in the middle and the final output is on the right.   You can read the paper in full here and also check out the GitHub respository to get the python codes.This is one of the coolest things we have covered on AVBytes. Some of the stylized images did not turn out as well but most of them are deeply impressive. This is the first example Ive seen of such an idea that can be packaged into a business idea (perhaps as a filter in tools like Photoshop). But the developer has mentioned that this is only intended for personal and not commercial use so he must have something in mind there!Go ahead and try it out on your machines. Let us know your experience in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/add-objects-paintings-images-seamlessly-amazing-python-script/
Googles Neural Network Extracts the Audio Source by Looking at the Persons Face,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Add Objects to Paintings and Images seamlessly with this Amazing Python Script|Essentials of Deep Learning: Getting to know CapsuleNets (with Python codes)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve taken Andrew Ngs Machine Learning course, you will remember the example of two different audio sources at a cocktail party. He goes on to show how, using an algorithm, the voices have been separated from the background noise so you can clearly hear only that audio.In their latest paper, the Google Research team has put forth an audio-visual deep learning model that takes this example to another level altogether. With this study, the team was able to produce videos in which the speech of specific people in enhanced while all the other noise is reduced to an almost negligible level, or cut out completely.Once the user selects the face of the person in the video that he/she wants to hear, the algorithm works on enhancing that persons audio and reducing everything else. It currently works on videos that have a single audio track. Check out an example of this in the below video:What separates this research from anything done before it is the unique audio-visual component. As you can imagine, when a person speaks, the movement of their mouth should ideally correlate with the sound being produced. This helps in identifying which parts of the audio correspond to that specific person. As you can see in the image below, the input is a video with multiple people speaking simultaneously. The algorithm works on audio-visual source separation and the output is a decomposition of the input audio track into cleaned speeches (one for each person speaking).What separates Google Research from most others is the ease of access they have to large amounts of data. They curated 100,000 high-quality videos of lectures from YouTube to build their dataset. Then they extracted parts with clean speech and with a single speaker visible in the video. This gave them around 2000 hours of video clips. These clips were in turn used to generate mixtures of face videos and their corresponding speech from separate video sources, along with non-speech background noise we obtained fromAudioSet.The team built a convolutional neural network model on this data to split the above mixture into separate audio streams for each user. You can read the research paper in full hereor go through their official blog post.This could be used for speech enhancement, recognition in videos, video conferencing, in healthcare to improve hearing aids for hard of hearing people, and in situations where you need the microphone to pick up specific speech patterns in a populated setting.The model can even be used for automatic video captioning. The initial results have been very promising (to say the least). In a field that has seen as many challenges as breakthroughs, this will speed up research in the community.",https://www.analyticsvidhya.com/blog/2018/04/googles-latest-deep-learning-model-separates-audio-persons-face/
Essentials of Deep Learning: Getting to know CapsuleNets (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Why is CapsuleNet getting so much attention?|Intuition behind Capsule Networks|Code Walkthrough of CapsNet on MNIST|End Notes,"1.Multi Layer Perceptron|2. Convolutional Neural Network|3. Capsule network|Participate in the McKinsey Analytics Online Hackathonto win an all-expenses paid trip to an international analytics conference!|Share this:|Like this:|Related Articles|Googles Neural Network Extracts the Audio Source by Looking at the Persons Face|Top 7 Data Science & Machine Learning GitHub Repositories in March 2018|
Faizan Shaikh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Multi-layer Perceptron|Convolutional Neural Network (CNN)|Capsule Network|Case 1  Simple image|Case 2  Rotated Image|Case 3 : Upside Down Image,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural networks have been around since the last century but in the last decade, they have reshaped how we see the world. From classifying images of animals to extracting parts of speech, researchers are building deep neural networks in diverse and vast fields to push and break boundaries.But as advancements in deep learning reach new heights, a new concept has lately been introduced that is a twist on the old neural network architecture  Capsule Networks. It improves on the effectiveness of the old traditional methods and understands even when presented with a situation that is shown from a different angle.In this article, we will see why Capsule Networks are suddenly getting so much attention in the deep learning community, what the intuition behind this concept is and we will then do a code walk through to strengthen and solidify our concepts.So are you ready? Lets go!Note: I suggest you go through the below articles before reading further in case you need to brush up your neural network concepts:You must have already heard the tales of CapsuleNets  but let us see it through with our eyes. To compare the power of this new architecture  let us monitor CapsNet on Analytics VidhyasIdentify the Digits Problem.Note  The code for this section is included in the Code Walkthrough section. Alternatively, you can check out the code on GitHub.For the uninitiated, the Identify the Digits problem, as the name suggests, is simply a digit recognition problem. When given a simple black and white image, the user has to predict the number shown in it.As this is an unstructured data problem, specific to image recognition, it is common knowledge that you should apply Deep Learning algorithms to get the best performance on the data. We will survey three Deep Learning architectures for this problem to check their performance, namely:For our first attempt, let us build a very simple Multi-layer Perceptron (MLP) for our problem. Below is the code to build an MLP model in keras:Below is our model summary:Now after training for 15 epochs, here is the result we get:Looks impressive for such a simple model!Now well move on to a more complex architecture, aka, Convolutional Neural Network (CNN). We have defined the model in the code below.Lets print our model summary:We see that our CNN model is deeper and more complex than our initial MLP model. Lets see if that horsepower is appropriate.We see that it takes more time to train our model. But its worth the effort as it gives a significant performance boost.Now let us check out the last architecture, the Capsule Network. As you might have guessed  it is bigger and even more complex than CNN. Below is the code to build it.If the above code seemed like black magic to you  dont worry. We will break this down for a better understanding in the next few sections. For now, here is our model summary:Now you can set this model to train. You can go and fix yourself a cup of coffee because it is going to take a while to train! (it took me half an our to train on a Titan X). I got these results:Looks even better than the previous models right? It outperforms all the architectures we saw before. The image below summarizes our experiments:This proves that our hypothesis was correct  CapsNets are truly worth exploring!PS  You can try experimenting thiswith any other dataset. If you do, please share your experience in the comments section below!As with all the Deep Learning puns to understand its concepts  we will take examples of cat pictures to understand the promise (and potential) of Capsule Networks.Lets start with a question  which animal is depicted in the image below? (PS  Ive already given you a hint!).If you guessed it right, yes its a cat! But how did you know its a cat? Was it the eyes or the cuteness? Well break it down so that we can clarify our hypothesis.So lets say this is the cat. How did you figure out that it is a cat? One possible approach could be to break it down into individual features, such as eyes, nose, ears, etc. This idea is summarised in the image below:So what we are essentially doing is decomposing high level features to low level ones. Concretely, you can define this as P(face) = P(nose) & ( 2 x P(whiskers) ) & P(mouth) & ( 2 x P(eyes) ) & ( 2 x P(ears) )where we can define P(face) as the presence of a face of the cat in the image. Iteratively, you can also define even more low level features, like shapes and edges, in order to simplify the procedure.Now what if I rotate the image by 30 degrees?If you go by the same features as we definedbefore, we would not be able to identify the cat. This is because the orientation of the low level features also changes. So the features which you defined previously will also change.So now, your overall cat recognizer would probably look like this:More specifically, you can represent this as:P(face) = ( P(nose) & ( 2 x P(whiskers) ) & P(mouth) & ( 2 x P(eyes) ) & ( 2 x P(ears) ) )         OR         (P(rotated_nose) & ( 2 x P(rotated_whiskers) ) & P(rotated_mouth) & ( 2 x
         P(rotated_eyes) ) & ( 2 x P(rotated_ears) ) )Now just to increase the complexity, what if we have a completely upside down image?Our approach seems like a brute force search for all possible rotations of the low level features. We will need a more robust method to do the job.Take a moment to think about a possible workaround for this before reading further. If you come up with anything that could solve this problem, please share it with the community below!One suggestion by the researchers was to include additional properties of the low level features itself  such as rotational angle. This is how you can check not only the presence of a feature, but also its rotation. For example, see the image below:In a more rigorous way, you can define this as:P(face) = [ P(nose), R(nose) ] & [ P(whiskers_1), R(whisker_1) ] & [ P(whiskers_2), R(whisker_2) ]& [ P(mouth), R(mouth) ] & where we also catch the rotational value of the individual as R() of a feature. So for this approach  change in rotational angle is represented in a meaningful way. This property is called rotational equivariance.Food for thought  we can also scale up this idea to capture more aspects of the low level features, such as scale, stroke thickness, skew, etc. This would help us grasp an object in the image more clearly. This is how capsule networks were envisioned to work when they were designed.Here, we saw an important aspect of Capsule Networks. Another important feature of Capsule networks is dynamic routing. We will take a look at this next.Now lets take a Dog vs Cat Classification problem.Overall, if you see them  they look very similar. But there are some significant differences in the images that can help us figure out which is the cat and which is the dog. Can you guess the difference?As we did in the previous sub-section  we will define features in the images which will help us figure out the differences.As we see in the image below, we can define iteratively complex features to come up with the solution. We can first define very low level facial features such as eyes and ears and them combine them to find a face. After that, we can combine the facial and body features to arrive at our solution, i.e, is it a dog or a cat.Now suppose that you have a new image, with all the extracted low level features. Now you are trying to figure out the class of this image.We picked a feature randomly and it turned out to be the eye. Can it single handedly help us figure out the class?We see that eye alone is not a differentiating factor. So our next step is to add more features to our analysis. The next feature we randomly pick out is a nose. For the moment, well only look at the low-level feature space and the intermediate-level feature space.We still see that they are not enough for classification. So our next logical step will be to take all the features and combine their guess estimates of which class they will output. In the example below, we see that by combining four features  eyes, nose, ears and whiskers  we can say that it is more probable that it is a cats face rather than a dogs face.So we will give more weightage to these features when we are performing cat recognition in an image. We will do this step iteratively at each feature level, so that we can route the correct information to those feature detectors that need the information for classification.In simple English, we are trying to figure out that at each lower level, what will be the most probable output at its immediate higher level? Will the higher level feature activate when it gets the information from all the features? If that higher level feature will indeed activate, the lower level feature will give its information to that feature. Otherwise it will not pass on this information, as it is somewhat irrelevant for that feature detector.In capsule terms, lower level capsule will send its input to the higher level capsule that agrees with its input. This is the essence of the dynamic routing algorithm.These essentially are the most important aspects of a capsule network, which sets it apart from traditional deep learning architectures  namely equivariance and dynamic routing. The result is that a capsule network is more robust to orientation and pose of the data  and it can even train on comparatively lesser number of data points with a better performance in solving the same problem. Researchers have developed a state of the art performance of Capsule Networks on the ultra-popular MNIST dataset with a couple of hundred times less data. This is the power of Capsule network.Of course, Capsule networks come with their own baggage  like requiring comparatively more training time and resources than most other deep learning architectures. But it is just a matter of time before we figure out how to tune them properly so that they can come out of their current research phase into production. I am eagerly waiting for that! Are you?We will cover the first section in more depth, along with a code walkthough to get more clarity. I would recommend following along with the code on your own machine to get the maximum benefit out of it.We will first have to clear the prerequisite data and import the libraries to set up our system. I would suggest having anaconda installed in your system, as most of the prerequisite libraries are already installed along with it.Before we begin, you need to download the dataset from theIdentify the Digits practice problem. Lets take a look at our problem statement:Our problem is an image recognition problem, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model. The dataset contains a zipped file of all the images and both the train.csv and test.csv have the name of corresponding train and test images. Any additional features are not provided in the datasets, just the raw images are provided in .png format.Before starting this experiment, make sure you have Keras installed in your system. Refer to the official installation guide. We will use TensorFlow for the backend, so make sure you have this in your config file. If not, follow the steps given here.We will then use an open source implementation of Capsule Networks by Xifeng Guo. To set it up on your system, type the below commands in the command prompt:You can then fire up a Jupyter notebook and follow along with the code below.We will first import the necessary module required for our implementation.Then, we will use a seed value for our random initialization.The next step is to set directory paths, for safekeeping!Now read in our datasets. These are in .csv format, and have a file name along with the appropriate labels.Let us see what our data looks like! We read our image and display it.Now for easier data manipulation, well store all our images as numpy arrays.As this is a typical ML problem, we will create a validation set to test the proper functioning of our model. Lets take a split size of 70:30 for the train set vs. the validation set.Now comes the main part!For our analysis, we will be building three deep learning architectures to do a comparative study (like we did in the first section of this article):Let us define our neural network architecture. We define a neural network with 3 layers input, hidden and output. The number of neurons in input and output are fixed, as the input is our 28 x 28 image and the output is a 10 x 1 vector representing the class. We take 50 neurons in the hidden layer. Here, we use Adam as our optimization algorithms, which is an efficient variant of Gradient Descent algorithm. There are a number of other optimizers available in keras (refer here). In case you dont understand any of these terminologies, check out the article on fundamentals of neural network to know more in depth of how it works.Now that we have defined our model, well print the architecture.After training for 15 epochs, this is the output we get:Pretty decent  but we can definitely improve upon this.But we see what a CNN can do, we have to reshape our data into a 2D format so that we can pass it on to our CNN model.We will now define our CNN model.
We will also tweak our process a bit, by augmenting the data.This is the result we get from our CNN model:Now we will build our final architecture  Capsule network. The code is punctuated with comments if you want to understand the details. Here is how the architecture of Capsule Networks looks like:Lets build this model.This is the result we get from our CapsNet model:To summarize, we can plot a graph of validation accuracies.This ends our tryst with CapsNet!In this article, we went though a non-technical brief overview of capsule networks, and then went on to understand what are the most important aspects of it. We also covered CapsNet in detail, along with a code walkthrough on a Digit Recognition dataset.I hope this article helped you grasp the concepts of CapsNet so you can implement it in your own real life use cases. If you do, please share your experience with us in the comments below.",https://www.analyticsvidhya.com/blog/2018/04/essentials-of-deep-learning-getting-to-know-capsulenets/
Top 7 Data Science & Machine Learning GitHub Repositories in March 2018,Learn everything about Analytics|Introduction|Person Blocker|AstroNet|ANN Visualizer|Fast Pandas|TensorFlow.js|Caffe64|TensorFlow Hub,"Participate in the McKinsey Analytics Online Hackathonto win an all-expenses paid trip to an international analytics conference!|Share this:|Like this:|Related Articles|Essentials of Deep Learning: Getting to know CapsuleNets (with Python codes)|XceptionNet is a Deep Learning Algorithm that Detects Face Swaps in Videos|
Pranav Dar
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I live GitHub! Not only can you follow the work happening in different domains, but you can also collaborate on multiple open source projects. All tech companies, from Google to Facebook, upload their open source project codes on GitHub so the wider coding / ML community can benefit from it.But, if you are too busy, or find following GitHub difficult, we bring you a summary of top repositories month on month. You can keep yourself updated with the latest breakthroughs and even replicate the code on your own machine!This months list includes some awesome libraries. From Google Brains AstroNet to an artificial neural network visualizer, we have curated a list of unique repositories that will expand your machine learning horizons.Are you ready? Lets look at last months top 7 then!You can check out the top 5 repositories that we picked out in January hereand February here.Person Blocker is a python librarythat automatically blocks out entire people in images using a pre-trained neural network. The algorithm uses Mask R-CNN that is pre-trained on the MS COCO dataset. And the cherry on top? No GPU required!And not just people, the algorithm is able to block out entire objects as well. The algorithm recognizes 80 different types of objects, including vehicles, animals, electronic gadgets, among other things.You can read more about this library on Analytics Vidhyas blog here.                                            Source: YahooBack in December 2017, the Google Brain team revealed it had discovered 2 new planets by applying Astronet  its deep neural network model for working on astronomical data. It was a monumental discovery that went to show the far-reaching impacts of machine learning in todays world.Now, Google Brain has released the entire code that went into making that technology and theyve made it available for everyone. The model is based on a convolutional neural network (CNN).We have you covered on this AVBytes article regarding AstroNet.ANN Visualizer is a python library that enables us to visualize an Artificial Neural Network using just a single line of code. It isused to work with Keras and makes use of pythons graphviz library to create a neat and presentable graph of the neural network youre building.Check out Analytics Vidhyas detailed coverage of this awesome library here.Any python novice will tell you how flexible and powerful the pandas library is. Being a data scientist, you need to be equally flexible and think of different ways to approach a problem. The Fast Pandas repository aims to benchmark the different available methods in such situations.This is a very useful library and one we highly recommend trying out at least once.TensorFlow.js is an open-source library that you can use to train and build machine learning models in your web browser, using JavaScript and APIs. If youre familiar with Keras, the high level layers API will seem very familiar to you.Its available with GPU acceleration and also automatically supports WebGL. You can import existing pre-trained models and also re-train entire existing ML models within your web browser.Check out our coverage of this here.Caffe64 is a simple, small yet incredibly functional neural network library. We all know how onerous it is to install a neural network library. According to the developers, Caffe64 ditches all the hard work and is the easiest to compile and most lightweight neural network library, period.If youve used caffe before, this will be a piece of cake for you!TensorFlow Hub is a library to foster the publication, discovery, and consumption of reusable parts of machine learning models. In particular, it providesmodules, which are pre-trained pieces of TensorFlow models that can be reused on new tasks.By reusing a module on a related task, you can:Have you used any of these libraries before? How was your experience? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/top-7-github-repositories-march-2018/
XceptionNet is a Deep Learning Algorithm that Detects Face Swaps in Videos,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Top 7 Data Science & Machine Learning GitHub Repositories in March 2018|Stanfords NLP Course Projects are Available Online and theyre Super Impressive|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Swapping a persons face for anothers has become possible in recent times thanks to deep learning. Unfortunately, this has been misused to harm the reputation of people on the internet. It has even posed a problem for biometric systems! And if this trend continues unchecked, how long before people lose trust in the videos they watch?To combat this, researchers from the Technical University of Munich have developed a deep learning algorithm that potentially identifies forged videos of face swaps on the internet.The first and third rows are real images. Source: MIT Technology ReviewThe researchers began by curating a dataset of over 1,000 videos that had face swaps and their original versions. The database they ended up creating contained over half a million images of faces that had been manipulated by software. They are calling this database the FaceForensics dataset.Once the data had been collected, the team trained its deep learning neural network model to understand and grasp the difference between the original video and the manipulated one. This algorithm is being called XceptionNet. Once trained, the algorithm was tested against other pre-existing forgery detection approaches.The results, as you can see in their research paper (link below), are pretty impressive. Even when the video has been compressed to make the task significantly more challenging, the algorithm has produced results to be hopeful about.You can read the full research paper here.Something akin to this algorithm was desperately required to wage the battle against face swaps being used for the wrong reasons. In releasing the research paper to the public, the researchers are hoping others also take up the baton and work on this study to make it more accurate and precise.But theres also a caveat with this algorithm  it can also potentially be used to improve the quality of the face swaps which will make it harder to detect the fake. Also, as soon as a forgery detection algorithm is launched, the scammers always try to refine their model to stay a step ahead.As a data scientist, work like this is exciting since it offers a different way of working with image manipulation problems.",https://www.analyticsvidhya.com/blog/2018/04/this-deep-learning-algorithm-detects-face-swaps-videos/
Stanfords NLP Course Projects are Available Online and theyre Super Impressive,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|XceptionNet is a Deep Learning Algorithm that Detects Face Swaps in Videos|Anomalize is a R Package that Makes Anomaly Detection in Time Series Extremely Simple and Scalable|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Stanford has long been considered one of the best universities in terms of teaching, quality of faculty and the content they teach. With the recent boom in the machine learning field, Stanfords ML courses have generated a lot of interest (you can find videos on YouTube if you havent done so already).Each year, Stanford releases a list of projects that its students have worked on and recently, in that same regard, has released a list of course projects for its Natural Language Processing (NLP) course. And wow, is it impressive.Students were given two options for the project  either choose your own topic (called Custom Project) or take part in the Default Project, which was building Question Answering models based on the SQuAD challenge. Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. Remember we covered this on AVBytes  Alibaba and Microsofts models had the bests scores on this dataset, which have since been eclipsed.Some of the papers submitted will undoubtedly impress you. The winner of the Custom Project did a project on Speech Synthesis in which the text-to-speech synthesis model produces audio from a sequence of input characters. They also demonstrated how to build a convolutional sequence to sequence model with a natural voice and pronunciation from scratch in well under $75!There was a project on machine translation of the Eskimo language which also won a prize. The students built a sequence to sequence neural machine translation model that translates the Eskimo language into English. Incredible!Another project that caught the eye was Generating SQL queries from Natural Language. The students developed a model for generating SQL query from from natural language question.There was even a project on creating memes  Dank Learning: Generating Memes using Deep Neural Networks, which produces a humorous caption given any image.Check out Stanfords page here which lists each project.I like that Stanford opens these projects up to the community so all aspiring data scientists and practitioners can read about the different (and sometimes unique) approaches taken. I highly recommend going through this list and choosing the papers you find interesting. This will be a great learning for you in the NLP field.One of my favourites, and this will undoubtedly interest data scientists, was based on generating SQL queries from natural language. Check it out!Which project caught your eye? Let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/04/stanford-nlp-course-projects-available-online-theyre-super-impressive/
Anomalize is a R Package that Makes Anomaly Detection in Time Series Extremely Simple and Scalable,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Stanfords NLP Course Projects are Available Online and theyre Super Impressive|Develop Your Own Personal Deep Learning Image Dataset using this Python Script|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anyone familiar with the machine learning world has been introduced to, or works with, time series forecasting. Its one of the most popular fields and has diverse and vast applications.But actually performing a time series analysis is not a straightforward task. Even with the packages currently available, there is still a bit of work that goes into making a time series model ready for the eventual analysis and for building a model. With the manual effort that goes in, the chances of missing anomalies and making errors increases.The anomalize package makes it really simple, easy and scalable to detect anomalies in your data. It has three functions (mentioned in this article below) and together, they make it a straightforward process to decompose, the given time series, detect any anomalies, and finally create bands that separate the normal data from the anomalous one.The package has three main functions, namely:In order to use this package, you need to have the tidyverse package installed and loaded as well. You can install the anomalize package either from devtools:or from the CRAN itself:You can check out the GitHub repository hereand view the below video for further details on this package.There have been packages built for anomaly detection previously, namely Twitters AnomalyDetection and the tsoutliers() packages. But anomalize takes that to the next level by making it even more simpler and scalable within the tidyverse universe.The developers have mentioned that they are also looking into the possibility of making a python library for this. The initial reaction in the ML community has been extremely positive. This is a brilliant tool for QA analysis in any field, from marketing to manufacturing. Go ahead and try it out.",https://www.analyticsvidhya.com/blog/2018/04/anomalize-r-package-makes-anomaly-detection-extremely-simple-scalable/
Develop Your Own Personal Deep Learning Image Dataset using this Python Script,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Anomalize is a R Package that Makes Anomaly Detection in Time Series Extremely Simple and Scalable|Replicating Human Memory Structures in Neural Networks to Create Precise NLU algorithms|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Developing your own dataset can be a really tedious and time consuming task. And when it comes to images, multiply the amount of effort by 100.So this python script will come in handy for people who dont have a lot of time on their hands but want to build an exhaustive image dataset for deep learning purposes. Using this, you can download hundreds of Google images to your own machine.This script is a command line python program. You can search keywords and/or key phrases in Google Images and optionally download the resulting images. Additionally, you can also invoke this script from another python file.Its a fairly straightforward program and does not require any dependencies if you only prefer downloading 100 images per keyword. In case your requirement goes beyond the 100 threshold, youll need to install the Selenium library along with chromedriver.This program is compatible with both python 2 and 3. You can see the underlying structured behind the code in the below image:You can install this library from pip by typing the below command:Read more about this library and access the source code on GitHub here.Please take note of the below disclaimer regarding copyright terms before using these images:This program lets you download tons of images from Google. Please do not download or use any image that violates its copyright terms. Google Images is a search engine that merely indexes images and allows you to find them. It does NOT produce its own images and, as such, it doesnt own copyright on any of them. The original creators of the images own the copyrights.Images published in the United States are automatically copyrighted by their owners, even if they do not explicitly carry a copyright warning. You may not reproduce copyright images without their owners permission, except in fair use cases, or you could risk running into lawyers warnings, cease-and-desist letters, and copyright suits. Please be very careful before its usage!This is a really cool script that you can use for your own personal purposes. Practice deep learning problems on your own machine but please do not use it commercially or distribute images without the owners permission!Its a pretty easy script to replicate once you understand how it works underneath. Let us know your experience using it in the comments below.",https://www.analyticsvidhya.com/blog/2018/04/curate-deep-learning-image-dataset-using-python/
Replicating Human Memory Structures in Neural Networks to Create Precise NLU algorithms,Learn everything about Analytics|Introduction|Table of Contents|Lets start with a simple exercise  tweet classification|How does our brain process the English Language?|Notation we will use in this article|Lets start with RNN|Lets consider Gated Recurrent Unit (GRU) first|Lets step up the game and understand Long Short Term Memory (LSTM)|A short note on Bidirectional RNN|LSTM vs GRU  Who wins?|End Notes,"Share this:|Like this:|Related Articles|Develop Your Own Personal Deep Learning Image Dataset using this Python Script|AVBytes: AI & ML Developments this week  Comet.ml for ML Models, TensorFlow.js, a Python ANN Visualizer, etc.|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning and Artificial Intelligence developments are happening at breakneck speed! At such pace, you need to understand the developments at multiple levels  you obviously need to understand the underlying tools and techniques, but you also need to develop an intuitive understanding of what is happening.By the end of this article, you will develop an intuitive understanding of RNNs, especially LSTM & GRU.Ready? Lets go!Have a look at thisarticleon NLP.I took a handful of tweets and used the word count of positive versus negative words to classify the sentiment of the tweet.This approach might work for simple sentences but will fail in the case of long sentences with mixed emotions or negated phrases like not at all happy.Any thoughts on why this approach fails?The answer is simple. The algorithm is far from how humans evaluate a sentiment score. Let me elaborate further. Consider the below sentence:Anyone can make out that the sentiment of this sentence is positive. But an n-gram scoring algorithm, solely based on word/phrase counting, gets confused because the sentence starts with a double negative and finally finds one positive to end the sentence. Analyzing each word, or a group of words individually, does no justice to the meaning of the sentence.Before we talk about sophisticated tools like RNN, LSTM and GRU, lets first developan intuition of what do we expect these models to do by understanding how the human brain reallyprocesses written/spoken language.Dont worry, this is not a biology lesson. We are only interested in understanding how our brain stores all the information it gets from all sensory organs. Following is a simplistic view of how humans store data:Read this statement  Bitcoin peaked in Dec 2017 to a value of $19,870.62 but dropped down steeply to a range of $6000-$7000. The current value of Bitcoin is about $6635.38.If I ask you a question right now  What is the current value of Bitcoin?, it is highly likely that you will reply $6635.38. However, if I ask you this question tomorrow, its highly likely that your answer will be in a range, something like  $6000  $7000. The reason we tend to forget precise values over time is because we initially retrieve the information from short term memory and later from long term memory.Our short term memory is like a scratch pad and our brain uses gates to find relevant concepts that need to be retained in our long term memory. Implicit memory is responsible for unconscious activities like riding bicycle or putting on shoes, etc. Explicit memory is the conscious memory and comprises of Episodic Memory and Semantic Memory. Episodic is a very case specific learning we do, for instance  your last trip to a beach or your memories of schooling etc. Semantic is generic rules we learn over time, like  snakes are poisonous or the Earth is round, etc.Now lets narrow down the discuss to NLP. If you are reading a novel, say Julius Caesar, you do not register each and every line that is written to your short-term memory butyou will learn and store the generic style of writing/grammar in your Semantic Memory, and learn about characters like Caesar/Brutus and important events in our Episodic memory. If we want to build a neural network that can compete with humans in terms of Natural Language Understanding, it needs to do exactly the same, i.e., use short term memory to register everything that it reads, use gates to identify what do we need in long term memory, and finally a long term memory to create something like a knowledge graph.Recurrent Neural Networks (RNNs) do exactly those things to understand natural language or any other sequential data. Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) are two special architectures of RNN that are great tools to have in your kit when trying to understand the semantics of a language. Now that you understand what this new architecture does, we will now look at how they perform exactly as the human brain does (or close enough).Here is a sample problem to define our notation  you have 1000 tweets on Cryptocurrency and you are tasked with identifying the cryptocurrency that is being most talked about. Here are couple of sample tweets :Notice that a single coin can be referred to in multiple ways  Bitcoin or BTC or something else. You want to train a neural network that can identify the occurrence of coin names. One way to define output sequence is as follows (Replace irrelevant words with U and coin name by their ticker):Our inputs will be denoted as X <i> (j) where i is the word number in the sequence and j is the sequence number. For instance, bullish is X <2> (1), Both is X <1> (2). Similarly, Y <1> (2) is U and Y<4> (1) is BTC. Try to memorize these notations as well reference them later in this article. We will denote W and C to identify weights and biases in all equations and drop subscripts for simplicity.We will also refer to two types of main functions in our RNNs Why do we need a special type of neural network at all for NLP based problems?Here are a few (not exhaustive) reasons why simple a neural net cannot do well on NLP problems:Following is a pictorial view of a single RNN:The blue outlines are individual RNN cells. I have collapsed the first and third cell for simplicity. Each RNN cell gets a hidden state from the previous RNN and a new input element. It then generates an output and passes on the modified hidden state.So for a sequence of 10 elements in a series, you will have 10 units modifying the hidden state, which is trying to capture the semantic of the sentence. Now this hidden state, which has all the information in the form of numbers, can be used directly to understand the sentiment of the sentence or any other information. Notice that instead of using individual words to understand the meaning of a sentence, we are now working more human like, i.e., creating a vector that has all the stored information of the entire sentence in a compressed form.The Squashing function will look something like:So, we found a jackpot to crack any NLP problem. But we have two major problems in this architecture. First, the vanishing gradient and, second, the exploding gradient.In the last 5 years we have solved both these problems in vastly different ways. The solution to exploding gradient is simple  just define a ceiling for the gradient. But the solution for vanishing gradient challenge is a much more involved process. Lets try to understand what Vanishing gradient really does to our algorithm. Consider this sentence:We want our neural network to fill in the blank. The actual subject wife will take the verb hates but this subject is far away from our verb. The English language can give grammatical tasks that are much more complex but are critical in understanding the meaning of the sentence. Neural Networks suffering with vanishing gradient problem tend to miss out relations between words that are far away from each other. LSTM and GRU are the architects of RNN that can solve this issue of vanishing gradient in a very human brain-like way.GRU is an elegant design which provides our RNN with a shortcut. RNN modifies our hidden state at every element. GRU simply gives a bypass option to our RNN on a few words.For example, in the sentence My wife, who loves to play with dogs, ______ (hate or hates) cats, GRU will simply not modify the hidden state for the words , who loves to play with dogs,  and correctly predict hates in the blank. Lets try learning this concept mathematically,The version of GRU that is popularly used is slightly different from the above equation. We introduce another gate called reset gate that modifies our first equation. Here is the updated equation:Lets try to see this equation pictorially to get a stronger intuition of GRUs .Note that I have not drawn the connections from Xs to the Gates for simplicity of representation. Refer to the equations for complete understanding of the mathematics behind the scene.Here is a quick check of all you have learnt till now The answer is right there in the picture. Now, we have only an update gate between h<t> and h<t-1> which has a value between 0 and 1. Our back propagation does not have to travel through the mathematically complex squashing function in a traditional RNN.LSTM is very similar to GRU but has more number of parameters to optimize. More parameters means more time to train these models. However, LSTM is supposed to perform better than GRU in sentences with long dependencies.We will focus on key differences in mathematical formulation between LSTM and GRU, rather than trying to write the equations from scratch. This will not only build up our concept on top of our GRU understanding but also help us appreciate the key differences.We had two gates in GRU  Update gate and the Reset Gate. LSTM does not have the Reset gate, but you are free to modify the architecture to derive your own LSTM. The Update gate in LSTM is broken down into two:This is one of the key differences between LSTM and GRU, the sum of coefficient for last hidden state and new calculated RNN hidden state is constraint to sum up to 1, whereas LSTM have these two coefficients as independent variables that can take any value. Now coming to a second key difference, LSTM maintains two different memories  Cell State and Hidden state. Cell state is the long term memory and hidden state is the short term memory, very similar to human brain. The concept will get clearer once we start formulating the mathematical equation. We will denote c<t> for cell state and h<t> for the hidden state.Here is a visualization of the equations written above:Note that I have not drawn the connections from Xs to the Gates for simplicity of representation. Refer to the equations for complete understanding of the mathematics behind the scene.Forget gate is an important difference between LSTM and GRU. This gate regulates how much previous information needs to be sent to the next cell, whereas GRU exposes its entire memory to the next cell. Note that hidden state, and not the cell state, is used to evaluate all the gates in LSTM. Hence controlling the hidden state that is moving forward to the next cell can give us complete control of all gates in the next cell. The difference is not a strong one when it comes to practical applications of LSTM vs. GRU. The cell state acts as a shortcut for the back propagation to travel and hence avoids the problem of diminishing gradient.Here is a brain twister  There ________ (seem/seems) to be a defect in all the cars manufactured by company XYZ in 2010. What is the main subject of this sentence  a defect or all the cars or company XYZ? The answer might look obvious to you  a defect is the main subject and, hence seems is the right answer. Here is what I want you to think about  out of GRU and LSTM, which one is in a better position to answer this question?Probably, none. Because both LSTM and GRU accumulate all the knowledge going from left to right. This approach works fairly well for most of the sentences, except ones that look like the example given above. In such exception cases, the relevant information that is required to answer such questions lies on the right side and is yet to be read by the algorithm. How do we solve this challenge?Bidirectional RNN to the rescue! It works on a simple idea  just read the sentence from both left to right and right to left. This process increases our optimizing parameters space by approximately a factor of 2. Our next layer will simply have more features to work with and the rest of the architecture remains the same.If the above explanation is too complex to grasp, just make a note in your long term memory that Bidirectional is an add-on to our LSTM and GRU cell that can simply enhance their capability to create thought vector from either direction at a cost of additional processing time.There has been a lot of debate around which among the two wins without an objective answer yet. Here are a few widely accepted principles and my opinions on them:Even though this article included minimal mathematics, it is enough for you to start experimenting with all the architectures referenced in this article. If your end goal is to use these tools on real world problems, I will recommend further reading on sequence to sequence models (Encoder Decoder architectures). I will cover these sophisticated architectures in my future articles  stay tuned.If you have any ideas or suggestions regarding the topic, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/replicating-human-memory-structures-in-neural-networks-to-create-precise-nlu-algorithms/
"AVBytes: AI & ML Developments this week  Comet.ml for ML Models, TensorFlow.js, a Python ANN Visualizer, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|Replicating Human Memory Structures in Neural Networks to Create Precise NLU algorithms|AlterEgo  MITs New AI Reads and Transcribes What Youre Thinking!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We continue to bring you the biggest and most important developments in the ML world on AVBytes.We keep you updated so you can modify your skillset accordingly and keep yourself relevant in this ever-changing field.In this past week, we covered TensorFlow.js being launched, Comet.ml for sharing and tracking all aspects of your ML model, DeepMinds latest research using neural networks, a python library that can block out entire people and objects in images, among other developments.Scroll down to view all the articles from last week. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.Source: MediumThe above AVBytes were published from 2nd to 8th April, 2018.Also, check out our coverage of all the sessions that happened at the TensorFlow Developer Summit last week!",https://www.analyticsvidhya.com/blog/2018/04/avbytes-ai-ml-developments-this-week-090418/
AlterEgo  MITs New AI Reads and Transcribes What Youre Thinking!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Comet.ml for ML Models, TensorFlow.js, a Python ANN Visualizer, etc.|Comet.ML is the GitHub for Machine Learning Models|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computers being able to pick up what a person is thinking has been so far a product of our imagination. We used to look at these other worldly concepts in movies, or read about them in books, and our imagination used to soar.Those fictional worlds are inching closer to becoming real-life application.Researchers at MIT have developed a device and a computing system (together called AlterEgo) that picks up the words you dont say aloud but vocalise internally. According toMITs blog post, electrodes in the device pick up neuromuscular signals in the jaw and face that are triggered by internal verbalizations  saying words in your head  but are undetectable to the human eye.The researchers conduced several experiments on test subjects to see how the system performed in different situations. In one example, the system picked up the thoughts of a chess opponent and read off almost all his moves!So how did they go about building this system? The developers built a neural network that searches and identifies correlations between neuromuscular signals and particular words. It even customises to the users needs by re-training just the last two layers of the neural network.The system was tested on 10 subjects and the accuracy of the algorithm, in terms of reading and transcribing the inner words, was a really impressive 92%. Currently the system is still in its nascent stages so its limited to performing simple tasks like calculating sums, having short conversations. etc.Check out the device in action below:Research on mind reading algorithms has been going on since a while, dating back to the 19th century! But with the recent boom in technology, this has taken a huge leap forward.The system will keep getting better as more and more data is accumulated to train the model. Can you imagine the vast uses of this system? If built and utilised properly, it could help deaf people understand what the other person its saying, could be sued for communication in loud places (like airport tarmacs and manufacturing plants), for the military, among many, many other things.",https://www.analyticsvidhya.com/blog/2018/04/mits-new-algorithm-reads-transcribes-youre-thinking/
Comet.ML is the GitHub for Machine Learning Models,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AlterEgo  MITs New AI Reads and Transcribes What Youre Thinking!|DeepMinds Latest Research  Building a Neural Network that Navigates without a Map|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GitHub has gained unparalleled popularity over the years for its amazing flexibility in allowing teams to collaborate and contribute to projects.Along the same lines comes Comet.ML, a tool that enables data scientists and machine learning practitioners to automatically track their machine learning code, experiments, hyperparameters, and model results. It additionally creates a graph of all the results which makes it easier to visualize and compare all your experiments.It has been released today from beta and is available to use for everyone.                                                Source: MediumYou can integrate Comet into your ML pipeline fairly easily. To get started, you only need to add the Comet.ml tracking code to whatever tool youre using for building the model. One of the best parts about this service is that it doesnt matter where you train your model (it could be on your machine, on the cloud, etc.).It allows the user to choose from any of the below tools and libraries:Comet works with GitHub and other git service providers. Once you have run your experiments and finalized your best model, you can generate a pull request straight to your GitHub repository.To install it on your machine via pip, follow the below command, depending on your version of python:Get a high level overview of how Comet.ml captures your models results and visualizes it below:There are different services available depending on your requirements. They range from a free version (the most popular option) to a business version. Check out and sign up for Comet.ml here!We are loving this release from Comet.ml! As a data scientist, you dont need to change the tool youre working on or the process you usually follow. Instead, you have a supplementary service that lets you see deeper into how well your experiments are going by comparing model to model.Tuning the hyperparameters of your model to get better results, or to compare it to previous models, is a tedious process. Comet.ml helps immensely in that aspect as well.Its a bit like Tensorboard with code and hyperparameter tracking. Go ahead and use it today and let us know your experience in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/comet-ml-is-the-github-for-machine-learning-models/
DeepMinds Latest Research  Building a Neural Network that Navigates without a Map,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Comet.ML is the GitHub for Machine Learning Models|Highlights of TensorFlow Developer Summit 2018|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In todays digitally connected world, we have come to rely on Google Maps and other GPS elements to guide us to our destination. Or if we are familiar with a place, we use our memories and visual queues (familiar objects we recognise from previous visits) to get to the end point. But what if there was no GPS and it was a completely new place?DeepMinds research team has built an interactive navigation environment that makes use of first-person point-of-view pictures from Google Street View to train its AI model. The team built a deep neural network artificial agent that continuously learns to navigate multiple cities using information it gathers from Street View images.The neural network model is rewarded. The more it is exposed to the visual environment, the better the model gets. Once it gets the hang of a few cities, it can adapt to a new city very quickly. The NN is made up of three parts:They are keen to stress that this study is about navigation in general rather than self-driving. If you read their research paper (link below), youll notice that they have not used any techniques to manipulate or model the control of vehicles or used any information about traffic.You can read the official research paper hereand DeepMinds blog post here.The aim of this study was to train the neural network to navigate the way that humans do and it has produced excellent results so far. The model was trained using deep reinforcement learning, taken queues from recent papers like learning to navigate in complex 3D mazes and reinforcement learning with unsurprised auxiliary tasks.But these studies were conducted on relatively small data while DeepMind has the capability to use real-life visual environments (hence, images from Google Street View). Do go through their paper and let us know your views in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/deepmind-research-build-neural-network-navigate-without-map/
Highlights of TensorFlow Developer Summit 2018,"Learn everything about Analytics|Introduction|Session presented at the Summit|
|1. Keynote|2. The tf.data Library|3. Eager Execution|5. Training Performance|6. Practitioners guide with TF High Level API|7. Distributed TensorFlow|8. Debugging TensorFlow with TensorBoard plugins
|9. TensorFlow Lite||10. Searching over Ideas|11. Reconstructing Fusion Plasmas|12. Nucleus: TensorFlow toolkit for Genomics|13. Opensource Collaboration|14. Swift for TensorFlow |15. TensorFlow Hub|16. TensorFlow Extended|17. Applied AI at the Coca-Cola Company||18. Real World Robot Learning|19. Project Magenta|End Notes","Share this:|Like this:|Related Articles|DeepMinds Latest Research  Building a Neural Network that Navigates without a Map|Deep Learning Makes it Possible to Predict Life on Other Planets|
Aishwarya Singh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow is one of the most popular open source libraries in the Machine Learning and Deep Learning community. We see breakthroughs in diverse fields on a weekly basis and more often than not, TensorFlow is at the heart of the final model.Since its release, the TensorFlow team has continuously worked on improving the library by making it simpler and interactive for the users. TensorFlow Developer Summit is an event that brings together TensorFlow users from all over the globe to see all the new products, tools, libraries, and use cases presented by the TensorFlow team and industry leaders.After receiving an overwhelming response to the first TensorFlow Developer summit conducted in2017, this years TensorFlow Developer Summit was held on March 30th 2018, at the Computer History Museum in Mountain View, CA. More than 500 TensorFlow users attended the summit and thousands others connected via a live stream .The Summit had many exciting announcements, demos and tech talks. Below we have provided the highlights from every session presented at the summit.Speakers: Anitha Vijayakumar, Rajat Monga, Megan Kacholia and Jeff DeanAnitha Vijayakumar, technical program manager for TensorFlow, kicked-off the summit by talking about the various fields where machine learning is currently being used extensively. A few examples she quoted were:Followed by this, Rajat Monga, director of engineering at TensorFlow, spoke about the growth of TensorFlow in the past two years as highlighted in the below graph:We have mentioned a few takeaways from his talk below:Taking the discussion forward, Megan Kacholia, Engineering director at Google Brain, introduced the latest update to TensorFlow Lite that lets users work on any platform, like CPU, Android or iOS. She also mentioned the various platforms TensorFlow works on, which now includes Cloud TPU. A beta version of cloud TPU was launched in February and it provides 180 teraflops of computation per device. Have a look at the Reference models and tools for Cloud TPUs here.Jeff Dean, leader of the Brain team talks about how TensorFlow addresses real problems, focusing on Advance health informatics and Engineer the tools for scientific discovery. The main idea is to make machine learning easier to use and replace ML expertise with computation.Speaker: Derek Murraytf.data is a new library that helps users get all of their data into TensorFlow. It works as an input pipeline. Derek Murray introduced the tf.data library and talked about its performance. He explained in detail about theflexibility, ease of use and speed that it provides. Derek also announced the launch of a performance guide for tf.data on the website for users. You can watch the complete talk here.Speaker: Alexandre PassosFocusing on making TensorFlow simpler to use, the team introduced an intuitive programming model, Eager Execution. With Eager Execution, the distinction between the construction of execution graph can be removed. Thus, one can now use the same code to generate the equivalent graph for training at scale using the Estimator high-level API. This was launched in the TensorFlow 1.5.0 update which we covered here.Alexandre Passos, Software Engineer at TensorFlow, talks about eager execution in detail along with a demo code in the following video.4. ML in Javascript: TensorFlow.jsSpeakers: Daniel Smilkov and Nikhil ThoratInspired by the JavaScript library, deeplearn.js, which was released in August 2017, the team has now launched tensorflow.js which brings machine learning to JavaScript. tensorflow.js allows the user to build and train modules in the browser itself. A user can also import TensorFlow and Keras models trained offline for inference using WebGL acceleration. You can read more about this on Analytics Vidhyas AVBytes article here.Here is the talk by Daniel Smilkov and Nikhil Thorat on TensorFlow.jsSpeaker: Brennan SaetaIn this talk,Brennan Saeta provided an overview for users on how to optimize training speed of their models on GPUs and TPUs. Starting with an introduction to the ML training loop, he further spoke about improving the performance following three basic steps: Find Bottleneck, Optimize Bottleneck and repeat, and he also provided a few pointers about its future impact.Speaker: Mustafa IspirMustafa Ispir spoke about high level APIs and how these can be used by ML practitioners for performing modeling experiments, with just a few lines of code. He explained, with a case study, how high level APIs can be used to be more efficient and effective. The discussion was mainly focused on High Level APIs built for each step in a ML pipeline. Briefly summarizing, APIs are built for the Estimators, Features, Premade, Modeling, Scaling and Serving.Speaker: Igor SaprykinDistributed TensorFlow is a method by which the models can train faster and work parallely. Igor Saprykin in his talk discussed the different ways to train a model on a single machine and multiple GPUs. The tf.contrib.distribute is a module that handles distributed computing in TensorFlow. It uses all-reduce on multiple GPUs to perform In-graph replication with synchronous training.Speakers: Justine Tunney and Shanqing CaiTo make debugging models easier, the TensorFlow team has released a new interactive graphical debugger plug-in as part of the TensorBoard visualization tool. In the following video, Justine Tunney and Shanqing Cai give a demo of the TensorFlow Debugger. It will help the user inspect, set breakpoints and step through the graph nodes in real time. This was definitely one of our favorite things from the conference!Speakers: Sarah Sirajuddin and Andrew SelleTensorFlow Lite was initially launched last year, and since then many new features have been added to improvise the same. Sarah Sirajuddin, software engineer in the TensorFlow Lite team, talked about TensorFlow Lite and the benefits of having machine learning models on mobile and other edge devices. The tool also provides support for Raspberry Pi and ops/models (including custom ops) . Here is the general workflow:Talk on TensorFlow Lite by Sarah Sirajuddin and Andrew can be viewed below:Speaker: Vijay VasudevanMost machine learning algorithms require extensive tuning of hyperparameters for obtaining best results. Vijay Vasudevan in his talk discussed about how TensorFlow can be useful in hyperparameter optimization. He suggested that automated Machine Learning techniques can be used in order to evaluate our ideas more efficiently. In the following video, Vijay explains the hyperparameter optimization process in detail:Speaker: Ian LangmoreStarting with a brief talk on nuclie fusion and plasma, Ian Langmore explained how Google and Tae together reconstruct plasma attributes on the basis of measurements, using Bayesian Inference. This is a Bayesian inverse problem and it uses TensorFlows distribution and tensorflow_probability libraries.Speaker: Cory McLeanCory McLean, Engineer in the Genomics Team at Google Brain, announced the launch of Nucleus, a Python library for reading, writing, and filtering common genomics file formats for conversion to TensorFlow examples. In his talk, he gave a basic introduction on genomics and opportunities for deep learning in this field. Furthermore, he spoke about Nucleus interoperable data representation with Variant Transforms, an open source tool from Google Cloud.Speaker: Edd Wilder-JamesEdd Wilder-James gave a brief talk on the TensorFlow community. He shared the number of users and contributors of TensorFlow which, as you can see is the image below, is further evidence of the growth of the TensorFlow community.With the new additions and improvements in TensorFlow, he expects the numbers to increase rapidly. Here is the video where Edd talks how they are planning to engage and collaborate with their users.Speaker: Chris Lattner and Richard WeiSwift for TensorFlow (TFiwS) is an early stage open source project with the aim to improve usability of TensorFlow. It has many design advantages, and will be released with technical whitepaper, code, and an open design approach in April 2018. The Swift for TensorFlow team strongly believes that Swift could be the future of data science and machine learning development.You can have a look at the talk by Chris Lattner and Richard Wei here:Speakers: Andrew Gasparovic and Jeremiah Harmsen)TensorFlow Hub, a built-in library, is launched to foster the publication, discovery, and consumption of reusable parts of machine learning models. Jeremiah Harmsen and Andrew Gasparovic explained how TensorFlow Hub lets you build, share and use Machine Learning modules. This can be easily integrated into your model with a single line of code. You can look at the below image for a better understanding of this and visit theirblog.Below is the video from the summit where Andrew Gasparovic and Jeremiah Harmsen discuss TF Hub:Speaker: Clemens Mewald and Raz MathiasClemens Mewald and Raz Mathias announced the roadmap for TensorFlow Extended (TFX), which is an end-to-end ML platform built around TensorFlow. Also, TensorFlow Model Analysis (TFMA), a library to visualize evaluation metrics, was launched. Here is the video where a demo of the same has been presented. Have a look!Speaker: Patrick BrandtIn 2016, Coca-Cola updated its core loyalty marketing program which states that in order to avail entry into the promotions, buyers need to input a 14-character proof-of-purchase code. Typing this manually would be a tedious job, so the team built a mobile app for recognizing codes (which were embedded in the bottles cap). Basically, Coca-Cola built an Optical Character Recognition (OCR) model that uses Convolutional Neural Networks and TensorFlow to perform the task. Watch this fascinating talk below:Speaker: Alexander IrpanAddressing real world research problems, Alex Irphan from the Google Brain Robotics Team, explained his approach to solve the problem. The problem setup was: neural network commands a robot arm to grasp objects. The robotics team combined feature level domain adaptation and pixel level domain adaptation to train the model. You can watch the video below to understand how the simulators and other ML techniques are used to reduce the amount of real world data required.Speaker: Sherol ChenProject Magenta is a smart tool that allows artists to create music using pre-trained models. Sherol Chen in her talk explaied how these models can be tuned and controlled. She also presented a glimpse of this on a keyboard, while tuning the outputs. The combination of music and machine learning is always a positive and has been well received by the TensorFlow community.There were quite a few new things launched at this years summit. While we had already seen Eager Execution before, new tools and libraries like TensorFlow.js andTensorFlow Model Analysis were introduced and will surely be incorporated into ML models soon.You can watch the full live stream of TensorFlow 2018 here! What did you find most exciting about this summit? Let us know your thoughts and feedbacks in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/tensorflow-developer-summit-2018-highlights/
Deep Learning Makes it Possible to Predict Life on Other Planets,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Highlights of TensorFlow Developer Summit 2018|This Python Library Visualizes Artificial Neural Networks (ANNs) with just One Line of Code|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The topic of life on other planets, or extraterrestrial life, has always intrigued us. For centuries, scientists and researchers have been searching for answers but have always come up short due to lack of technological help. We all love those movies where we discover other planets habitable for human life and move away from Earth. These fictional stories are inching closer to becoming a reality.Thanks to the advancements in deep learning, a group of researchers from the Centre for Robotics and Neural Systems (CRNS) at Plymouth University have developed a deep neural network that can analyse life patterns and identify how habitable a planet is.The researchers trained artificial neural networks (ANNs) that can classify five types of planets:ANNs are able to learn continuously through pattern recognition so the team fed them data using NASAs Planetary Spectrum Generator (PSG). The data included hundreds of unique high resolution spectrum profiles of each planet mentioned above. Each profile had tons of parameters that defined the habitable nature of the planet.The neural network then attempted to classify each of these planets. The results, so far, have been promising according to the project supervisor, Angelo Cangelosi. When the model was presented with a completely new profile, it adapted and performed well.Our outlook on life is limited by human elements so the research team came up with a metric called probability-of-life. It is basically based on factors like the atmosphere and the orbit of the five planets/worlds it was tested on.This model can be used for categorizing the different types of exoplanets. As a data scientist interested in this field, you can imagine how this technique can be used to identify prospective planets (other than the 5 mentioned above) for signs of life.The findings and details about the ANN will be presented by this team today at theEuropean Week of Astronomy and Space Science (EWASS) in Liverpool. We will update this article as and when we get more details about how accurate the final model has been so far. Watch this space!",https://www.analyticsvidhya.com/blog/2018/04/predicting-life-on-other-planets-now-possible-thanks-to-deep-learning/
This Python Library Visualizes Artificial Neural Networks (ANNs) with just One Line of Code,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Deep Learning Makes it Possible to Predict Life on Other Planets|Person Blocker is a Python Script that Blocks Out Entire People in Images|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","ann_viz(model, view=True, filename=network.gv, title=MyNeural Network)",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Vizualizing data lies at the core of any good data scientists skillset. It gives a holistic view of what could be hidden in the data. But with advanced in deep learning, you can visualise the entire deep learning process or just the Convolutional Neural Network youve built.Now, you can even visualize an Artificial Neural Network using just a line of code. ANN Visualizer is a visualization library used to work with Keras. It makes use of pythons graphviz library to create a neat and presentable graph of the neural network youre building.You can install the library using the below command:To generate the visualization, you need to follow the below command structure:Below is an example of what the final visualization looks like:Beautifully done, isnt it? You can go through this GitHub repository here to look at a demo of how a model is created and visualized.This is useful in a lot of ways. It can be used for teaching purposes when you want to explain how your NN looks like without having to run a lot of code. There have been previous efforts in this area but with the ease of effort and the beautifully optimised output, this is a library worth checking out.It currently only visualizes dense layers but the developers has indicated that convolution and LSTM layers might be added soon. Keep in mind that this is still an unstable release so you might encounter a bug here and there but dont let that put you off from trying it out!",https://www.analyticsvidhya.com/blog/2018/04/python-library-visualizes-artificial-neural-networks/
Person Blocker is a Python Script that Blocks Out Entire People in Images,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|This Python Library Visualizes Artificial Neural Networks (ANNs) with just One Line of Code|TensorFlow.js  Now Build Machine Learning Models in JavaScript!|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Weve all seen those edited videos paraded around in the news and movies where a persons face is blacked out due to various reasons. You can now do the same on your machine by typing just a few lines of code!A group of developers have created a python library called Person Blocker that automatically blocks out entire people in images using a pre-trained neural network. The algorithm uses Mask R-CNN that is pre-trained on the MS COCO dataset. And the cherry on top? No GPU required!And not just people, the algorithm is able to block out entire objects as well. The algorithm recognizes 80 different types of objects, including vehicles, animals, electronic gadgets, among other things. You can find the entire list in the class.py file on GitHub (link given at the end of this section).There are two steps associated with blocking objects in the image:You can run Person Blocker from the command line itself:The output of this is two-fold  a static image and an animated image.There are a few dependencies attached to this library and we have listed them below:You can view the GitHub repository for Person Blocker here.We tried out this library in python and we pretty impressed with the results. This will be a good go-to library for anyone getting into the image processing field. You can see below how the algorithm runs and the labels that are used to recognise objects in the image:You can choose to block out anything in that image that has been labelled. Pretty impressive, isnt it? Let us know in the comments below how it worked out for you.",https://www.analyticsvidhya.com/blog/2018/04/person-blocker-is-a-python-script-that-blocks-out-people-from-images/
TensorFlow.js  Now Build Machine Learning Models in JavaScript!,Learn everything about Analytics|Overview|Introduction|How do I install TensorFlow.js?|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Person Blocker is a Python Script that Blocks Out Entire People in Images|Automatic Image Captioning using Deep Learning (CNN and LSTM) in PyTorch|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youre a JavaScript developers whos new to the world of Machine Learning, or a Machine Learning practitioner whos new to JavaScript, this latest release of TensorFlow will intrigue you.TensorFlow.js is an open-source library that you can use to train and build machine learning models in your web browser, using JavaScript and APIs. If youre familiar with Keras, the high level layers API will seem very familiar to you.You might be wondering at this point whats the advantage here of using a browser to build ML models.Well, running a ML program in your browser means there is need to install any libraries or drivers! Open your browser page and youre ready to build your model. Its also available with GPU acceleration. Additionally, TensorFlow.js supports WebGL, a JavaScript API that is used for rendering 2D and 3D graphics within any browser.You can also open the webpage on your mobile or tablet which will enable your model to take advantage of sensor data.You can import an existing pre-trained model and TensorFlow.js converters will make it browser ready for you. You can also re-train existing ML models using transfer learningto augment an existing model trained offline using a small amount of data collected in the browser using a technique called Image Retraining. You can re-train the model very quickly and efficiently with this while only requiring a small amount of data.Its pretty straightforward. You can use it by installing in from NPM:Check out the TensorFlow.js website and their GitHub pageto read more about this release.This is awesome news for folks who are familiar with JavaScript and are trying to carve their way in the ML world! It makes things a lot simpler for folks coming from a non-ML background who are looking to understand this field. The use cases for this are plenty. I highly encourage you to check out the demos on the official site which include a real-time performance on a piano by a neural network and playing the popular Pac-Man game using images trained in a browser.TensorFlow.js is basically the successor of deeplearn.js. The major difference between the two is the TensorFlow.jsincludes a layers API and imports pre-trained models and can also re-train them. Also, you can work on almost any GPU but it will not be close to the speed youll get on CUDA.Are you planning to use this for building your models? What do you think about this latest TensorFlow release? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/04/tensorflow-js-build-machine-learning-models-javascript/
Automatic Image Captioning using Deep Learning (CNN and LSTM) in PyTorch,Learn everything about Analytics|Introduction|Table Of Contents|What does an Image Captioning Problem entail?|Methodology to Solve the Task|Walkthrough of Implementation|Where to go from here?|End notes,"|Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|TensorFlow.js  Now Build Machine Learning Models in JavaScript!|Google Releases TensorFlow 1.7.0! All You Need to Know|
Faizan Shaikh
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning is a very rampant field right now  with so many applications coming out day by day. And the best way to get deeper into Deep Learning is to get hands-on with it. Take up as much projects as you can, and try to do them on your own. This would help you grasp the topics in more depth and assist you in becoming a better Deep Learning practitioner.In this article, we will take a look at an interesting multi modal topic where we will combine both image and text processing to build a useful Deep Learning application, aka Image Captioning. Image Captioning refers tothe process of generating textual description from an image  based on the objects and actions in the image. For example:This process has many potential applications in real life. A noteworthy one would be to save the captions of an image so that it can be retrieved easily at a later stage just on the basis of this description.Lets get on with it!Note: This article assumes that you know the basics of Deep Learning and have previously worked on image processing problems using CNN. If you want to brush up on the concepts, you can go through these articles first:Suppose you see this picture What is the first thing that comes to you mind? (PS: Let me know in the comments below!).Here are a few sentences that people could come up with :A man and a girl sit on the ground and eat .
A man and a little girl are sitting on a sidewalk near a blue bag eating .
A man wearing a black shirt and a little girl wearing an orange dress share a treat .A quick glance is sufficient for you to understand and describe what is happening in the picture. Automatically generating this textual description from an artificial system is the task of image captioning.The task is straightforward  the generated output is expected to describe in a single sentence what is shown in the image  the objects present, their properties, the actions being performed and the interaction between the objects, etc. But to replicate this behaviour in an artificial system is a huge task, as with any other image processing problem and hence the use of complex and advanced techniques such as Deep Learning to solve the task.The task of image captioning can be divided into two modules logically  one is an image based model  which extracts the features and nuances out of our image, and the other is a language based model  which translates the features and objects given by our image based model to a natural sentence.For our image based model (viz encoder)  we usually rely on a Convolutional Neural Network model. And for our language based model (viz decoder)  we rely on a Recurrent Neural Network. The image below summarizes the approach given above.Usually, a pretrained CNN extracts the features from our input image. The feature vector is linearly transformed to have the same dimension as the input dimension of the RNN/LSTM network. This network is trained as a language model on our feature vector.For training our LSTM model, we predefine our label and target text. For example, if the caption is A man and a girl sit on the ground and eat., our label and target would be as follows Label  [ <start>, A, man, and, a, girl, sit, on, the, ground, and, eat, . ]
Target  [ A, man, and, a, girl, sit, on, the, ground, and, eat, ., <end> ]This is done so that our model understands the start and end of our labelled sequence.Lets look at a simple implementation of image captioning in Pytorch. We will take an image as input, and predict its description using a Deep Learning model.The code for this examplecan be found on GitHub. The original author of this code is Yunjey Choi. Hats off to his excellent examples in Pytorch!In this walkthrough, a pre-trainedresnet-152 model is used as an encoder, and the decoder is an LSTM network.To run the code given in this example, you have to install the pre-requisites. Make sure you have a working python environment, preferably with anaconda installed. Then run the following commands to install the rest of the required libraries.After you have setup your system, you should download the dataset required to train the model. Here we will be using the MS-COCO dataset. To download the dataset automatically, you can run the following commands:Now you can go on and start your model building process. First  you have to process the input:Now you can start training your model by running the below command:Just to peek under the hood and check out how we defined our model, you can refer to the code written in themodel.py file.Now we can test our model using:For our example image, our model gives us this output:<start> a group of giraffes standing in a grassy area . <end>And thats how you build a Deep Learning model for image captioning!The model which we saw above was just the tip of the iceberg. There has been a lot of research done on this topic. Currently, the state-of-the-art model in image captioning is Microsofts CaptionBot. You can look at a demo of the system on their official website (link : www.captionbot.ai).I will list down a few ideas which you can use to build a better image captioning model.In this article, I have coveredImage Captioning, a multimodal task which constitutes deciphering the image and describing it in natural sentences. I have then explained the methodology to solve the task and given a walk-through of its implementation. For the curious, I have also included a list of the methods you can use to improve the model performance.I hope this article motivates you to discover more such tasks that can be solved using Deep Learning, so that more and more breakthroughs and innovations happen in the industry. If you have any suggestions/feedback, let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/
Google Releases TensorFlow 1.7.0! All You Need to Know,Learn everything about Analytics|Overview|Introduction|TensorRT integration with TensorFlow|TensorFlow Debugger Plugin|Eager Mode Moves out of Contrib|Other major features|How do I install TensorFlow?|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Automatic Image Captioning using Deep Learning (CNN and LSTM) in PyTorch|25 Open Datasets for Deep Learning Every Data Scientist Must Work With|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google has released the latest update to its ultra-popular library TensorFlow  version 1.7.0. TensorFlow is an open-source library written in python, C++ and CUDA. The last big update was TensorFlow 1.5 which we covered at the time of the releasehere.There are a few fascinating additions to the latest update, including the integration of TensorRT with TensorFlow. Lets check out a few features below.TensorRT is a library that optimizes deep learning models for inference and creates a runtime for deployment on GPUs in production environments. In tests performed by Google, they found that ResNet-50 performed 8x faster under 7 ms latency with the TensorFlow-TensorRT integration using NVIDIA Volta Tensor Cores as compared with running TensorFlow only.This is still in alpha mode. Its a graphical user interface of the TensorFlow Debugger. This feature will enable users to:One of the biggest additions in the last two version was the introduction of Eager Execution. It enables users to execute TensorFlow operations as soon as they are called from python. This is not only welcome by researchers experienced with the language, but makes things easier for newcomers as well. With this update, Eager mode has come out of contrib. Try using the below command:tf.enable_eager_execution()You can read more about the release on the TensorFlow GitHub pagehere.It just takes a line of code:I cant wait to get our hands on this! I am a huge fan of visualizations so Im especially intrigued by the TensorFlow Debugger dashboard that gives an overview of the Tensor values.We will update this article with how the latest version performs for us soon.",https://www.analyticsvidhya.com/blog/2018/03/tensorflow-1-7-released-all-you-need-to-know/
25 Open Datasets for Deep Learning Every Data Scientist Must Work With,Learn everything about Analytics|Introduction|Image Datasets|MNIST|MS-COCO|ImageNet|Open Images Dataset|VisualQA|The Street View House Numbers (SVHN)|CIFAR-10|Fashion-MNIST|Natural Language Processing|IMDB Reviews|Twenty Newsgroups|Sentiment140|WordNet|Yelp Reviews|The Wikipedia Corpus|The Blog Authorship Corpus|Machine Translation of Various Languages|Audio/Speech Datasets|Free Spoken Digit Dataset|Free Music Archive (FMA)|Ballroom|Million Song Dataset|LibriSpeech|VoxCeleb|Analytics Vidhya Practice Problems|Twitter Sentiment Analysis|Age Detection of Indian Actors|Urban Sound Classification,"How to use these datasets?|Learn,engage, competeandget hired!|Share this:|Like this:|Related Articles|Google Releases TensorFlow 1.7.0! All You Need to Know|Google AutoML  Two Real Life Examples of Googles Automated Machine Learning Tool in Action|
Pranav Dar
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The key to getting better at deep learning (or most fields in life) is practice. Practice on a variety of problems  from image processing to speech recognition. Each of these problem has its own unique nuance and approach.But where can you get this data? A lot of research papers you see these days use proprietary datasets that are usually not released to the general public. This becomes a problem, if you want to learn and apply your newly acquired skills.If you have faced this problem, we have a solution for you. We have curated a list of openly available datasets for your perusal.In this article, we have listed a collection of high quality datasets that every deep learning enthusiast should work on to apply and improve their skillset.Working on these datasets will make you a better data scientist and the amount of learning you will have will be invaluable in your career. We have also included papers with state-of-the-art (SOTA) results for you to go through and improve your models.First things first  these datasets are huge in size! So make sure you have a fast internet connection with no / very high limit on the amount of data you can download.There are numerous ways how you can use these datasets. You can use them to apply various Deep Learning techniques. You can use them to hone your skills, understand how to identify and structure each problem, think of unique use cases and publish your findings for everyone to see!The datasets are divided into three categories  Image Processing, Natural Language Processing, and Audio/Speech Processing.Lets dive into it!MNIST is one of the most popular deep learning datasets out there. Its a dataset of handwritten digits and contains a training set of 60,000 examples and a test set of 10,000 examples. Its a good database for trying learning techniques and deep recognition patterns on real-world data while spending minimum time and effort in data preprocessing.Size:~50 MBNumber of Records: 70,000 images in 10 classesSOTA:Dynamic Routing Between Capsules COCO is a large-scale and rich for object detection, segmentation and captioning dataset. It has several features:Size: ~25 GB (Compressed)Number of Records:330K images, 80 object categories, 5 captions per image, 250,000 people with key pointsSOTA :Mask R-CNNBored with Datasets? Solve real life project on Deep LearningImageNet is a dataset of images that are organized according to the WordNet hierarchy. WordNet contains approximately 100,000 phrases and ImageNet has provided around 1000 images on average to illustrate each phrase.Size: ~150GBNumber of Records:Total number of images: ~1,500,000; each with multiple bounding boxes and respective class labelsSOTA :Aggregated Residual Transformations for Deep Neural Networks Open Images is a dataset of almost 9 million URLs for images. These images have been annotated with image-level labels bounding boxes spanning thousands of classes. The dataset contains a training set of 9,011,219 images, a validation set of 41,260 images and a test set of 125,436 images.Size: 500 GB (Compressed)Number of Records: 9,011,219 images with more than 5k labelsSOTA :Resnet 101 image classification model (trained on V2 data): Model checkpoint, Checkpoint readme, Inference code.VQA is a dataset containing open-ended questions about images. These questions require an understanding of vision and language. Some of the interesting features of this dataset are:Size: 25 GB (Compressed)Number of Records:265,016 images, at least 3 questions per image, 10 ground truth answers per questionSOTA :Tips and Tricks for Visual Question Answering: Learnings from the 2017 ChallengeThis is a real-world image dataset for developing object detection algorithms. This requires minimum data preprocessing. It is similar to the MNIST dataset mentioned in this list, but has more labelled data (over 600,000 images). The data has been collected from house numbers viewed in Google Street View.Size: 2.5 GBNumber of Records: 6,30,420 images in 10 classesSOTA :Distributional Smoothing With Virtual Adversarial TrainingThis dataset is another one for image classification. It consists of 60,000 images of 10 classes (each class is represented as a row in the above image). In total, there are 50,000 training images and 10,000 test images. The dataset is divided into 6 parts  5 training batches and 1 test batch. Each batch has 10,000 images.Size: 170 MBNumber of Records: 60,000 images in 10 classesSOTA :ShakeDrop regularization Fashion-MNIST consists of 60,000 training images and 10,000 test images. It is a MNIST-like fashion product database. The developers believe MNIST has been overused so they created this as a direct replacement for that dataset. Each image is in greyscale and associated with a label from 10 classes.Size: 30 MBNumber of Records: 70,000 images in 10 classesSOTA :Random Erasing Data Augmentation This is a dream dataset for movie lovers. It is meant for binary sentiment classification and has far more data than any previous datasets in this field. Apart from the training and test review examples, there is further unlabeled data for use as well. Raw text and preprocessed bag of words formats have also been included.Size: 80 MBNumber of Records: 25,000 highly polar movie reviews for training, and 25,000 for testingSOTA :Learning Structured Text Representations This dataset, as the name suggests, contains information about newsgroups. To curate this dataset, 1000 Usenet articles were taken from 20 different newsgroups. The articles have typical features like subject lines, signatures, and quotes.Size: 20 MBNumber of Records: 20,000 messages taken from 20 newsgroupsSOTA :Very Deep Convolutional Networks for Text Classification,Sentiment140 is a dataset that can be used for sentiment analysis. A popular dataset, it is perfect to start off your NLP journey. Emotions have been pre-removed from the data. The final dataset has the below 6 features:Size: 80 MB (Compressed)Number of Records: 1,60,000 tweetsSOTA :Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment DatasetsMentioned in the ImageNet dataset above, WordNet is a large database of English synsets. Synsets are groups of synonyms that each describe a different concept. WordNets structure makes it a very useful tool for NLP.Size: 10 MBNumber of Records:117,000 synsets is linked to other synsets by means of a small number of conceptual relations.SOTA : Wordnets: State of the Art and PerspectivesThis is an open dataset released by Yelp for learning purposes. It consists of millions of user reviews, businesses attributes and over 200,000 pictures from multiple metropolitan areas. This is a very commonly used dataset for NLP challenges globally.Size: 2.66 GB JSON, 2.9 GB SQL and 7.5 GB Photos (all compressed)Number of Records: 5,200,000 reviews, 174,000 business attributes, 200,000 pictures and 11 metropolitan areasSOTA :Attentive ConvolutionThis dataset is a collection of a the full text on Wikipedia. It contains almost 1.9 billion words from more than 4 million articles. What makes this a powerful NLP dataset is that you search by word, phrase or part of a paragraph itself.Size: 20 MBNumber of Records: 4,400,000 articles containing 1.9 billion wordsSOTA :Breaking The Softmax Bottelneck: A High-Rank RNN language ModelThis dataset consists of blog posts collected from thousands of bloggers and has been gathered from blogger.com. Each blog is provided as a separate file. Each blog contains a minimum of 200 occurrences of commonly used English words.Size: 300 MBNumber of Records:681,288 posts with over 140 million wordsSOTA :Character-level and Multi-channel Convolutional Neural Networks for Large-scale Authorship AttributionThis dataset consists of training data for four European languages. The task here is to improve the current translation methods. You can participate in any of the following language pairs:Size: ~15 GBNumber of Records: ~30,000,000 sentences and their translationsSOTA :Attention Is All You NeedEngage with real life projects on Natural Language Processing hereAnother entry in this list for inspired by the MNIST dataset! This one was created to solve the task of identifying spoken digits in audio samples. Its an open dataset so the hope is that it will keep growing as people keep contributing more samples. Currently, it contains the below characteristics:Size: 10 MBNumber of Records: 1,500 audio samplesSOTA :Raw Waveform-based Audio Classification Using Sample-level CNN ArchitecturesFMA is a dataset for music analysis. The dataset consists of full-length and HQ audio, pre-computed features, and track and user-level metadata. It an an open dataset created for evaluating several tasks in MIR. Below is the list of csv files the dataset has along with what they include:Size: ~1000 GBNumber of Records: ~100,000 tracksSOTA :Learning to Recognize Musical Genre from AudioThis dataset contains ballroom dancing audio files. A few characteristic excerpts of many dance styles are provided in real audio format.Below are a few characteristics of the dataset:Size: 14GB (Compressed)Number of Records: ~700 audio samplesSOTA :A Multi-Model Approach To Beat Tracking Considering Heterogeneous Music StylesThe Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.Its purposes are:The core of the dataset is the feature analysis and metadata for one million songs. The dataset does not include any audio, only the derived features. The sample audio can be fetched from services like 7digital, usingcode provided by Columbia University.Size:280 GBNumber of Records: PS  its a million songs!SOTA : Preliminary Study on a Recommender System for the Million Songs Dataset ChallengeThis dataset is a large-scale corpus of around 1000 hours of English speech. The data has been sourced from audiobooks from the LibriVox project. It has been segmented and aligned properly. If youre looking for a starting point, check out already preparedAcoustic models that are trained on this data set at kaldi-asr.org and language models, suitable for evaluation, at http://www.openslr.org/11/.Size: ~60 GBNumber of Records: 1000 hours of speechSOTA :Letter-Based Speech Recognition with Gated ConvNetsVoxCeleb is a large-scale speaker identification dataset. It contains around 100,000 utterances by 1,251 celebrities, extracted from YouTube videos. The data is mostly gender balanced (males comprise of 55%). The celebrities span a diverse range of accents, professions and age. There is no overlap between the development and test sets. Its an intriguing use case for isolating and identifying which superstar the voice belongs to.Size: 150 MBNumber of Records: 100,000 utterances by 1,251 celebritiesSOTA : VoxCeleb: a large-scale speaker identification datasetFor your practice, we also provide real life problems and datasets to get your hands dirty. In this section, weve listed down the deep learning practice problems on our DataHack platform.Hate Speech in the form of racism and sexism has become a nuisance on twitter and it is important to segregate these sort of tweets from the rest. In this Practice problem, we provide Twitter data that has both normal and hate tweets. Your task as a Data Scientist is to identify the tweets which are hate tweets and which are not.Size: 3 MBNumber of Records: 31,962 tweetsThis is a fascinating challenge for any deep learning enthusiast. The dataset contains thousands of images of Indian actors and your task is to identify their age.All the images aremanuallyselected and cropped from the video frames resulting in a high degree of variability interms of scale, pose, expression, illumination, age, resolution, occlusion, and makeup.Size: 48 MB (Compressed)Number of Records: 19,906 images in the training set and 6636 in the test setSOTA: Hands on with Deep Learning  Solution for Age Detection Practice ProblemThis dataset consists of more than 8000 sound excerpts of urban sounds from 10 classes.This practice problem is meant to introduce you to audio processing in the usual classification scenario.Size: Training set  3 GB (Compressed), Test set  2 GB (Compressed)Number of Records:8732 labeled sound excerpts (<=4s) of urban sounds from 10 classesIf you are aware of other open datasets, which you recommend to people starting their journey on deep learning/ unstructured datasets, please feel free to suggest them along with the reasons, why they should be included.If the reason is good, Ill include them in the list. Let us know your experience with using any of these datasets in the comments section. And happy deep learning!Not sure, how to start your Deep Learning Journey? Use ourLearning Path for Deep Learning",https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/
Google AutoML  Two Real Life Examples of Googles Automated Machine Learning Tool in Action,Learn everything about Analytics|Overview|Introduction|Our take on this,"The Restaurant Location Recognizing Model|Classifying Branded Goods|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|25 Open Datasets for Deep Learning Every Data Scientist Must Work With|Thanks to Deep Learning, this Robot can Perform Tasks with Almost Human Level Skills|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Googles AutoML has been improving steadily since its release as more and more data scientists are incorporating it to solve real life problems. This week, Google published a couple of examples on their blog detailing the use cases. Lets look at them below.If someone asked you to eat a bowl of noodles from a restaurant you havent visited before, how likely are you to identify which restaurant it comes from? Sounds like an impossible question, right?Not for Googles AutoML tool. A machine learning model, trained using AutoML, was able to identify the restaurant with 95% accuracy by looking at the image of the noodle bowl. The model can take apart and analyse minute details of the image and is able to predict which restaurant it was made in. Incredible!The developer of the model, Kenji Doi, collected 48,244 photos of noodle bowls from different Ramen Jiro locations (Ramen Jiro is a popular restaurant franchise in Japan). Then, Kenji removed photos that was unsuitable for training, such as duplicates. He then labelled 48,000 photos (1170 photos from 41 Ramen Jiro locations).Kenji then uploaded his dataset into AutoML and found that the trained model gave him a 94.5% accuracy (94.8% precision and 94.5% recall). In simple words, the model was able to recognise the location where the bowl of noodles was prepared an incredible 94.5 times out of 100!You can check out the confusion matrix below for the model. Note that the rows represent the actual shop and the columns are the predicted shop.In another instance, Mercari leveraged AutoML to classify images with its brand name.The company has recently launched a new application to sell branded goods in Japan. According to Mercari, they have been developing their own ML model that suggests a brand name from 12 major brands in the photo uploading user interface. Their current model uses transfer learning on TensorFlow and gives an accuracy of around 75%.But then data scientists at Mercari decided to try their hand at AutoML. The results blew their old model out of the water. They trained the model on 50,000 images and achieved an accuracy of 91.3%. An incredible 16% increase on their already existing model!Check out the confusion matrix for Mercaris AutoML model below:So far, AutoML had been labelled as one of the best tools out there for people who did not have the technical data science skills. But as you can see in these examples, data scientists are also leveraging it in real-life cases to build and improve their models.AutoML takes out the need for hyperparameter tuning and data augmentation. All you need to do is label images, upload them and AutoML takes care of the rest. You can read more about the tool in our post here.Have you used AutoML so far? Tell us about your experience in the comments below.",https://www.analyticsvidhya.com/blog/2018/03/two-examples-google-automl/
"Thanks to Deep Learning, this Robot can Perform Tasks with Almost Human Level Skills",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Google AutoML  Two Real Life Examples of Googles Automated Machine Learning Tool in Action|AVBytes: AI & ML Developments this week  IBMs Library 46 Times Faster than TensorFlow, Baidus Massive Self-Driving Dataset, the Technology behind AWS SageMaker, etc.|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"AI powered robots have been around for a while. Weve covered robots that can cook the perfect burger, read the news, and even deliver speeches. But this latest creation, by a professor and his student from UC Berkeley, is the smartest one developed so far.It has a range of functions, but the most useful one is that it can sort through your junk drawer and pick out stuff with the speed and accuracy that rivals humans. The key to its smartness lies in how the software behind it has been programmed.                                 Source: MIT Technology ReviewThe software is called Dex-Net and it identifies how to differentiate and pick-up objects, especially ones that might not belong in the pile, with unerring accuracy. You might be wondering at this point how does the robot actually work?Well, this is where deep learning comes into play. The software powering the robot has been developed with a deep neural network  it designed to pick up objects in a virtual environment, sort through them, and continuously learn with each picking. But here is what separates it from anything done previously in this field  Dex-Net generalises from an object it has seen before to a new one. What makes it even more human is that if its not able to understand what the object it, the robot will look at it from another angle to get a better view of it.As you can see in the image above, the robot has two arms that are powered by a different neural network. One arm is supposed to grasp objects while the other arm performs the function of suction.The two researchers also came up with a way to calculate how to measure the performance of Dex-Net  using a measure called mean picks per hour. Basically, this is calculated by multiplying the mean time per pick and the mean probability of success. Humans are capable of 400-600 mean picks per hour while this robot has reached 200-300 so far.A demonstration of how the robot works was given at MIT Technology Reviews EmTech Digital event. You can view a video of the robot strutting its stuff below:Information from MIT Technology Reviews blog post was taken in this article.This robot is as close to matching human skills and understanding as anything created before. Once it has been programmed with even better commands, it can potentially be used for diverse functions, both commercially and in the industry.And heres the advantage it has over humans  Dex-Net does not get tired and can relentlessly plough through objects without experiencing any lag time. With the mean picks per hour metric, it has given researchers something to measure and improve with each iteration of the software.In addition to Berkeleys efforts, researchers at DeepMind and OpenAI are also experimetning with robotic technology and new and even mroe intelligent AI is expected to be revealed soon.",https://www.analyticsvidhya.com/blog/2018/03/thanks-deep-learning-robot-can-perform-tasks-almost-human-level-skills/
"AVBytes: AI & ML Developments this week  IBMs Library 46 Times Faster than TensorFlow, Baidus Massive Self-Driving Dataset, the Technology behind AWS SageMaker, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|Thanks to Deep Learning, this Robot can Perform Tasks with Almost Human Level Skills|Introduction to k-Nearest Neighbors: A powerful Machine Learning Algorithm (with implementation in Python & R)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In recent times, one of the more popular themes in the machine learning world has been regarding computational power. As the amount of data collected continues to rise unabated, organizations are lagging behind as their hardware is not up to scratch. But the big tech giants like IBM, Google and Amazon are working on products that can handle gigantic data using smaller computation power.We, at Analytics Vidhya, are covering these developments, along with all other major stories in the ML world onAVBytes! We provide links to official research papers so you can deep dive into the theory behind the technology. We also provide links to the source code on GitHub so you can replicate it (and even improve it!) on your own machine.In the past week, we saw the big names grabbing the headlines  Amazon unveiled the technology behind its AWS SageMaker, IBM developed a library that ran the same model on the same data 46 times faster than TensorFlow, Baidu open sourced its massive self-driving dataset, SAS developed a ML model to rank the best places to live, etc.Scroll down to view all the articles from last week. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.                           Source: inasightThe above AVBytes were published from 19th to 25th March, 2018.This week was full of exciting developments. We cant wait to use Baidus massive dataset and are eagerly anticipating more details from IBM about their Snap ML library. What excited you the most in the ML world in the past week? Let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/03/avbytes-ai-ml-developments-this-week-260318/
Introduction to k-Nearest Neighbors: A powerful Machine Learning Algorithm (with implementation in Python & R),Learn everything about Analytics|Overview|Introduction|Table of Contents|When do we use KNN algorithm?|How does the KNN algorithm work?|How do we choose the factor K?|Breaking it Down  Pseudo Code of KNN||Implementation in Python from scratch|Comparing our model with scikit-learn|Implementation of kNN in R|||Comparing our kNN predictor function with Class library|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|AVBytes: AI & ML Developments this week  IBMs Library 46 Times Faster than TensorFlow, Baidus Massive Self-Driving Dataset, the Technology behind AWS SageMaker, etc.|DeepMind is Using Neuron Deletion to Understand Deep Neural Networks|
Tavish Srivastava
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Note: This article was originally published on Oct 10, 2014 and updated on Mar 27th, 2018In the four years of my data science career, I have built more than 80% classification models and just 15-20% regression models. These ratios can be more or less generalized throughout the industry. The reason behind this bias towards classification models is that most analytical problems involve making a decision.For instance, will a customer attrite or not, should we target customer X for digital campaigns, whether customer has a high potential or not etc. These analysis are more insightful and directly linked to an implementation roadmap.In this article, we will talk about another widely used machine learning classification technique called K-nearest neighbors (KNN) . Our focus will be primarily on how does the algorithm work and how does the input parameter affect the output/prediction.KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. To evaluate any technique we generally look at 3 important aspects:1. Ease to interpret output2. Calculation time3. Predictive PowerLet us take a few examples to place KNN in the scale :KNN algorithm fairs across all parameters of considerations. It is commonly used for its easy of interpretation and low calculation time.Lets take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) :You intend to find out the class of the blue star (BS) . BS can either be RC or GS and nothing else. The K is KNN algorithm is the nearest neighbors we wish to take vote from. Lets say K = 3. Hence, we will now make a circle with BS as center just as big as to enclose only three datapoints on the plane. Refer to following diagram for more details: The three closest points to BS is all RC. Hence, with good confidence level we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next we will understand what are the factors to be considered to conclude the best K.First let us try to understand what exactly does K influence in the algorithm. If we see the last example, given that all the 6 training observation remain constant, with a given K value we can make boundaries of each class. These boundaries will segregate RC from GS. The same way, lets try to see the effect of value K on the class boundaries. Following are the different boundaries separating the two classes with different values of K.If you watch carefully, you can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority. The training error rate and the validation error rate are two parameters we need to access on different K-value. Following is the curve for the training error rate with varying value of K :As you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:This makes the story more clear. At K=1, we were overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions.We can implement a KNN model by following the below steps:We will be using the popular Iris dataset for building our KNN model. You can download it from here.We can see that both the models predicted the same class (Iris-virginica) and the same nearest neighbors ( [141 139 120] ). Hence we can conclude that our model runs as expected.Step 1: Importing the dataStep 2: Checking the data and calculating the data summaryOutputStep 3: Splitting the DataStep 4: Calculating the Euclidean DistanceStep 5: Writing the function to predict kNNStep 6: Calculating the label(Name) for K=1OutputIn the same way, you can compute for other values of K.OutputWe can see that both models predicted the same class (Iris-virginica).KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems. The only difference from the discussed methodology will be using averages of nearest neighbors rather than voting from nearest neighbors. KNN can be coded in a single line on R. I am yet to explore how can we use KNN algorithm on SAS.Did you find the article useful? Have you used any other machine learning tool recently? Do you plan to use KNN in any of your business problems? If yes, share with us how you plan to go about it.",https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/
DeepMind is Using Neuron Deletion to Understand Deep Neural Networks,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Introduction to k-Nearest Neighbors: A powerful Machine Learning Algorithm (with implementation in Python & R)|IBMs Machine Learning Library is 46 Times Faster than TensorFlow!|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural networks have always been a tricky subject to understand. Deep neural networks are beyond the scope of most people. They consist of multiple neurons and which are used for various and diverse applications in the industry.But these multiple hidden neurons is what has given them the black box stigma.In a blog post, researchers at DeepMind have explained how they went about understanding and judging the performance of a neural network by deleting individual neurons one by one, as well as in groups. The researchers developed image classification models and then removed several neurons. Then they measured how each deletion affected the outcome of the model.According to DeepMind, their findings yielded two outcomes:In the above image, the top most neuron (which is greyed out) has been deleted. On DeepMinds blog post, you can delete each neuron and see how it affects the output results.You can read DeepMinds official research paper on the topichere. They are scheduled to present this next month at the International Conference on Learning Representations (ICLR).Deep neural networks have been hard to interpret so this is a nice start towards demystifying them by one of the leading research companies.Their results imply that individual neurons are much less important than we would have initially thought.I highly encourage you to go through their blog post and the research paper to understand each step that was taken in performing this.",https://www.analyticsvidhya.com/blog/2018/03/deepmind-using-neuron-deleting-understand-deep-neural-networks/
IBMs Machine Learning Library is 46 Times Faster than TensorFlow!,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|DeepMind is Using Neuron Deletion to Understand Deep Neural Networks|SASs Machine Learning Model Ranks the Best Places to Live in the World|
Pranav Dar
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The race to become the quickest and most efficient library is now in full flight. IBM claims that performing machine learning tasks on its POWER servers is an incredible 46 times quicker than on TensorFlow used in Google Cloud.Earlier this year, a Google software engineer wrote a blog post on how they usedGoogle Cloud Machine Learning and TensorFlow for solving click prediction problems. They trained their deep neural network model to predict display ad clicks on Criteo Labs clicks logs. These logs are over 1TB in size and includefeature values and click feedback from millions of display ads.For them, data preprocessing took about an hour, followed by training the model for 70 minutes. The evaluation loss was reported to be 0.13. They did manage to reduce this evaluation loss and get more accurate results but that was achieved at the cost of increasing training time.But IBM blew those results out of the water. Their training algorithm, running on POWER9 servers and GPUs, outperformed Google Cloud Platforms in the initial training phase.The IBM researchers trained their model on the Criteo Labs click logs, the same data source used by Google earlier. It contains 4.2 billion training examples and 1 million variables. They trained it using logistic regression (again, the same technique used by Google). However, IBM used a different ML library  Snap Machine Learning.IBMs model completed the same logistic regression in 91.5 seconds! Thats a remarkable 46 times faster than Googles previous attempt.IBM posted the below comparisons between their Snap ML library and the other competitors:You can read more about the Snap Machine Learning library in IBMs research paper here.A 46 times improvement over TensorFlow is truly impressive. Of course a point to be noted here is that these 2 models were not run on similar hardware configurations so we cant validate IBMs results until they publicly release more information.Having said that, IBM has definitely caught the machine learning worlds attention and has given them an opportunity to introduce their POWER9 servers and the Snap ML library to the general public.",https://www.analyticsvidhya.com/blog/2018/03/ibm-machine-library-46-times-faster-tensorflow/
SASs Machine Learning Model Ranks the Best Places to Live in the World,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|IBMs Machine Learning Library is 46 Times Faster than TensorFlow!|Essentials of Deep Learning: Visualizing Convolutional Neural Networks in Python|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The debate on which place(s) is the best in the world has been raging on since times immemorial. But thanks to SAS and machine learning, we might finally have the answer.Researchers at SAS claim to have created an artificial intelligence program that ranks the best places to live in the world using publicly available data.The model was developed using more than 5 million data points sourced from various social media platforms, travel review websites (like TripAdvisor) websites, public statistics from UNESCO and WTO and also used geodata. The final data that was analysed and trained on included the weather, job prospects, pollution, public transport, economy, healthcare and green locations.Using a variable reduction technique, the key criteria for the final algorithm were cut down to eight, namely:The results of the above criteria were then compared to quality of life indicators, like theprice of local fruits and groceries, the number of walkable pavements, the number of trees in the area, the width of footpaths and even the likelihood of traffic jams.The top 5 places to live, according to the algorithm, are:While debating and analysing the best places to live or travel in the world, our unconscious bias inevitably creeps into the equation. What this research from SAS does is it takes out that human element and by analysing more data than ever before, it has truly trumped every argument.Once more details are revealed about the research and depending on its commercial release, it could also help house buyers in comparing and buying real estate. Lets hope we see the source code behind this algorithm soon so we can try it out as well!",https://www.analyticsvidhya.com/blog/2018/03/sas-built-machine-learning-model-rank-the-best-places-world/
Essentials of Deep Learning: Visualizing Convolutional Neural Networks in Python,Learn everything about Analytics|Introduction|Table of Contents|Importance of Visualizing a CNN model|Methods of Visualizing a CNN model|End Notes,"1. Preliminary Methods|2. Activation Maps|3. Gradient Based Methods|Participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|SASs Machine Learning Model Ranks the Best Places to Live in the World|IBM Launches Deep Learning as a Service for ML and AI Developers|
Faizan Shaikh
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",1.1 Plotting model architecture|1.2 Visualize filters|2.1 Maximal Activations|2.2 Image Occlusion|3.1 Saliency Maps|3.2 Gradient based Class Activations Maps,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most debated topics in deep learning is how to interpret and understand a trained model  particularly in the context of high risk industries like healthcare. The term black box has often been associated with deep learning algorithms. How can we trust the results of a model if we cant explain how it works? Its a legitimate question.Take the example of a deep learning model trained for detecting cancerous tumours. The model tells you that it is 99% sure that it has detected cancer  but it does not tell you why or how it made that decision.Did it find an important clue in the MRI scan? Or was it just a smudge on the scan that was incorrectly detected as a tumour? This is a matter of life and death for the patient and doctors cannot afford to be wrong.In this article, we will explore how to visualize a convolutional neural network (CNN), a deep learning architecture particularly used in most state-of-the-art image based applications. We will get to know the importance of visualizing a CNN model, and the methods to visualize them. We will also take a look at a use case that will help you understand the concept better.Note: This article assumes that you know the basics of Deep Learning and have previously worked on image processing problems using CNN. Also, we will be using Keras as our deep learning library. If you want to brush up on the concepts, you can go through these articles first:Lets get on with it!As we have seen in the cancerous tumour example above, it is absolutely crucial that we know what our model is doing  and how its making decisions on its predictions. Typically, the reasons listed below are the most important points for a deep learning practitioner to remember:Let us look at an example where visualizing a neural network model helped in understanding the follies and improving the performance (the below example has been sourced from: http://intelligence.org/files/AIPosNegFactor.pdf).Once upon a time, the US Army wanted to use neural networks to automatically detect camouflaged enemy tanks. The researchers trained a neural net on 50 photos of camouflaged tanks in trees, and 50 photos of trees without tanks. Using standard techniques for supervised learning, the researchers trained the neural network to a weighting that correctly loaded the training setoutput yes for the 50 photos of camouflaged tanks, and output no for the 50 photos of forest.This did not ensure, or even imply, thatnewexamples would be classified correctly. The neural network might have learned 100 special cases that would not generalize to any new problem. Wisely, the researchers had originally taken 200 photos, 100 photos of tanks and 100 photos of trees. They had used only 50 of each for the training set. The researchers ran the neural network on the remaining 100 photos, and without further training the neural network classified all remaining photos correctly. Success confirmed! The researchers handed the finished work to the Pentagon, which soon handed it back, complaining that in their own tests the neural network did no better than chance at discriminating photos.It turned out that in the researchers dataset, photos of camouflaged tanks had been taken on cloudy days, while photos of plain forest had been taken on sunny days. The neural network had learned to distinguish cloudy days from sunny days, instead of distinguishing camouflaged tanks from an empty forest.Broadly the methods of Visualizing a CNN model can be categorized into three parts based on their internal workingsWe will look at each of them in detail in the sections below. Here we will be using keras as our library for building deep learning models and keras-vis for visualizing them. Make sure you have installed these in your system before going ahead.NOTE: This article uses the dataset given in Identify the Digits competition. To run the code mentioned below, you would have to download it in your system. Also, please performthe steps provided in this page before starting with the implementation below.The simplest thing you can do is to print/plot the model. Here, you can also print the shapes of individual layers of neural network and the parameters in each layer.In keras, you can implement it as below:For a more creative and expressive way  you can draw a diagram of the architecture (hint  take a look at the keras.utils.vis_utils function).Another way is to plot the filters of a trained model, so that we can understand the behaviour of those filters. For example, the first filter of the first layer of the above model looks like:Generally, we see that the low level filters work as edge detectors, and as we go higher, they tend to capture high level concepts like objects and faces.Source :http://web.eecs.umich.edu/~honglak/cacm2011-researchHighlights-convDBN.pdfTo see what our neural network is doing, we can apply the filters over an input image and then plot the output.This allows us to understand what sort of input patterns activate a particular filter. For example, there could be a face filter that activates when it gets the presence of a face in the image.We can transfer this idea to all the classes and check how each of them would look like.PS: Run the script below to check it out.In an image classification problem, a natural question is if the model is truly identifying the location of the object in the image, or just using the surrounding context. We took a brief look at this in gradient based methods above. Occlusion based methods attempt to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. The examples clearly show the model is localizing the objects within the scene, as the probability of the correct class drops significantly when the object is occluded.To understand this concept, let us take a random image from our dataset and try to plot a heatmap of the image. This will give us an intuition of which parts of the image are important for that model in order to make a clear distinction of the actual class.As we saw in the example of tanks, how can we get to know which part does our model focuses on to get prediction? For this, we can use saliency maps. Saliency maps was first introduced in the paper:Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.The concept of using saliency maps is pretty straight-forward  we compute the gradient of the output category with respect to the input image. This should tell us how the output category value changes with respect to a small change in the input image pixels. All the positive values in the gradients tell us that a small change to that pixel will increase the output value. Hence, visualizing these gradients, which are the same shape as the image, should provide some intuition of attention.Intuitively this method highlights the salient image regions that contribute the most towards the output.Class activation maps, or grad-CAM, is another way of visualizing what our model looks at while making predictions. Instead of using gradients with respect to the output, grad-CAM uses penultimate Convolutionallayer output. This is done to utilize the spacial information that is being stored in the penultimate layer.In this article, we have covered how to visualize a CNN model, and why should you do it along with an example. It has wide ranging applications from helping in medical cases to solving logistical issues for the army.I hope this will give you an intuition of how to build better models in your own deep learning applications.If you have any ideas / suggestions regarding the topic, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-visualizing-convolutional-neural-networks/
IBM Launches Deep Learning as a Service for ML and AI Developers,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Essentials of Deep Learning: Visualizing Convolutional Neural Networks in Python|Amazon Unveils the Technology Behind the AWS SageMaker|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"IBM has announced the launch of its Deep Learning as a Service (DLaaS) platform for AI developers.It aims to help developers run hundreds of deep learning models at the same time while building their neural networks. With DLaaS, developers can now train their deep neural networks using the usual popular frameworks like TensorFlow, Caffe2, and PyTorch without having to splurge money on expensive hardware.                                       Source: IBMWhat makes this an attractive choice is that users only have to pay for the GPU time while using only those resources which they need to train their models.Developers can choose from a set of deep learning frameworks, a neural network model, training data and cost constraints. DLaaS takes care of the rest, and provides them an interactive experience.The service saves a ton of time as well for the developers. They just have to clean their data, upload it, begin training and then download and view the training results. Its a fairly straightforward process and does not require extremely advanced machine learning techniques.It takes potentially days and weeks to run iterative training models. According to IBM:This training process has been a challenge for data scientists and developers. To simplify this neural network building process and making it possible even for professionals without deep coding experience to do it, Deep Learning as a Service now includes a unique Neural Network Modeler. Neural Network Modeler is an intuitive drag-and-drop interface that enables a non-programmer to speed up the model-building process by visually selecting, configuring, designing and auto-coding their neural network using the most popular deep learning frameworks.You can read more about DLaaS in IBMs white paper hereand their blog post here.IBM has joined the likes of Google, Facebook and Amazon in trying to take the difficulty out of training deep neural network models. Finding experienced data scientists and machine learning practitioners is a challenging and expensive task for organizations. So models like this DLaaS, Googles AutoML and H2Os Driverless AI are making machine learning as autonomous as possible.",https://www.analyticsvidhya.com/blog/2018/03/ibm-launches-deep-learning-service-ml-ai-developers/
Amazon Unveils the Technology Behind the AWS SageMaker,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|IBM Launches Deep Learning as a Service for ML and AI Developers|Introduction to Regression Splines (with Python codes)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computational power is a major challenge for data scientists. You have all the data you need but the lack of computing power is quite often a major hurdle. Also, data is usually not a static element. Data always keeps being accrued and is dynamic in nature.This is why Amazon developed SageMaker, a fully-managed service for its Amazon Web Services (AWS) customers that enables developers and data scientists to quickly build, train and deploy machine learning models at any scale. It was launched last year and Amazon unveiled the technology behind this tool yesterday.When you input data into SageMaker, it uses a streaming algorithm that only makes a single pass over the data. Streaming algorithms are infinitely scalable, that is, they can consume unlimited amounts of data. For instance, processing the 20th gigabyte and 2000th gigabyte is pretty much the same thing. As Amazons CTO Werner Vogels says in his blog post, the memory footprint of the algorithms is fixed and it is therefore guaranteed not to run out of memory (and crash) as the data grows.SageMaker also uses containers to spread the workload of the machine learning tasks across its network. This in turn, significantly increases the speed at which models are trained and deployed. This also allows the models to move between the GPUs and the CPUs, depending on what works best with that particular model.Currently, SageMaker has the capability of offering production-ready and infinitely scalable algorithms such as:You can use SageMaker to train with any deep learning frameworks, including:Official benchmarking figures are not out there yet, but there can be comparisons between this and Googles CloudML. They seem to be structured in quite similar ways.Amazon is yet to release any official research papers regarding this technology so we will have to wait to understand how it compares to other services. Have you used this technology yet? Let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/03/amazon-unveils-technology-behind-aws-services/
Introduction to Regression Splines (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Understanding the data|Introduction to Linear Regression|Improvement over Linear Regression: Polynomial Regression|Walk-through of Regression Splines along with its Implementations|End Notes,"Piecewise Step Functions|Basis Functions|Piecewise Polynomials||Constraints and Splines|Cubic and Natural Cubic Splines||Choosing the Number and Locations of the Knots|Comparison of Regression Splines with Polynomial Regression|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Amazon Unveils the Technology Behind the AWS SageMaker|Baidu has Released a Gigantic Self-Driving Dataset named ApolloScape|
Gurchetan Singh
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As a beginner in the world of data science, the first algorithm I was introduced to was Linear Regression. I applied it to different datasets and noticed both its advantages and limitations.It assumed a linear relationship between the dependent and independent variables, which was rarely the case in reality. As an improvement over this model, I tried Polynomial Regression which generated better results (most of the time). But using Polynomial Regression on datasets with high variability chances to result in over-fitting. Source: PingaxMy model always became too flexible, which does not work well with unseen data. I then came across another non-linear approach known as Regression Splines. It uses a combination of linear/polynomial functions to fit the data.In this article, we will go through some basics of linear and polynomial regression and study in detail the meaning of splines and their implementation in Python.Note: To fully understand the concepts covered in this article, knowledge of linear and polynomial regression is required. You can learn more about them here.Lets get started!To understand the concepts, we will work on the wage prediction dataset which you can download here(this has been taken from the popular book:Introduction to Statistical learning).Our dataset contains information like the ID, year, age, sex, marital status, race, education, region, job class, health, health insurance, log of wage and wage of various employees. In order to focus on spline regression in detail, I will use only age as the independent variable to predict the wage (dependent variable).Lets start working on the data.What are your thoughts on the above scatter plot? Is it positively, negatively or not correlated at all? Please share your thoughts in the comments section below.Linear regression is the simplest and most widely used statistical technique for predictive modelling. It is a supervised learning algorithm for solving regression based tasks.It is called a linear model as it establishes a linear relationship between the dependent and independent variables. It basically gives us a linear equation like the one below where we have our features as independent variables with coefficients:Here, we have Y as our dependent variable, the Xs are the independent variables and all betas are the coefficients. Coefficients are the weights assigned to the features. They signify the importance of each of the features. For example, if the outcome of an equation is highly dependent upon one feature (X1) as compared to any other feature, it means the coefficient/weight of the feature (X1) would have a higher magnitude as compared to any other feature.So, lets try to understand linear regression with only one feature, i.e., only one independent variable. It is called Simple Linear Regression. Therefore, our equation becomes,As we are using only age to predict the wages of the employees, we will implement simple linear regression on the training dataset and calculate the error (RMSE) on the validation dataset.We can now calculate the RMSE on the predictions.We can infer from the above graph that linear regression is not capturing all the signals available and is not the best method for solving this wage prediction.Although linear models are relatively simple to describe and implement and have advantages over other approaches in terms of interpretation and inference, theyhave significant limitations in terms of predictive power. This is because they assume the linear combination between the dependent and independent variables which is almost always an approximation, and sometimes a poor one.In the other methods we will see below, we will set aside the linearity assumption while still attempting to maintain as much interpretability as possible. We will do this by examining very simple extensions of linear models like polynomial regression and step functions, as well as more sophisticated approaches such as splines.Consider these visualisations The plots above seem to be using a lot more signals between wage and age as compared to the linear plot. These plots are not linear in shape, hence they use a non-linear equation instead of a linear equation for establishing the relationship between age and wage. This type of regression technique, which uses a non linear function, is called Polynomial regression.Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables , as predictors. This approach provides a simple way to provide a non-linear fit to data.The standard method to extend linear regression to a non-linear relationship between the dependent and independent variables, has been to replace the linear model with a polynomial function.As we increase the power value, the curve obtained contains high oscillations which will lead to shapes that are over-flexible. Such curves lead to over-fitting.Similarly, we can plot polynomial curves for different degree values.Unfortunately, polynomial regression has a fair number of issues as well. As we increase the complexity of the formula, the number of features also increases which is sometimes difficult to handle. Also, polynomial regression has a tendency to drastically over-fit, even on this simple one dimensional data set.There are other issues with polynomial regression. For example, it is inherently non-local, i.e., changing the value of Yat one point in the training set can affect the fit of the polynomial for data points that are very far away. Hence, to avoid the use of high degree polynomial on the whole dataset, we can substitute it with many different small degree polynomial functions.In order to overcome the disadvantages of polynomial regression, we can use an improved regression technique which, instead of building one model for the entire dataset, divides the dataset into multiple bins and fits each bin with a separate model. Such a technique is known as Regression spline.Regression splines is one of the most important non linear regression techniques. In polynomial regression, we generated new features by using variouspolynomial functions on the existing featureswhichimposed a global structure on the dataset. To overcome this, we can divide the distribution of the data into separate portions and fit linear or low degree polynomial functions on each of these portions. Source: R-BloggersThe points where the division occurs are called Knots. Functions which we can use for modelling each piece/bin are known as Piecewise functions. There are various piecewise functions that we can use to fit these individual bins.In the next few sub-sections, we will read about some of these piecewise functions.One of the most common piecewise functions is a Step function. Step function is a function which remains constant within the interval. We can fit individual step functions to each of the divided portions in order to avoid imposing a global structure. Here we break the range of X into bins, and fit a different constant in each bin.In greater detail, we create cut points C1 , C2, . . . , Ck in the range of X, and then construct K + 1 new variables.where I( ) is an indicator function that returns a 1 if the condition is true and returns a 0 otherwise. For example, I(cK  X ) equals 1 if cK  X, otherwise it equals 0. For a given value of X, at most only one of C1, C2, . . ., CK can be non-zero, as X can only lie in any one of the bins.Binning has its obvious conceptual issues. Most prominently, we expect most phenomena we study to vary continuously with inputs. Binned regression does not create continuous functions of the predictor, so in most cases we would expect no relationship between the input and output.For example, in the above graph, we can see that the first bin clearly misses the increasing trend of wage with age.To capture non-linearity in regression models, we need to transform some, or all of the predictors. To avoid having to treat every predictor as linear, we want to apply a very general family of transformations to our predictors. The family should be flexible enough to adapt (when the model is fit) to a wide variety of shapes, but not too flexible as to over-fit.This concept of a family of transformations that can fit together to capture general shapes is called a basis function. In this case, our objects are functions: b1 (X ), b2 (X ), . . . , bK (X ).Instead of fitting a linear model in X, we fit the below model:Now well look into a very common choice for a basis function: Piecewise Polynomials.Instead of fitting a constant function over different bins across the range of X, piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of X. As we use lower degrees of polynomials, we dont observe high oscillations of the curve around the data.For example, a piecewise quadratic polynomial works by fitting a quadratic regression equation:where the coefficients 0 , 1 and 2 differ in different parts of the range of X. A piecewise cubic polynomial, with a single knot at a point c, takes the below form:In other words, we fit two different polynomial functions to the data: one on the subset of the observations with xi < c, and one on the subset of the observations with xi  c.The first polynomial function has coefficients 01, 11, 21, 31 and the second has coefficients 02, 12, 22, 32. Each of these polynomial functions can be fit using the least squares error metric.Remember that this family of polynomial functions has 8 degrees of freedom, 4 for each polynomial (as there are 4 variables).Using more knots leads to a more flexible piecewise polynomial, as we use different functions for every bin. These functions depend only on the distribution of data of that particular bin. In general, if we place K different knots throughout the range of X, we will end up fitting K+1 different cubic polynomials. We can use any low degree polynomial to fit these individual bins. For example, we can instead fit piecewise linear functions. In fact, the stepwise functions used above are actually piecewise polynomials of degree 0. Now we will look at some necessary conditions and constraints that should be followed while forming piecewise polynomials.We need to be cautious while using Piecewise polynomials as there are various constraints that we need to follow. Consider the image below:Source: Elements of Statistical LearningWe might encounter certain situations where the polynomials at either end of a knot are not continuous at the knot. Such a condition should be avoided because the family of polynomials as a whole should generate a unique output for every input.We can see from the above image that it outputs two different values at the first knot. Thus, to avoid this, we should add an extra constraint/condition that the polynomials on either side of a knot should be continuous at the knot.Source: Elements of Statistical LearningNow after adding that constraint, we get a continuous family of polynomials. But does it look perfect? Before reading further, take a moment to think about whats missing here.It looks like smoothness at the knots is still absent. So to smoothen the polynomials at the knots, we add an extra constraint/condition: the first derivative of both the polynomials must be same. One thing we should note:Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, as we reduce the complexity of the resulting piecewise polynomial fit. Therefore, in the above plot, we are using only 10 degrees of freedom instead of 12.Source: Elements of Statistical LearningAfter imposing the constraint of equal first derivative, we obtain the above plot. This plot uses 8 degrees of freedom instead of 12 as two constraints are imposed. Although the above plot looks better, there is still some scope for improvement. Now, we will impose an extra constraint: that the double derivatives of both the polynomials at a knot must be same.Source: Elements of Statistical LearningThis plot seems perfect for our study. It uses 6 degrees of freedom instead of 12. Such a piecewise polynomial of degree m with m-1 continuous derivatives is called a Spline. Hence, we have constructed a Cubic Spline in the above plot. We can plot any degree of spline with m-1 continuous derivatives.Cubic spline is a piecewise polynomial with a set of extra constraints (continuity,continuity of the first derivative, and continuity of the second derivative). In general, a cubic spline with K knots uses cubic spline with a total of 4 + K degrees of freedom. There isseldom any good reason to go beyond cubic-splines (unless one is interested in smooth derivatives).We know that the behavior of polynomials that are fit to the data tends to be erratic near the boundaries. Such variability can be dangerous. These problemsare resembled by splines, too. The polynomials fit beyond the boundaryknots behave even more wildly than the corresponding global polynomialsin that region. To smooth the polynomial beyond the boundary knots, we will use a special type of spline known as Natural Spline.A natural cubic spline adds additional constraints, namely that the function is linear beyond the boundary knots. This constrains the cubic and quadratic parts there to 0, each reducing the degrees of freedom by 2. Thats 2 degrees of freedom at each of the two ends of the curve, reducing K+4 to K.When we fit a spline, where should we place the knots? One potential place would be the area of high variability, because in those regions the polynomial coefficients can change rapidly. Hence, one option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable.While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.Another option is to try out different numbers of knots and see which produces the best looking curve.A more objective approach is to use cross-validation. With this method:We repeat this process multiple times until each observation has been left out once, and then compute the overall cross-validated RMSE. This procedure can be repeated for different numbers of K knots. Then the value of K giving the smallest RMSE is chosen.Regression splines often give better results than polynomial regression. This is because, unlike polynomials, which must use a high degree polynomial to produce flexible fits, splines introduce flexibility by increasing the number of knots but keep the degree fixed. Generally, this approach produces more stable estimates. Splines also allow us to place more knots, and hence flexibility, over regions where the function seems to be changing rapidly, and fewer knots where the function appears more stable. The extra flexibility in the polynomial produces undesirable results at the boundaries, whereas the natural cubic spline still provides a reasonable fit to the data.In this article, we learned about regression splines and their benefits over linear and polynomial regression. Another method to produce splines is called smoothing splines. It works similar to Ridge/Lasso regularisation as it penalizes both loss function and a smoothing function. You can read more about it in the book Introduction to Statistical learning. You can implement these methods on datasets with high variability and notice the difference.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/
Baidu has Released a Gigantic Self-Driving Dataset named ApolloScape,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Introduction to Regression Splines (with Python codes)|AVBytes: AI & ML Developments this week  Microsofts NLP AI, Build your Own Face Emoji, Googles Music Making Model, Reuters AI Redefining Journalism, etc.|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Autonomous vehicles are the talk of the town these days. Whichever AI and machine learning conference you hear about, self-driving cars are almost always a topic of conversation. But progress has been slow and it still feels like were a while away from seeing these vehicles in practical situations.So Baidu has decided to include the AI and ML community in the hopes of speeding up research. The company has announced the release of the worlds largest open-source dataset for self-driving technology.Named ApolloScape, the dataset has been released under the umbrella of Baidus self-driving platform Apollo. Apollo is a high performance flexible architecture which supports fully autonomous driving capabilities. The team behind this has recently released a version 2.0 of the platform which enables a vehicle to drive on simple urban roads autonomously. It is able to cruise, avoid collisions with obstacles, stop at traffic lights and change lanes.ApolloScape will provide researchers and developers a solid framework and a base for building self-driving vehicles. The dataset has 26 pre-defined semantic items, like cars, buildings, people walking on the sidewalk, traffic lights, street lights, etc. This has been done with pixel-by-pixel semantic segmentation technique.You can check out Apollos GitHub page here and follow along with the instructions to deploy it on your own machine.The data provided in the ApolloScape project is almost 10 times more than any previously released open source datasets like CityScapes and Kitti. It will save researchers a massive amount of time and money that would otherwise have gone into collecting and parsing through data.According to a report by the RAND corporation, autonomous vehicles would have to be driven hundreds of millions of miles in order to demonstrate their reliability in terms of fatalities and injury. In fact, to even gain a 20% advantage over human drivers, 100 cars would have to be driven continuously for 500 years!I hope this dataset will speed up research in this field and benefits your learning as a data science enthusiast as well.",https://www.analyticsvidhya.com/blog/2018/03/baidu-apollo-released-gigantic-self-driving-dataset/
"AVBytes: AI & ML Developments this week  Microsofts NLP AI, Build your Own Face Emoji, Googles Music Making Model, Reuters AI Redefining Journalism, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Baidu has Released a Gigantic Self-Driving Dataset named ApolloScape|Microsofts Language Translation AI has Reached Human Levels of Accuracy|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The job of a data scientist is one of the most challenging in todays world. You have to keep yourself up to date almost on a daily basis regarding the developments and ongoings in the machine learning universe. Slack off, and you might well find yourself lagging behind the competition.Let AVBytesbe your go-to guide! The articles under the AVBytes umbrella include official research papers, open source code, GitHub links, among various other things to help you, as a data science practitioner and enthusiast, keep yourself updated. You can download the code and replicate the model on your own machine, browse through the deep learning research papers, or just read through the article as a source of information.In the past week, we saw quite a few breakthroughs  Microsofts NLP AI has reached human levels of quality and accuracy, a new deep learning technique to build your own 3D digital avatar, Googles algorithm to generate new music from scratch, Reuters using AI to redefine how news stories are curated, among others.Scroll down to view the articles. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.                        Source: Pexels             Source: thecoversation.comThe above AVBytes were published from 12th to 18th March, 2018.Whats your take on last weeks developments? Let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/03/avbytes-ai-ml-developments-this-week-190318/
Microsofts Language Translation AI has Reached Human Levels of Accuracy,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Microsofts NLP AI, Build your Own Face Emoji, Googles Music Making Model, Reuters AI Redefining Journalism, etc.|You can now Build your own 3D Digital Face Emoji using Deep Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Even with the advances in the Natural Language Processing field, there have always been nagging doubts about the quality and accuracy of translations from one language to another. Take Googles translation, for example. While it has steadily improved over the years, you still see a few things grammatically wrong with complex sentences.             Source: WikipediaTo bridge that gap, Microsoft claims it has developed a system that can translate from Chinese to English with the quality and accuracy of humans. The researchers behind this system developed it by training the model on a set of news stories called newstest2017.In order to ensure that they results of the translations were precise, Microsoft hired external bilingual evaluators to compare the results of the machines translations with two independently produced human translations.The researchers used two methods to develop the AI:Two new techniques were also developed during the training phase to further improve the accuracy of the model.To understand the mathematics behind the system, you can view Microsofts official research paper here.This is quite a huge breakthrough in NLP. But caution should be taken at this stage. This research was conducted on a set of old news stories and as of today, has not yet been tested on real-time news stories. Its applications could go beyond just translations.If youre interested in NLP, I encourage you to check out the research paper which lists how the team went about developing the deep neural network behind this system.",https://www.analyticsvidhya.com/blog/2018/03/microsofts-claims-language-translation-ai-reached-human-levels-accuracy/
You can now Build your own 3D Digital Face Emoji using Deep Learning,Learn everything about Analytics|Overview|Introduction|Our take on this,"How does this work?|Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|Microsofts Language Translation AI has Reached Human Levels of Accuracy|Google has Released the Deep Learning Model used to Create Pixel 2s Portrait Mode|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever wondered how the animojis on the iPhone X work? Dont worry, deep learning has the answer again. How about a technique that doesnt require any specific hardware, doesnt need a video of you (just a picture), and generates a 3D digital avatar with remarkable accuracy that can be animated in real time?This is not some far-off futuristic technology. This is now.A group of developers have released a research paper demonstrating how they used deep learning to build a digital 3D avatar of a persons head and face.The input image is segmented into a face part and a hair part. Then, the har part is run through a neural network that attempts to extract attributes like length, curve, spikiness, baldness, etc. This is an extremely deep neural network with over 50 layers. It was trained on over 40,000 images of various hairstyles.The framework is also robust to lighting. That is, even under differing light conditions, the same photo will lead to extremely similar outcomes. And the same applies to facial expressions.The developers claim that their digitized models are fully rigged with intuitive animation controls, like blend shapes and joint-based skeletons, and can be readily integrated into existing game engines.You can view the official research paper here and check out their video below:To see the technology live in action, you can also download the PinScreen application on iOS devices (not available on Android currently).Their model offers a practical end-to-end solution for avatar personalization in gaming and VR applications. I have previously used a similar application wherein we hosted avatars in virtual classrooms so people could connect from remote locations. But with the level of personalization this deep learning model offers, it could be a game changer in multiple industries.As a person on the video has commented, it is amazing how machine learning is making hardware obsolete. From 3D face scanners to dual lenses (as we covered in the Pixel 2 article), hardware is just not a necessary component when algorithms are as intelligent as this one.I strongly recommend checking out their research paper to understand how they went about building the deep neural network and then trying it on your own.",https://www.analyticsvidhya.com/blog/2018/03/build-your-own-facial-emoji-using-deep-learning/
Google has Released the Deep Learning Model used to Create Pixel 2s Portrait Mode,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Also, go ahead and participate in our Hackathons, including the DataHack Premier League and Lord of the Machines!|Share this:|Like this:|Related Articles|You can now Build your own 3D Digital Face Emoji using Deep Learning|Essentials of Deep Learning  Sequence to Sequence modelling with Attention (using python)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have covered Googles releases quite a lot recently on AVBytes and thats because they continue to open source their releases. They want the community to benefit from their resources.Keeping that going, Google has now released the deep learning model it used to create the Portrait Mode in its signature Pixel 2 phone. It is based on semantic image segmentation.The deep learning model assigns semantic labels to each pixel in the image. Then, categorization classifies the objects in the image, like animal, human, sky, road, etc. It basically recognises the foreground and background in that particular image.The latter feature was used to develop the Pixel 2s portrait mode for shallow depth of field effects with only one physical lens. Its main use is in recognizing the outline of objects in the image, or being able to successfully determine where an object ends and the background begins.The code open sourced by Google is named DeepLab -V3+. Its an image segmentation tool built on top of a convolutional neural network (CNN).Check out the underlying structure used by Googles team to build the model:This has been implemented in TensorFlow (of course, its Google!) and the release includes the training and evaluation code.You can read Googles official blog post about it here.Modern image segmentation models built on top of CNNs are scaling never before seen accuracy levels. This latest release will again help the deep learning community understand and replicate (if not improve) what Google have done in the photography field.",https://www.analyticsvidhya.com/blog/2018/03/google-released-deep-learning-model-used-create-pixel-2s-portrait-mode/
Essentials of Deep Learning  Sequence to Sequence modelling with Attention (using python),Learn everything about Analytics|Introduction|Table of Contents|Problem Formulation for Sequence to Sequence modelling|A glance of Sequence to Sequence modelling technique|Improving the performance of models  Beam Search and Attention mechanism|Hands-on view of Sequence to Sequence modelling|End Notes,"Beam Search|Attention mechanism|Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|Google has Released the Deep Learning Model used to Create Pixel 2s Portrait Mode|Top 5 Data Science & Machine Learning Repositories on GitHub in Feb 2018|
Faizan Shaikh
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning at scale is disrupting many industries by creating chatbots and bots never seen before. On the other hand, a person just starting out on Deep Learning would read about Basics of Neural Networks and its various architectures like CNN and RNN.But there seems like a big jump from the simple concepts to industrial applications of Deep Learning. Concepts such as Batch Normalization, Dropout and Attention are almost a requirement to know in building deep learning applications.In this article, we will cover two important concepts used in the current state of the art applications in Speech Recognition and Natural Language Processing  viz Sequence to Sequence modelling and Attention models.Just to give you a sneak peek of the potential application of these two techniques Baidus AI system uses them to clone your voiceIt replicates a persons voice by understanding his voice in just three seconds of training.You can check out someaudio samplesprovided by Baidus Research team which consist of original and synthesized voices.Note: This article assumes that you already are comfortable with basics of Deep Learning and have built RNN models. If you want a refresher, you can go through these articles first:We know that to solve sequence modelling problems, Recurrent Neural Networks is our go-to architecture. Lets take an example of a Question Answering System to understand what a sequence modelling problem looks like.Suppose you have a series of statements:Joe went to the kitchen. Fred went to the kitchen. Joe picked up the milk.
Joe travelled to the office. Joe left the milk. Joe went to the bathroom.And you have been asked the below question:Where was Joe before the office?The appropriate answer would be kitchen. A quick glance makes this seem like a simple problem. But to understand the complexity  there are two dimensions which the system has to understand:This can be considered as a sequence modelling problem, as understanding the sequence is important to make any prediction around it.There are many such scenarios of sequence modelling problems, which are summarised in the image below. The example given above is a many input  one output problem (If you consider a word as a single output).A special class of these problems is called a sequence to sequence modelling problem, where the input as well as the output are a sequence. Examples of sequence to sequence problems can be:1. Machine Translation An artificial system which translates a sentence from one language to the other.2. Video Captioning Automatically creating the subtitles of a video for each frame, including a description of the sound cues (such as machinery starting up, people laughing in the background, etc.).A typical sequence to sequence model has two parts  an encoder and a decoder.Both the parts are practically two different neural network models combined into one giant network.Broadly, the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output. Lets take an example of a conversational agent to understand the concept. Source: https://github.com/farizrahman4u/seq2seqIn the image given above, the input sequence is How are you.So when such an input sequence is passed thoughthe encoder-decoder network consisting of LSTM blocks (a type of RNN architecture), the decoder generates words one by one in each time step of the decoders iteration. After one whole iteration, the output sequence generated is I am fine.A sequence to sequence modelling network should not be used out of the box. It still needs a bit of tuning to squeeze out the best performance out there to meet expectations. Below are two techniques which have proven to be useful in the past in sequence to sequence modelling applications.As we saw before, the decoder network generates the probability of occurrence of a word in the sequence. At each time step, the decoder has to make a decision as to what the next word would be in the sequence.One way to make a decision would be to greedily find out the most probable word at each time step.For example, if the input sequence was Who does John like?, there could be many sentences that can be generated by a single decoder network in multiple iterations, making a tree like structure of sentences as shown above. The greedy way would be to pick a word with the greatest probability at each time step.What if it comes out to be, likes Mary John? This does not necessarily give us the sentence with the highest combined probability.For this, you would have to intelligently sort out the appropriate sequence for the sentence.Beam search takes into account the probability of the next k words in the sequence, and then chooses the proposal with the max combined probability, as seen in the image below:When a human tries to understand a picture, he/she focuses on specific portions of the image to get the whole essence of the picture. In the same way, we can train an artificial system to focus on particular elements of the image to get the whole picture. This is essentially how attention mechanism works.Lets take an example of an image captioning problem, where the system has to generate a suitable caption for an image. In this scenario, to generate the caption, attention mechanism helps the model to grasp individual parts of the image which are most important at that particular instance.To implement attention mechanism, we take input from each time step of the encoder  but give weightage to the timesteps. The weightage depends on the importance of that time step for the decoder to optimally generate the next word in the sequence, as shown in the image below:Source https://github.com/google/seq2seqKeeping these two techniques in mind, you can then build an end-to-end sequence to sequence model that works wonderfully well. In the next section, we will go through a hands on example of the topics we have learnt and apply it on a real life problem using python.Lets look at a simple implementation of sequence to sequence modelling in keras.The task is to translate short English sentences into French sentences, character-by-character using asequence-to-sequence model.The code for this examplecan be found on GitHub. The original author of this code is Francois Chollet.For implementation, we will use a dataset consisting of pairs of English sentences and their French translation, which you can download from here (download the file named fra-eng.zip). You also should have keras installed in your system. Heres a summary of our implementation:1) Turn the sentences into 3 Numpy arrays,encoder_input_data,decoder_input_data,decoder_target_data:2) Train a basic LSTM-based Seq2Seq model to predictdecoder_target_datagivenencoder_input_dataanddecoder_input_data.3) Decode some sentences to check that the model is working (i.e. turn samples fromencoder_input_datainto corresponding samples fromdecoder_target_data).Now lets have a look at the python code.Here is a preview of the output which is generated:In this article, we went through a brief overview of sequence to sequence modelling and attention models, two of the most important techniques which are used in state-of-the-art deep learning products focused on natural language and speech processing.There is a lot of scope of applying these models in practical day-to-day life scenarios. If you do try them out, let us know in the comments below!",https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/
Top 5 Data Science & Machine Learning Repositories on GitHub in Feb 2018,Learn everything about Analytics|Introduction|FastPhotoStyle|Twitter Scraper|Handwriting Synthesis|ENAS PyTorch|Sign Language,"Share this:|Like this:|Related Articles|Essentials of Deep Learning  Sequence to Sequence modelling with Attention (using python)|Google is making Music with Machine Learning  and has released the code on GitHub|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Continuing our theme of collecting and sharing the top machine learning GitHub repositories every month, the February edition is fresh off the shelves ready for you!GitHub repositories are one of the easiest and best things for all the people working in data science to keep ourselves updated with the latest developments and projects. Its also an awesome collaboration tool where we can connect with other like minded data scientists on various projects.Without any further ado, lets dive into this months list.This is part of a series from Analytics Vidhya that will run every month.You can check out the top 5 repositories that we picked out in January here.FastPhotoStyle is a python library developed by NVIDIA. The model takes a content photo and a style photo as inputs. It then transfers the style of the style photo to the content photo.The developers have cited two examples to show how the algorithm works. The first is a very simple iteration  you download a content and a style image, re-size them, and then simply run the photorealistic image stylization code. In the second example, semantic label maps are used to create the stylized image.You can read more about this library on Analytics Vidhyas blog here.If youve ever scraped tweets from Twitter, you have experience working with its API. It has its limitations and is not easy to work with. This python library was created with that in mind  it has no API rate limits (does not require authentication), no limitations, and is ultra quick. You can use this library to scrape the tweets of any user triviallyThe developer has mentioned that it can be used for making Markov Chains. Do note that it works only with python version 3.6+.This is an implementation of the handwriting synthesis experiments presented in the Generating Sequences with Recurrent Neural Networks paper by Alex Graves. As the name of the repository suggests, you can generate different styles of handwriting. The model is based on priming and biasing. Priming controls the style of the samples and biasing controls the neatness of the samples.The samples presented by the author on the GitHub page are truly fascinating in their diversity. He is looking for contributors to enhance the repository so if youre interested, get in touch with him!This is a PyTorch implementation of Efficient Neural Architecture Search (ENAS) via Parameters Sharing. What do ENAS do? They reduce the computational requirement, that is, the GPU Hours of the Neural Architecture Search by an incredible 1000 times. They do this via parameter sharing between models that are subgraphs within a large computational graph.The process of how to use it have been neatly explained on the GitHub page. The prerequisites for implementing this library are: Source: WikipediaThis is a relatively straightforward, yet utterly fascinating, use of machine learning. Using a convolutional neural network in python, the developer has built a model that can recognize the hand gestures and convert it into text on the machine.The author of this repository built the CNN model using both TensorFlow and Keras. He has specified, in detail, how he went about creating this project and each step he followed. Its definitely worth checking out and trying once on your own machine.Did you find these helpful? Or are you aware of any other GitHub repositories the AV community should know about? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/03/top-5-github-repositories-february-2018/
Google is making Music with Machine Learning  and has released the code on GitHub,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Top 5 Data Science & Machine Learning Repositories on GitHub in Feb 2018|Supported by Andrew Ng, Woebot is a Mental Health Chatbot to help with Depression|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The field of audio processing has seen quite an interest with the rise of deep learning. But what if you are working in the music industry and are faced with a musicians block (on the same line as a writers block)? You have a few initial ideas but the music is just not flowing to you.Google has an answer for that as well.Magenta, Googles research arm that finds ways of using AI to help peoples creativity, has developed an instrument it calls NSynth Super. It is based on the NSynth algorithm which uses a deep neural network technique to generate sound. NSynth was released by Google a few months ago.Rather than generating music notes, NSynth replicates the sound of an instrument. What makes the algorithm unique is that it continuously learns the core qualities of what makes up an individual sound and is able to combine various sounds to generate something completely new.NSynth Super is an open source experimental instrument. It gives musicians (and deep learning followers) the ability to explore completely new sounds generated by the NSynth machine learning algorithm. It has been built using open source libraries like TensorFlow and openFrameworks.The instrument can be played via any MIDI source, like a keyboard or a sequencer.You can put together your own NSynth Super interface by following the step-by-step code and instructions provided by Google on their GitHub repository.To deep dive into the algorithm and get the dataset behind NSynth, go to Googles blog here.Check out Googles video of NSynth below:Also check out how the NSynth Super instrument works below:The directions given by Google on their GitHub page are detailed and will help you to create the instrument step-by-step. You (probably) wont be able to make one as gorgeous as Googles, but you will be able to generate crazy sound sequences to get you started.This is a goldmine for the deep learning enthusiasts who are interested in the audio processing field. Go ahead and check the code, try it out and build your own musical database!",https://www.analyticsvidhya.com/blog/2018/03/google-making-music-help-machine-learning/
"Supported by Andrew Ng, Woebot is a Mental Health Chatbot to help with Depression",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google is making Music with Machine Learning  and has released the code on GitHub|Google is Using Deep Learning to Make Computers Better with Age!|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Talking to a therapist is not something most people can do, or can afford. People suffering from mental health issues often go undiagnosed throughout their lives.Thanks to the recent uptick in machine learning, healthcare has seen a sharp rise in applications and products targeted towards people who previously were not able to afford treatment.Woebot was launched by a team of Stanford psychologists and AI experts last year. Andrew Ng joined its board of directors as the chairman soon after. It is a talk therapy chatbot for people who struggle with mental health issues. The app was launched for iOS earlier this year and has been made accessible for Android users recently.The chatbot can do a variety of things:Anna Maxted, a writer for The Times recently shared her experience with the tool. You can get a glimpse of how the chatbot works in the below conversation:Woebot monitors daily, one-on-one interactions with users using natural language processing (NLP) to deliver a self-guided version of cognitive behavioral therapy (CBT), which is the most effective treatment for mental health problems.It has been found that the burden of mental health problems has doubled in the past decade. According to the World Health Organization, depression is the leading cause of disability globally with over 300 million people affected each year. The intention of creating Woebot was to provide continuous support to the people suffering from anxiety and depression.It is already being used in 130 countries, getting more than two million messages per week, which is a remarkable start.Woebot is a very useful foray into the world of digital mental healthcare but it cannot (yet) replace a human connection. Having said that, its still a welcome change and should help people around the world deal with their everyday issues. Not everyone has the time and money to get personal counseling.The field of digital healthcare is still ripe for data scientists  the amount of data available encourages researchers in this field to find cures and prevention measures for various illnesses, both physical and mental. You can see machine learning competitions being hosted on popular sites encouraging people to come up with new solutions to old problems. Heres hoping the trend continues with even better and accurate results in the future.",https://www.analyticsvidhya.com/blog/2018/03/woebot-mental-health-chatbot-supported-andrew-ng/
Google is Using Deep Learning to Make Computers Better with Age!,Learn everything about Analytics|Overview|Introduction|Our take on this,"How did they do this?|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Supported by Andrew Ng, Woebot is a Mental Health Chatbot to help with Depression|A Guide to Building an Intelligent Chatbot for Slack using Dialogflow API|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Our machines slow down with time. Its a known fact (and a source of frustration) and is generally accepted by us, albeit grudgingly. The hardware in our machines struggles to handle the latest software updates and inevitable leads to wear and tear. Source: PexelsBut Google has been looking into this, and might have come up with an answer. In their latest research paper, they have proposed a solution using deep learning that might make our machines better with age!There is a widely known issue in computing called prefetching. Basically, our computers process information at a rate thats way more quicker than whats being extracted from the memory. To avoid long queues, the computer will pull information well in advance by predicting what might be needed. But as technology has become more and more advanced, the predictions have become a rather tricky task.This is what the research team at Google is aiming to solve  its deep learning model, made up of a gigantic simulated neural network, is working on revamping the prefetching process. In its research paper, Google has not posted any figures showing the improvement in speed yet.You can read Googles official research paper here.I personally feel this is just the tip of the iceberg. The researchers are confident that their deep learning model, once improved further, could potentially be applied to all components of a machine, from the design chips to the integrated OS.The model, or approach, could also be applied to machine learning algorithms. Imagine a model designed for marketing data could also be applied to financial datasets? But expectations at this point should be tempered. To even create these models is a computationally expensive task since a lot of data is required to even begin to think about improvements.An MIT professor, Tim Kraska, is also working on a similar problem of improving machines using deep learning. These are exciting times in the machine learning community!",https://www.analyticsvidhya.com/blog/2018/03/google-deep-learning-make-computers-better-with-age/
A Guide to Building an Intelligent Chatbot for Slack using Dialogflow API,Learn everything about Analytics|Introduction|Table of Contents|What is a Chatbot and why should you care?|Anatomy of our Chatbot|A simple rule based Chatbot using Slack|Understanding Googles Dialogflow (api.ai) for NLP and Machine Learning|Bringing the Chatbot to life (Integrating DialogFlow and Slack)|End Notes,"Creating a Slack Workspace|Creating a Slack Application|Using python to create a rule based bot|Setting up DialogFlow|Training the agent|Setting up Slack app credentials with DialogFlow|Continuing Slack Setup|Conversing with the chatbot|Configuring the Small Talk|Share this:|Like this:|Related Articles|Google is Using Deep Learning to Make Computers Better with Age!|Reuters is Using AI to Spot Patterns and Redefine Journalism|
Mohd Sanad Zaki Rizvi
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Agents|Understanding intents and entities|a.) Entity|b.) Intent|Add OAuth URL|Add Event Request URL|Enable Event Subscriptions|Add Your Slack Bot to a Team,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Breakthroughs in the field of Natural Language Processing (NLP) have seen a sudden rise in recent times. The amount of text data available to us is enormous, and data scientists are coming up with new and innovative solutions to parse through it and analyse patterns. From writing entire novels to decoding ancient texts, we have seen a variety of applications for NLP.One of the most popular applications is a chatbot.Organizations like Zomato, Starbucks, Lyft, and Spotify are leveraging this technology on their website and mobile apps. As a user, we no longer need to worry about being put on hold  just type your query and the chatbot will instantly analyse the text and give the appropriate response.Given how popular and useful these chatbots have become, I wanted to showcase how to make one. In this article, we will build a chatbot using Slack, the worlds largest communication app for teams. Then, we will utilize Googles DialogFlow (previously api.ai) to add intelligence to the bot.Sounds interesting? Lets start!A chatbot provides a way for your users to give complex commands and get complex tasks done using simple language.For example, assume you want to buy a pair of shoes from an ecommerce site that has its own chatbot. You can tell the bot that you are looking to buy some shoes, and it would have a conversation with you to gather more details, like what brand/colour?, whats your size? and what kind of shoes? Sneakers or boots?. Instead of browsing through the website, you will have a conversation with the bot,mirroring the type of experience you would have when you go to the retail store.
Another example of a use case would be Starbucks chatbot:Its easy to order coffee while conversing with the bot, and then clicking on a bunch of buttons and searching for the orders manually. For these reasons, chatbots are considered to be one of the best ways to engage with the user. Now that we know what a chatbot is, lets get into the basics. Below is my definition of a chatbot:A chatbot is a service, powered by rules and sometimes artificial intelligence, that you interact with via a chat interface.The service could be any number of things, ranging from functional to fun, and it could live in any major chat product (Facebook Messenger, Slack, Telegram, Text Messages, etc.).In layman terms, a chatbot is a program that interacts with a user on a chat platform. It can be as dumb (fun) as you want it to be and as smart (resourceful) as you require it to be!There are various ways of creating a chatbot based on what kind of functionality we want it to have. In this tutorial, we are going to build a basic chatbot. Below is a high level design of it:We will use Slacks API to send the messages to DialogFlow (DF). DialogFlows NLP engine would understand the intent and semantics behind the users messages, and craft an appropriate reply for our chatbot to send to the user. The Slack interface and DialogFlow together make the chatbot. To put this in an analogy, the Slack interface would be the face of our bot, and DialogFlow would be the brain.For this study, Slack is our platform of choice, as it is used widely across the world for organization level communications. It also has very good support for adding bots through its APIs. Before we can do anything with Slack however, we need to get a few things ready:A workspace (group) is Slacks way of organizing teams. If you are a part of Analytics Vidhyas group on Slack, you are part of a workspace. A workspaces URL address looks like this:You can create your own workspace at this url:Once you enter your email, Slack will send you a 6 digit code on your email id. Follow their given steps to select your name, password, team size. Then, Slack asks you to provide a name for the group:You can give it any name you like. Ill name mine something. Once you select the URL address and create your workspace, Slack will recommend sending email invitations. You can just skip this step by clicking on Skip for Now and voila! You just created your own workspace on Slack. Try sending some messages!Now that you have a workspace to experiment with, you need an application where you can attach your bot. Create the app on the below link:Click on Create App and give a name to the app and select your workspace:This will redirect you to your app dashboard. From there, you can select theBots option:Click Add a Bot User > Give a name to your bot. In my case, I have named it skynet. Now that we have created a bot for our app, we need to add it to our workspace so we can chat with it! Go back to the above app dashboard and scroll down to find the Install App to Workspace option.Once you do that, Slack will ask you to authorize the application. Go ahead and accept the authorization. Now that we have authorized the bot, lets create a simple rule based chatbot using python.Before we are able to connect any external program to our Slack bot, we need to ensure that Slack has the right authorization to access the bot. For that, Slack provides an auth token that we need to provide when trying to connect with it. Go back to the app dashboard and select the OAuth & Permissions option:This will open the permissions settings of the app. Select the Bot User OAuth Access Token and save it (I have hidden them for security reasons). This token is instrumental in connecting to our bot.Now that you have everything set up, lets write some python code. To make it easy to connect to the Slack API, I have written a small python program. Lets setup the code environment.Note: The entire project is in python 2.7.1.Install slackclient library.2. Clone my slack repository.3. Set your slack auth token to the value of Bot User OAuth Access Token and the name of your bot in the environment variables.For example, my BOTNAME is skynet so Ill set it like this:4. Go to the Slack-AI-ChatBot directory.5. Start the python bot.You should now get the below notification:Now that everything is set up, you can just go to your Slack workspace and start chatting with your bot. Note that it would reply only when you talk to it by using its mention @chatbotname (just like we normally do in Slack).These are some of the hard coded rules I have preset in the bot. You can change them or add your own rules. If you open the file mainbot.py, you will find the below function:This is where all the magic is happening. Go ahead and try adding your own rules and make the chatbot as customizable as you can! Also, I have added comments throughout the code in the project to make it understandable. Feel free to explore the code here to understand how the process works.Now that we have got a taste of building a chatbot for Slack using custom rules, lets see how can we use the power of NLP and ML to make our bot even more intelligent!Note: Every time you make changes in the code, you will have to restart the program for the changes to take effect.Before we proceed, recall the anatomy of our chatbot we discussed right at the beginning of the article. For your reference, below is that diagram:
The first part, which included setting up Slack, has been completed. Now, its time to explore the DialogFlow api.1. Go to https://dialogflow.com/ and signup for free using your Google account2. You will get access to your console:DialogFlow works by creating agents. Agents are best described as NLU (Natural Language Understanding) modules. These can be included in your app, product, or service. They transform natural user requests into actionable data.This transformation occurs when a user input matches one of the intents inside your agent. Intents are the predefined or developer-defined components of agents that process a users request.For example, you tell the chatbot that you are feeling cold, or you want the fan to be turned off. The agent needs to know what the intent is behind these statements. More specifically, What does the user wants me to do?. Once the intent is known, a corresponding action/reply can be generated.Lets start by creating our own agent. Well take the example of a chatbot for a pizza company.On the top left of the window, youll see an option to Create Agent. Click on it and select a name for your agent. I have named mine as pizzaBot.Now that we have the agent ready, we need to define some entities it needs to recognize and some intents it needs to understand.Entities are a group of objects that you need the agent to recognize. For example, if we are creating a pizzaBot, some of our entities would be objects that are usually required with a pizza:Lets create our entities.Now that our entities are ready, we need to define some intents that will help our agent understand what to do with these entities.In simple words, an intent is a mapping between what the user says, and what operation your bot takes on that instruction. If you click on the intent tab, youll see that there are two intents already present.Lets play around with the default intents. Type pizza please! in the demo field on the right and see how the agent reacts:Since we havent trained our agent yet, it used the default fall back intent and responded with one of the many responses it has stored for that scenario. Lets create an intent to cook a pizza and call it makePizza. Click on create intent, select a name, and save it. You will see something like this:The way DialogFlow works is by taking some sample user sentences to start out with. Then, it trains its engine to generate an algorithm that best matches these sentences to the correct intents. Lets add some basic training phrases for our agent:The idea is to train our agent in as diverse and varied examples as possible to make it more precise. Once you have set up the training phrases, you need to define what action the agent should take if it encounters this intent.You can do that by clicking on the Add Parameters and Action option. Add the following actions and parameters:What do these column names mean?Click on Define prompts next to toppings and write What toppings would you like on your pizza?Similarly, add one for cheese. Wow. That was a lot of work. Lets see what we achieved. Save the intent and get back to the demo area of the agent. Try asking the agent for a pizza. Since we have trained the agent on a few sample phrases, it should be able to immediately recognize any phrase related to ordering a pizza. It should also ask you for the toppings and for the type of cheese you want? Checkout the following video to see what Im talking about.Notice how natural the conversation feels. Once you trigger an intent, the agent tries to extract the parameters you have marked as important from the user by having a conversation. Note that you only need to give a few examples for the training phrase; the NLP engine of DialogFlow takes care of the rest.Also, you can add custom responses for the user in case the intent is successfully acted upon and completed. Click on the Add Responses button below the Action and Parameters, add the following response, and save it:Once your pizza is ready, youll be greeted by the below message:What happened behind the scenes? When we used $cheese and $toppings in the response field, they were automatically replaced by the values the parameters extracted from the users conversation. Convenient, isnt it?Now that everything about the agent is set up , its time to train it. Click on the settings icon next to your agent name.In the settings window, select ML Settings.Here are few important things to note:Click on the Train button and once the agent is trained, save it. Now our NLP agent is good to go. Lets go to the next step and integrate this with our Slack app to make the chatbot complete!DialogFlow (DF) is an excellent choice when it comes to integration with most popular applications. It supports integration with Slack right out of the box. Lets get our DF agent to work with our Slack bot!On the left side of the window, click on the Integrations button and you will see a bunch of options. Select the Slack icon and click settings beneath that. The below box will pop up:On the top right hand side, toggle on the switch to turn on the integration with Slack.Scroll down and you will find fields to enter your Slack apps credentials:For that, head back to your Slack apps console. If you dont have it opened already, just go to the following URL and select your app:Scroll down on the page and youll find your apps credentials (I have hidden them for security reasons).Copy them to the respective fields in the DialogFlow page.Note: To copy the Client Secret, you have to first click on the Show option button next to it. After you have copied the credentials, click Start on the bottom right of the Slack box in DialogFlow.Now that youve set up Slack and Dialogflow, youll need to enable OAuth (helps in authentication), Event Requests URLs (provided by Dialogflow), and Event Subscriptions (what kind of events does our bot listen to?).After a few moments you should see a green Verified text above the Request URL field.Event subscription is the most crtitical thing for the chatbot. It basically tells Slack what kind of events our bot will listen to and get triggered.Events can range from someone joining a channel to a new personal message.Once you are done with this step, you can go back to your workspace and start conversing with your bot!Go to your workspace and either personal DM the bot, or mention his name like @botname and it will start responding.Did you notice?That apart from answering the normal pizza questions, the bot is also good at replying to generic messages like hi, hello, and thank you? This is possible because of a DialogFlow feature called SmallTalk.If you click on the Small Talk button on the left pane in DialogFlow, youll come across a set of options:Small Talk lets you add how your bot should react when it receives generic comments like Hi, Hello or general questions like Who are you? and What can you do?. Go ahead and set it up with details specific to your project.Our chatbot is finally in its full shape! Now that you are able to understand how a chatbot works and have implemented both a simple rule based bot and a ML based intelligent bot, you can do a lot more with it by exploring the features both Slack and DialogFlow have to offer.This has been a great learning experience for me. Using the power of DialogFlow, we were able to avoid a lot of complexity thatd we would have faced if we were implementing NLP and ML models from scratch. I challenge you to go ahead and build some interesting chatbots of your own!And to wrap it up, here are some resources/links you can check out:Did you find this article useful? Let me know your thoughts and feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2018/03/how-to-build-an-intelligent-chatbot-for-slack-using-dialogflow-api/
Reuters is Using AI to Spot Patterns and Redefine Journalism,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Guide to Building an Intelligent Chatbot for Slack using Dialogflow API|AVBytes: AI & ML Developments this week  Pandas on Ray, Windows ML, TensorFlow code for Googles AstroNet, An online tool for Dirty Data, etc.|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Journalism has always felt like a field driven by human insight and opinion. How could machines ever dictate the nuances of what to write, how to write it and when it should be published?                          Source: thecoversation.comTurns out, Reuters has the answer.Named Lynx Insight, the AI tool has already been used by multiple journalists and Reuters is in the process of rolling it out across all its offices. The tool will assist journalists in curating stories from around the internet, suggesting ideas, analyzing data and even write complete sentences if required.Lynx Insight will trawl through hundreds and thousands of stories every day and try to spot patterns that stand out. Reuters has already experimented with financial stories and claims it was an unqualified success which prompted it to move to all sectors.The AI will be given access to run through massive datasets to flag any interesting stories  trending topics in any part of the world, major changes in the stock price of a company, election patterns, etc. It will then pass that information on to journalists in any form they have chosen  message, email or a news flash on their computer. The tool will also generate a quick summary to help journalists understand why the story was flagged and if its worth pursuing.The system also has a feature where the journalist can enter the name of a company and it will return information about it. This can be handy in doing a quick background check ahead of any potential interviews.Reuters claims, like so many other AI users, that the technology will support and assist journalists rather than replace them in the long term.This is not the first time were seeing AI being pioneered in journalism. Quite a lot of news organizations, like the Washington Post, the Press Association and Yahoo, use AI to write short summaries of news stories.This is quite the gamble taken by Reuters as fake news stories is a genuine concern globally right now. If the AI is able to sift out that news, all the better for them. There is certainly a lot of data available when it comes to news so this could be the first of many tools to be launched in this field.We will certainly be keeping an eye out on its performance and will keep you updated in case of any major developments.",https://www.analyticsvidhya.com/blog/2018/03/reuters-using-ai-redefine-journalism/
"AVBytes: AI & ML Developments this week  Pandas on Ray, Windows ML, TensorFlow code for Googles AstroNet, An online tool for Dirty Data, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|Reuters is Using AI to Spot Patterns and Redefine Journalism|HEKAs Artificial Intelligence Mattress will help you Sleep Better|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Breakthroughs in technology are taking place at a breakneck pace thanks to machine learning and artificial intelligence. I come across articles from around the globe on a daily basis about how the field of data science is helping researchers and the community make progress towards a brighter future.However, as someone in this ever expanding world, it is daunting to keep up with each and every new technology. This is why we, at Analytics Vidhya, have recently launched AVBytes. We cover the latest breakthroughs, developments, updates and happenings in the world of machine learning, deep learning and artificial intelligence.In the past week, we had a range of developments  a Pandas library to work with large datasets with limited resources, Flippy the AI robot burger cook, Farrago  an online tool that helps you deal with dirty and messy data, Googles TensorFlow code for working with space data, an AI experiment to convert 2D images to 3D (open to the general public), among many others.Scroll down to view the articles and get details on each one of them. Also,Subscribe hereto get AVBytes delivered directly to your inbox daily!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.HEKAs Artificial Intelligence Mattress will help you Sleep Better: We all have bad sleeping habits. But AI is here to help. HEKAs AI mattress adapts in real time to your sleeping position to ensure you get the right amount of sleep every night. The AI revolution continues!              Source: PRNewsfoto/HEKA Inc.                           Source: Yahoo                            Source: dornob                       Credit: Bill Sellers                         Source: ComicBook                       Source: plus2salesforceThe above AVBytes were published from 5th to 11th March, 2018.Whats your take on the developments this past week? Is there anything else you came across that isnt covered here? Get involved and let us know in the comments below!",https://www.analyticsvidhya.com/blog/2018/03/avbytes-ai-ml-developments-this-week-120318/
HEKAs Artificial Intelligence Mattress will help you Sleep Better,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Pandas on Ray, Windows ML, TensorFlow code for Googles AstroNet, An online tool for Dirty Data, etc.|Google has made the TensorFlow Code for AstroNet Available for Everyone|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Weve all been there  sleep deprivation due to a bad mattress, waking up at odd hours, struggling to even get a good nights sleep. A recent study revealed that humans, on average, sleep an hour less at night than we did 50 years ago. Its a global problem but AI might just have the fix for it.HEKA, a company from Delaware, has developed a mattress that uses AI technology to automatically understand and adapt to the sleeping postures and movements of the human body.                                  Source: PRNewsfoto/HEKA Inc.The AI mattress has been built using AI Trackbot, an autonomous furniture adaptation technology. It collects body pressure distribution data in real time to recognise the body shape and sleeping positions. The mattress then automatically adjusts the height and firmness to as per the sleeping position.According to the figures by HEKA (as yet unpublished), out of 362 users, there has been quite a decrease in the average number of times a person tossed and turned throughout the night (from 30 to 12). The average deep sleep of a person increased by more than 50%!Sleeping problems are a major issue globally. With our fast living lifestyles, sleep can get overlooked. Intelligent mattresses have been around in the last few years but their market penetration has been limited to the developed countries.Thanks to the rapid advancement in AI, this mattress could bring about a global change. The initial results have been impressive and the company will expand this to hospitals and for commercial use as well.Its always a pleasure reading about the positive effect of AI in the world and heres hoping the trend continues in years to come. We look forward to this being adopted outside the US soon.",https://www.analyticsvidhya.com/blog/2018/03/ai-powered-mattress-will-help-get-better-sleep/
Google has made the TensorFlow Code for AstroNet Available for Everyone,Learn everything about Analytics|Overview|Introduction|Our take on this,"Where has Google Brain collected the data from?|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|HEKAs Artificial Intelligence Mattress will help you Sleep Better|Volume is Using Machine Learning to Transform 2D Images to 3D|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Back in December 2017, the Google Brain team revealed it had discovered 2 new planets by applying Astronet  its deep neural network model for working on astronomical data. It was a monumental discovery that went to show the far-reaching impacts of machine learning in todays world.Now, Google Brain has released the entire code that went into making that technology and theyve made it available for everyone. The model is based on a convolutional neural network (CNN) and you can see an outline of Googles model in the below image:Note that you will require the below packages to work with the data:NASAs Keplar space telescope of course. It has been scanning the Milky Way for years collecting and analyzing data about planets and stars. The dataset generated from all those years is a data scientistss dream  plenty of outliers making for noisy data and enough unknown elements to keep the best in the business guessing.But at the end of the day, the neural network requires human labeled data to understand, flag and verify the planets and stars. The research team have only worked on 600 out of 200,000 stars observed by Keplar.You can view Googles official blog post about it here and access the entire code on GitHub here.The logic Google has given behind making the code available for everyone is that they hope this will speed up the process of improving the accuracy of their convolutional neural network. Note that it will take a bit of time to download the data because of its gigantic volume.This is one of the most fascinating datasets a data scientist could work with. It does require domain knowledge but the vast nature of the problem and with Googles walk-through on their GitHub page, it could be enough to get you started. So go ahead and dive in!",https://www.analyticsvidhya.com/blog/2018/03/google-has-made-the-code-for-astronet-available-for-everyone/
Volume is Using Machine Learning to Transform 2D Images to 3D,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google has made the TensorFlow Code for AstroNet Available for Everyone|Microsoft Announces Windows ML, an AI Platform for Windows 10|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Remember those 3D masks in the Mission Impossible movies? Using just a 2D image, they were able to effortlessly create them from scratch. Well, thanks to deep learning, we are not too far away from fiction becoming reality. Source: dornobVolume is a tool for reconstructing any 2D image (or video) in a 3 dimensional space. To build a model that understands human movements requires a gigantic amount of data. According to one of the co-developers, Or Fleisher, they are building a neural network classification model to identify the elements in an image that are the most dominant.Volume has opened its tool to the general public  so anyone can go and upload their image(s) to the site. You can try to do it yourself here.The initial resulting images range from impressive to downright funny. As you zoom into the image, you can see how the pixels are broken down into different parts. It makes for fascinating viewing. But as users keep uploading images, Volumes model keeps learning with the increase in the training data.Since this concept is still in its teething stages, the developers are working to continuously improve the understanding and accuracy of the model.Check out the below incredible video to see the tool in action:While the model and the released tool are still in early development, one can imagine this technology being applied in the gaming industry. EVen video editors, currently using an amalgamation of tools, could find this to be an immensely helpful addition.A similar AI was revealed a few months back by researchers in the UK which converted 2D images of faces to 3D. But Volume does a lot more than just facial recognition. It looks at the entire image and attempts to convert it to 3D. That, as we mentioned, takes a massive amount of data to train and improve. Its an audacious project being attempted but it opens the door for others to try it out as well.",https://www.analyticsvidhya.com/blog/2018/03/volume-using-machine-learning-transform-2d-images-3d/
"Microsoft Announces Windows ML, an AI Platform for Windows 10",Learn everything about Analytics|Overview|Introduction|Our take on this,"The technology behind the AI platform|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Volume is Using Machine Learning to Transform 2D Images to 3D|Interview with Margherita Pagani and Clment Levallois, Program Co-Directors, MSc in Digital Marketing and Data Science, emlyon business school|
Pranav Dar
|No Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Microsoft wants to help developers bring their machine learning models from the cloud to their desktop. The AI platform will be launched with the next major update to Windows 10.Rather than taking a circuitous route to the cloud and back, the platform will make direct use of the GPU on your machine so you can run your models in real time. Microsoft has been talking up their machine learning capabilities in the cloud since a long time and with this addition, it completes Satya Nadellas vision for their AI journey.Developers can now build their models in the cloud and integrate them in their desktop applications using Visual Studio and other tools.A project named Onnx, backed by the big tech giants  Microsoft, Facebook and Amazon, is at the heart of this technology. Developers can convert PyTorch, Caffe2 and other models into the Onnx format and move them between different frameworks.Keeping up with the recent trend of automated machine learning tools, Microsoft will also give developers full access to train image recognition models on its Azure Custom Vision Service. The models can then seamlessly be exported to WindowsML. The developer doesnt need to be an advanced machien learning expert to do this  the tool just requires the training data to be properly tagged for it to work.You can read Microsofts full announcement in their blog post here.Microsoft was already using its AI capabilities in their Office 365 suite and their Photos app. We expect the data science community to benefit directly from this release. A lot of the popular machine learning tools and resources are often not found on the Windows platform or are late in arriving, which has been a drawback for data science practitioners.I recently read a comment on LinkedIn from a data science thought leader urging people to forget about Windows when it comes to ML. With this release, Microsoft will hope to make its penetration into the market again and win over the naysayers.",https://www.analyticsvidhya.com/blog/2018/03/microsoft-adding-ai-platform-windows-10/
"Interview with Margherita Pagani and Clment Levallois, Program Co-Directors, MSc in Digital Marketing and Data Science, emlyon business school",Learn everything about Analytics|Introduction,"Share this:|Like this:|Related Articles|Microsoft Announces Windows ML, an AI Platform for Windows 10|Machine Learning is Helping Researchers Create Lifelike Animal Simulations|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"emlyon business school, founded in 1872, stands out among the top few business schools worldwide. It has been awarded three international accreditations and is ranked in the top 100 for Global Employability by the Times Higher Education  University Rankings 2016. Choosing to study at emlyon business school is choosing to study in a resolutely international environment that is fully connected to todays business world.Margherita PaganiClmentLevalloisThe school has recently launched a M.Sc. in Digital Marketing and Data Science course. It is being led by Margherita Pagani and ClmentLevallois, the programs co-directors.Ms. Pagani isa Professor of Digital Marketing at emlyon business school(France) and an Adjunct Professor of Digital Marketing at Bocconi University (Milan).She holds a HDR (Habilitation to Direct Researches) and a Ph.D in Management.Her current research examines digital marketing, consumer behavior and new technologies. Social media, mobile marketing and dynamics influencing value creation and capture in digital ecosystems.She published several books (published in US, Italy, Korea) two encyclopedias and articles in leading international journals and won several awards.ClmentLevalloisis an Associate Professor in the Market and Innovation Department and a computational social scientist.He is a specialist in data mining : data visualisation, text mining, network analysis. He is the creator ofCODAPPS, Coding mobile apps for entrepreneurs, a MOOC hosted on Coursera.He is a also a member of the Gephi Consortium and Director of thedata r&d institute.Here are excerpts of my conversation with both of them.PD: Recently you have launched the MSc in Digital Marketing and Data Science course. Can you tell us briefly about the program?Ms. Pagani and Mr. Levallois: The MSc in Digital Marketing and Data Sciencecourse is a unique format combining 2 essential disciplines digital marketing & data science.As many courses around the world focus nowadays on business intelligence in a global business environment, this program is different and delivers a unique take on a very specific field. This program is designed to grow a new generation of leading marketing specialists  digital savvy professionals that can benefit from an explosive growth of online technologies to develop business.The MSc in Digital Marketing and Data Science is a 16 months program entirely taught in English based in Paris with a learning trip toBostonand one semester inShanghai.PD: Why do you think theres a need for a specialization in Digital Marketing and Data Science?Ms. Pagani and Mr. Levallois:The program has been created in order to adapt to the internet and advances in digitalization and social networking that are currently transforming how companies interact with customers and partners.The program pedagogy uniquely combines a strong academic background in business studies, marketing, data analysis and strategy with an indepth and specific digital knowledge addressing the specific need of companies of all size, activity sector and profile to recruit marketing & data literate managers trained for the digital future. Offering this new Digital Marketing & Data Science program is a response to companies growing digital demands.PD: Who is ideally suited to take this course?Ms. Pagani and Mr. Levallois:Students do not need to have a specific scientific background to follow this course. The program will provide all participants (from various backgrounds) with the necessary strategic and analytical skills to succeedin a digital world. emlyonbusiness school take the applicants entire potential into account. It is very important to consider motivation as an essential element to pursue this course.PD: Are there any prerequisites for the students?Ms. Pagani and Mr. Levallois:Students will need:More information can be found on youremlyon business school account.PD: What will the students gain from enrolling in this program?Ms. Pagani and Mr. Levallois:Students will become accomplished digital marketing professionals, able to manage and innovate in a datarich business environment. You will be wellprepared to work in a sales or marketing department of startups and major brands in business to consumer or business to business environments. You will also fit the requirements of advertising agencies looking for marketing professionals knowledgeable about multichannel communication, consulting firms managing the digital transformation of their clients, as well as digital media and technology companies looking for managers with a strong business background who are also familiar with their trade.The program meets a real need for employers that want to recruit young talents with a dual competency, business and digital, capable of bringing a real plus. The application of big data in the marketer profession is crucial.Throughout the year students have the opportunity to attend conferences and workshops which allow them toget hands-on experience and meet professional experts. PD: How is this particular program different from others that the candidates might consider?Ms. Pagani and Mr. Levallois: In addition to the unique format combining two essential disciplines(digital marketing & data science), it is important to underline that the data science part of the program will entirely be delivered by emlyon business schools professors rather than by an engineering school. This choice means that data science is always considered in a business environment and never out of that context which enable students to really develop their skills according to the reality of the employers needs.PD: Will there be any industry leaders and practitioners who will be teaching?Ms. Pagani and Mr. Levallois: Yes, many professionals will be teaching in this course. For example, Yihun Lim (Researcher/ Architecture Designer and Associate Director at MIT mobile experience laboratory) will share her experience with the new cohort.PD: Do tell us briefly about the subject areas to be covered in this course.Ms. Pagani and Mr. Levallois: Here are a few examples among the many subjects covered during this course:You can find the program structure, which includes all the courses taught each semester, here.PD: Regarding the mix between digital marketing and data science, how much time will be spent teaching each area separately?Ms. Pagani and Mr. Levallois: An equal amount of time will be spent studying both areas during the first semester. In the second semester the students will select elective courses specific to the specific track of their choice (digital marketing or data science). During this semester two courses will be mandatory and their aim is to create a bridge between the two disciplines. In the third semester, students will have the opportunity to choose their specialization (in digital marketing or data science), participate in a project for a company and enlarge their horizons with Asian perspectives. The courses in this third semester will be in Shanghai and the students will have the possibility to further deepen their knowledge in an international environment.PD:Are there any tie-ups with companies to do live projects during the course?Ms. Pagani and Mr. Levallois:Yes, during their last semester in Shanghai, all students will have to complete an in-company project. Combining academic knowledge with practical, real-life business challenges is key to the future success of our students. Therefore, the MSc in Digital Marketing & Data Science includes business projects for a sponsor company and in-company internships.The Business Project is a key module of the Shanghai experience. Students will work for a company on a real project.Working on these business projects will help students to:PD: Is this a full-time or part-time course? How will it be delivered?Ms. Pagani and Mr. Levallois:This is a full-time course that will take place over 16 months. The first two semesters will take place at emlyon business school Paris campus (France). In November you will benefit from a learning trip to Boston: a unique opportunity to discover how and why Boston is a privileged spot to innovate and create value.Students will spend their last semester in Shanghai (China). That semester includes an in-company project (a project, based on a real problem faced by a company in the digital industry). You will work as a consultant for a real company,apply the knowledge gained throughout the program, and benefit from hands-on experience in an international environment.PD: Is there any placement assistance provided to the students?Ms. Pagani and Mr. Levallois: emlyon business school has a specific careers services department that helps you prepare for your future career, and develop your professional network. A team of experts will provide you with continual support in identifying career goals, developing action plans to achieve them as well as support you in your internship and job search.PD: When is the next batch starting and how do candidates enroll themselves?Ms. Pagani and Mr. Levallois:The next intake will be in September 2018.Selection sessions for the 2018 intake run are from November 2017 to July 2018.The first step of the admission process is your online application, which youcan access throughyourpersonal space (program dashboard).
Should your application be complete, you will receive the admission boards final decision within 15 working days.PD:What is the total course fee (including accommodation) for French residents and international students?Ms. Pagani and Mr. Levallois: The tuition fees is 24,000 euros(Pricing is for the 2018 intake. All indicated fees include tuition, enrolment fees and lifelong membership ofemlyon business schoolforever alumni network.)The application fees is 120.All financing information can be found on our website.PD: Thanks, Ms. Pagani and Mr. Levallois for taking out the time for this interview. We hope our community will find the information helpful.
This sponsored post has been written by Analytics Vidhya on behalf of, and with inputs from, emlyon business school.Note: This post was updated on 12th April, 2018 to reflect some changes in the course.",https://www.analyticsvidhya.com/blog/2018/03/interview-margherita-pagani-clement-levallois-program-co-directors-msc-digital-marketing-data-science-emlyon-business-school/
Machine Learning is Helping Researchers Create Lifelike Animal Simulations,Learn everything about Analytics|Overview|Introduction|Our take on this,"How was the model developed?|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Interview with Margherita Pagani and Clment Levallois, Program Co-Directors, MSc in Digital Marketing and Data Science, emlyon business school|Pandas on Ray  A Library to Make Pandas Faster with Just One Line of Code|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"From the rhinoceros in Black Panther to Gollum in Lord of the Rings, humans have always had a fascinating with computer simulated animals. The results are usually varied  but we still havent got to a point where they look exactly lifelike.That might be about to change.Researchers at the University of Manchester have developed and fine-tuned a machine learning algorithm that improves the computer simulations of animals.A report onphys.orgrevealed that the animal being used behind the machine learning model was a chimpanzee.This will not only help researchers improve their understanding of how the locomotion of animals functions, but alsoimprove the technology behind it.                                   Credit: Bill SellersThe idea behind the algorithm was to calculate the energy it takes to walk in a stable fashion, as compared to other movement patterns. A full-body scan of the chimpanzee was taken to create the model. Using this scan, the researchers generated an outline of the skeleton and the skin.The skeletal model was then used to define the joints, muscles and limbs for the simulation. The final step involved analyzing the movement of the animal while it was made to walk.Check out the video of the simulated chimpanzee walking below:This study will be of great help to the animal bio-mechanical research community. Until now, machine learning algorithms were on the basic scale when it came to replicating the movement of animals. This should help speed along the process for everyone.But outside that field, can you imagine how much more realistic those simulated animals will become in movies? From animated features to novel adaptations, this could be a potential game changer once it arrives in mainstream cinema.",https://www.analyticsvidhya.com/blog/2018/03/machine-learning-researchers-create-animal-simulation/
Pandas on Ray  A Library to Make Pandas Faster with Just One Line of Code,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Machine Learning is Helping Researchers Create Lifelike Animal Simulations|Meet Flippy, an AI Powered Robot that can Cook Burgers|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dealing with large scale data has always been a challenging task for data scientists. With limited resources and computational power, it often becomes a daunting experience.Pandas is one of the most commonly used python libraries but using it on a single core to deal with large datasets becomes insufficient. Most users do not want to optimise their entire workflow just to meet the existing hardware requirements; they do, however, want Pandas to run faster regardless of the size of the data.So researchers at Berkeley have come up with Pandas on Ray, a library that wraps Pandas and transparently distributes the data and computation. Its targeted towards existing Pandas users who want their programs to run quicker and and scale better without making huge changes to the code.Ray is basically a flexible and high performance distributed execution framework.According to the researchers, The user does not need to know how many cores their system or cluster has, nor do they need to specify how to distribute the data. Even on a single machine, users can continue using their usual Pandas notebooks but will experience a significant upgrade in processing speed.All the user needs to do is modify the old Pandas import statement in the below format:And youre good to go! Ray is initialized automatically with the number of cores available to you.You can read the official research paper, which includes a dataset and a demo on how to use the library, on Berkeleys blog here.We have also covered another product from Ray, a reinforcement library called RLlib, which you can read about here.While still very much in its nascent stages, this is shaping up to be a very promising library. Heavy datasets always tend to be problematic with limited computational resources, so Pandas on Ray should provide a workaround for that.This is a good alternative to Dask, but not at the same level yet. You can read about the different between Ray and Dask here.It is not available for Windows yet and there is no word on when that might happen. Currently, it can be used on both Mac and Linux machines.Are you planning to use this library? Let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2018/03/pandas-on-ray-python-library-make-processing-faster/
"Meet Flippy, an AI Powered Robot that can Cook Burgers",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Pandas on Ray  A Library to Make Pandas Faster with Just One Line of Code|Commit Assistant, an AI from Ubisoft, can now Predict Errors in the Code Before you Make Them|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Is AI going to take over the kitchen and displace a chefs role?In another incredible development, a robot called Flippy is being employed at CaliBurger, a restaurant in Pasadena, that can cook burgers at an ultra-efficient speed and quality. The reviews so far (of the food) have been overwhelmingly positive.Using thermal vision, 3D and computer vision, and machine learning algorithms, Flippy was created by Miso Robotics and has been trained how to cook and handle the burgers. The deep learning model was built using data about kitchen equipment, the temperatures the grill can heat up to and what the ideal temperature is for cooking the burgers.When the kitchen works put the patties on the grill, Flippy is able to detect where they are.                  Photo by Sarah Reingewirtz, Pasadena Star-News/SCNGThe above burgers were all made by Flippy in a recent demonstration. The robot also manages to clean the spatula used for cooking and wipes the surface of the grill when a batch has been produced.The end result? The burgers are never over or under-cooked. This not only maintains the consistency in each product, but improves the quality of the food and ramps up the efficiency of the restaurant.Check out the below video for a demonstration by Flippy:Thanks to machine learning and AI, we have been seeing changes in the food industry recently (case in point, Amazons Go store). Restaurant owners have long complained that it takes time and money to train chefs, so this will be a welcome addition to their armoury.But will technology like this take over a chefs job in the future? Its tempting to answer yes at this moment, but the nuances of the culinary field are diverse so we will have to wait and watch.What is your take on this? Do you think technology like this should be allowed? Let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2018/03/flippy-ai-robot-cook-burgers/
"Commit Assistant, an AI from Ubisoft, can now Predict Errors in the Code Before you Make Them",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Meet Flippy, an AI Powered Robot that can Cook Burgers|How to create a poet / writer using Deep Learning (Text Generation using Python)?|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence continues its march deep into the gaming territory. Now, theres a new application of AI  predicting bugs in a games code before the developer even makes the error.The French gaming company Ubisoft has released Commit Assistant, an AI to help speed up the development process of games by reducing (and in some cases eliminating) the number of errors in the script.                               Source: ComicBookWeve all played video games where weve seen bugs. No matter how much beta testing goes on before a games release, there are almost always inevitably bugs that make it through the screening process and lead to bad publicity for the game and the company.The developers of this AI trained their model on almost 10 years worth of code from Ubisofts software library. The purpose of doing this was to look at the history of errors made in the code, to learn from them, and to flag them if they crop up in the current coding process.The company claims that testing and correcting bugs manually can cost the company almost 70% of the budget so this will be a welcome change in that respect. However, the AI is still very much in its infant stages. How will the developers react to a machine telling them their code is about to be wrong? These questions will only be answered as the AI continues to get better.The below video demonstrates how the Commit Assistance works behind the scenes:As we mentioned in previous AVBytes articles, AI in gaming is a hot topic these days. This particular AI has the potential to be a game changer however, and not just in the gaming industry.Imagine working on a product code and before you commit an error, its already been flagged so you can make the required changes? Sounds like a magnificent deal to me.The downside here of course is that, like most machine learning models, it requires tons and tons of data to learn what kind of errors were made in the past. This can be expensive and might make its adoption slower than one imagines at this point.",https://www.analyticsvidhya.com/blog/2018/03/commit-assistant-ubisoft-ai-predict-errors-code/
How to create a poet / writer using Deep Learning (Text Generation using Python)?,Learn everything about Analytics|Introduction|Table of Contents|What are text generators?|Different Steps of Text Generation|Experimenting with different models|End Notes,"Importing Dependencies|Loading the Data|Creating character/word mappings|Data preprocessing |Modelling|Generating Text|A more trained model|A deeper model|A wider model|A gigantic model|Learn,Engage,Compete&Get Hired.|Share this:|Like this:|Related Articles|Commit Assistant, an AI from Ubisoft, can now Predict Errors in the Code Before you Make Them|Intela Launches Farrago, an Online Tool to Deal with Dirty Data|
Pranjal Srivastava
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"From short stories to writing 50,000 word novels, machines are churning out words like never before. There are tons of examples available on the web where developers have used machine learning to write pieces of text, and the results range from the absurd to delightfully funny.Thanks to major advancements in the field of Natural Language Processing (NLP), machines are able to understand the context and spin up tales all by themselves.                                                           Source: The VergeExamples of text generation include machines writing entire chapters of popular novels like Game of Thrones and Harry Potter, with varying degrees of success.In this article, we will use python and the concept of text generation to build a machine learning model that can write sonnets in the style of William Shakespeare.Lets get into it!Nowadays, there is a huge amount of data that can be categorized as sequential. It is present in the form of audio, video, text, time series, sensor data, etc. A special thing about this type of data is that if two events are occurring in a particular time frame, the occurrence of event A before event B is an entirely different scenario as compared to the occurrence of event A after event B.However, in conventional machine learning problems, it hardly matters whether a particular data point was recorded before the other. This consideration gives our sequence prediction problems a different solving approach.Text, a stream of characters lined up one after another, is a difficult thing to crack. This is because when handling text, a model may be trained to make very accurate predictions using the sequences that have occurred previously, but one wrong prediction has the potential to make the entire sentence meaningless. However, in case of a numerical sequence prediction problem, even if a prediction goes entirely south, it could still be considered a valid prediction (maybe with a high bias). But, it would not strike the eye.This is what makes text generators tricky!For a better understanding of the code please go through my previous article, where I have discussed the theory behind LSTMs.Text generation usually involves the following steps:Lets look at each one in detail.This is self-explanatory. We are importing all libraries required for our study.Here, we are loading a combined collection of all Shakespearean sonnets that can be downloaded from here. I cleaned up this file to remove the start and end credits, and it can be downloaded from my git repository.The text file is opened and saved in text. This content is then converted into lowercase, to reduce the number of possible words (more on this later).Mapping is a step in which we assign an arbitrary number to a character/word in the text. In this way, all unique characters/words are mapped to a number. This is important, because machines understand numbers far better than text, and this subsequently makes the training process easier.I have created a dictionary with a number assigned to each unique character present in the text. All unique characters are first stored in characters and are then enumerated. It must also be noted here that I have used character level mappings and not word mappings. However, when compared with each other, a word-based model shows much higher accuracy as compared to a character-based model. This is because the latter model requires a much larger network to learn long-term dependencies as it not only has to remember the sequences of words, but also has to learn to predict a grammatically correct word. However, in case of a word-based model, the latter has already been taken care of.But since this is a small dataset (with 17,670 words), and the number of unique words (4,605 in number) constitute around one-fourth of the data, it would not be a wise decision to train on such a mapping. This is because if we assume that all unique words occurred equally in number (which is not true), we would have a word occurring roughly four times in the entire training dataset, which is just not sufficient to build a text generator.This is the most tricky part when it comes to building LSTM models. Transforming the data at hand into a relatable format is a difficult task.Ill break down the process into small parts to make it easier for you. Here, X is our train array, and Y is our target array. seq_length is the length of the sequence of characters that we want to consider before predicting a particular character.The for loop is used to iterate over the entire length of the text and create such sequences (stored in X) and their true values (stored in Y). Now, its difficult to visualize the concept of true values here. Lets understand this with an example:For a sequence length of 4 and the text hello india, we would have our X and Y (not encoded as numbers for ease of understanding) as below:Now, LSTMs accept input in the form of (number_of_sequences, length_of_sequence, number_of_features) which is not the current format of the arrays. Also, we need to transform the array Y into a one-hot encoded format.We first reshape the array X into our required dimensions. Then, we scale the values of our X_modified so that our neural network can train faster and there is a lesser chance of getting stuck in a local minima. Also, our Y_modified is one-hot encoded to remove any ordinal relationship that may have been introduced in the process of mapping the characters. That is, a might be assigned a lower number as compared to z,but that doesnt signify any relationship between the two.Our final arrays will look like:We are building a sequential model with two LSTM layers having 400 units each. The first layer needs to be fed in with the input shape. In order for the next LSTM layer to be able to process the same sequences, we enter the return_sequences parameter asTrue. Also, dropout layers with a 20% dropout have been added to check for over-fitting. The last layer outputs a one hot encoded vector which gives the character output.We start off with a random row from the X array, that is an array of 100 characters. After this, we target predicting another 100 characters following X. The input is reshaped and scaled as previously and the next character with maximum probability is predicted. seq is used to store the decoded format of the string that has been predicted till now. Next, the new string is updated, such that the first character is removed and the new predicted character is included. You can find the entire code on my git repo here. Ive provided the training file, notebooks and trained model weights for your reference.The baseline model, when trained for 1 epoch with a batch size of 100, gave the following output:This output doesnt make much sense. It is nothing but a repetition of the same prediction, as if its stuck in a loop. This is because language prediction models are way too complex when compared to the miniature model that we have trained. Lets try to train the very same model, but for a longer period of time.This time we trained our model for 100 epochs and a batch size of 50. We at least obtained a non-repetitive sequence of characters, which contains a decent number of legitimate words. Also, the model learnt to produce a sonnet-like word structure.However, this model is still not good enough to produce quality content. So now well do what everyone does when a deep learning model is not producing decent results. Build a deeper architecture!A wise man once said: if the model is not doing good, increase the number of layers! Im going to do the same with my model. Lets add another LSTM layer with 400 units followed by a dropout layer of 0.2 fraction and see what we get.The result is interesting. The grammar has enhanced itself, keeping the sonnet structure and punctuation intact. However, this still requires a lot of improvement. Lets try and explore a wider network, one with more number of units.I increased the number of units to 700 on each of the two LSTM layers. This tweak produced the following poetry:This is a little disappointing at first, because the words have lost their meaning. But, whats interesting to note here is that there is some rhyme that is building up. The model is trying to understand poetry after all! But, we cannot compromise with meaningful words, right?Lets put it all together in a one gigantic model.I increased the number of layers to three, each having 700 units and trained it for 100 epochs. The result produced is a magnificent piece of poetry. Take a look:This not only has sensible words, but has also learnt to rhyme. We could have had a more sensible piece of art had the data that was fed into the network been cleaned properly! But as a starting piece, this model has more than done what it was asked. It is way more poetic than most humans could ever get!What makes a text generator more efficient is its capability to generate relevant stories. This is being implemented by many models at the output level, to generate actual language-like text, which can be difficult to differentiate from one written by humans.Andrej Karpathys character level RNN modelisone such masterpiece, a sufficiently trained model on this framework gives some eye-popping results. Also, there are models which can generate clickbaits via an automated process and grab peoples attention! In all, text generators can find great applications, right from creating original art, to regenerating content that has been lost. One revolutionary application of such text generators could be the point where we could train them to write and manipulate code. Imagine a world where computer programs and algorithms can modify themselves, as and when required.Please use the below comments section to ask any questions or leave any feedback.",https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/
"Intela Launches Farrago, an Online Tool to Deal with Dirty Data",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|How to create a poet / writer using Deep Learning (Text Generation using Python)?|AVBytes: AI & ML Developments this week  Google Brains Image Manipulation, Record-Breaking AI, Stanfords ML Model Predicts Poverty|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Its well documented that cleaning up dirty data takes up the majority of a data scientistss time. Its a cumbersome and often tiring process, but it has to be dealt with before the exploration and model building stages. With the recent rise in different sources of data collection, the amount of data is at an all-time high. And its only going to increase.Wellington based company Intela wants to help companies deal with this dirty data. They have launched an online tool, called Farrago, that uses machine learning to clean up messy data. The company claims that the tool can deal with data coming from any source and in any context.For demo purposes, the online tool lets you either upload your own dataset or select from a pre-uploaded dataset. Then,Farrago will suggest meaningful fields (based on initial data analysis) to use in duplication searching.It learns from the responses to enhance its accuracy on your data. The more training the better. Intela recommends a minimum of ten of each type of response. It then organizes everything and allows you to analyse and download the cleaned dataset.You can take a demo run of the tool here to get a general idea of how it works.This could really help advance a lot of initiatives in the field of data science. Data scientists spend thousands of hours trying to deal with and clean up messy data before it can be ready to be analysed and models built on it.In fact, even with the advancements in technology, data cleaning has remained a cumbersome and tricky step. There are times when dirty data holds back the entire data science life-cycle process in an organization.If you, as a data scientist, are unable to clean up the dirty data, or miss a few components in your haste to get to the model building part, it will lead to the wrong computations. Dirty data in will inevitably lead to dirty data out.There have been previous attempts at automating the data cleaning process  including a popular python library called datacleaner. But they have reported bugs and have not caught on in the community. Heres hoping Farrago goes a long way in solving this oft-mentioned issue!",https://www.analyticsvidhya.com/blog/2018/03/intela-launched-farrago-deal-dirty-data/
"AVBytes: AI & ML Developments this week  Google Brains Image Manipulation, Record-Breaking AI, Stanfords ML Model Predicts Poverty",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|Intela Launches Farrago, an Online Tool to Deal with Dirty Data|SketchAR Application Predicts what youre Trying to Draw, using Computer Vision|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence and Machine Learning are making an impact on a daily basis. I dont remember when was the last time a day went by without hearing news about a new product / service coming out using application of data science and machine learning.This obviously is very exciting as a data science professional, but it is also difficult to track these developments on a regular basis. This is why we startedAVBytesfor our community members. Using AVBytes, you can keep yourself updated with latest developments and applications of AI & ML.Some of the exciting developments last week includesGoogle Brains Image Manipulation Algorithm Fooling Humans and Machines,Stanford is Using Machine Learning on Satellite Images to Predict PovertyandAI Beats Human LawyersCheck out the detailed coverage and the articles below. Also,Subscribe hereto get AVBytes in your inbox directly!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.The above AVBytes were published from 26th February to 4th March, 2018.Whats your take on these? What do you think of this new initiative? What would you want us to cover more as part of AVBytes? Let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/03/avbytes-developments-machine-learning/
"SketchAR Application Predicts what youre Trying to Draw, using Computer Vision",Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: AI & ML Developments this week  Google Brains Image Manipulation, Record-Breaking AI, Stanfords ML Model Predicts Poverty|Google Brains Image Manipulation Algorithm Fools Both Humans and Machines|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We covered NVIDIAs python library recently that makes being an artist so much easier. While that requires programming skills and is on a different level altogether, theres an application that makes sketching and drawing a piece of cake.The SketchAR team has developed an application that uses machine learning to help you become an artist. All you have to do, is draw a rough figure of the picture and blindly follow the instructions. The application gets the idea and takes care of the rest.Here is a video for you to understand how it works.I bet youre wondering how it works! What if you dont use a white sheet? How does it differentiate between your hand and the sketch?The developers have provided a detailed explanation of how the algorithm works here.They mentioned that the application uses machine learning and neural networks to analyze and train data. The multiple layers (hand, pen, sheet) are separated, and the algorithm, using computer vision, teaches the camera to distinguish between everything it sees. Check out the below series of images to get a general idea of the underlying process:This provides a clear separation of the surface and other objects, enabling it to focus only on the drawn content. When a user starts to draw, the drawing is mapped to the original picture, placing a virtual object on the surface that the user can trace.This isnt the first attempt at using machine learning to predict what the user is drawing. Back in 2017, Google introduced AutoDraw which used machine learning algorithms to match your attempts to a professional art. An example of this is below:And now Machine Learning is here to teach the newbies to draw, providing a canvas to the non-artists. It is incredible how researchers are using machine learning to influence artworks. Soon there will be a new generation of digital artists.Few years down the line, we can imagine computers bring able to draw like Van Gogh and Picasso!",https://www.analyticsvidhya.com/blog/2018/03/machine-learning-can-help-you-draw/
Google Brains Image Manipulation Algorithm Fools Both Humans and Machines,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|SketchAR Application Predicts what youre Trying to Draw, using Computer Vision|An AI was Trained to Play Q*Bert and it Broke all Records|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The dangers of AI have been well documented recently. This study from Google Brain will only add to that concern.Researchers at Google Brain have developed an algorithm that can manipulate images in such a way that neither humans, nor machines, are able to identify the object in the picture correctly.A deep convolutional network (CNN) algorithm was tested on a slightly manipulated picture of a cat. Incredibly, it mis-identified it as a dog. See the image below for reference  the left frame is an unmodified image of a cat, and the right frame is a slight tweak of the cats face; enough to fool the CNN.More importantly (and disconcertingly), humans were likewise fooled into thinking it was a dog.Previously, it has been easy to trick CNNs into mis-identifying objects in images. The way to mess with them is to introduce a slight distraction in the image. It could be a wrongly placed pixel, white noise, etc. But these instances involved a single image classifier.In this particular study, the developers at Google Brain created this model that can fool multiple systems by generating adversarial images. How did they do this? They added features that are human meaningful, like altering the edges of objects, playing around with the texture, altering the parts of the photo which enhanced the distraction of the object.Some images managed to fool 10 out of 10 CNNs at a time!You can read the research paper on the image manipulation here.If humans are unable to tell the difference between a cat and a dog thanks to an algorithm, its time to take the regulating AI discussion a little more seriously. Experts have expressed concern that this technology could be misused  a politician enhancing his image on social media to look more appealing to the audience, advertisers using it to manipulate the biases in the human brain, etc.However, this is still major progress in the AI field. On the positive side of things, it could be used for making boring photos (government announcements, traffic news, etc. come to mind) a bit more engaging to the audience.",https://www.analyticsvidhya.com/blog/2018/03/google-brains-image-manipulation-fools-both-humans-and-machines/
An AI was Trained to Play Q*Bert and it Broke all Records,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Brains Image Manipulation Algorithm Fools Both Humans and Machines|A Step-by-Step Guide to learn Advanced Tableau  for Data Science and Business Intelligence Professionals|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"After DeepMinds AlphaGo, the competition to make AI technology in order to play and beat games has been intense. The AI developers are given an existing and preset set of conditions and are asked to create the technology to beat it. Its a match made in AI heaven.So it comes as no surprise that a group of three researchers from the University of Freiburg designed one such AI. The system was being designed and trained to play games made by Atari. It came across a bug in the Q*bert game which allowed the system to rack up almost an infinite number of points.In this game, the character jumps from cube to cube. With each jump, the color of the platforms changes and points are rewarded when all the colors are changed. But the AI found a curious flaw. According to the researchers:First, it completes the first level and then starts to jump from platform to platform in what seems to be a random manner. For a reason unknown to us, the game does not advance to the second round but the platforms start to blink and the agent quickly gains a huge amount of points (close to 1 million for our episode time limit).You can read the research paper in full hereand watch the video of the AI wreaking havoc in the game below:This game has been around for years but the very fact that a human never spotted this flaw and an AI did? It bodes well for evolutionary algorithms in the future.An evolutionary algorithm puts algorithms against one another to see which one completes a task in the best possible way. It then adds a few changes to the remaining algorithms to make them better step by step.Researchers love using machine learning on games to test how far they can take their algorithms. This trend will continue for a long time to come and we cant wait to see how far it goes!",https://www.analyticsvidhya.com/blog/2018/03/machine-learning-algorithm-broke-qbert-game/
A Step-by-Step Guide to learn Advanced Tableau  for Data Science and Business Intelligence Professionals,Learn everything about Analytics|Introduction|Table of Contents|1. Advanced Graphs  Visualizing beyond Show Me|2. Introducing R programming in Tableau|End Notes,"1.1 Motion Chart|1.2 Bump Chart |1.3 Donut Chart|1.4 Waterfall Chart|1.5 Pareto Chart|Learn,Engage,Compete&Get Hired.|Share this:|Like this:|Related Articles|An AI was Trained to Play Q*Bert and it Broke all Records|Stanford is Using Machine Learning on Satellite Images to Predict Poverty|
Pavleen Kaur
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"See the data. Show the visual. Tell the story. Engage the audience.Tableau is one of the most popular Data Visualization tools used by Data Science and Business Intelligence professionals today. It enables you to create insightful and impactful visualizations in an interactive and colorful way.Its use is not just for creating traditional graphs and charts. You can use it to mine actionable insights thanks to the plethora of features and customizations it offers.Famous for its ease of use and simple functionalities, making insightful dashboards like the below takes only a few clicks:In this article, we will look at a few advanced graphs that go beyond the drag and drop feature. We will create calculations to dive deeper into our data to extract insights. We will also look at how R can be integrated and used with Tableau.This article assumes that you possess a fair amount of knowledge about using Tableau, such as basic chart formation, calculations, parameters etc. In case you dont, I would recommend referring to the following articles first and then heading back here:Almost all Tableau users are privy to the various elementary graphs, such as those shown in the introductory dashboard. Such charts can be easily made using the Show Mefeature of Tableau. But since this is an article meant for advanced users, we are going to move beyond Show Me and explore graphs that require some extra computations.First, lets take a quick look at what we are going to be making in the next few sections.Below is some basic analysis of the Sales and Profit of our Superstore. Simple graphs will serve the same purpose as those in the dashboard, but I think you would agree that there is something exciting and enrapturing about the grandeur of these charts.Before we begin, have a look atHans Roslings World Economics Representationvisualization. Hit play, and see the magic unfold.Interested in making one of your own now?If you have already started worrying about animation, dont! What you saw is called aMotion Chart. Using this, you can view the changes in your data in real-time.So lets start by downloading the Superstore dataset which can be foundhere.By now making trend lines like the following should be easy for you: But what we are first going to learn in this section is how to make the below trend lines in motion:So lets get started!Suppose you want to explore the Sales of the various segments of the Superstore (for an entire year). One way to do this is the following:While an alternate option could be the below:Although the Line chart managed to show the difference of Sales between each Segment, the Bump Chart (in the above image), gave a more clear and concise picture of the same outcome.Such charts are mostly used to understand how the popularity of a particular product is changing over the years.Lets try and make one of our own now:The chart that you will get wont look like the chart in the dashboard because it lacks the Labels. Lets remedy that quickly, with the help of a Dual Axis:You see Rank and Rank (2) in the Marks Pane? We are going to use these to create those circled Labels.A donut chart is yet another representation of an elementary chart. To put it candidly, its a pie chart with a hole in the middle, but it helps put more emphasis on the various segments, as you can see below:Lets understand the difference as we create this.You must have understood by now that all the above charts, although different in their final looks, were all derived from the core graphs of the Show Me feature. But wait, its not over yet. I have more to show you.A waterfall chart derives its name from its analogous orientation and flow. Here we have plotted the Running Sales of the Superstore over its years, and you can see the two small red areas in the middle of 2013 and the beginning of 2014, indicating that the Sales actually dipped and also the measure by how much.This implies that such charts are used to analyze the cumulative effect of a Measure, and see how it increases and decreases as a whole. To understand this better, lets visualize it.A waterfall chart is a derivative of a Line Chart, so we will begin with this graph:Note: Here the X-axis is Order Date (in Month-Year format and converted to Discrete). And the Y-axis is Profit.The calculated field was used to fill in the space in the Gantt Chart. A negative value in Profit would extend the bar downwards, whereas a positive one would extend it upwards.The length of each small bar in the chart represents the amount of change in Profit from one month to the next.The graph that you will get could be very easily represented in the form of a Bar Chart as well. Do note that I have reversed the colors here, to make the anomalies stand out:But I am sure you would agree that using a Waterfall chart was a more intuitive way of representing the data, especially to see the changes in Measures such as Sales and Profit over the years.Below I have visualized a popular 80-20 principle of data analytics. If you have not heard of it, let me try and explain it with our example. It is often observed that the majority of the sales of a Superstore come from a select few products.One cannot expect bread and eggs to have the same sales figures as cakes, right? This is officially termed as the 80-20 principle, meaning that 80% of the Sales come from 20% of the Products. In our Superstore, this principle can be observed in the below chart, where most of the sales are generated by Phones and Chairs :Quite a popular visualization,Pareto charts are often used for Risk Management to determine the most common problems that are having the most negative impact on a project; but as we will see, it can have other applications too.Lets see how its done:One thing I like about Tableau is that its not just a tool meant to create pretty graphs with mere drag and drop actions.With the release of Tableau 8.1 in 2013 came a plethora of new functionalities. The introduction of R, to enable making richer and dynamic visualizations, was one of the predominant features. R can be used with Tableau for techniques like Clustering, Prediction and Forecasting, to name a few.I wanted to start the exploration of R and Tableau through Clustering, so I used the ultra popular Iris Dataset. It contains different features to distinguish between 3 types of flowers, namely Virginica, Setosa and Versicolor. As you can see in the below image, the R integration quite easily creates clusters of these 3 species:Interested in making this yourself? First lets go through the basics and the installation process, before delving into the visualization!The following depicts the flow of control between Tableau and R to make this integration possible:R scripts are written in Tableau as Table Calculations, which are sent to the R serve package of R.Here the module carries out the necessary computations and returns the result to Tableau.Note:To properly understand and thereby use this feature, you must possess some knowledge of R and its various syntaxes. For the same you may refer to the following tutorial:Learn Data Science in R from scratchNow lets look at the steps for this integration:So now thatyou have the proper ingredients ready, lets start cooking!As was shown in the image above, you make use of Tableaus Table Calculation to communicate with R :If you scroll down the list of functions, you will come across the following four:Tableau automatically understands that the script is meant for R when these functions are included in the calculation area.I hope that your initial excitement of making the clusters is still there! Lets proceed.What we have above is a Scatter Plot, which shows clusters of data points divided into 3 distinct clusters.Lets try doing the same with R now, and compare the two visualizations that we will get. We will be using the most common clustering algorithm, K-Means:Although there are a few overlaps, the two visualisations do appear to be quite accurate.This was a small gist of the potential of integrating R with Tableau. Its applications are limitless, and I am sure you must have already started to think of the different ways you can interact with it. It would be naive of me to say that this is all there is to Tableau. As new versions roll in, so do new functionalities. Not only that, people are always experimenting and exploring Tableau, and coming up with new visuals. There are multiple blogs where people publish their experiments with data too. Do check them out.You can also find new and gorgeous visualizations weekly on Tableaus official Gallery page.I would definitely advise you to keep referring to these posts, creating your own visuals, and sharing it with the community.Stay creative and all the best on your journey as a Data Explorer!",https://www.analyticsvidhya.com/blog/2018/03/tableau-for-advanced-users-easy-expertise-in-data-visualisation/
Stanford is Using Machine Learning on Satellite Images to Predict Poverty,Learn everything about Analytics|Overview|Introduction|Our take on this,"How does the algorithm work?|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Step-by-Step Guide to learn Advanced Tableau  for Data Science and Business Intelligence Professionals|AI Beats Human Lawyers at Contract Reading  By being 200 Times Faster!|
Pranav Dar
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Eliminating poverty is the number one goal of most countries around the world. However, the process of going around rural areas and manually tracking census data is time consuming, labor intensive and expensive.Considering that, a group of researchers at Stanford have pioneered an approach that combines machine learning with satellite images to make predicting poverty quicker, easier and less expensive.Using this machine learning algorithm, the model is able to predict per capita consumption expenditure of a particular location when provided with its satellite images. The algorithm runs through millions of images of rural regions throughout the world. It then compares the presence of light in a region during the day and at night to predict its economic activity. This approach is called transfer learning.Using the images captured during the night, the algorithm cross references it with the day time images to gauge the infrastructure there. In general, a brightly lit area means it is powered by electricity and must be better off than the alternative.Before making its predictions, the algorithm has been made to cross check its results with actual survey data in order to improve its accuracy.So far, this study was performed for regions in 5 countries  Nigeria, Uganda, Tanzania, Rwanda and Malawi. Check out a small video on this study below:Anything that helps eliminate poverty is good in our books and when it comes to machine learning doing the work, even better. Stanford claims that its model predicts poverty almost as well as the manually collected data so that makes it a feasible option for the survey administrators.Its also an open-sourced project and they have made their code available on GitHub here. Its available both in R and python so anyone with an interest in the subject can try it on their own systems.Apart from Stanford, researchers at the University of Buffalo are also using machine learning and satellite images to predict poverty. Their approach differs from Stanfords as they have added cell phone data to their model. The Pentagon is also offering $100,000 to anyone who can read the data from satellite images in the same way that Stanfords model does.",https://www.analyticsvidhya.com/blog/2018/02/stanford-using-machine-learning-satellite-images-predict-poverty/
AI Beats Human Lawyers at Contract Reading  By being 200 Times Faster!,Learn everything about Analytics|Overview|Introduction:|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Stanford is Using Machine Learning on Satellite Images to Predict Poverty|Gartners 2018 Magic Quadrant Ranks Elite Machine Learning Tools|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In another stunning show, an AI technology outperformed human lawyers by reading through a contract with impressive accuracy. All the while doing this at an almost 200 times quicker speed.This study was conducted by legal AI platform, LawGeex. It gave us another glimpse of AIs the immense potential. The technology goes through contracts and flags any arguments or language that seems out of context.Source: LawGeexLawGeex made 20 highly experienced lawyers battle against a trained AI, for a task of spotting errors in a contract. Here are the results:Not only this, the AI achieved a 100% accuracy rate in one particular contract while the best accuracy by a human lawyer was 96%. Incredible!LawGeexs deep learning AI was trained using tens of thousands of Non Disclosure Agreements (NDAs) to reach that level of speed and precision.Lawyers dont need to worry for their jobs (yet). Like any other technology, this has its limitations as well. Lawgeex thinks it should be used by the lawyers, rather than instead of lawyers.This is not the first time AI has been used to help with legal expertise. As we reported earlier, Instraspexions AI can help companies predict if theyre about to be sued. Another case was when technology was used to predict the results of cases judged by the European Court of Human Rights.But it just shows how far the field of Natural Language Processing has come, and how much potential it still has.",https://www.analyticsvidhya.com/blog/2018/02/ai-beats-human-lawyers-reading-contract/
Gartners 2018 Magic Quadrant Ranks Elite Machine Learning Tools,Learn everything about Analytics|Overview|Introduction|Our take on this,"Comparing this years report with the 2017 version:
|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AI Beats Human Lawyers at Contract Reading  By being 200 Times Faster!|Ultimate guide to deal with Text Data (using Python)  for Data Scientists and Engineers|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The question of which tool should I use? is an ever-present in the data science field. With the rapid pace at which new technology is being created, its no surprise that even experienced data scientists are always on the lookout for the next best tool.Keeping that in mind, Gartner has released its annual Magic Quadrant for Data Science and Machine-Learning Platforms report for 2018 this week. This magic quadrant evaluated 16 platforms to help companies to identify the right one for their respective organizations. The two criteria used in the quadrant are:These platforms are the software data scientists use for developing and deploying machine learning solutions. Gartner defines a data science and machine learning platform as:A cohesive software application that offers a mixture of basic building blocks essential both for creating many kinds of data science solution and incorporating such solutions into business processes, surrounding infrastructure and products.Without any further ado, here is the magic quadrant for 2018:H20.ai are the biggest entry in this years version as this is their first time on the list. IBM and Microsoft retail their status as stable platforms.You can view the full report for this year on Gartners site here. They have listed the strengths and cautions for each platform in each quadrant in detail.The report is a comprehensive overview of each platform and will help companies decide which tool suits their needs the most. We love how well defined each company is in the quadrant.H20.ais Driverless AI automated machine learning platform has fueled its rise into the Leaders quadrant.For data scientists, this report is a welcome sight. It not only helps them keep up to date with what tools are available in the industry currently, but with the comprehensive review of each one available, it helps them decide which tools can be leveraged for the different types of analysis and data available.",https://www.analyticsvidhya.com/blog/2018/02/gartners-magic-quadrant-data-science-machine-learning/
Ultimate guide to deal with Text Data (using Python)  for Data Scientists and Engineers,Learn everything about Analytics|Introduction|Table of Contents:||1. Basic Feature Extraction|2. Basic Pre-processing|3. Advance Text Processing|End Notes,"|1.1 Number of Words|1.2 Number of characters|1.3 Average Word Length||1.4 Number of stopwords|1.5 Number of special characters|1.6 Number of numerics|1.7 Number of Uppercase words|2.1 Lower case|2.2 Removing Punctuation|2.3 Removal of Stop Words|2.4 Common word removal|2.5 Rare words removal|2.6 Spelling correction|2.7 Tokenization|2.8 Stemming|2.9 Lemmatization|3.1 N-grams|3.2 Term frequency|3.3 Inverse Document Frequency|3.4 Term Frequency  Inverse Document Frequency (TF-IDF)|3.5 Bag of Words|3.6 Sentiment Analysis|3.7 Word Embeddings|Share this:|Like this:|Related Articles|Gartners 2018 Magic Quadrant Ranks Elite Machine Learning Tools|Mabl Uses Machine Learning to Automate Functional Testing for Developers|
Shubham Jain
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the biggest breakthroughs required for achieving any level of artificial intelligence is to have machines which can process text data. Thankfully, the amount of text data being generated in this universe has exploded exponentially in the last few years.It has become imperative for an organization to have a structure in place to mine actionable insights from the text being generated. From social media analytics to risk management and cybercrime protection, dealing with text data has never been more important.In this article we will discuss different feature extraction methods, starting with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.In addition, if you want to dive deeper, we also have a video course on NLP (using Python).By the end of this article, you will be able to perform text operations by yourself.Lets get started!We can use text data to extract a number of features even if we dont have sufficient knowledge of Natural Language Processing. So lets discuss some of them in this section.Before starting, lets quickly read the training file from the dataset in order to perform different tasks on it. In the entire article, we will use the twitter sentiment dataset from the datahack platform.
Note that here we are only working with textual data, but we can also use the below methods when numerical features are also present along with the text.One of the most basic features we can extract is the number of words in each tweet. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones. To do this, we simply use the split function in python:This feature is also based on the previous feature intuition. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.Note that the calculation will also include the number of spaces, which you can remove, if required.We will also extract another feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model. Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:Generally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.Here, we have imported stopwords from NLTK, which is a basic NLP library in python.One more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.Here, we make use of the starts with function because hashtags (or mentions) always appear at the beginning of a word.Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example,Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words.So far, we have learned how to extract basic features from text data. Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data.So, lets get into it.The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, Analytics and analytics will be taken as different words. The next step is to remove punctuation, as it doesnt add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data.As you can see in the above output, all the punctuation, including # and @, has been removed from the training data.As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries.Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, lets check the 10 most frequently occurring words in our text data then take call to remove or retain.Now, lets remove these words as their presence will not of any use in classification of our text data.Similarly, just as we removed the most common words, this time lets remove rarely occurring words from the text. Because theyre so rare, the association between them and other words is dominated by noise.You can replace rare words with a more general form and then this will have higher countsAll these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective.Weve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, Analytics and analytcs will be treated as different words even if they are used in the same sense. To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous article on NLP for beginners using textblob.Note that it will actually take a lot of time to make these corrections. Therefore, just for the purposes of learning, I have shown this technique by applying it on only the first 5 rows. Moreover, we cannot always expect it to be accurate so some care should be taken before applying it. We should also keep in mind that words are often used in their abbreviated form. For instance, your is used as ur.We should treat this before the spelling correction step, otherwise these words might be transformed into any other word like the one shown below:Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.Stemming refers to the removal of suffices, like ing, ly, s, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library.In the above output,dysfunctional has been transformed into dysfunct, among other changes.Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming.Up to this point, we have done all the basic pre-processing steps in order to clean our data. Now, we can finally move on to extracting features using NLP techniques.N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.Unigrams do not usually contain as much information as compared to bigrams and trigrams.The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher then), the more context you have to work with. Optimum length really depends on the application  if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the general knowledge and only stick to particular cases.So, lets quickly extract bigrams from our tweets using the ngrams function of the textblob library.Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.Therefore, we can generalize term frequency as:TF = (Number of times term T appears in the particular row) / (number of terms in that row)To understand more about Term Frequency, have a look at this article.Below, I have tried to show you the term frequency table of a tweet.You can read more about term frequency in this article.The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if its appearing in all the documents.Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.So, lets calculate IDF for the same tweets for which we calculated the term frequency.The more the value of IDF, the more unique is the word.TF-IDF is the multiplication of the TF and IDF which we calculated above. We can see that the TF-IDF has penalized words like dont, cant, and use because they are commonly occurring words. However, it has given a high weight to disappointed since that will be very useful in determining the sentiment of the tweet.We dont have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearnhas a separate function to directly obtain it:We can also perform basic pre-processing steps like lower-casing and removal of stopwords, if we havent done them earlier.Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data.The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.For implementation, sklearn provides a separate function for it as shown below:To gain a better understanding of this, you can refer to this article.If you recall, our problem was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library), lets check the sentiment of the first few tweets.Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model.Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors.Word2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki, etc.Here, we will use pre-trained word vectors which can be downloaded from theglove website. There are different dimensions (50,100, 200, 300) vectors trained on wiki data. For this example, I have downloaded the 100-dimensional version of the model.You can refer an article here to understand different form of word embeddings.The first step here is to convert it into the word2vec format.Now, we can load the above word2vec file as a model.Lets say our tweet contains a text saying go away. We can easily obtain its word vector using the above model:We then take the average to represent the string go away in the form of vectors having 100 dimensions.We have converted the entire string into a vector which can now be used as a feature in any modelling technique.I hope that now you have a basic understanding of how to deal with text data in predictive modeling. These methods will help in extracting more information which in return will help you in building better models.I would recommend practising these methods by applying them in machine learning/deep learning competitions. You can also start with the Twitter sentiment problem we covered in this article (the dataset is available on the datahack platform of AV).Did you find this article helpful? Please share your opinions/thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/
Mabl Uses Machine Learning to Automate Functional Testing for Developers,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Ultimate guide to deal with Text Data (using Python)  for Data Scientists and Engineers|Mind Reading Algorithm can Reconstruct Images based on our Brain Activity|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Developers have always had headaches with functional testing (the process of testing the front end of the application). Its a tedious process that must be done to ensure the application or product runs without any glitches.Mabl, a startup from Boston, has launched a software that uses machine learning to make the process of functional testing easier for developers. Users will no longer have to write huge lines of code by hand.Instead, developers only have to show the software the workflow they want to test and Mabl does the rest. It will perform those tests and even adapts to the smallest of under interface modifications. The software is able to detect bugs, errors in JavaScript, visual changes, broken links, and a whole lot more, thus making the testing for users as simple as possible.The co-founders of Mabl are Izzy Azeri and Dan Belcher, who previously founded Stackdriver (that was bought by Google). Unsurprisingly, Mabl runs on Googles Cloud platform. The team working under the co-founders uses a wide range of tools, like Google Cloud Functions, BigQuery, CloudML and the Google Kubernetes Engine.Once a user signs up for Mabl, the software will scan the site for potential errors. It also allows the user to create their own tests. The user has to walk the service through a scenario, and the service automatically understands the steps it needs to take to perform the test.If you experiment with a new user interface on your site, Mabls software will perform the same test on it.Currently the service is available as a preview for free. Developers who want to use it can sign up here.For any product, quality assurance is one of the most important elements and Malbs release makes this process simpler, faster and more efficient. One of the things we really liked about the service is the custom testing. Regardless of UI changes, the software adapts itself and performs the testing steps.Mabls release is in a competitive automation testing field which has seen the likes of Selenium, Watir, IBM and others release products in the last few months. It remains to be seen if they will be able to sustain their user base going forward.",https://www.analyticsvidhya.com/blog/2018/02/mabl-automate-functional-testing-machine-learning/
Mind Reading Algorithm can Reconstruct Images based on our Brain Activity,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Mabl Uses Machine Learning to Automate Functional Testing for Developers|AVBytes: Developments this week  Automated Feature Engineering, Baidus voice cloning AI, JupyterLab Release, Googles Heart Disease Predicting AI, etc.|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning continues to help us take a step closer to the AI powered worlds we have seen previously in movies like The Matrix and Blade Runner. We have previously covered a study by Japanese scientists on an algorithm that could potentially read out minds.Now, researchers from the University of Toronto have developed a new algorithm that reconstructs images of what people perceive based on their brain activity. The team, led by Dan Nemrodov, has developed this technique based onelectroencephalography (EEG) data.                                    Source: University of Toronto ScarboroughA few people were chosen for a test run of the algorithm. They were hooked up to the EEG equipment and were shown images of a few faces. Simultaneously, their brain activity was recorded and used to digitally recreate the image that was in the subjects mind.While the results have been impressive, the researchers are more focused on the fact that EEG data can, in fact, be used for this type of image reconstruction. There have been major doubts about EEG in the past but this new study has put them to rest.EEG can capture activity in the millisecond scale. In this study, the team was able to estimate that it takes our brain approximately 0.17 seconds to form a passable imitation of a face we saw earlier.This is not the first study in the field but what makes this stand out is the use of EEG data. Because its relatively inexpensive and portable, it has the potential to reach far ranging industries. It could provide a means of communication for verbally challenged folks and help the police is reconstructing a suspects face, rather than relying on verbal descriptions from eyewitnesses.Deep learning using neural networks continues to assist in more ways than we could possible have imagined a few years ago.",https://www.analyticsvidhya.com/blog/2018/02/machine-learning-algorithm-can-tell-youre-thinking/
"AVBytes: Developments this week  Automated Feature Engineering, Baidus voice cloning AI, JupyterLab Release, Googles Heart Disease Predicting AI, etc.",Learn everything about Analytics,"Subscribe hereto get daily AVBytes in your inbox!|Share this:|Like this:|Related Articles|Mind Reading Algorithm can Reconstruct Images based on our Brain Activity|A Neural Network Algorithm can now Make Movies from a few lines of Text|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Over the last 4 years, Analytics Vidhya has played a huge role in spreading analytics and data science knowledge among professionals and learners. But one of the things, our readers constantly wanted to learn was recent developments in industry i.e. what technology is being applied where to create a real life impact.I am really excited to present youAVBytes!AVBytes will keep you updated with all the major developments and advancements in the world of #AI, #MachineLearning & #DataScience, across the globe!In addition to the news, we will also add our take on that particular development.Subscribe hereto get the updates and start your learning now!Below is a round-up of all the happenings in the last week. Click on each title to read the full article.The above AVBytes were published from 17th to 25th Feb 2018.Whats your take on these? What do you think of this new initiative? What would you want us to cover more as part of AVBytes. Let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/02/avbytes-developments-deep-learning-machine-learning-data-science/
A Neural Network Algorithm can now Make Movies from a few lines of Text,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|AVBytes: Developments this week  Automated Feature Engineering, Baidus voice cloning AI, JupyterLab Release, Googles Heart Disease Predicting AI, etc.|Perform Automated Feature Engineering in Python with Featuretools|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Watch out Hollywood. Machine Learning might be extending its arms to take over the movie making business.A new algorithm developed by researchers can now create a movie from just a few lines of text. Prior to this, images could be created from text and technologies like FAIRs Detectron could identify objects in images. But putting that all together to make a movie  that hasnt been done before.The underlying idea behind the algorithm was devised using machine learning. Its basically a series of neural networks that operates in two stages:So how does the neural network learn? Basically, it sees the video generated to illustrate the action (for instance, a player scoring a goal) alongside the actual video of the player scoring a goal. Then, it is trained to pick the real video. The more data it is fed, the better it becomes in terms of prediction accuracy. The feedback it gives keeps setting a higher standard for the generator network.The developers trained this algorithm on ten types of scenes, including kitesurfing on the sea and playing golf on grass. The results were not that great  it resulted in a grainy video. However, according to a post by Science, a simple classification algorithm correctly guessed the intended action among six choices about half the time.As of today, the videos are 32 frames long. Their size is roughly that of a stamp. When the researchers tried to increase the size, it reduced the prediction accuracy drastically.You can read the research paper on how video is generated from text here.Even though these are very early days for this algorithm, the applications could well be beyond movie making. It could potentially generate training data to help train other neural networks.For example, it could help the autonomous cars train on fatal or dangerous situations it hasnt seen before. It could train robots menial tasks, help in healthcare research, among other varying things.Given how quickly research seems to be progressing in deep learning these days, it should not be long before the algorithm improves to a much more efficient rate with far ranging applications.",https://www.analyticsvidhya.com/blog/2018/02/neural-network-algorithm-making-movies-from-text/
Perform Automated Feature Engineering in Python with Featuretools,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|A Neural Network Algorithm can now Make Movies from a few lines of Text|Baidus Deep Voice AI System can Clone your Voice|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Feature engineering has been at the core of any hackathon winning solution. It has become the defacto go-to option when youre looking to differentiate your solution from the competition. But its often difficult to engineer new features from the dataset youve been given. Its a time (and energy) consuming process.This is where the tool set from Feature Labs comes into play. Developed by the folks at Feature Labs, Featuretools is an open-source framework for automating feature engineering.The company has developed this by using a process called Deep Feature Synthesis (DFS). According to Feature Labs CEO, Max Kanter, DFS creates features from raw relational and transactional datasets, like visits to a website or abandoned cart items, and automatically understands and converts that into a predictive signal. The above image gives you a general idea of how the tool works.It can be integrated into both python 2 and 3. It has been designed to work with common frameworks like Pandas for data preparation and skikit-learn for machine learning.According to their official website, the tool was tested against 1000 data scientists in three world wide competitions. On average, Feature Labs performed as well asas well as top human competitors and only required 1/10th of the time.Early customers of the company include Spanish bank BBVA and developers at MIT. In fact, theyve published a case study on how BBVA used Featuretools to create a credit card fraud detection system. You can view it here.Feature engineering is one of the mose important steps in any machine learning pipeline. Whether its differentiating your ML algorithm in a hackathon, or creating features to mine the most out of your data as an organization, its a critical technique.This release will not only save a lot of time for the user (or company), it will enable them to shift their focus to other areas of the data science life cycle. The fact that its available for python and can be used with common frameworks is a huge plus.",https://www.analyticsvidhya.com/blog/2018/02/feature-labs-launches-python-integrated-software-to-automate-feature-engineering/
Baidus Deep Voice AI System can Clone your Voice,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Perform Automated Feature Engineering in Python with Featuretools|Google Introduces AI powered Auto Ads|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Chinese internet search giant Baidu has developed an AI system that can clone an individuals voice! An year in the making, the text to speech system, called Deep Voice, can generate synthetic human voices using deep neural networks.According to the information shared by Baidu Research, they claim that it takes their trained model just three seconds to replicate and output a persons voice.Baidus research team used voice cloning techniques to develop the AI system which they expect will have noteworthy applications in personalizing human-machine interface. They used a two-pronged approach to build their neural cloning system:Speaker Adaptation and Speaker Encoding ApproachBoth Speaker Adaptation and Speaker Encoding (requiring minimal audio) provide quality performance and can be integrated in the Deep Voice model along with speaker embeddings without having to compromise the quality of the source audio.You can check out some audio samples provided by Baidus Research team which consist of original and synthesized voices. They have also published an official research paper which you can accesshere.Text to speech technology has been around for a while. Googles Deepmind, Adoble and Lyrebird have made significant contribution in the field. Baidu also developed a Text-To-Speech system in 2017 and has since made an exponential growth in the field.As far the uses go for this technology, it can potentially be useful for improving digital virtual assistants like Apples Siri, Amazons Alexa or Google Assistant. The cloning voice technology might also be serviceable in the film industry.One of the major areas where it can be of assistance is healthcare. Baidu claims that its technology will help people, who have lost their voice, to communicate again. Its a bold claim and it remains to be seen if the technology is advanced enough to do this yet.The early reviews from people have been mixed. While it has received some positive reviews for the thought, the execution hasnt been great so far. If you listen to the audio samples provided above, the output doesnt seem to be similar to the voice being cloned. Baidu has asked for a few months more to perfect the technology so the jury remains out on this for now.",https://www.analyticsvidhya.com/blog/2018/02/baidu-ai-system-clone-your-voice/
Google Introduces AI powered Auto Ads,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Baidus Deep Voice AI System can Clone your Voice|Jupyterlab beta Released for the General Public|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google continues to flex its AI muscles. This time, their focus is on the advertising industry with the launch of AdSense Auto Ads.Using machine learning, Auto Ads is a concept of making smart placements and monetization decision on your behalf. All you need to do to leverage this technology is place a piece of code on all your sites pages. After that, Google takes care of the rest.Auto ads analyses your sites pages, finds potential ad placement opportunities, and shows new ads when they have the maximum chance of performing well.According to the official blog post by Google, a few benefits of Auto Ads are:Googles machine learning arm continues to stretch its reach into diverse fields. Its fascinating to think the amount of data that must have gone into creating this technology. Google continues to guard its secrets close to its chest so we might never get an official figure from them.This latest updated from them has been met with cautious optimism in the industry. Given the power Google has over the internet, this will represent a major landscape change in advertising.However, given the amount of fake news and negative publicity happening around the web, publishers are not too keen on handing over control entirely to Google. It would be helpful if Google published a demo video of how this will look once put into practice.",https://www.analyticsvidhya.com/blog/2018/02/google-introduces-ai-powered-auto-ads/
Jupyterlab beta Released for the General Public,Learn everything about Analytics|Overview|Introduction|Our take on this,"First Impressions|Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Google Introduces AI powered Auto Ads|An Introduction to PyTorch  A Simple yet Powerful Deep Learning Library|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It took three years in the making, but finally JupyterLab has been launched for everyone! The JupyterLab team has announced the beta release series of JupyterLab, a web-based user interface for Project Jupyter.It includes all the usual components of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and ultra powerful UI that can be extended through third party extensions.JupyterLab is an extensible environment for interactive computing, specially designed based on the Jupyter notebook architecture, an open-source web application.                     Source: Jupyter blogIn addition to the core JupyterLab developers, more than hundred contributers spent three years to build JupyterLab. With over 11,000 commits and 2,000 releases of npm and Python packages, JupyterLab is made available for regular users.With the exponential growth of data science and machine learning, Jupyter notebook is extensively being used for operations such as data cleaning, data transformation, numerical simulation, statistical modeling, data visualization, etc. JupyterLab provides full support for Jupyter notebooks along with access to custom components, such as text editors or data file viewers.Apart from that, JupyterLab provides a high level of integration between the notebooks and documents. The Jupyter blog also mentions a list of activities that a user can perform, such as, using diverse file formats (like Markdown, Vega, VegaLite etc.) or linking your code console to the notebook, and whole other plethora of options!Apart from this, JupyterLab offers multiple extensions to customise and enhance your notebook. As an independent company or user, you also have the option to develop your own extensions! To know more about this, refer to the JupyterLab Extension Developer Guide.To install JupyterLab on your machine, follow these steps.JupyterLab will make working on remote servers much easier. It also lets a user have terminals, notebooks and file editors all in one browser, thus making it more convenient and suitable.We took JupyerLab for a proverbial spin and below is a summary of what we liked:There are quite a few IDEs floating around for python but with this release, JupyterLab is showing a lot of promise. It will definitely be our go-to IDE for the foreseeable future.After this beta release, the JupyterLab team plans to release version 1.0 later on this year. It will focus on stabilizing the extension development API, user interface improvements, and additional core features. We cant wait to get our hands on that!",https://www.analyticsvidhya.com/blog/2018/02/jupyterlab-available-for-general-public/
An Introduction to PyTorch  A Simple yet Powerful Deep Learning Library,Learn everything about Analytics|Introduction|Table of Contents|An Overview of PyTorch|Diving into the Technicalities|Building a neural network in Numpy vs. PyTorch|Comparison with other deep learning libraries|Case Study  Solving an Image Recognition problem in PyTorch|End Notes,"PyTorch Tensors|Mathematical Operations|Autograd module|Optim module|nn module|STEP 0:Getting Ready|STEP 1: Data Loading and Preprocessing|STEP 2: Model Building|Learn,compete, hackandget hired!|Share this:|Like this:|Related Articles|Jupyterlab beta Released for the General Public|NVIDIAs FastPhotoStyle Library Will Make you an Artist (with Python codes)|
Faizan Shaikh
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every once in a while, a python library is developed that has the potential of changing the landscape in the field of deep learning. PyTorch is one such library.In the last few weeks, I have been dabbling a bit in PyTorch. I have been blown away by how easy it is to grasp. Among the various deep learning libraries I have used till date  PyTorch has been the most flexible and effortless of them all.In this article, we will explore PyTorch with a more hands-on approach, covering the basics along with a case study. We will also compare a neural network built from scratch in both numpy and PyTorch to see their similarities in implementation.Lets get on with it!Note  This article assumes that you have a basic understanding of deep learning. If you want to get up to speed with deep learning, please go through this article first.Also, if you want a more detailed explanation of PyTorch from scratch, understand how tensors works, how you can perform mathematical as well as matrix operations using PyTorch, I highly recommend checking out A Beginner-Friendly Guide to PyTorch and How it Works from ScratchPyTorchs creators say that they have a philosophy  they want to be imperative. This means that we run our computation immediately. This fits right into the python programming methodology, as we dont have to wait for the whole code to be written before getting to know if it works or not. We can easily run a part of the code and inspect it in real time. For me as a neural network debugger, this is a blessing!PyTorch is a python based library built to provide flexibility as a deep learning development platform. The workflow of PyTorch is as close as you can get to pythons scientific computing library  numpy.Now you might ask, why would we use PyTorch to build deep learning models? I can list down three things that might help answer that:A few other advantages of using PyTorch are its multiGPU support, custom data loaders and simplified preprocessors.Since its release in the start of January 2016, many researchers have adopted it as a go-to library because of its ease of building novel and even extremely complex graphs. Having said that, there is still some time before PyTorch is adopted by the majority of data science practitioners due to its new and under construction status.Before diving into the details, let us go through the workflow of PyTorch.PyTorch uses an imperative / eager paradigm. That is, each line of code required to build a graph defines a component of that graph. We can independently perform computations on these components itself, even before your graph is built completely. This is called define-by-run methodology.Source:http://pytorch.org/about/Installing PyTorch is pretty easy. You can follow the steps mentioned in the official docs and run the command as per your system specifications.For example, this was the command I used on the basis of the options I chose:The main elements we should get to know when starting out with PyTorch are:Below, well take a look at each one in some detail.Tensors are nothing but multidimensional arrays. Tensors in PyTorch are similar to numpys ndarrays, with the addition being that Tensors can also be used on a GPU.PyTorch supports various types of Tensors.You can define a simple one dimensional matrix as below:As with numpy, it is very crucial that a scientific computing library has efficient implementations of mathematical functions. PyTorch gives you a similar interface, with more than 200+ mathematical operations you can use.Below is an example of a simple addition operation in PyTorch:Doesnt this look like a quinessential python approach? We can also perform various matrix operations on the PyTorch tensors we define. For example, well transpose a two dimensional matrix:PyTorch uses a technique calledautomatic differentiation. That is, we have a recorder that records what operations we have performed, and then it replays it backward to compute our gradients. This technique is especially powerful when building neural networks, as we save time on one epoch by calculating differentiation of the parameters at the forward pass itself.Source:http://pytorch.org/about/torch.optimis a module that implements various optimization algorithms used for building neural networks. Most of the commonly used methods are already supported, so that we dont have to build them from scratch (unless you want to!).Below is the code for using an Adam optimizer:PyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks. This is where the nn module can help.The nn package defines a set of modules, which we can think of as a neural network layer that produces output from input and may have some trainable weights.You can consider a nn module as the keras of PyTorch!Now that you know the basic components of PyTorch, you can easily build your own neural network from scratch. Follow along if you want to know how!I have mentioned previously that PyTorch and Numpy are remarkably similar. Lets look at why. In this section, well see an implementation of a simple neural network to solve a binary classification problem (you can go through this article for its in-depth explanation).Now, try to spot the difference in a super simple implementation of the same in PyTorch (the differences are mentioned in bold in the below code).In one benchmarking script, it is successfully shown that PyTorch outperforms all other major deep learning libraries in training a Long Short Term Memory (LSTM) network by having the lowest median time per epoch (refer to the image below).The APIs for data loading are well designed in PyTorch. The interfaces are specified in a dataset, a sampler, and a data loader.On comparing the tools for data loading in TensorFlow (readers, queues, etc.), I found PyTorchs data loading modules pretty easy to use. Also, PyTorch is seamless when we try to build a neural network, so we dont have to rely on third party high-level libraries like keras.On the other hand, I would not yet recommend using PyTorch for deployment. PyTorch is yet to evolve. As the PyTorch developers have said, What we are seeing is that users first create a PyTorch model. When they are ready to deploy their model into production, they just convert it into a Caffe 2 model, then ship it into either mobile or another platform.To get familiar with PyTorch, we will solve Analytics Vidhyas deep learning practice problem Identify the Digits. Lets take a look at our problem statement:Our problem is an image recognition problem, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model.So first, download the train and test files. The dataset contains a zipped file of all the images and both the train.csv and test.csv have the name of corresponding train and test images. Any additional features are not provided in the datasets, just the raw images are provided in .png format.Lets begin:a) Import all the necessary librariesb) Lets set a seed value, so that we can control our models randomnessc) The first step is to set directory paths, for safekeeping!a) Now let us read our datasets. These are in .csv formats, and have a filename along with the appropriate labelsb) Let us see what our data looks like! We read our image and display it.d) For easier data manipulation, lets store all our images as numpy arrayse) As this is a typical ML problem, to test the proper functioning of our model we create a validation set. Lets take a split size of 70:30 for train set vs validation seta) Now comes the main part! Let us define our neural network architecture. We define a neural network with 3 layers input, hidden and output. The number of neurons in input and output are fixed, as the input is our 28 x 28 image and the output is a 10 x 1 vector representing the class. We take 50 neurons in the hidden layer. Here, we use Adam as our optimization algorithms, which is an efficient variant of Gradient Descent algorithm.b) Its time to train our modelThe training score comes out to be:whereas, the validation score is:This is a pretty impressive score especially when we have trained a very simple neural network for just five epochs!I hope this article gave you a glimpse of how PyTorch can change the perspective of building deep learning models. In this article, we have just scratched the surface. To delve deeper, you can read the documentation and tutorialson the official PyTorch page itself.In the next few articles, I will apply PyTorch for audio analysis, and we will attempt to build Deep Learning models for Speech Processing. Stay tuned!Have you used PyTorch to build an application or in any of your data science projects? Let me know in the comments below.",https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/
NVIDIAs FastPhotoStyle Library Will Make you an Artist (with Python codes),Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|An Introduction to PyTorch  A Simple yet Powerful Deep Learning Library|Predict the Risk of a Law Suit? Intraspexions Deep Learning Model Makes it Possible|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"NVIDIA has released a python library that will make you want to become an artist.The model takes a content photo and a style photo as inputs. It then transfers the style of the style photo to the content photo. You can see a couple of examples in the below images:In the users manual, the developers have cited two examples to show how the algorithm works. The first is a very simple iteration  you download a content and a style image, re-size them, and then simply run the photorealistic image stylization code:In the second example, semantic label maps are used to create the stylized image. Take a look at the below image to get a general idea of how the labeling process works.Before you use this library, you need to have the below python dependencies:To read more about the details of the algorithm that went into developing this code, you can view the official research paper here.You can access the python code on the librarys official GitHub page here.In their paper, the developers compare their approach to previous attempts (Luan, et all) and for a 1024512 image, they are almost 30-60 times faster! They are also more accurate with their algorithm. The algorithm is being refined behind the scenes and more refinements are expected. Its prety awesome on NVIDIAs part to have made the entire deep learning code accessible to the general public.The only issue here could be with the license this has been released under. Its a non-commercial license (CC BY-NC-SA 4.0 license) which means professional artists cannot sell any of their works made using this library.",https://www.analyticsvidhya.com/blog/2018/02/nvidias-fastphotostyle-python-library/
Predict the Risk of a Law Suit? Intraspexions Deep Learning Model Makes it Possible,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|NVIDIAs FastPhotoStyle Library Will Make you an Artist (with Python codes)|Googles AI can Predict Heart Disease by Scanning our Retina|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Companies lose billions of dollars in litigation suits worldwide every year. Quite often, they are taken by surprise when the court order comes in and are helpless in response.Step in Intraspexion.Intraspexion is a unique company in the legal world. The organization uses predictive analytics and deep learning to identify the risk of a client being sued. Using its patented software system, it aims to predict and prevent potential legal suits.So how does the system actually work? The companys deep learning model runs through the emails throughout the enterprise to identify potential risks while the communication is still internal. The software involves mining and leveraging classification of the existing data to train the deep learning model. Following that, the algorithm is run on all the internal communication (emails). This in turn generates a score that helps the risk team to notify the concerned parties that a potential law suit could be on their hands soon.The company claims to preserve the clients net profits by an average of $350,000 to $450,000 per case.Since some of the software is patented, the technology behind it is a secret. However, on their site they have revealed that parts of the software have been developed using GloVe and Googles TensorFlow.Take a look at a demo of the Intraspexion system at work:Deep learning has penetrated into the legal landscape as well. Given the extensive amount of data lawyers have to sift through on a daily basis, machine learning has huge potential here. Within litigation, research and discovery are two hot fields that are already seeing ML penetration. From companies like DISCO (using predictive analytics for e-discovery) to Ravel Law (it gives lawyers deeper insights into legal data), companies are waking up to the power of machine learning.In that sense, Intraspexion is as unique as it is transcendent. Their software has the capability of saving millions of dollars for firms and sets the precedent for others to follow.",https://www.analyticsvidhya.com/blog/2018/02/predict-risk-law-suit-intraspexions-deep-learning-model-makes-possible/
Googles AI can Predict Heart Disease by Scanning our Retina,Learn everything about Analytics|Overview:|Introduction:|Our take on this:,"Subscribe to AVByteshereto get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Predict the Risk of a Law Suit? Intraspexions Deep Learning Model Makes it Possible|Polisis uses AI to Protect your Online Privacy|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Heres yet another potential breakthrough in the field of healthcare. And Google is at the forefront again.Google has developed an AI algorithm that scans an individuals retina and tells them if theyre at risk of contracting a heart disease. According to an article published in Nature journal Biomedical Engineering, Google researchers trained a deep learning algorithm to predict cardiovascular risk by using data points from 284,335 patients, which included retina scans and medical data.Source: Digital Tech InsiderTo test the algorithm, the deep learning model was applied on two independent datasets of 12,026 and 999 patients. The model was shown images of the retina of individuals who suffered a heart attack within five years of the study and those who did not have a cardiac event. The algorithm was able to predict heart attacks and cardiovascular events 70% of the time, a good accuracy but not good enough to be used for any practical purpose yet.This result is similar to testing methods using a patients blood. Google AI can not only predict heart disease, but also the likelihood of a cardiovascular event, such as a heart attack or stroke. Additionally, the model can tell an individuals age, blood pressure, and whether or not the patient smokes.Googles Lily Peng, an MD and lead researcher on the project, hopes that the AI can be applied to other areas of scientific discovery, including cancer research.The accuracy of the model is a bit low but the optimism is quite high about the future of this project. Ms. Peng admitted that the data used for this study was smaller than expected but as more data points are added, she expects the accuracy of the model to increase. Should the results improve, physicians worldwide will be able to run quick scans to detect any risks of heart diseases in a matter of seconds.However, expectations should be tempered a bit at this point. Ms. Peng admitted that its a matter of a few years, rather than months, before this technology can be made available for practical purposes.",https://www.analyticsvidhya.com/blog/2018/02/google-ai-predicts-heart-disease-eye-scan/
Polisis uses AI to Protect your Online Privacy,Learn everything about Analytics|Overview|Introduction|Our take on this,"Subscribe to AVBytes here to get regular data science, machine learning and AI updates in your inbox!|Share this:|Like this:|Related Articles|Googles AI can Predict Heart Disease by Scanning our Retina|RLlib Library Aims to Make Reinforcement Learning Easier|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Nobody actually reads privacy policies. You see those terms and conditions and without a second glance, the box is ticked and you move on to the actual product. In the millions of sentences, you dont really see what personal information you just agreed to share with the website.               Source: Economic TimesIn fact, a study conducted by a researcher at NYU revealed that only 0.7% of users actually read the privacy policy before click on I Accept!Now, theres a software that can break down a websites privacy policies inside a minute and consequently help you protect your personal information you were unwittingly giving out.Named Polisis (short for privacy policy analysis), this program has been developed by a group of researchers from the Ecole Polytechnique Federale de Lausanne in Switzerland, the University of Michigan and the University of Wisconsin. It informs users which websites (and applications) collect and even sell their personal data. The machine learning model behind this software was trained on over 130,000 privacy policies the developers found on Googles App Store.Its a free piece of software and can be utilized as a browser extension in Firefox and Google Chrome.Polisis can read the privacy policy of any website (that it hasnt seen before) inside a minute and do the following steps:To go along with Polisis, the developers have also created a chatbot called Pribot. If you have any questions on any privacy policy, Pribot is able to come up with the answer. Its like a digitally powered legal assistant. We asked Pribot a question on whether we could opt out of LinkedIns data collection feature:We installed the Polisis extension on Chrome and tested it on LinkedIns policies. Below are the results:It gave us a full plethora of privacy features LinkedIn uses  from our location to what personal informatiton will be shared with advertisers, among other things. I couldnt recommend installing this extension highly enough. In todays day and age of data breaches and a lack of privacy awareness, this will really help keep track of what and where your personal data is being shared.Of course this software isnt the first of its kind. Columbia University and the Carnegie Mellon University have previously developed similar programs but Polisis is the first to be made free and open to the general public.",https://www.analyticsvidhya.com/blog/2018/02/polisis-uses-ai-to-protect-your-online-privacy/
RLlib Library Aims to Make Reinforcement Learning Easier,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Polisis uses AI to Protect your Online Privacy|Baidu has Developed an Algorithm that Might Make Store Managers Obsolete|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Reinforcement learning could be about to get a whole lot simpler thanks to the release of a new library called RLlib. According to the authors, the goal of this library is to help the users in breaking down of the various components that go into reinforcement learning. This will make them more scalable and easier to reuse and integrate.RLlib has been designed to support multiple deep learning frameworks and can be accessed using a simple Python API. Its currently available with the below RL algorithms:According to the official RLlib page, this library aims to provide both performance and composability:PerformanceComposabilityThe authors published a paper on this last month which can be accessed here. In the paper, they put forward the following:We argue for building composable RL components by encapsulating parallelism and resource requirements within individual components, which can be achieved by building on top of a flexible task-based programming model. We demonstrate this principle by building Ray RLlib on top of Ray and show that we can implement a wide range of state-of-the-art algorithms by composing and reusing a handful of standard components. This composability does not come at the cost of performance  in our experiments, RLlib matches or exceeds the performance of highly optimized reference implementations.You can access the GitHub library for RLlib here.Next to deep learning, reinforcement learning is the most followed topic these days. RL learning has been previously described as a data hungry field. So scalability issues are inevitable with the amount of the data that is required to create RL models. RLlib will help in this regard as it aims to break down and scale the process for the user.",https://www.analyticsvidhya.com/blog/2018/02/rllib-library-aims-to-make-reinforcement-learning-easier/
Baidu has Developed an Algorithm that Might Make Store Managers Obsolete,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|RLlib Library Aims to Make Reinforcement Learning Easier|Top 5 Data Science & Machine Learning Repositories on GitHub in Jan 2018|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Cashiers were the first casualty of Amazons AI powered Go store launched last month. On the other side of the Pacific, Chinese search engine Baidu is making store managers sweat for their jobs. Source: Getty ImagesThe ability to predict how much a food item will be consumed on a specific day is usually something an experienced store manager does in todays world. But with Baidus latest experiment using a machine learning algorithm, this job may well become obsolete.With historical data from 70 metrics including store food purchases, sales, the weather and festivals, Beijing-based Baidu said it has developed a model that can predict store sales for the next day as a reference for the store manager to decide on the quantity of food products, such as rice boxes and sandwiches, needed to meet the customer demands without creating excess waste.Using data from over 70 metrics, Baidu has developed a ML model to predict the sales in the store for the following day. The metrics that are being tracked include food purchases, sales, weather, and festivals, among others. The model achieves a two-fold use for the store manager:This experiment was conducted over a period of 10 days at 10 convenience stores in China. According to Baidu, the model helped increase the average profit across these store by 20% and cut down on the waste by 30%. Mr. Liu Yongfeng, the senior project manager of Baidus deep learning platform, revealed that they are planning to embed this technology across 200 more stores in Chinas Wuhan city this year.This is a slightly different take in the retail space  rather than focusing on the customer, Baidu are intent on solving an industry level problem. Amazon, Bingo Box, etc. have opened stores that help the customer avoid queues using facial and voice recognition. Baidu, on the other hand, are focusing on the back end. Fresh food, if not consumed within a stipulated amount of time, has to be thrown away. Using their model, the company is aiming to reduce the excess waste and improve the bottom line of each store.Another issue in this field is the high turnover of store managers. Quite often, they leave without passing on their entire knowledge to the successor. This technology will help even a not-so-experienced person take over and ensure the stores are replenished with the optimum quantity.",https://www.analyticsvidhya.com/blog/2018/02/baidu-developed-algorithm-might-make-store-managers-obsolete/
Top 5 Data Science & Machine Learning Repositories on GitHub in Jan 2018,Learn everything about Analytics|Introduction|Detectron|DeepReinforcementLearning|Caire|Minigo|Alpha Pose|VisualDL|TensorFlow Project Template,"Share this:|Like this:|Related Articles|Baidu has Developed an Algorithm that Might Make Store Managers Obsolete|Highlights from the rstudio::conf 2018|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Breakthroughs in data science and machine learning are happening at a break-neck pace. If you are working in this field, its extremely important to keep yourself updated with whats new.Following GitHub repositories is one such way to do so. You can see the latest developments, interesting projects and their applications. I can not tell how much learning can happen through this.You can download the code and run it on your own machine or simply just keep it as a reference point for your project. Whatever the application, GitHub communities are invaluable resources.In this post, we look at 5 GitHub repositories created in January 2018 that you must follow. This is part of a series from Analytics Vidhya that will run every month.Detectron is a software system developed by Facebooks AI Research team (FAIR) that implements state-of the art object detection algorithms. It is written in Python and leverages the Caffee2 deep learning framework underneath.Along with the Python code, FAIR has also released performance baselines for over 70 pre-trained models. Once the model(s) is trained, it can be deployed on the cloud and even on mobile devices.Detectron has been covered by us here.This is a replica of the AlphaZero methodology developed in Python. The author has written the code to train an algorithm to play the Connect4 game. Its not quite as complex as the famed Go game, but there are 4,531,985,219,092 possible game positions so its perfect for this situation.The main advantages of this repository are two-fold, namely:Run it and you will see the beauty in AlphaGo!Caireis a content-aware image resizing library. Currently, most applications either give you the option of cropping an image or changing its aspect ratio. This often leads to either the main parts being left out or the image becoming blurred. This is where Caire comes into play.It has support for both shrinking and enlarging any image, resizing it horizontally or vertically and does not require any third party library. It uses edge detection to generate an energy map of the image. Based on that, it finds seams in the image and uses its algorithm accordingly. The process of how this works has been illustrated in the three images below:It is based on theSeam Carving for Content-Aware Image Resizingpaper. This has been covered by Analytics Vidhya here.Covered by Analytics Vidhya here, this is an open-source Python implementation inspired by DeepMinds AlphaGo. Its a Neural Network based AI, developed using Tensorflow.                               Source: WIREDThe goals of this project, as described by the authors, are listed below:You can access the entire Python code on this GitHub repository.Alpha Pose is a remarkably accurate tool to estimate the poses of multiple people (you can see this in their GitHubs GIFs). Its the first open-source systems that has achieved 70+ mAP on the COCO dataset 80+ mAP on the MPII dataset. Additionally, the authors have also developed Pose Flow, which is an online pose tracker.And here are two bonus repositories for you!VisualDL is a tool that can visualize the entire deep learning process for us. Its an incredibly powerful visualization tool that helps us design deep learning jobs. VisualDL was built to support Python. Just by adding a few lines of Python code and inserting them into our neural network model, we can generate plenty of visualizations to understand the framework. VisualDL has also been written in low level C++.Currently, VisualDL provides four components (more will be added soon):You can read more about these components, and how VisualDL works, in our post here.There are a ton of things to do when starting a TensorFlow project. The underlying idea behind this repository is to wrap up thonse things into a simple and well-defined structure. The TensorFlow Project Template combines simplicity, best practices for creating and maintaining folder structure and excellent OOP design.Do you know of any other repositories created last month that we should be aware of? Feel free to let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/02/top-5-github-repositories-january-2018/
Highlights from the rstudio::conf 2018,Learn everything about Analytics|Day 1|Day 2|Our take on this,"Share this:|Like this:|Related Articles|Top 5 Data Science & Machine Learning Repositories on GitHub in Jan 2018|This Job Site uses Machine Learning to Find Qualified and Deserving Candidates|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The rstudio::conf 2018 was held in San Diego two weeks ago. Most of the slides and materials presented have now been shared by the facilitators. While the focus was understandably on deep learning, quite a few other interesting packages were shared during the event.The conference was held over a two day span and we have highlighted the most exciting things from each day in this articleThe first day of the conference had a heavy focus on the tidyverse world. The keynote speaker was Diane Cook on the topic To the tidyverse and beyond: Challenges for the Future in Data Science. The takeaway from her talk was that tidy data provides the glue from raw data to the data in the statistics textbooks and it will continue to help in various fields in the future.Davis Vaughan presented on The future of time series and financial analysis in the tidyverse where he revealed a couple of packages which, as the name suggests, make it far easier to deal with messy time series and financial data. Keeping the theme of finance going, Emily Riederer has created a package called tidycf which makes dealing with cash flow analysis a whole lot simpler and interpretable.Emily Robinson shined some light on The lesser known stars of the tidyverse. The presentation looks at some of the ways of tidying your data using not-so-well known tidyverse functions. You can view her presentation slides here.A few of the other talks on day 1 included:You can watch the entire days video below:The second day had a heavy dose of deep learning in R. The keynote on this day was Machine Learning with TensorFlow and R presented by JJ Allaire. He kicked things off with a tour of the basic of tensors and introduced the tensorflow package in R.Mr. Allaire wrapped up his talk by demoing various ways of deploying tensorflow and keras models, including publishing them directly to RStudio Connect.This was followed by Googles Michael Quinn. His talk was on Large scale machine learning using TensorFlow, BigQuery and CloudML Engine within RStudio. Once youve developed a tensorflow or keras model, you can then deploy this to Googles CloudML. This can be accomplished using the cloudml package.Keeping the theme of deep learning going, Javier Luraschi (from RStudio) gave a talk on Deploying TensorFlow models with tfdeploy. The tfdeploy package provides a unified way of deploying models directly to various platforms including CloudML and RStudio Connect.One of the more intriguing talks of the conference was by Ali Zaidi on Reinforcement learning in Minecraft with CNTK-R. Mr. Zaidi demonstrated how he trained a deep-learning model to control a character in the popular video game Minecraft. The character was taught how to navigate a puzzling maze as well as understand a few tidbits of natural language. The package used for training the model was CNTK-R.Some of the other fascinating talks included:Watch the entire days video here.All the material that was presented at the conference can be found on Github here. It will be updated with the recordings of each session as well once available.Deep learning was the general theme running throughout day 2. It has made significant strides in the world of machine learning and given its increasing influence, it came as no surprise that plenty of packages are being created for this purpose. R has a plethora of supporting packages for TensorFlow which is quite encouraging for people using this language.Apart from that, the tidyverse world got a lot of love from the presenters and new ways of tidying up messy data were shown. For R users, this conference is something you should absolutely check out using the links above.",https://www.analyticsvidhya.com/blog/2018/02/highlights-rstudio-conference-2018/
This Job Site uses Machine Learning to Find Qualified and Deserving Candidates,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Highlights from the rstudio::conf 2018|Neural Networks on all Battery Powered Devices Could Turn into Reality|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Finding the right talent is a headache for hiring managers. With so many job applications to sift through, the manual process often overlooks deserving candidates and in todays ultra-competitive job market, is not a very reliable or efficient mode of hiring.A plethora of AI powered startups have sprung up recently offering solutions to hiring companies. Out of these, a company called Uncommon, aims to filter candidates who come from diverse backgrounds and reduces human bias.Uncommon has used machine learning to build its unique candidate filtering model. Their ML model has been trained on over 50 million resumes and over 6 million job descriptions to predict and identify the most deserving and qualified candidates. Uncommons founder, Amir Ashkenazi, claims that the model can make predictions with 95% accuracy, a rate far better than any human can manage.The company claims that for each job offering, it will only recommend 100% interested and qualified candidates. The process works as below:Uncommon has so far partnered with brands such as Amazon, Google, Lyft, among others.Check out how Uncommons process works in the below video:This is quite a promising entry into a very competitive market. The fact that it can reduce a humans unconscious bias is a major selling point for the company. The company is working on a Cost-Per-Interested and Qualified model, which means you only pay for qualified applicants. It seems to be a good strategy but it remains to be seen if they can distinguish themselves from the herd in the near future.",https://www.analyticsvidhya.com/blog/2018/02/job-site-machine-learning-uncommon/
Neural Networks on all Battery Powered Devices Could Turn into Reality,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|This Job Site uses Machine Learning to Find Qualified and Deserving Candidates|10 Free Must-Read Machine Learning E-Books For Data Scientists & AI Engineers|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most neural network models that we know of are huge and computationally heavy, which means they consume a lot of energy and are not practical for handheld devices. Almost all the smartphone applications (like speech and facial recognition programs) that rely on neural networks simply upload their data to cloud servers. Its processed there and the result is sent back to the app.                           Source: MediumMIT researchers have come up with a special chip that increases the speed of neural network computations by three to seven times over its predecessors. More impressively, it claims to reduce power consumption by an amazing 94-95 percent. This new study will make it far more easier and practical to run NNs directly in the smartphone apps. The chip can even be embedded into other household applications like the fridge, blenders, etc.The development of this chip was led by Avishek Biswas, an MIT graduate student in electrical engineering and computer science. In an interview with MIT, Mr. Bisvas said:Since these machine-learning algorithms need so many computations, this transferring back and forth of data is the dominant portion of the energy consumption. But the computation these algorithms do can be simplified to one specific operation, called the dot product. Our approach was, can we implement this dot-product functionality inside the memory so that you dont need to transfer this data back and forth?Mr. Bisvas unveiled the chip this past week at the International Solid State Circuits Conference. You can read about the technology behind the neural networks on MITs site here.This is a very important breakthrough because it means that the smartphones and other portable gadgets in the future can perform deep learning applications (like advanced speech and facial recognition) directly on the device, instead of using rudimentary algorithms or sending the data to the cloud and waiting for the results. We will no longer have to worry about our data going to third party apps or creating bandwidth traffic. Once this technology goes commercial, expect a lot of companies to leverage it in most electronic devices.",https://www.analyticsvidhya.com/blog/2018/02/mit-neural-network-chip/
10 Free Must-Read Machine Learning E-Books For Data Scientists & AI Engineers|Statistics|Basic Machine Learning and Statistics|Advanced Machine Learning,Learn everything about Analytics|Introduction|Think Stats  Probability and Statistics for Programmers|Bayesian Reasoning and Machine Learning|An Introduction to Statistical Learning|Understanding Machine Learning|A Programmers Guide to Data Mining|Mining of Massive Datasets|A Brief Introduction to Neural Networks|Deep Learning|Natural Language Processing with Python|Machine Learning Yearning,"Share this:|Like this:|Related Articles|Neural Networks on all Battery Powered Devices Could Turn into Reality|Googles Smart Reply is Coming to Popular Chat Apps|
Pranav Dar
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So you love reading but cant afford to splurge too much money on books? Quite a lot of the data science and machine learning books out there fall in the expensive category. Its only fair, given how much thought and effort goes into writing and publishing them.But there are a few kind souls who have made their work available to everyone..for free! If you want to become a data scientist or AI Engineer  you couldnt have asked for more.Here is a collection of 10 such free ebooks on machine learning. We begin the list by going from the basics of statistics, then machine learning foundations and finally advanced machine learning.To access the books, click on the name of each title in the list below.Author: Allan B. DowneyThink Stats is an introductory book to statistics and probability for people with a basic background in Python programming. Itsbased on a Python library for probability distributions (PMFs and CDFs). To make things easier for the reader, most of the exercises have short programs.The book also includes a case study using data from the National Institutes of Health.One of the stand-out features of this book is it covers the basics of Bayesian statistics as well, a very important branch for any aspiring data scientist.Author: David BarberSpeaking of Bayesian statistics, this one is a classic. This takes a Bayesian statistics approach to machine learning. A book worth checking out for anyone getting into the machine learning field.Authors: Gareth James, Daniela Witten, Trevor Hastie and Robert TibshiraniOne of the most popular entries in this list, its an introduction to data science through machine learning.This book gives clear guidance on how to implement statistical and machine learning methods for newcomers to this field. Its filled with practical real-world examples of where and how algorithms work.For those with an inclination towards R programming, this book even has practical examples in R. In case youre not a programmer, dont let that put you off. This book is a gem.Authors:Shai Shalev-Shwartz and Shai Ben-DavidThis book gives a structured introduction to machine learning.It looks at the fundamental theories of machine learning and the mathematical derivations that transform these concepts into practical algorithms. Following that, it covers a list of ML algorithms, including (but not limited to),stochastic gradient descent, neural networks, and structured output learning.Author:Ron ZacharskiWhat I like about this book are the chapters covering recommendation systems. It takes a fun and visually entertaining look at social filtering and item-based filtering methods and how to use machine learning to implement them. Other concepts like Naive Bayes and Clustering are also covered. There is a chapter on Unstructured text and how to deal with it, in case you are thinking about getting into Natural Language Processing.Examples in Python are also available in case you want to practice.Authors: Anand Rajaraman and Jeffrey David UllmanAs the era of Big Data rages on, mining data to gain actionable insights is a highly sought after skill.This book focuses on algorithms that have been previously used to solve key problems in data mining and which can be used on even the most gigantic of datasets.Author: David KrieselIf youre interested in neural networks, this book is for you. It starts off by covering the history of neural networks before deep diving into the mathematics and explanation behind different types of NNs. The author expects the reader to have a background of basic linear algebra and calculus.Authors:Ian Goodfellow, Yoshua Bengio and Aaron CourvilleThis is probably one of the most comprehensive book written by distinguished people in deep learning field. Concepts like Monte Carlo Methods, Recurrent and Recursive Nets, Autoencoders and Deep Generative Models (among others) are covered in detail.Authors: Steven Bird, Ewan Klein, and Edward LoperFolks interested in getting into Natural Language processing should read this book. Its written in a lucid and clear manner with extremely well-presented codes in Python. Readers are given access to well-annotated datasets to analyse and deal with unstructured data, linguistic structure in text, among other NLP things.Author: Andrew NgNo machine learning list is complete without mentioning Andrew Ng. According to him, this book will help the reader get up to speed with building AI systems. It will effectivelyteach you how to make the various decisions required with organizing a machine learning project.The book is still being updated regularly and you can sign up on the site to receive updates as each chapter is posted.We hope you found this list helpful. In case you know of other free books that youve read, or are planning to read, let us know in the comments below.",https://www.analyticsvidhya.com/blog/2018/02/10-free-must-read-machine-learning-e-books/
Googles Smart Reply is Coming to Popular Chat Apps,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|10 Free Must-Read Machine Learning E-Books For Data Scientists & AI Engineers|Visualize the Entire Deep Learning Process thanks to VisualDL|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google has been using machine learning to improve, modify and create features for a long time now. Recently, Area 120, an experimental program by Google, came up with an idea for a new experiment. Using machine learning algorithms, they are modifying the Smart Reply feature to be extended into all popular chatting mobile applications.Smart Reply means our smartphone will scan all incoming messages or emails and give us several response options. Depending on the requirement, we can choose any one. As users, we dont need to type anything. Its just a one-click operation. This feature has been in operation in Gmail and Googles Allo app for a while.The replies can also take into account the users location for framing several responses. For example, if the text message is Are you at the restaurant?, and your location is at the restaurant, it will suggest responses like yes, yes, I am here, I am indeed. Incredible, isnt it?         Source: TechnoskepticGoogles servers will scan your e-mails, generate three short responses, and add them at the bottom of your screen or just above the keyboard. You can select one of the replies, edit it if required, or send it directly. Keeping in mind an individuals safety, the messages or the location are only scanned by machine and no person is reading them.Smart Reply utilizes machine learning to suggest replies that are used by you more often, that is, it will understand from your previous responses, and prepare the next ones. We have had this feature in Googles Gmail for over two years. Googles neural networks, which are programmed to learn behaviors through training, do the work. Area 120 is bringing it in for other chat applications such as:A point to note here is that this feature will not be added into each chat application. Instead, a new app called Reply will use the existing API to read the text and images in your messages and produce a response accordingly. It will be available on Googles Play Store soon.The Reply app will be limited to Android devices for now (sorry Apple users). Having personally used the Smart Reply feature to quickly reply to e-mails previously, I cannot wait to see it in action in chat applications. Time is at a premium for most of us in todays world and this excellent addition from Google will save plenty of it.",https://www.analyticsvidhya.com/blog/2018/02/google-smart-reply-coming-all-apps/
Visualize the Entire Deep Learning Process thanks to VisualDL,Learn everything about Analytics|Our take on this,"Graph|Scalar|Image|Histogram|Share this:|Like this:|Related Articles|Googles Smart Reply is Coming to Popular Chat Apps|Amazon Plans to Design AI chips for Alexa|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A Python library, called VisualDL, can now visualize the deep learning process for us. Its an incredibly powerful visualization tool that helps us design deep learning jobs.                                Source: VisualDLMost deep neural networks these days are using Python as their primary language. Keeping this in mind, VisualDL was built to support Python. Just by adding a few lines of Python code and inserting them into our neural network model, we can generate plenty of visualizations to understand the framework.VisualDL has also been written in low level C++.Currently, VisualDL provides four components (more will be added soon):Lets look at these in a bit more detail.This is compatible with the Open Neural Network Exchange. In fact, VisualDL is compatible with most deep neural network frameworks, including PyTorch.One of the most useful components, scalar shows us the error trends during the training of the model.The image component can be utilised to visualize any tensor or intermediate generated image.Histogram is used to visualize the distribution of parameters and trends for any tensor.Just write the below code to install VisualDL in Python:To give it a quick test run, run the below code:You can access the source code and other details about this library on their official GitHub page here.This is a pretty fascinating tool that gives the user a deeper insight into the deep learning process. Visualization has always been a go-to technique for data scientists when theyre stuck on an error, or want to understand what exactly is going on behind neural network theyre training. This wonderfully adaptive tool is a dream for them. And the even more exciting part is that new features are being added as we write this. Definitely one to look out for and we highly recommend you follow the page on Github.",https://www.analyticsvidhya.com/blog/2018/02/visualize-entire-deep-learning-process-thanks-visualdl/
Amazon Plans to Design AI chips for Alexa,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|Visualize the Entire Deep Learning Process thanks to VisualDL|Learn Audio Beat Tracking for Music Information Retrieval (with Python codes)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Alexa, an intelligent personal assistant, developed by Amazon, lets users perform various tasks such as making to-do lists, setting alarms, streaming podcasts etc. It is also capable of voice interaction and can provide information on weather or give news updates. The list of functions that Alexa can perform is enormous.To add to this list and to improvise the functioning of Alexa, Amazon is now planning to design an AI chip for speeding Alexas responses and to work on hardware powered by Amazons Alexa. The AI chip by Amazon will be designed to help its voice-enabled products more efficiently handle tasks. The idea is to perform more data processing on the device than in the cloud. Since maximum data processing will be done on the device, Alexa would be able to parse the users voice pattern against commonly used phrases on the device itself, without having to bounce up to the cloud and back. This could improve Alexas Ability to infer and respond faster to the commands.Another benefit of having data process on the device is that it will be able to perform certain functions without Internet connection. Also, the users personal information can be now saved on the device. Additionally, the process would be simplified as accessing the cloud every time will not be required.According to the reports which first surfaced on The Information , the AI chips will also be used for Amazon Web Services (AWS), which would be geared toward machine training, to allow users a more intuitive experience. The technical specs for the chip itself werent detailed, but the report noted that Annapurna Labs, an Israeli chip maker has been tasked with the chips development.SourceGoogle and Apple already have custom hardware and Amazon is beginning to develop one to stay competitive in the smart home hardware market. Alexa made performing basic tasks extremely simple and with the AI chip, it will be faster and more efficient. As proposed, most of the data processing will be performed on the device itself  even without the internet. We could expect that few years down the line, AI systems will be so small that they could even fit in the palm of your hands!",https://www.analyticsvidhya.com/blog/2018/02/amazon-plans-design-ai-chips-alexa/
Learn Audio Beat Tracking for Music Information Retrieval (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Overview of Beat Tracking|Approaches to solve Beat tracking|End Notes,"What is Beat tracking?|Applications of Beat Tracking|Challenges in beat tracking|Approach 1: Onset Detection and Dynamic Programming|Approach 2: Ensemble of Recurrent Neural Networks coupled with Dynamic Bayesian Network|Step 1: Preprocessing audio signal|Step 2: Training and Fine Tuning the RNN models|Step 3: Choosing the best RNN model|Step 4: Applying Dynamic Bayesian Network|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Amazon Plans to Design AI chips for Alexa|A Robot called Erica set to become News Anchor in Japan|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Music is all around us. Whenever we hear any music that connects to our heart and mind  we lose ourselves to it. Subconsciously, we tap along with the beats we hear. You must have noticed your foot automatically follows along with the beats of that music. There is no logical explanation as to why we do this. Its just that we fall into the groove of the music and our mind starts to resonate with the tunes.What if we could train an artificial system that could catch the rhythm as we do? A cool application could be to build an expressive humanoid robot thatruns a real-time beat tracking algorithm, enabling it to stay in sync with the music as it dances.Looks interesting right?In this article, we will understand the concept of beats, and the challenges we face trying to track it. Then we will go through the approaches of solving the problem, along with the state-of-the-art solutions in the industry.Note : This article assumes that you have basic knowledge of audio data analysis in python. If not, you can go through this article and then have a go on this.Audio beat tracking is commonly defined as determining the time instances in an audio recording, where a human listener is likely to tap his/her foot to the music. Audio beat tracking enables the beat-synchronous analysis of music.Being an important and relevant MIR task, research has been rampant in this area. The goal of the automatic beat tracking task is to track all beat locations in a collection of sound files and output these beat onset times for each file.To give you an intuition of the task, download and listen to the below audio file:Now check out the audio below which has annotated timings for the respective beats.There are numerous applications of beat tracking, but the most intriguing for me is beat synchronous light effects. Check out the video below for a real life demonstration.On a more technical note, beat tracking could be used to for music information retrieval tasks such as music transcription. For example, you can annotate the beat of drums  so that it can then be shared with other music creators and enthusiastsOther applications of beat tracking include:We now have a gist of what audio beat tracking is. So let us have a look at the approaches that are used to solve the challenge.There is an annual evaluation campaign for music information retrieval algorithms, coupled to the ISMIR conference, called Music Information Retrieval Evaluation eXchange (MIREX). It has a task called audio beat tracking. Researchers participate in MIREX and submit their approaches. Here I will explain two of those approaches. The first one is simple and the most primitive, whereas the second one is state-of-the-art.Suppose we have an audio such as this given below:What we can do is, find out the locations where there is a sudden burst of sound (aka onset) and mark those moments of time:This is most likely to be the representations of the beat. But it would contain many false positives, such as vocal sound of a person or background noise. So to minimize these false positives, we can find the longest common subsequence of these onsets to identify the beats. If you want to get into the details of how dynamic programming works, you can refer this article.We will look at an implementation below to have a more clearer perspective.Instead of relying on sound cues manually, we can use a machine learning/deep learning approach. Shown below is an architecture of the framework for solving beat tracking. To get into the nitty-gritty of it, you can read the official research paper.The gist of the approach is this  we preprocess the audio signal, and then use recurrent neural network to find out the most probable values of these beat times. The researchers additionally use a committee of Recurrent Neural Networks (RNN), and then ensemble their outputs using a bayesian network.madmom library contains the implementation of various state of the art algorithms in the field of beat tracking and is available on github. It incorporates low-level feature extraction and high-level feature analysis based on machine learning methods. The code for approach 2 is available in this repository as python files which output the beat locations from an input audio file.Lets look at an implementation in python:Lets get into the details of the approach we just saw. There are four steps you perform to get audio beats from a music signal.As with all the unstructured data, an artificial system does not grasp the concept of audio signal easily. So it has to be converted in a format that is explainable to a machine learning model (viz preprocessing the data). As mentioned in the image below, there are quite a few methods which you can use to preprocess an audio data.Now that you have preprocessed the data, you can apply a machine learning/deep learning model to decipher the patterns in the data.Theoretically, if you train a single deep learning model with enough training examples and a proper architecture, it could perform well on the problem. But this is not always possible, due to reasons such as insufficient training data availability etc. So to boost the performance, what we can do is to train multiple RNN models on a single genre of music files, so that it can catch the patterns in that genre itself. This helps to mitigate some of the data insufficiency problems.For this approach, we first train an LSTM model(a improved version of RNN) and set it as our base model. Then we fine tune multiple LSTM models derived from our base model. This fine tuning is done on different genres of musical sequences. At test time, we pass the audio signal from all the models.At this step, we simply chose that model which has the least error among all the models, by comparing them with the score we get from the base model.Regardless of you use one LSTM or multiple LSTMs, there is a fundamental shortcoming: the final peak-finding stage does not try to find a global optimum when selecting the final locations of the beats. It rather determines the dominant tempo of the piece (or a segment of certain length) and then aligns the beat positions according to this tempo by simply choosing the best start position and then progressively locating the beats at positions with the highest activation function values in a certain region around the pre-determined position.To circumvent this problem, we feed the output of the chosen neural network model into a dynamic Bayesian network (DBN) which jointly infers tempo and phase of a beat sequence. Another advantage of using DBNs is that we are able to model both beat and non-beat states, which was shown to perform superior to the case where only beat states are modelled. I will not go into the full description of how DBNs work, but you can refer this video if you are interested.This is how we get the beats of a musical sequence from raw audio signals.I hope this article gave you an intuition of how to solve a beat tracking problem in python. This has potential applications in the music information retrieval domain, where we can identify similar kinds of music using the beat rhythm we get while tracking it. If you have any comments/suggestions do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2018/02/audio-beat-tracking-for-music-information-retrieval/
A Robot called Erica set to become News Anchor in Japan,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Learn Audio Beat Tracking for Music Information Retrieval (with Python codes)|Natural Language Processing for Beginners: Using TextBlob|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In a first from the news reporting world, a Japanese robot will be anchoring a news segment starting in April. The robot, named Erica, was developed byHiroshi Ishiguro, the director of the Intelligent Robotics Laboratory at Osaka University.       Mandatory Credit: Photo by Franck Robichon/Epa/REX/Shutterstock (8453847e)Erica resembles a young woman reporter and her voice has been dubbed to sound like a 23-year olds. According to this report by Sanvada, Erica can process sound from its source and also from its interaction with human beings. Using audio processing, the robot can easily distinguish different sounds from a conversation. According to Mr. Ishiguro, this is done using a model called theadvanced speech synthesis system.It can also track movement around it with 15 inbuilt infrared sensors using object detection algorithms. However, Erica does not have functional arms and legs. That is, it can talk but cannot move, because it doesnt have to. The job involves reading the news from behind the desk and it has been able to do that without any issues during the test runs. Erica will not be involved in collecting or curating the news, so it wont qualify as a reporter.Mr. Ishiguro revealed that he tried to get Erica on-air almost four years ago but things didnt quite work out then. He has since programmed the robot to develop a consciousness of its own. He claims Erica has a soul, and has feelings like a real person. This, as we can imagine, has been received controversially. The debate will rage on for quite a while when it comes to the metaphysical aspect of artificial intelligence.This is quite a big deal in the journalistic world. AI has been extending its capabilities in different fields and this is the first major breakthrough in the world of news reporting. So far, the robot will only be acting as a news anchor but it wont be long before its abilities include collating and collecting the news as well. As for the robot developing a personality of its own, it remains to be seen how it impacts the people whos jobs it will be replacing. Will it replace reporters out in the field covering events? Will it be able to ask questions in press conferences? Well just have to wait and watch.",https://www.analyticsvidhya.com/blog/2018/02/erica-robot-news-anchor-japan/
Natural Language Processing for Beginners: Using TextBlob,Learn everything about Analytics|Introduction|Table of Contents|1. About TextBlob?|2. Setting up the System|3. NLP tasks using TextBlob|4. Other cool things to do|5. Text classification using TextBlob|6. Pros and Cons|7. End Notes,"3.1 Tokenization|3.2 Noun Phrase Extraction|3.3 Part-of-speech Tagging|3.4 Words Inflection and Lemmatization|3.5 N-grams|3.6 Sentiment Analysis|4.1 Spelling Correction|4.2 Creating a short summary of a text|4.3 Translation and Language Detection|Pros:|Cons:|Learn,compete, hackandget hired!|Share this:|Like this:|Related Articles|A Robot called Erica set to become News Anchor in Japan|Wearables can now Detect Early Signs of Diabetes (using Machine Learning, of course)|
Shubham Jain
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing (NLP) is an area of growing attention due to increasing number of applications like chatbots, machine translation etc. In some ways, the entire revolution of intelligent machines in based on the ability to understand and interact with humans.I have been exploring NLP for some time now.My journey started with NLTK library in Python, which was the recommended library to get started at that time. NLTK is a perfect library for education and research, it becomes very heavy and tedious for completing even the simple tasks.Later, I got introduced to TextBlob, which is built on the shoulders of NLTK and Pattern. A big advantage of this is, it is easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction, etc. It has now become my go-to library for performing NLP tasks.On a side note, there isspacy, which is widely recognized as one of the powerful and advanced library used to implement NLP tasks. But having encountered both spacy and TextBlob, I would still suggest TextBlob to a beginner due to its simple interface.If it is your first step in NLP, TextBlob is the perfect library for you to get hands-on with.The best way to go through this article is to follow along with the code and perform the tasks yourself.So lets get started!Note : This article does not narrate NLP tasks in depth. If you want to revise the basics and come back here, you can always go through this article.TextBlob is a python library and offers a simple API to access its methods and perform basic NLP tasks.A good thing about TextBlob is that they are just like python strings. So, you can transform and play with it same like we did in python. Below, I have shown you below some basic tasks. Dont worry about the syntax, it is just to give you an intuition about how much-related TextBlob is to Python strings.So, to perform these things on your own lets quickly install and start coding.Installation of TextBlob in your system in a simple task, all you need to do is open anaconda prompt ( or terminal if using Mac OS or Ubuntu) and enter the following commands:This will install TextBlob. For the uninitiated  practical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora.To download the necessary corpora, you can run the following commandTokenization refers to dividing text or a sentence into a sequence of tokens, which roughly correspond to words. This is one of the basic tasks of NLP. To do this using TextBlob, follow the two steps:So, lets quickly create a textblob object to play with.Now, this textblob can be tokenized into a sentence and further into words. Lets look at the code shown below.Since we extracted the words in the previous section, instead of that we can just extract out the noun phrases from the textblob. Noun Phrase extraction is particularly important when you want to analyze the who in a sentence. Lets see an example below.As we can see that the results arent perfectly correct, but we should be aware that we areworking with machines.Part-of-speech tagging or grammatical tagging is a method to mark words present in a text on the basis of its definition and context. In simple words, it tells whether a word is a noun, or an adjective, or a verb, etc. This is just a complete version of noun phrase extraction, where we want to find all the the parts of speech in a sentence.Lets check the tags of our textblob.Here, NN represents a noun, DT represents as a determiner, etc. You can check the full list of tags from here to know more.Inflectionis a process ofwordformation in which characters are added to the base form of awordto express grammatical meanings. Word inflection in TextBlob is very simple, i.e., the words we tokenized from a textblob can be easily changed into singular or plural.TextBlob library also offers an in-build object known as Word. We just need to create a word object and then apply a function directly to it as shown below.We can also use the tags to inflect a particular type of words as shown below.Words can be lemmatized using the lemmatize function.A combinationof multiple words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words, and can be used as features for language modelling. N-grams can be easily accessed in TextBlob using the ngrams function, which returns a tuple of n successive words.Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.The sentiment function of textblob returns two properties, polarity, and subjectivity. Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].Lets check the sentiment of our blob.We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a public opinion and not a factual information.Spelling correction is a cool feature which TextBlob offers, we can be accessed using the correct function as shown below.We can also check the list of suggested word and its confidence using the spellcheck function.This is a simple trick which we will be using the things we learned above. First, take a look at the code shown below and to understand yourself.Simple, Aint it? What we did above that we extracted out a list of nouns from the text to give a general idea to the reader about the things the text is related to.Can you guess what is written in the next line?Haha! Can you guess which language is this? Dont worry, lets detect it using textblobSo, it is Arabic. Now, lets find translate it into English so that we can know what is written using TextBlob.Even if you dont explicitly define the source language, TextBlob will automatically detect the language and translate into the desired language.This is seriously so cool!!! Lets build a simple text classification model using TextBlob. For this, first, we need to prepare a training and testing data.Textblob provides in-build classifiers module to create a custom classifier. So, lets quickly import it and create a basic classifier.As you can see above, we have passed the training data into the classifier.Note that here we have used Naive Bayes classifier, but TextBlob also offers Decision tree classifier which is as shown below.Now, lets check the accuracy of this classifier on the testing dataset and also TextBlob provides us to check the most informative features.As, we can see that if the text contains is, then there is a high probability that the statement will be negative.In order to give a little more idea, lets check our classifier on a random text.So, based on the training on the above dataset, our classifier has provided us the right result.Note that here we could have done some preprocessing and data cleaning but here my aim was to give you an intuition that how we can do text classification using TextBlob.I hope that you that a fun time learning about this library. TextBlob, actually provided a very easy interface for beginners to learn basic NLP tasks. I would recommend every beginner to start with this library and then in order to do advance work you can learn spacy as well. We will still be using TextBlob for initial prototyping in the almost every NLP project.You can find the full code of this article from my github repository.Also, did you find this article helpful? Please share your opinions/thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/
"Wearables can now Detect Early Signs of Diabetes (using Machine Learning, of course)",Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Natural Language Processing for Beginners: Using TextBlob|A Boston Startup is Solving a Classic Optimization Problem with Machine Learning|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Wearables have already come leaps and bounds in recent years. From just counting the steps we take to the calories we burn, they can now even monitor our heart rates. Using this latter feature and putting it into a neural network, startup Cardiogram has launched an application that can detect early signs of diabetes.                              Source: CardiogramDiabetes has always been a silent killer, but the number of people suffering from it has increased exponentially in the last few decades. A recent study by Cardiogram and the University of California revealed that almost 90% of prediabetics are unaware of what is about to hit them. More often than not, people continue with their normal lifestyle, unaware that their health is at severe risk with each day diabetes goes undetected.Yes, there are devices that help with diabetes. But most of these help once the disease has already been detected. As it turns out, any wearable device you have (Apple Watch, Android wear, FitBit, etc.) that can monitor heart rate, can now be used to predict the risk of diabetes.Cardiograms method of detecting the risk of diabetes is founded on how the variability of our heart rate can be connected to the likelihood of contracting diabetes. As one can imagine, its not possible to gather the potentially millions of data points required to train the neural network (its extremely time consuming and expensive). Instead, Cardiogram and the University of California turned to semi-supervised machine learning. Using 33,628 person weeks worth of sensor health data, they trained their deep neural network (called DeepHeart). Once trained, they tested the neural network on another data set, this one consisting of 12,790 person weeks. The result was a super impressive 85% accuracy rate.                              Source: CardiogramAs of today, the researchers are working on increasing this accuracy rate. Apart from diabetes, the neural network can also detect other heart related illnesses likehypertension and sleep apnea.The best thing about this application is that you dont need to spend an extravagant amount of money to reap the benefits. All you need is a smartwatch, and even that can be a relatively cheap one (as long as it monitors your heart rate). Analytics in healthcare has been booming recently and this is just another example of it. Diabetes is a debilitating disease wreaking havoc across the world. Hopefully, Cardiograms application will reduce the number of people suffering from it (or at least delay it as much as possible by helping people take precautionary measures).",https://www.analyticsvidhya.com/blog/2018/02/wearables-can-now-detect-early-signs-of-diabetes-using-machine-learning-of-course/
A Boston Startup is Solving a Classic Optimization Problem with Machine Learning,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Wearables can now Detect Early Signs of Diabetes (using Machine Learning, of course)|R Interface to TensorFlow made Possible|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Heres a classic optimisation problem  most transportation and logistics companies are still using the old methods of deciding routes for trucks. While the big companies are getting on board with data analytics, others are stuck in a rut.When drivers get stuck in traffic jams, or the original path has a diversion in the middle, the truck drivers get behind on their schedule. This, as you might expect, results in major losses for the company and leads to customer dissatisfaction.A startup from Boston, Wise Systems, has devised a solution. They have paired data (partly taken from the drivers mobiles) with machine learning algorithms to optimise each route every truck will take. The variables the ML model looks at include the drivers speed, GPS location from the cell phone, traffic on the route, weather, the trucks destination, and what time the customer will be available to receive their order.So the question is, how is this any different from the old methods? Well, the biggest advantage is that the model allows flexibility. The routes can be changed immediately depending on any changes in any variable. For example, if any road is closed or diverted, the driver will receive an immediate alert on his phone. If the driver is not scheduled to deliver on time, he will get an alert asking to speed up his truck.This stems from the age-old Traveling Salesman problem which a lot of experts have been trying to solve since the early 20th century. The possibilities are endless.Using Wise Systems ML model, companies can save a lot of money by minimising the delivery time (and as a result making more deliveries in a day).As mentioned, this is a problem that has plagued many a company since decades. Its a surprise that its taken this long for a proper solution to come up for medium sized companies (the big companies like Amazon and Kuehne Nagel) have their own models). But now that it has, we can expect it to catch on pretty quickly.",https://www.analyticsvidhya.com/blog/2018/02/boston-startup-optimization-analytics-machine-learning/
R Interface to TensorFlow made Possible,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|A Boston Startup is Solving a Classic Optimization Problem with Machine Learning|7 methods to perform Time Series forecasting (with Python codes)|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow, a general purpose numerical computing library, was nominally developed for python and has been proving support for approximately 2 years now. This is one of the reasons why Python has always been preferred over R.Rstudio (a free and open-source integrated development environment ) made R Interface with TensorFlow plausible. Rstudio formally announced their work on creating R interfaces to TensorFlow at rstudio::conf on Saturday. Here is JJ Allaire, the CEO of Rstudio, addressing the conference.Interfacing R and TensorFlow has a suite of packages that provides high-level interfaces to deep learning models (Keras) and standard regression and classification models (Estimators). Here we have some interfaces to TensorFlow:Considering the fact that not all users will have complete access to high-end NVIDIA GPU, using GPUs in the cloud has been made possible. Here are a few methods for the same :On the other hand, for a user having required NVIDIA GPU hardware, here are steps to set up GPU in the local workstation.To make this simpler for the users, Rstudio has provided all the resources on TensorFlow for R website. You can also refer Deep Learning using Keras and TensorFlow in R.It has always been a major topic of discussion to choose between R and Python. Python was given the preference as it could be interfaced with TensorFlow and Keras. The creation of R interface with TensorFlow is a good news for all R users.",https://www.analyticsvidhya.com/blog/2018/02/r-interface-to-tensorflow-made-possible/
7 methods to perform Time Series forecasting (with Python codes),Learn everything about Analytics|Introduction|Table of Contents|Understanding the Problem Statement and Dataset||Installing library(statsmodels)||Method 1: Start with a Naive Approach|Method 2:  Simple Average|Method 3 Moving Average|Method 4 Simple Exponential Smoothing||Method 5 Holts Linear Trend method||Method 6 Holt-Winters Method|Method 7 ARIMA||Projects|End Notes,"Learn,engage, hackandget hired!|Share this:|Related Articles|R Interface to TensorFlow made Possible|TensorFlow 1.6.0 Released!|
Gurchetan Singh
|75 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most of us would have heard about the new buzz in the market i.e. Cryptocurrency. Many of us would have invested in their coins too. But is investing money in such a volatile currency safe? How can we make sure that investing in these coins now would surely generate a healthy profit in the future? We cant be sure but we can surely generate an approximate value based on the previous prices. Time series modeling is one way to predict them.Source: BitcoinBesides Cryptocurrencies, there are multiple important areas where time series forecasting is used  forecasting Sales, Call Volume in a Call Center, Solar activity, Ocean tides, Stock market behaviour, and many others.Assume the Manager of a hotel wants to predict how many visitors should he expect next year to accordingly adjust the hotels inventories and make a reasonable guess of the hotels revenue. Based on the data of the previous years/months/days, (S)he can use time series forecasting and get an approximate value of the visitors. Forecasted value of visitors will help the hotel to manage the resources and plan things accordingly.In this article, we will learn about multiple forecasting techniques and compare them by implementing on a dataset. We will go through different techniques and see how to use these methods to improve score.Lets get started!We are provided with a Time Series problem involving prediction of number of commuters of JetRail, a new high speed rail service by Unicorn Investors. We are provided with 2 years of data(Aug 2012-Sept 2014) and using this data we have to forecast the number of commuters for next 7 months.Lets start working on the dataset downloaded from the above link. In this article, Im working with train dataset only.As seen from the print statements above, we are given 2 years of data(2012-2014) at hourly level with the number of commuters travelling and we need to estimate the number of commuters for future.In this article, Im subsetting and aggregating dataset at daily basis to explain the different methods.Lets visualize the data (train and test together) to know how it varies over a time period.The library which I have used to perform Time series forecasting is statsmodels. You need to install it before applying few of the given approaches. statsmodels might already be installed in your python environment but it doesnt support forecasting methods. We will clone it from their repository and install using the source code. Follow these steps :-Consider the graph given below. Lets assume that the y-axis depicts the price of a coin and x-axis depicts the time (days).We can infer from the graph that the price of the coin is stable from the start. Many a times we are provided with a dataset, which is stable throughout its time period. If we want to forecast the price for the next day, we can simply take the last day value and estimate the same value for the next day. Such forecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method.Now we will implement the Naive method to forecast the prices for test data.We will now calculate RMSE to check to accuracy of our model on test data set.We can infer from the RMSE value and the graph above, that Naive method isnt suited for datasets with high variability. It is best suited for stable datasets. We can still improve our score by adopting different techniques. Now we will look at another technique and try to improve our score.Consider the graph given below. Lets assume that the y-axis depicts the price of a coin and x-axis depicts the time(days).We can infer from the graph that the price of the coin is increasing and decreasing randomly by a small margin, such that the average remains constant. Many a times we are provided with a dataset, which though varies by a small margin throughout its time period, but the average at each time period remains constant. In such a case we can forecast the price of the next day somewhere similar to the average of all the past days.Such forecasting technique which forecasts the expected value equal to the average of all previously observed points is called Simple Average technique.We take all the values previously known, calculate the average and take it as the next value. Of course it wont be it exact, but somewhat close. As a forecasting method, there are actually situations where this technique works the best.We will now calculate RMSE to check to accuracy of our model.We can see that this model didnt improve our score. Hence we can infer from the score that this method works best when the average at each time period remains constant. Though the score of Naive method is better than Average method, but this does not mean that the Naive method is better than Average method on all datasets. We should move step by step to each model and confirm whether it improves our model or not.Consider the graph given below. Lets assume that the y-axis depicts the price of a coin and x-axis depicts the time(days).We can infer from the graph that the prices of the coin increased some time periods ago by a big margin but now they are stable.Many a times we are provided with a dataset, in which the prices/sales of the object increased/decreased sharply some time periods ago. In order to use the previous Average method, we have to use the mean of all the previous data, but using all the previous data doesnt sound right.Using the prices of the initial period would highly affect the forecast for the next period. Therefore as an improvement over simple average, we will take the average of the prices for last few time periods only. Obviously the thinking here is that only the recent values matter. Such forecasting technique which uses window of time period for calculating the average is called Moving Average technique. Calculation of the moving average involves what is sometimes called a sliding window of sizen.Using asimple moving average model, we forecast the next value(s) in a time series based on theaverage of a fixed finite numberpof the previous values. Thus, for alli > pA moving average can actually be quite effective, especially if you pick the right p for the series.We chose the data of last 2 months only. We will now calculate RMSE to check to accuracy of our model.We can see that Naive method outperforms both Average method and Moving Average method for this dataset. Now we will look at Simple Exponential Smoothing method and see how it performs.An advancement over Moving average method is Weighted moving average method. In the Moving average method as seen above, we equally weigh the past n observations. But we might encounter situations where each of the observation from the past n impacts the forecast in a different way. Such a technique which weighs the past observations differently is called Weighted Moving Average technique.Aweightedmoving average is a moving average where within the sliding window values are given different weights, typically so that more recent points matter more. Instead of selecting a window size, it requires a list of weights (which should add up to 1). For example if we pick [0.40, 0.25, 0.20, 0.15]as weights, we would be giving 40%, 25%, 20% and 15% to the last 4 points respectively.After we have understood the above methods, we can note that both Simple average and Weighted moving average lie on completely opposite ends. We would need something between these two extremes approaches which takes into account all the data while weighing the data points differently. For example it may be sensible to attach larger weights to more recent observations than to observations from the distant past. The technique which works on this principle is called Simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations:where0  1is the smoothing parameter.The one-step-ahead forecast for timeT+1is a weighted average of all the observations in the seriesy1,,yT. The rate at which the weights decrease is controlled by the parameter.If you stare at it just long enough, you will see that the expected valueyxis the sum of two products:ytand(1)yt-1.Hence, it can also be written as :So essentially weve got a weighted moving average with two weights:and1.As we can see,1is multiplied by theprevious expected valueyx1 which makes the expression recursive. And this is why this method is called Exponential. The forecast at timet+1is equal to a weighted average between the most recent observationytand the most recent forecastyt|t1.We will now calculate RMSE to check to accuracy of our model.We can see that implementing Simple exponential model with alpha as 0.6 generates a better model till now. We can tune the parameter using the validation set to generate even a better Simple exponential model.We have now learnt several methods to forecast but we can see that these models dont work well on data with high variations. Consider that the price of the bitcoin is increasing.If we use any of the above methods, it wont take into account this trend.Trend is the general pattern of prices that we observe over a period of time. In this case we can see that there is an increasing trend.Although each one of these methods can be applied to the trend as well. E.g. the Naive method would assume that trend between last two points is going to stay the same, or we could average all slopes between all points to get an average trend, use a moving trend average or apply exponential smoothing.But we need a method that can map the trend accurately without any assumptions. Such a method that takes into account the trend of the dataset is called Holts Linear Trend method.Each Time series dataset can be decomposed into its componenets which are Trend, Seasonality and Residual. Any dataset that follows a trend can use Holts linear trend method for forecasting.We can see from the graphs obtained that this dataset follows an increasing trend. Hence we can use Holts linear trend to forecast the future prices.Holt extended simple exponential smoothing to allow forecasting of data with a trend. It is nothing more than exponential smoothing applied to both level(the average value in the series) and trend. To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected forecast yThe values we predicted in the above algorithms are called Level. In the above three equations, you can notice that we have added level and trend to generate the forecast equation.As with simple exponential smoothing, the level equation here shows that itis a weighted average of observationand the within-sample one-step-ahead forecastThe trend equation shows that itis a weighted average of the estimated trend at timetbased on (t)(t1)andb(t1), the previous estimate of the trend.We will add these equations to generate Forecast equation. We can also generate a multiplicative forecast equation by multiplying trend and level instead of adding it. When the trend increases or decreases linearly, additive equation is used whereas when the trend increases of decreases exponentially, multiplicative equation is used.Practice shows that multiplicative is a more stable predictor, the additive method however is simpler to understand.sourceWe will now calculate RMSE to check to accuracy of our model.We can see that this method maps the trend accurately and hence provides a better solution when compared with above models. We can still tune the parameters to get even a better model.So lets introduce a new term which will be used in this algorithm. Consider a hotel located on a hill station. It experiences high visits during the summer season whereas the visitors during the rest of the year are comparatively very less. Hence the profit earned by the owner will be far better in summer season than in any other season. This pattern will repeat itself every year. Such a repetition is called Seasonality. Datasets which show a similar set of pattern after fixed intervals of a time period suffer from seasonality.sourceThe above mentioned models dont take into account the seasonality of the dataset while forecasting. Hence we need a method that takes into account both trend and seasonality to forecast future prices. One such algorithm that we can use in such a scenario is Holts Winter method. The idea behind triple exponential smoothing(Holts Winter) is to apply exponential smoothing to the seasonal components in addition to level and trend.Using Holts winter method will be the best option among the rest of the models beacuse of the seasonality factor. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations  one for the levelt, one for trendbt and one for the seasonal component denoted byst, with smoothing parameters,and.sourcewhere s is the length of the seasonal cycle, for 0    1, 0    1 and 0    1.The level equation shows a weighted average between the seasonally adjusted observationand the non-seasonal forecastfor timet. The trend equation is identical to Holts linear method. The seasonal equation shows a weighted average between the current seasonal index, and the seasonal index of the same season last year (i.e., stime periods ago).In this method also, we can implement both additive and multiplicative technique. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series.We will now calculate RMSE to check to accuracy of our model.We can see from the graph that mapping correct trend and seasonality provides a far better solution. We chose seasonal_period = 7 as data repeats itself weekly. Other parameters can be tuned as per the dataset. I have used default parameters while building this model. You can tune the parameters to achieve a better model.Another common Time series model that is very popular among the Data scientists is ARIMA. It stand for Autoregressive Integrated Moving average.While exponential smoothing models were based on a description of trend and seasonality in the data, ARIMA models aim to describe the correlations in the data with each other. An improvement over ARIMA is Seasonal ARIMA. It takes into account the seasonality of dataset just like Holt Winter method.You can study more about ARIMA and Seasonal ARIMA models and its pre-processing from these articles (1) and (2).We will now calculate RMSE to check to accuracy of our model.We can see that using Seasonal ARIMA generates a similar solution as of Holts Winter. We chose the parameters as per the ACF and PACF graphs. You can learn more about them from the links provided above. If you face any difficulty finding the parameters of ARIMA model, you can use auto.arimaimplemented in R language. A substitute of auto.arima in Python can be viewed here.We can compare these models on the basis of their RMSE scores.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problem:I hope this article was helpful and now youd be comfortable in solving similar Time series problems. I suggest you take different kinds of problem statements and take your time to solve them using the above-mentioned techniques. Try these models and find which model works best on which kind of Time series data.One lesson to learn from these steps is that each of these models can outperform others on a particular dataset. Therefore it doesnt mean that one model which performs best on one type of dataset will perform the same for all others too.You can also explore forecast package built for Time series modelling in R language.You may also explore Double seasonality models from forecast package. Using double seasonality model on this dataset will generate even a better model and hence a better score.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/
TensorFlow 1.6.0 Released!,Learn everything about Analytics,"Share this:|Like this:|Related Articles|7 methods to perform Time Series forecasting (with Python codes)|Resize Image efficiently using Dynamic Programing|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow, developed by Google Brain team, is an open source software library for a building machine learning models for range of tasks in data science. It is written in Python, C++, CUDA and is mainly used for machine learning applications such as neural networks.The open source software library has had many releases, each one with more improvements and fixes. Google recently announced the release of TensorFlow 1.5.0 in January, which included Eager Execution of TensorFlow and developer preview of TensorFlow Lite as major additions to the library. Googles contribution to the world of Machine Learning and Data Science did not end with that. A new version, TensorFlow 1.6.0 has been released, with significant changes over the previously existing versions.Source: Google ImagesTensorFlow 1.6.0 has introduced a second version of Getting started for Machine Learning newcomers. It provides a document explaining Machine Learning fundamentals along with TensorFlow programming. Here is the document on Getting Started for ML Beginners.Furthermore, TensorFlow 1.6.0 has a number of Bug fixes, few of which are:Apart from that, we have some improvements made in TensorFlow 1.6.0:Additionally, there are some technical advancements in API:For more information, visit TensorFlow 1.6.0Our take on this:The newer version of TensorFlow has made an effort to emerge as a beginner friendly software library. It has introduced the Getting started version for machine learning beginners which provides a detailed explanation of the machine learning fundamentals. Also, the updated version has support for CUDA 9.0 and cuDNN 7, bringing the glimpse of state-of-the-art and cutting edge technologies both at the same time.",https://www.analyticsvidhya.com/blog/2018/02/tensorflow-1-6-0-released/
Resize Image efficiently using Dynamic Programing,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|TensorFlow 1.6.0 Released!|MIT launches Neuromorphic Chip that can work like a Human Brain|
Aishwarya Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every now and then we resize or crop our images so as to fit them into the limited space as per our requirement. Often while resizing, the subject of the image gets distorted or looses its original quality. How would you feel if I tell you that you can now resize your image intelligently. Interesting? Right?You can now use Caire to resize your image while preserving its most significant parts. Unlike the usual image cropping, Carie resizes the image maintaining the ratio of people or building in the image. Let us know more about Caire and see how it works!Caire is an image resize library which uses the Seam Carving algorithm. Seam carving is an algorithm for content-aware image resizing. The main idea of Seam Carving algorithm is to identify the least important parts in an image and remove them in order to reduce the size of the image. Seam carving also allows manually defining areas in which pixels may not be modified, and features the ability to remove whole objects from photographs.Carie uses this algorithm to resize objects in an image, based on heat-map and edge detection techniques, that is, it does not resize each part of the image with same metric, instead it identifies the important objects to preserve and least important parts to be removed.When an image is provided, edge detection technique is used to generate an energy map for the image. Based on the energy level values, it recognizes the parts with lower energy levels as least important. These least important parts in the image are referred to as seams which are generated using dynamic programming approach.Heres the step by step illustration of the algorithm:1. Original Image:2. Energy map created using dynamic mapping:3. Seams Identified:4. Resized Image:SourceThe previously existing techniques for image photoshop or rising would blindly follow the users instructions resulting in an image with distorted proportions. Using dynamic programming, Carie can resize image more efficiently, providing better results than ever.",https://www.analyticsvidhya.com/blog/2018/02/resize-your-image-efficiently-using-dynamic-programing/
MIT launches Neuromorphic Chip that can work like a Human Brain,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|Resize Image efficiently using Dynamic Programing|An Introductory Guide to Regularized Greedy Forests (RGF) with a case study in Python|
Aishwarya Singh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Researchers in the field of neuromorphic computing at MIT(Massachusetts Institute of Technology) have attempted to design a neuromorphic chip that works like human brain.Let us first see how the human brain works  The neurons in our brain sends signals to other neurons via synapse (space between the cells). The synapse works as a bridge for the nerve cells to communicate. There are about 100 billion neurons in the brain, each firing off 5-50 messages per second. This activity allows us to process our environment, move our muscles, make conscious movements, make decisions, form memories, etc.A team of researchers at the MIT university have attempted to design an artificial synapse that can precisely control the strength of an electric current flowing across it, similar to the way ions flow between neurons in the brain.The researchers fabricated a neuromorphic chip consisting of these artificial synapses made from silicon germanium, each synapse measuring about 25 nanometers across. They applied voltage to each synapse and found that all synapses exhibited more or less the same current, or flow of ions, with about a 4 per cent variation between synapses. This brain-on chip introduced, works on analogue fashion (unlike the previously fabricated computer chips which worked on digital signals) so that it can process multiple parallel computations, like our brain does.The team tested a single synapse over multiple trials, applying the same voltage over 700 cycles, and found the synapse exhibited the same current, with just 1 per cent variation from cycle to cycle. In simulations, they found that the chip and its synapses could be used to recognize samples of handwriting, with 95 per cent accuracy.Ultimately we want a chip as big as a fingernail to replace one big supercomputer, Kim says, leader of the research team at MIT.To read more, visit here.It can prove to be a major breakthrough towards portable Artificial Intelligence devices (smartphones, laptops etc). These neuromorphic chips would make it easier for us to enable the devices with AI technology, which will be faster, more efficient and use less power.",https://www.analyticsvidhya.com/blog/2018/02/mit-launches-computer-chips-that-can-work-like-human-brain/
An Introductory Guide to Regularized Greedy Forests (RGF) with a case study in Python,Learn everything about Analytics|Introduction|Table of Contents|RGF vs. Gradient Boosting|Implementation in Python|End Notes,"Hyperparameters|Training and evaluation using python wrapper|Share this:|Like this:|Related Articles|MIT launches Neuromorphic Chip that can work like a Human Brain|Utilizing Artificial Intelligence for Disaster Prevention|
Ankit Choudhary
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As a data scientist participating in multiple machine learning competition, I am always on the lookout for not-yet-popular algorithms. The way I define them is that these algorithms by themselves may not end up becoming a competition winner. But they bring different way for doing predictions on table.Why I am interested in these algorithms? The key is to read by themselves in the statement above. These algorithms can be used in ensemble models to get extra edge over mostly popular gradient boosting algorithms (XGBoost, LightGBM etc.).This article talks about one such algorithm called Regularized Greedy Forests (RGF). It performs comparable (if not better) to boosting algorithms for large number of datasets. They produce less correlated predictions and do well in ensemble with other tree boosting models.In order to get the most out of this article, you should know the basics of gradient boosting and basic decision tree terminology.In Boosting algorithms, each classifier/regressor is trained on data, taking into account the previous classifiers/regressors success. After each training step, the weights are redistributed. Mis-classified data increases its weights to emphasize the most difficult cases. In this way, subsequent learners will focus on them during their training.However, the boosting methods simply treat the decision tree base learner as a black box and it does not take advantage of the tree structure itself. In a sense, boosting does a partial corrective step to the model at each iteration.In contrast, RGF performs 2 steps:Search for the optimum structure change:Let me explain this with an example.Figure 3 shows that at the same stage as Figure 2, we may either consider splitting one of the leaf nodes marked with symbol X or grow a new tree T4 (split T4 s root).Weight OptimizationWeights for each of the nodes are also optimized in order to minimize the loss function further:RegularizationExplicit regularization to the loss function is essential for this algorithm as it overfits really quickly. It is possible to have different L2 regularization parameters for the process of growing a forest and the process of weight correction.There are three methods of regularization:Tree SizeRGF does not require the tree size parameter (e.g., number of trees, max depth) needed in gradient boosted decision trees. With RGF, the size of each tree is automatically determined as a result of minimizing the regularized loss. What we do declare, is the maximum number of leaves in the forest and regularization parameters (L1 and L2).Model SizeSince RGF performs fully corrective steps on the model/forest, it can train a simpler model as compared to boosting algorithms which require a small learning rate/shrinkage and large number of estimators to produce good results.Original RGF implementation for binary classification and regression was done in C++ by authors of the original research paper  Rie Johnson and Tong Zhang. The most popular wrapper of the same implementation for Python developed by fukatani even supports multiclass classification (using one vs. all technique). Much of the implementation is based on RGF wrapper by MLWave.Lets talk about the most important parameters that would affect the accuracy of the model, or speed of training, or both:Let us try RGF on the Big Mart Sales Prediction problem. The dataset for the same can be downloaded from this datahack link. I have imported some pre-processing steps used in this article.Once we have the pre-processed the stored data, we can then import RGF using the following command:The two most important parameters to set for this is the maximum number of leaves allowed and L2 regularization. We can use a grid search to find out the parameters with the best cross validation MSE.Looks like we are trying to fit too complex a model with too many leaves. The high regularization term is high and max_leaf is low. Let us do a different grid search with lower max_leaf:Looks like these parameters are fitting the best. This scores a RMSE of 1146 on the public leaderboard.RGF is yet another tree ensemble technique that sits next to gradient boosting algorithms and can be used for effectively modelling non-linear relationships. The documentation for the library used can be found at this link. There are several other parameters to try and tune with the RGF. Let me know in the comments how it works out for you.",https://www.analyticsvidhya.com/blog/2018/02/introductory-guide-regularized-greedy-forests-rgf-python/
Utilizing Artificial Intelligence for Disaster Prevention,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|An Introductory Guide to Regularized Greedy Forests (RGF) with a case study in Python|AI is already in your Smartphones and its just getting better!|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With the exponential growth of Artificial Intelligence based applications in various fields, it is now making its way to the space. Japan government plans to use Artificial intelligence to analyze images of earth captured by the satellites to predict and prevent disasters. AI based analysis could be used to develop new services for disaster management in areas prone to high occurrence of disasters.The main idea is to use AI to monitor any change in the images and predict the risk of disasters. For example, changes in the steep slopes could be an indication for landslides, or changes in sea behavior may imply arrival of tsunami. Tokio Marine & Nichido Fire Insurance Co. has revealed plans to utilize a satellite to assess large-scale flood damage, with artificial intelligence.Apart from monitoring these changes, it can monitor aging infrastructure. Satellites can detect millimeter scale deformations and provide a synoptic view of terrain and infrastructure stability. Hence the damage caused by collapsing buildings or subsiding roads/bridges could be reduced, if not completely eliminated. With the practical implementation of AI in satellites, a significant amount of time and money can be saved.For more information on implementation of AI for disaster prevention, click here.With the help of AI, the data from satellite can be put to appropriate use. At present, we have advanced image processing techniques and deep learning methods that can provide better results for image analysis than we have ever seen before. If we can forecast the occurrence of a disaster, it will not only be helpful in saving thousands of lives, but also mitigate the loss of money and infrastructure. Further developments in AI based technologies will certainly be beneficial for social good.",https://www.analyticsvidhya.com/blog/2018/02/artificial-intelligence-based-analysis-for-disaster-prevention/
AI is already in your Smartphones and its just getting better!,Learn everything about Analytics|Our take on this:,"Share this:|Like this:|Related Articles|Utilizing Artificial Intelligence for Disaster Prevention|Demystifying Information Security Using Data Science|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Contemplating the remarkable pace at which AI is entering our world, it is expected to be a vital feature in smartphones in near future. Most smartphone companies like Apple, Google, Huawei, have already started using AI for various applications such as photography, security etc. source : GoogleThe cameras in phones will use AI to enhance the picture. It will detect the object in the frame (be it face, landscape, vehicle, food etc.) and alter the settings to capture the best image. This will not only improve the quality of the picture, but also make it easier for the user as the settings need not be changed before every picture. Apart from capturing images, AI tends to replace the current translation apps (translates text in an image from one language to another). One major benefit of using AI over these applications is that AI will not need Internet connection for translation.AI can also be used to monitor the users daily activities over the phone. For example, when the phone is silent (in a class/ office) or when no apps are used (when sleeping) and so on. Thus AI will learn and adapt the users behavior over time and automate these daily processes.Additionally, AI based algorithms improve security. Apples iPhone X uses face detection to unlock the phone. It is able to create a 3D map of a persons face which is used to unlock the device. This feature can also detect salient changes in the face such as beard or spectacles. Huawei Honor View 10 uses a similar feature for face unlock. It recognizes if the users eyes are open or closed, and the device will authenticate only if the eyes are open. Face unlock can perform additional tasks like turning off the screen when you arent looking at the display and setting the display orientation according to the positioning of the device using the camera.Read more about AI applications in Smartphones here.AI will continue to provide us with features to simplify our problems and make our work more efficient. Initially the benefits might be less but in the future we will see AI becoming a major part of our. This will improve efficiencies between the user and their phone across text, voice, image, video, and sensors. This change will be embraced by millions of users across the globe.Do you also believe that AI in smartphone is the next big thing? Why or why not? Share your thoughts in comments section below!",https://www.analyticsvidhya.com/blog/2018/02/ai-already-smartphones-just-getting-better/
Demystifying Information Security Using Data Science,Learn everything about Analytics|Introduction|Table of Contents|Why are so many ransomware attacks and data breaches happening now?|What are the challenges in InfoSec?|Why does InfoSec need data science?|What are the data science challenges for InfoSec?|What are the key data sources and use cases for security data science?|How did security data science evolve over time?|How to model an InfoSec use case into a data science problem?|End Notes|References,"About the Author|Share this:|Like this:|Related Articles|AI is already in your Smartphones and its just getting better!|Using Data? Master the Science in Data Science|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","Phase 1  Rule-based and Anomaly Detection systems|Phase 2  Security Data Lakes/SIEM|Phase 3  UEBA, Malware detection|Phase 4  Deception-triggered data science",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you search for security data science on the internet, its difficult to find resources with crisp and clear information about the use cases, methods and limitations in Information Security (hereby referred to as InfoSec). Theres usually always some marketing material attached to it. So, I thought of summarising my knowledge and InfoSec experience in this article.The intended audience for this article is:There are several reasons for this and a few major ones are listed below:                             Source: CSOWhen the attackers are within the enterprise network, they first need to figure out where they are. Once they accomplish this, they move towards their targets, and carry out the attack. During these reconnaissance queries and movements, they usually leave some traces or signals. These signals are present in the data, and their presence can be detected using data science to raise timely alerts.Earlier, we used to bring all the data to a security data lake called SIEM (security information and event management). But now with the advancements in data science, correlations across multiple events can be performed in real-time. Using algorithms, we can connect the dots and find the patterns which used to be difficult to find manually owing to the lack of security analysts.One of the key advantages of data science-based systems is that they learn from the decisions taken by the security analysts. After training the systems extensively, they can also start taking the same preventive measures/actions as the security analysts.The problems in InfoSec are multi-dimensional, that is, thousands of features present in tons of data sources. We need to detect the adversary presence by mining petabytes of machine logs. This is a complex and difficult problem, because the signal to noise ratio is very low. Also, connecting the attack sequences among isolated and rare signal events is a significant challenge.The majority of the security data has no labels, which makes it difficult to apply deep learning networks to a large number of InfoSec use cases. However, the industry is tackling this problem by generating class labels for a few use cases at a time.For example, detection of malware, and the ranking of malicious websites and DNS domains, is primarily done using Machine Learning techniques. Another successful use case of data science for security is making a baseline of each user/network device/entity within the network and comparing it with the real-time data to find rare/abnormal behavior and raising anomalies.These user behavior-based anomalies are certainly more than 100 times lesser than rule-based anomalies. However, their magnitudes are still quite high and a large number of them end up being false positives. In short, security data science is not a silver bullet for InfoSec. We need to marry multiple technologies along with it to improve the defense.Figure 1: Data Sources and Use Cases for Security Data ScienceThe InfoSec domain has a large number of logs. The data volume and variety depend on the organisations size and domain. Most of the big MNCs use 20-50 InfoSec tools and record the data into hot and cold storage. They use so-called security data lakes or Security Information and Event Management (SIEM) tools to store recent data (e.g. DNS logs, authentication logs, Windows security logs, etc.) for monitoring the threats. Data older than a few months, or high volume data (e.g. NetFlow, Bro logs etc.), is pushed to cold storage in Hadoop-based systems.Here is a list of typical data sources in InfoSec:All these logs provide a lot of visibility about the adversarys presence and activities. The table below summarises various use cases according to the data source type. Figure 1 (above) shows that these use cases are typically solved using anomaly detection and ML techniques. Table 1: Use Cases for the Security Data ScienceSecurity data science has evolved in three phases as shown in Figure 2 below. Since the 1990s, data science has played an increasingly important role in information security. This started with rules-based approaches to finding anomalies in intrusion detection system (IDS) and intrusion prevention system (IPS). Most of the firewall, network/host IDS/IPS are either rule-based or anomaly detection-based systems.Rules are written by security experts and the system raises alerts based on the rules, for instance, failed authentication beyond a specific count indicates a brute force attack. However, these rules dont capture the dynamic nature of events and context around the events. Anomaly detection systems are based on the normal behavior models of hosts and networks. Whenever there is significant deviation from the normal behavior, then they raise alerts. Anomaly detection algorithms, such as Clustering, Robust-PCA, SVD, One-Class SVM, DB Scan and KDE, are used to detect anomalous events. Anomaly-based algorithms are used in networks to detect:Unfortunately, most of the AD systems raise high false alarms and need a lot of security analysts to validate the alerts. Figure 2: Evolution of the Security Data ScienceIn the early 2000s, the second generation of security tools evolved. These facilitated triaging the alerts by correlating multiple data sources in a security data lake called as security information and event management (SIEM) tools. SIEM was successful when the data was large, but in the Big data era, they are slow and are missing an intelligence layer. With the advances in Big data frameworks, a new form of security data science has evolved. Now, it is possible to boil the ocean of raw logs in real time and raise alerts. This gave rise to user and entity behavior analytics (UEBA) that leverages Hadoop/Spark and anomaly detection techniques to raise real-time alerts whenever there is abnormal behavior of hosts/users within the enterprise network.This has enabled enterprises to detect insider attacks. However, the anomaly-based solutions have a drawback of generating a large number of false-positive alerts. Each investigation of a false-positive alert adds a significant burden to an already overloaded security analyst.Another emerging area that is rapidly gaining traction isendpoint security where deep learning is used to detect and classify malware in real time. Supervised ML algorithms such as Deep Learning Networks (ANN, RNN, CNN), Random Forest and XGBoost are used to classify malicious scripts vs benign scripts, detect DNS tunnels, detect C&C servers, detect malware, detect known network scans, application attacks, and many more known threats that have labels available for training the system.In this evolution, we are bringing a new paradigm shift for the InfoSec field. In this security defense, we first deploy deceptions (reincarnation of honeypots, honeynets, honeywords etc.) in the enterprise network. Then, we leverage data science to profile adversary behavior and their movements within the network. We termed this research deception-triggered data science. Deception-triggered data science is significantly different from conventional security data science. The latter primarily leverages anomaly detection techniques to identify anomalous behavior in network traffic, or user/host/network element behavior. Whereas deception-triggered data science starts from a real attack, i.e., anomaly announced by a deception event, and hence does not require anomaly detection algorithms.Deception alerts are high fidelity alerts. Data science correlates other security event data with these high fidelity alerts to generate a lot of insights about the adversary behavior. In this approach, we collect and describe the context around a deception alert instead of looking for anomalies like a needle in a haystack. Instead, this kind of data science can focus on capturing everything about how an attack begins and proceeds as it progresses.To draw on a metaphor, comparing deception-triggered data science to brute force security data science is like boiling a cup of tea rather than boiling an entire ocean. The former is practical, clever and elegant; the latter is expensive, cumbersome and impractical.Deception triggered data science significantly reduces the false positives thereby reducing the overall infrastructure and maintenance cost associated with security-related chores.More details about this topic can be found in my talk at Splunk .conf 2016 (#4 in the references at the bottom of this article). To read more about deception, please refer to Almeshekah and Spaffords paper listed in point #5.Figure 3: Security Data Science MethodsMost of the InfoSec problems can be modeled using anomaly detection and machine learning techniques, as shown with an example in Figure 3 above. I have shared the details of algorithms, feature engineering and data science pipeline for several InfoSec case studies during my webinars.The video titles mentioned below, along with the timeline, contain the InfoSec use cases. The links to these videos are in the references section at the bottom of this article.I have put together a link to the datasets, papers and talks related to Security Data Science. In the references below, use [6,7,8] Github links to learn further. Enjoy learning [1] TechGig Webinar, Demystifying Security Data Science  Part 1[2] TechGig Webinar, Demystifying Security Data Science  Part 2[3] DataHack Summit[4] Splunk .conf 2016 Talk, Deception-Triggered Security Data Science to Detect Adversary Movements.[5] Mohammed H. Almeshekah and Eugene H. Spafford, Planning and Integrating Deception into Computer Security Defenses, Proceedings of the 2014 New Security Paradigms Workshop, 2014.[6] GitHub  Awesome ML for Cyber Security[7] The Definitive Security Data Science and Machine Learning Guide[8] SANS Institute: Reading RoomDr. Satnam Singh, Chief Data Scientist  Acalvio TechnologiesDr Satnam Singh is currently leading security data science development at Acalvio Technologies. He has more than a decade of work experience in successfully building data products from concept to production in multiple domains. In 2015, he was named as one of the top 10 data scientists in India. To his credit, he has 25+ patents and 30+ journal and conference publications.Apart from holding a PhD degree in ECE from University of Connecticut, Satnam also holds a Masters in ECE from University of Wyoming. Satnam is a senior IEEE member and a regular speaker in various Big Data and Data Science conferences.",https://www.analyticsvidhya.com/blog/2018/02/demystifying-security-data-science/
Using Data? Master the Science in Data Science,"Learn everything about Analytics|Introduction|Table of Contents|1) Model Thinking  Understand the role and meaning of models|2) The Hypothesis  Deploy the power of hypothesis-led learning|3) The Data Generating Process  Know what it is that you seek to model|4) Searching for the Mechanism  The how and why of a models performance|5) Replicability, Reproducibility, Generalizability  Push for enduring impact|End Notes","Share this:|Like this:|Related Articles|Demystifying Information Security Using Data Science|Google is using AI for filtering Play Store Applications|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This past November, Avi Patchava and Paul Meinshausen participated in panel discussions at the Data Hack Summit in Bangalore and were encouraged to see Kunal Jains keynote on developing the data science ecosystem in India. They have been a part of the community for several years and have worked in data science roles across start-ups, management consulting, industry, and venture capital. They share the strong conviction that an important area for development in the ecosystem is in Data Sciences intellectual infrastructure.In the early stages of development, data science is often mistaken for a thin layer of popular statistical tools and packaged algorithms that are applied bluntly to a problem of choice. As data science evolves as a discipline in India, it will take more robust shape as an intellectual approach to discovering, as well as building solutions, for tough problems across business and society. The early stages of data science emphasize the size and diversity of data. As data science evolves, we see more focus on the application of scientific rigor in forming hypotheses, making deliberate decisions on model design, and causal inference.We still get a lot of confused responses when we talk about the science in data science. One of the most common questions is: What do Apache Spark and neural network models have to do with laboratories and experiments? So, in this article, we list down the concepts of science that are at the core of productive data science.Model thinking is at the very heart of data science. We use models to understand or predict the parts of the world that we want to make decisions about in our businesses. For example, at InMobi, Avi and team use models to predict behaviours of mobile users  such as whether they will click an advert, download an app, or purchase an ecommerce product. These models directly inform whether InMobi will bid for the opportunity to show an advertisement to a given user.George Box famously said that All models are wrong but some are useful. Once you have understood the lesson of intellectual humility Box was emphasizing, it is important to think about why only some models are useful and how you can make sure your models are among that useful subset. Models that are formed unconsciously or uncritically (think of cliches, stereotypes, and poorly considered KPIs, all of which are automatic mental models) are usually not very useful. We evaluate a model in different ways depending on what we want to do with it: description, explanation, prediction, prescription. Data scientists who do their work like scientists describe their models clearly, both the parts of the world that are included in the model and the parts that are left out. They also explicitly identify decisions their model can inform and acknowledge the kinds of decisions that the model will not help inform. For example, Avi was once involved in building an ML-driven control system for a metallurgical process in a steel production plant. The control system was midstream in the production value chain: it was one of 8 sub-plants performing different activities required in taking steel all the way from raw materials to steel slabs ready for transport. There were 3 major complications:The team made progress by sketching out the control system on a whiteboard, and showing clearly which types of variables would be used in the model and at what level of granularity. Also, the team was upfront about what assumptions the model needed to make about the data-generating process (more on this term below) given it would use a limited dataset. Further, the team was clear on which variables would enter the prediction system, and which variables were inputs into the final recommendation system.Hypotheses are an important tool for achieving useful models. The growth of computational power has been a boon to our science; our models have become incredibly sophisticated and complex. We are able to test many more possibilities for the inputs, the form, or the configurations of our models and we can look for many potential relationships that our models might represent.But effective data scientists do not let computational power drown the role of hypotheses in model building. Sometimes it is not possible to cost-effectively try every combination of inputs, forms and configurations. We need to know where to start. Moreover, when we auto-test many ideas we are vulnerable to that very human inclination to see patterns where they do not exist.Data scientists who think like scientists are able to take advantage of small data. Small data is where a smaller set of experiences suggest a certain idea or hypothesis. Sometimes humans have themselves become trained algorithms in virtue of having experienced a large number of the event in question. They cannot explain how they know a likely outcome (e.g., whether a consumer will be a defaulter, or whether their child is lying) but through accumulation of wide data from many situations, they become a human compass with accurate intuitions and hunches.So speak to the humans who have accumulated relevant experience; hear their intuitions. Once you have developed a model from this kind of research, you can start to build technology that can collect and create more data to fit and validate your models. Real scientists do not just use data; they also collect new data based on well specified hypothetico-deductive models.Models go hand-in-hand with data. Sometimes you form the models first and then go looking for data that matches your model and lets you test empirically and eventually deploy it. Sometimes you have a bunch of data and you explore it to develop a model of the phenomena the data represents. In either case, it is critically important that you remember the data only ever represents the world, it is not actually the world itself.Since data is only a representation of the world, it is important to think clearly about where it came from and how the way it was generated might limit or mislead your models.For example, when Paul was at Housing.com an important problem for the Data Science Lab was developing valuation models for real estate. The model hypothesized that the features of a house (number of bedrooms/bathrooms, size, etc) and the features of the locality the house is in (proximity to public transport, safety, proximity to schools, etc) contributed to the value people assign to it. Prices and peoples perceptions of the value of a house are also influenced by the houses around it.At the beginning those models relied on the assumption that Housings data reliably represented the houses in a given locality and city. Then they discovered that their data collectors did not collect houses in a random or representative way. Instead, young bachelor data collectors would often go to brokers who specialized in renting flats to young bachelors. The result was an oversampling of certain kinds of properties that led to poor predictive power for properties that did not come from that population. The team was able to deal with the problem because they paid close attention to the processes (human and technological) that generated their data.A model is often treated as a box: you add inputs, you get outputs. The mechanism is that inner causal process within the box which is how inputs are converted into outputs. Some models are built to accurately reflect or simulate the how in the data-generating process.Though some models are explicitly built not to mimic the how of the data-generating process (e.g. the predictive models for click behaviours above), often models work better if they are constructed so as to mimic the way the real-world mechanism works. The more you try to understand this, the more you can strengthen your model design.Sometimes models work well, even brilliantly. Sometimes they just will not deliver no matter how many ideas you try. Some people move from one model to the next, blindly hoping for a bullseye on the next shot, knowing they have many arrows yet in their quiver.By asking why something has worked i.e. looking for deeper clues in the data, developing hypotheses based on potential causes, and testing with further exploration, you seek a more reliable path to success. Your next step is better informed with learnings from the former. If you land on something that does work brilliantly, and you understand why it works so well, you are less likely to have succeeded by fluke (which often fails next turn).For models involving a high number of variables (i.e. high dimensionality), it will be challenging to understand or accurately map the mechanism. Contrast this to classical physics equations  such as Newtonian laws  where the limited variables means the mechanism is easier to identify. However even in high-dimensionality situations, the model design can seek to capture the structure of the underlying problem. Consider how convolutional neural networks have been so effective because they are structured to work with grids of pixels, whereas recurrent neural networks are structured to work with sequential data, such as time-series.At InMobi, the underlying dynamics of our global marketplace are changing rapidly. It is very easy to find models  or an arbitrary modelling technique  that works well over the last months data but, once deployed, the model subsequently fails to perform in the following month. We push ourselves to understand the why and how of a new technique before we are truly convinced, even if all performance metrics look green.People (this includes product managers and executives) love clear answers and good stories with happy endings. The best scientists learn to resist the urge to prioritize making their customers happy vs. maintaining a critical mindset on the world. As Ibn al-Haytham put it, The duty of man who investigates the writings of scientists, if learning the truth is his goal, is to make himself an enemy of all that he reads and  attack it from every side. He should also suspect himself as he performs his critical examination of it, so that he may avoid falling into either prejudice or leniency.If your model has worked where before it failed, recognise that you have learned something about the world. Maybe you have made a novel discovery. If you have asked why, find what is your best understanding of why it has worked. Did you learn something surprising about the world? Update your overall view of how the world works.You might have been trying to map a single data-generating process but there will be learnings across similar types of process  is your learning consistent with what you know about other processes? Or, if your process is part of a wider system, ask yourself: how do my learnings on this process affect my understanding of the wider system? What is generalizable? What are the implications? Remind yourself that one novel discovery in science often has wide ramifications, across many other fields of science.Data science that aims to be science doesnt leave work buried in files and folders of distributed code. Your work should be checked and verified and corrected on a regular basis. But if you truly want that to happen you have to do the extra work to document it and make it transparent, accessible, and extensible  all the successes and especially the failures too.Finally, remember failure is inevitable in your journey. For every new idea you have, perhaps only one in five will deliver. Do not be disheartened. Failure is an opportunity for learning, especially if you began with hypothesis. So why is this called science? Because discovering genuine and enduring insights is going to be difficult, but is a huge part of our journey.In conclusion, data science is a mindset and an approach that is far more than manipulating maths and writing clever code. Remind yourself that you are a scientist, and consequently you will be a much better data scientist.About the Authors:Paul Meinshausen is Data Scientist in Residence at Montane Ventures, an early stage VC fund based in India. He is a cofounder at PaySense, a mobile fintech startup in Mumbai, and was previously the Chief Data Officer. He has held data science positions at Housing.com, Teradata, the University of Chicago, and the U.S. Department of Defense, and conducted research in Psychology at Harvard University.Avi Patchava is Vice-President of Data Sciences, Machine Learning and Artificial Intelligence at InMobi  a leading Indian company in the world of Mobile AdTech. Previously, he was with McKinsey&Co driving large-scale machine learning initiatives in sectors such as Banking, Automotive, and Manufacturing. His background is in economics and the social sciences, with Masters degrees from the University of Oxford and the London School of Economics.",https://www.analyticsvidhya.com/blog/2018/02/using-data-master-the-science-in-data-science/
Google is using AI for filtering Play Store Applications,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Using Data? Master the Science in Data Science|Google has Extended its Machine Learning Capabilities to Predict Flight Delays|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With over 2.8 million apps, Googles Play Store is the biggest app store on the planet. However, trying to make sure that none of those applications are harmfulis a massive challenge for Google.source: GoogleTo overcome this and sort through the flood of new apps hitting the Play Store every day, Google has revealed that it has developed new detection models by leveraging AI and machine learning algorithms to shut down Android malware. In 2017 alone, Google took down over 700,000 malicious appsthat violated its Play store policy(a rise of 70 percent compared to 2016), and 100,000 bad developer accounts.Google usedAndroids built-in security and virus detection software, Google Play Protect, to reduce the installation of potentially harmful apps by a factor of 10 and close down over 250,000 copycat apps as well. This software could detect apps that commit fraud, steal information or allow hijacks. Google play protect will be enabled by default on all Android devices and is meant to be a real-time malware scanner that will scan every installed or about-to-be installed app and inform users in case theres something amiss. However, users can disable Google Protect if they do not want automatic scanning of their device.Artificial intelligence and machine were used learning to improve the overall experience of browsing through the store for an app or game, simultaneously keeping users safe from bad apps. The role of AI is to detect inappropriate content such as hate speech or violence, and flag those apps so that they could be reviewed more closely by human personnel. It examines privacy and security signals for every app on the Play Store to detect potentially harmful signals on time. Additionally, Machine learning techniques are used to catch apps with abusive content. as a result of which 99% of these apps were identified and rejected before anyone could install them.Google Play is committed to providing a safe experience for billions of Android users. In 2017, they took down more than a quarter of a million of impersonating apps and the annual installation of potentially harmful apps was reduced by 50 percent. Despite the new and enhanced detection capabilities, there is a need for improvement so as to simplify the process of hunting for the most safe and apropriate apps for the users. According to Google, the company is committed to make Google Play the most trusted and safe app store in the world.This has most certainly decreased the risk of downloading harmful apps and reduces the number of options to lower the complexity. Further upgradation is required to completely eliminate the irrelevant applications from Google Play Store. The numbers are evidence for the fact that well witness a bigger change in the future.",https://www.analyticsvidhya.com/blog/2018/02/google-uses-ai-for-filtering-android-apps/
Google has Extended its Machine Learning Capabilities to Predict Flight Delays,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Google is using AI for filtering Play Store Applications|Minigo: An Open-Source Python Implementation Inspired By DeepMinds AlphaGo|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google today announced the addition of two new features to its Flights application. And both of them will make a significant difference to the flyer.                              Source: GoogleWith the help of machine learning, Google Flights will now be able to predict if a flight will be delayed, and by how long. The other feature will assist users in making confident decisions that theyre getting the cheapest fare, regardless of their destination.The basic economy class has been introduced by a lot of airlines but the details around it remain fuzzy for the average flyer. Does it include overhead bin space? Can we select our seat without paying extra? Is the baggage fee involved in it? Well, worry no more  the Flights app will do all that for us. It will break down all the intricacies of the basic economy fare to help us pick the right option.Using historical flight data, Googles machine learning algorithms will predict the status of each flight. A flight will only be marked as a risk of being delayed if the algorithm is 80% (or more) confident in the prediction. Simply searching for the flight or the route on the app will bring up the information.Just to cover its bases, Google still tells us that we should reach the airport on time and not rely blindly on the apps predictions (which kills the point of the feature). But Google is constantly improving its model and frankly, who doesnt want to know when there is a risk of their flight being delayed, in order to save the waiting time?Currently, these features are only available on the Delta, American and United airlines flights but the expectation is that it will be extended globally in the near future. You can check our their full blog post here.Google continues to branch out its machine learning capabilities into different fields. Because of its market share, it will provide intense competition to the already crowded market. Expect the others to follow suit and bring these features to their respective apps. The winner out of all this? The flyers, of course.",https://www.analyticsvidhya.com/blog/2018/02/google-extended-machine-learning-capabilities-predict-flight-delays/
Minigo: An Open-Source Python Implementation Inspired By DeepMinds AlphaGo,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Google has Extended its Machine Learning Capabilities to Predict Flight Delays|A Collection of 10 Data Visualizations You Must See|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve been fascinated with DeepMinds AlphaGo program, theres good news for you. A few Go enthusiasts have replicated the results of the AlphaGo Zero paper, using a few resources provided by Google.                                    Source: WIREDThe developers are keen to stress that this project is in no way associated with the official AlphaGo program by DeepMind. Its an independent effort that is inspired by AlphaGo, just not affiliated to it.According to the developers, Minigo is a pure Python implementation of a neural-network based Go AI, using TensorFlow. It adds a few features and architecture changes to the Mastering the game of Go without human knowledge paper. Very recently, this architecture was extended further in the Mastering Chess and Shogi by self-play with a general reinforcement learning algorithm paper.The goals of this project, as described by the authors, are listed below:If youre interested in doing this on your machine, you can access Minigos source code, and other resources, here.Just a note here that you will need the following before you can get started:The developers repeatedly mention that this is not a DeepMind project and is explicitly not meant to compete with AlphaGo. They just wanted other developers in the community to understand (and perhaps replicate or improve) how the Go model works. Its definitely something to keep your eye on as more progress is made in this study.",https://www.analyticsvidhya.com/blog/2018/01/minigo-an-open-source-python-implementation-inspired-by-deepminds-alphago/
A Collection of 10 Data Visualizations You Must See,"Learn everything about Analytics|Introduction|Visualizing the tree line using solar panels|Calculating the Age of the Universe|Rendering the Moon using Earths Colors|Gaussian Distribution|1.3 Billion Taxi Trips in New York City|Instability of an unsteered bicycle|The World Seen Through 17,000 Travel Itineraries|The BB-8 Droid|Visualizing Solar Eclipses|The Jimi Hendrix Experience","Share this:|Like this:|Related Articles|Minigo: An Open-Source Python Implementation Inspired By DeepMinds AlphaGo|AI May Have Decoded One of the Worlds Most Mysterious Manuscripts|
Pranav Dar
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Writing codes is fun. Creating models with them is even more intriguing. But things start getting tricky when it comes to presenting our work to a non-technical person.This is where visualizations comes in. They are one of the best ways of telling a story with data. In this article, we look at some of the best charts and graphs people have created using tools like Python, R, and Tableau, among others.I have also included the link to the source code or the official research paper, so you can attempt to create these visualizations on your machines or just get a general understanding of how it was created.Lets get into it.This is a beautiful graph where the author has visualized the trees around his house using solar panel data and the position of the sun.You can access the source code here.Tool Used: RUsing data from Hyperleda, the author created this visualization in R to calculate the age of the universe. Astonishingly, his calculations were only off by -0.187% from the accepted age of the universe.You can access the source code here.Tool Used: RThis is a rendering of how the moon would look if it was filled with Earths colors. A spectacular effort using the moons topography which was converted to colormap using matplotlib and cpt city.The 3-D model was created with blender and python.You can access the source code here.Tool Used: PythonWhats impressive about this graphic is that it was created using JavaScript in a HTML document (you read that correctly).You can access the source code here.Tool Used: JavaScriptAn absolutely gorgeous firefly-like visualization. The author collected, cleaned and plotted the pick-up and drop-off locations of all taxi rides in New York between January 2009 and June 2016.You can access the source code here.Tool Used: PythonThis surreal visualization shows 800 runs of a bicycle being pushed to the right. For each run, the path of the front wheel is shown until the bicycle fell over. The research paper is written in a humorous tone as well, adding to the already fascinating effort of creating self-riding bicycles.You can access the research paper here.Tool Used: PythonAccording to the author, Each city is plotted with its (lat, lng) coordinates and connected to any other city that came after it in someones itinerary. For example, London is connected to Paris because theres at least one itinerary going from the former to the latter. The countries were clustered together and colored using theLouvain Modularity. All countries of the same color have trips that go between each other more often than other countries.You can further read about it, and access the source code files, here.Tools Used: Tableau, GephyIf you are a fan of the Star Wars franchise then this one is for you. The author created this droid using shapes (and a couple of other libraries) in R.You can access the source code here.Tool Used: RThe author hasvisualized solar eclipses over 5 millennia based on type, date, duration, and latitude. A spectacular effort.You can access the Tableau workbook here.Tool Used: TableauThis Tableau visualization contains all of Jimi Hendrixs live performances between 1967 and 1970. It includes which songs were played and their frequency, where the concerts were held, among other insightful data.You can access the Tableau workbook here.Tool Used: TableauThese are just a few of the amazing visualizations out there. Have you come across any others recently? Let us know using the comments section below!",https://www.analyticsvidhya.com/blog/2018/01/collection-data-visualizations-you-must-see/
AI May Have Decoded One of the Worlds Most Mysterious Manuscripts,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|A Collection of 10 Data Visualizations You Must See|Alibaba Cloud Launches AI-Powered Malaysia City Brain Initiative|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The Voynich Manuscript is one of the most mysterious manuscripts in history. But with the help of AI, researchers may finally have decoded a part of it.                     Source: Beinecke Rare Book & Manuscript LibraryThe book consists of around 240 pages of inscrutable illustrations and coded language. It even has fold out pages, which is incredible for a book written some 600 years ago.Its been owned by all sorts of people, from alchemists to emperors, since it was written in the 15th century. It finally got its modern name in the 20th century after a Polish book dealer, Wilfrid Voynich.All sorts of codebreakers, cryptographers and folks from other walks of life have tried making sense of the seemingly undecipherable language, but to no avail. Until now.A few Canadian data scientists may finally have broken a part of the code.Using a technique called algorithmic decipherment, these researchers from the University of Alberta have used artificial intelligence to crack the code hidden in this ancient text.The algorithm was tested on 380 different translations and was able to recognise the language of origin (by looking for patterns) an impressive 97% of the time. This model was then applied on the Voynich manuscript. The researchers suspected it was written in Arabic, but the model disagreed. It indicated Hebrew as the most likely language.Following this, they trained the model to look for alphagram (a mixture of alphabetically organised anagrams) patterns in the text. As it turned out, almost 80% of the words in the book were present in the Hebrew dictionary. But the issue still plaguing the model is that when put together, the sentences dont often make much sense.The research is still continuing and experts in the Hebrew language have been called in to look at the manuscript as well. You can read the research paper here.This is an exciting time in the world of Natural Language Processing. No doubt researchers all over the world will be clamouring to apply (and improve) this algorithm to shine a new light on more ancient texts that have eluded historians so far.",https://www.analyticsvidhya.com/blog/2018/01/ai-may-decoded-one-worlds-mysterious-manuscripts/
Alibaba Cloud Launches AI-Powered Malaysia City Brain Initiative,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|AI May Have Decoded One of the Worlds Most Mysterious Manuscripts|A Beginners Guide to Channel Attribution Modeling in Marketing (using Markov Chains, with a case study in R)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Alibaba Cloud, the cloud computing section of Alibaba Group, today announced the launch of the Malaysia City Brain. This is being done in collaboration with the Malaysia Digital Economy Corporation (MDEC) and Dewan Bandaraya Kuala Lumpur (DBKL).                        Source: TechNodeThe initative will drive Malaysias digital transformation with cloud technology and artificial intelligence. The whole concept has been designed to help the Malaysian government upgrade their decision-making processes and transform the city into an intelligent one. City Brain is powered by Alibabas computing engine Apsara.In the last couple of years, Alibaba has pioneered its cloud computing to solve traffic problems in a few Chinese cities. So in the first phase of the Malaysia City Brain initiative, dealing with traffic flows will be the main focus area. By calculating the time it takes to reach intersections, the team behind this project aims to help with cutting down the traffic queues.Not just traffic flows, City Brain can connect together several public systems, like the ambulance call and the traffic command system. Using real-time data like traffic volume and speed in particular lanes, the AI can calculate the quickest route for the ambulance from and to the hospital.Along with this initiative, Alibaba Cloud also announced a Big Data crowd intelligence platform called Malaysia Tianchi Big Data Platform.Backed by MDEC, the aim here is to develop 500 data experts and 300 startups within the next two years by leveraging Alibaba Clouds AI tools and facilities. Enterprises, start-ups, research communities, and even universities will all fall under this umbrella.You can find out more about City Brain in the video below:Alibaba had already implemented this in its hometown of China and its quite heartening to see them branching out into another Asian country. There is a lot of talk about regulating AI etc., but this is a step in the right direction with machines working in tandem with humans. This will show the skeptics that AI can be used for a good purpose and will affect thousands, if not millions, of lives in the process in a good way.If successful (and it already is in China), we should expect to see other countries taking up this initiative to help with governance and in making smart data-driven decisions.",https://www.analyticsvidhya.com/blog/2018/01/alibaba-cloud-malaysia-city-brain-ai/
"A Beginners Guide to Channel Attribution Modeling in Marketing (using Markov Chains, with a case study in R)",Learn everything about Analytics|Introduction|Table of Contents|What is Channel Attribution?|Case Study of an E-Commerce Company|Implementation using R|End Notes,"Markov Chains|Removal Effect|Share this:|Like this:|Related Articles|Alibaba Cloud Launches AI-Powered Malaysia City Brain Initiative|Google Releases TensorFlow 1.5  All you need to know|
Guest Blog
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In a typical from think to buy customer journey, a customer goes through multiple touch points before zeroing in on the final product to buy. This is even more prominent in the case of e-commerce sales. It is relatively easier to track which are the different touch points the customer has encountered before making the final purchase.Source: MarTech TodayAs marketing moves more and more towards the consumer driven side of things, identifying the right channels to target customers has become critical for companies. This helps companies optimise their marketing spend and target the right customers in the right places.More often than not, companies usually invest in the last channel which customers encounter before making the final purchase. However, this may not always be the right approach. There are multiple channels preceding that channel which eventually drive the customer conversion. The underlying concept to study this behavior is known as multi-channel attribution modeling.In this article, we look at what channel attribution is and how it ties into the concept of Markov chains. Well also take a case study of an e-commerce company to understand how this concept works, both theoretically and practically (using R).Google Analytics offers a standard set of rules for attribution modeling. As per Google, An attribution model is the rule, or set of rules, that determines how credit for sales and conversions is assigned to touchpointsin conversion paths. For example, the Last Interaction model in Analytics assigns 100% credit to the final touchpoints(i.e., clicks) that immediately precede sales or conversions. In contrast, the First Interaction model assigns 100% credit to touchpoints that initiate conversion paths.We will see the last interaction model and first interaction model later in this article. Before that, lets take a small example and understand channel attribution a little further. Lets say we have a transition diagram as shown below:In the above scenario, a customer can either start their journey through channel C1 or channel C2. The probability of starting with either C1 or C2 is 50% (or 0.5) each. Lets calculate the overall probability of conversion first and then go further to see the effect of each of the channels.P(conversion) = P(C1 -> C2 -> C3 -> Conversion) + P(C2 -> C3 -> Conversion)= 0.5*0.5*1*0.6 + 0.5*1*0.6
= 0.15 + 0.3
= 0.45Markov chains is a process which maps the movement and gives a probability distribution, for moving from one state to another state. A Markov Chain is defined by three properties:We know the stages through which we can pass, the probability of moving from each of the paths and we know the current state. This looks similar to Markov chains, doesnt it?This is, in fact, an application of a Markov chains. We will come back to this later; lets stick to our example for now. If we were to figure out what is the contribution of channel 1 in our customers journey from start to end conversion, we will use the principle of removal effect. Removal effect principle says that if we want to find the contribution of each channel in the customer journey, we can do so by removing each channel and see how many conversions are happening without that channel being in place.For example, lets assume we have to calculate the contribution of channel C1. We will remove the channel C1 from the model and see how many conversions are happening without C1 in the picture, viz-a-viz total conversion when all the channels are intact. Lets calculate for channel C1:P(Conversion after removing C1) = P(C2 -> C3 -> Convert)= 0.5*1*0.6= 0.330% customer interactions can be converted without channel C1 being in place; while with C1 intact, 45% interactions can be converted. So, the removal effect of C1 is0.3/0.45 = 0.666.The removal effect of C2 and C3 is 1 (you may try calculating it, but think intuitively. If we were to remove either C2 or C3, will we be able to complete any conversion?).This is a very useful application of Markov chains. In the above case, all the channels  C1, C2, C3 (at different stages)  are called transition states; while the probability of moving from one channel to another channel is called transition probability.Customer journey, which is a sequence of channels, can be considered as a chain in a directed Markov graph where each vertex is a state (channel/touch-point), and each edge represents transition probability of moving from one state to another. Since the probability of reaching a state depends only on the previous state, it can be considered as a memory-less Markov chain.Lets take a real-life case study and see how we can implement channel attribution modeling.An e-commerce company conducted a survey and collected data from its customers. This can be considered as representative population. In the survey, the company collected data about the various touch points where customers visit before finally purchasing the product on its website.In total, there are 19 channels where customers can encounter the product or the product advertisement. After the 19 channels, there are three more cases:The overall categories of channels are as below:Now, we need to help the e-commerce company in identifying the right strategy for investing in marketing channels. Which channels should be focused on? Which channels should the company invest in? Well figure this out using R in the following section.Lets move ahead and try the implementation in R and check the results. You can download the dataset here and follow along as we go.Output:We will do some data processing to bring it to a stage where we can use it as an input in the model. Then, we will identify which customer journeys have gone to the final conversion (in our case, all the journeys have reached final conversion state).We will create a variable path in a specific format which can be fed as an input to the model. Also, we will find out the total occurrences of each path using the dplyr package.Output:Output:Now, we will create a heuristic model and a Markov model, combine the two, and then check the final results.Output:Output:Before going further, lets first understand what a few of the terms weve seen above mean.First Touch Conversion: The conversion happening through the channel when that channel is the first touch point for a customer. 100% credit is given to the first touch point.Last Touch Conversion: The conversion happening through the channel when that channel is the last touch point for a customer. 100% credit is given to the last touch point.Linear Touch Conversion: All channels/touch points are given equal credit in the conversion.Getting back to the R code, lets merge the two models and represent the output in a visually appealing manner which is easier to understand.The scenario is clearly visible from the above graph. From the first touch conversion perspective, channel 10, channel 13, channel 2, channel 4 and channel 9 are quite important; while from the last touch perspective, channel 20 is the most important (in our case, it should be because the customer has decided which product to buy). In terms of linear touch conversion, channel 20, channel 4 and channel 9 are coming out to be important. From the total conversions perspective, channel 10, 13, 20, 4 and 9 are quite important.In the above chart we have been able to figure out which are the important channels for us to focus on and which can be discarded or ignored. This case gives us a very good insight into the application of Markov chain models in the customer analytics space. E-commerce companies can now confidently create their marketing strategy and distribute their marketing budget using data driven insights.Author Bio:This article was contributed byPerceptive Analytics. Chaitanya Sagar, Prudhvi Potuganti and Saneesh Veetil developed this article.Perceptive Analytics provides data analytics, data visualization, business intelligence and reporting services to e-commerce, retail, healthcare and pharmaceutical industries. Our client roster includes Fortune 500 and NYSE listed companies in the USA and India.",https://www.analyticsvidhya.com/blog/2018/01/channel-attribution-modeling-using-markov-chains-in-r/
Google Releases TensorFlow 1.5  All you need to know,"Learn everything about Analytics|Introduction|Eager Execution for TensorFlow|TensorFlow Lite|What else is there in TensorFlow 1.5?|So, how to install TensorFlow 1.5?|Our take on this","Share this:|Like this:|Related Articles|A Beginners Guide to Channel Attribution Modeling in Marketing (using Markov Chains, with a case study in R)|Twitter is using Machine Learning to Make Photos More Engaging|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow is a very popular open-source library that is written in Python, C++ and CUDA. Its uses span a range of tasks. Chief amongst them, is its use in machine learning applications for building neural networks.The TensorFlow library has seen many releases since 2015, and Google announced the latest update a couple of days back  TensorFlow 1.5. According to the team, they were monitoring feedback about the programming style of TensorFlow, and how developers really wanted an imperative, define-by-run programming style.This has led to the inclusion of Eager Execution for TensorFlow. Its available as a preview with the latest build and is available to the general public. With Eager Execution for TensorFlow enabled, users can now execute TensorFlow operations as soon as they are called from Python. Not only does this make it easier for TensorFlow newcomers, it also makes research projects more insightful and intuitive.You can learn more about Eager Execution here.The developer preview of TensorFlow Lite is also available as part of the latest release. TensorFlow Lite is TensorFlows solution for mobile and embedded devices. With it, you can take a trained TensorFlow model and convert it to a .tflite file. This can then be executed on a mobile device. Lets say you want to build a model to classify an image. Using TensorFlow Lite,a trained model can be deployed to the mobile device and the image will be classified directly on the device.You can read more about TensorFlow Lite here.Apart from that, below is a list of some of the other minor changes and additions:It just takes a line of code. To install TensorFlow 1.5, use the usual pip installation (pip3 if youve upgraded to python3):You can access TensorFlows GitHub page here.All the minor features and changes considered, Eager Execution for TensorFlow is one of the most anticipated additions to the library. With this enabled, the user no longer needs to execute a pre-constructed graph with Session.run().A welcome change, indeed.",https://www.analyticsvidhya.com/blog/2018/01/tensorflow-1-5-google-released/
Twitter is using Machine Learning to Make Photos More Engaging,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Google Releases TensorFlow 1.5  All you need to know|Using Machine Learning to help Refugees find Employment|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Twitter has revealed that its using machine learning to crop photos to show the most interesting parts. In the current functionality, photos are cropped off without recognising whats in them and what should be shown on the users timeline.In this blog post, researcher Lucas Theis and machine learning lead Zehan Wang explained how they initially used only facial recognition to crop the photos. As you can imagine, this had its limitations. Pictures of cats, dogs, scenery, etc. were not being picked up by the algorithm, hence reducing the accuracy of the model.So to drill down into all the aspects in a picture, the team decided to crop the images using saliency. To define what features are salient, they spent time gathering data from eye studies to understand eye-tracking, which records what parts of a picture people look at first. They then built a neural network model based on these features.But the real challenge was  how do you do this in real-time? Millions of images are uploaded to Twitter so how do you use this neural network algorithm to crop photos without affecting the load time for the end user?The team used two techniques to reduce the size of the neural network and also its computational requirements. The first is called knowledge distillation which they used to train a smaller network to imitate the more powerful one.With this, an ensemble of large networks is used to generate predictions on a set of images. These predictions, together with some third-party saliency data, are then used to train a smaller, faster network.The second technique they used was pruning to iteratively remove features that were not helping the performance of the neural network (and were costly to compute as well). Using these two techniques together enabled the model to analyse and crop the picture 10 times faster than before (basically, in real-time as soon as an image was uploaded to Twitter).Source: TwitterIn the above images, you can see the contrast between images uploaded before the neural network was applied (on the left), and the same images after the model does its work and focuses on the important aspects.Not every machine learning technique has to lead to a breakthrough, as this tweak from Twitter shows. It will help users laser down their focus on what stands out immediately in the image rather than waste time in enlarging it. Its a welcome addition and Twitter is currently rolling out the changes for everyone.",https://www.analyticsvidhya.com/blog/2018/01/twitter-using-machine-learning-make-photos-engaging/
Using Machine Learning to help Refugees find Employment,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Twitter is using Machine Learning to Make Photos More Engaging|The Ultimate Learning Path to Becoming a Data Scientist in 2018|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The global refugee crisis has been going on for quite a while now. According to the United Nations, an incredible 65 million people have been displaced from their home globally. Per day, more than 28,000 people are forced to flee their surroundings.A recently published study aims to streamline and answer the pressing question  where should the authorities place the refugees to optimize their skills? Currently, the process is done manually and in a lot of cases, at random. This leads to a mismatch between what refugees can offer, and where they are actually placed and what theyre asked to do.A team from Stanford University has developed a machine learning algorithm for geographically placing refugees to maximize their overall employment rate. The algorithm uses features like country of origin, gender, languages, and age to train the model. The training set for the algorithm had 33,000 working age refugees in the period between 2011 and 2016. The model was then tested on 900 refugees entering the United States and Switzerland at the end of 2016.The results of the model were pretty impressive. According to the report, The median refugees predicted probability of employment in the United States more than doubled, increasing from approximately 25% to 50%. 34% of the refugees in the test group found work within three months of arriving in the United States. Using this algorithm (and fine-tuning it more) the researchers predict this figure could climb up to 48%.Figures from the United States model on the test setIn the case of Switzerland, the model found that the ability to speak French resulted in a much better pay-off for refugees assigned to French-speaking cantons, rather than German speaking cantons.Figures from the Switzerland model on the test setYou can read the full Stanford research report here.This is a wonderful example of data and machine learning being put to a great use in our society. The refugee crisis is a very real thing and hopefully this research will help them settle down and stabilize themselves. The algorithm still needs to be worked upon to improve the accuracy  test it on more countries, use a bigger dataset, test for longer and shorter term results, etc.If the authorities start using their current processes with the machine learning model, it will lead to a better overall experience for everyone involved.",https://www.analyticsvidhya.com/blog/2018/01/machine-learning-refugees-find-employment/
The Ultimate Learning Path to Becoming a Data Scientist in 2018,Learn everything about Analytics|Introduction|What changes have we made in the learning path?|January|February|March|April|May|June|July|August|September  October|November  December|A few things to keep in mind:|Lets get started,"Here it is then  the ultimate learning path to becoming a Data Scientist in 2018!|Getting Started with Data Science and Python|Statistics, Data Exploration and Basic Data Visualization|Probability and Machine Learning Basics (Part I)|Machine Learning Basics (Part II) and Feature Engineering|Build Your Data Science Persona|Advanced Machine Learning and Time Series Modeling|Dealing with Unstructured Data|Introduction to Deep Learning|PRACTICE!|Apply for jobs and further enhance your portfolio|Share this:|Like this:|Related Articles|Using Machine Learning to help Refugees find Employment|IBM using AI to Predict the risk of Psychosis|
Kunal Jain
|46 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So youve taken the plunge. You want to become a data scientist. But where to begin? There are far too many resources out there. How do you decide the starting point? Did you miss out on topics you should have studied? Which are the best resources to learn?Dont worry, we have you covered!Analytics Vidhyas learning path for 2016 saw 250,000+ views. In 2017, we went even further and saw an incredible 500,000+ views! So this year, we have made the learning path more interactive than ever before and we cant wait for you to experience it yourself.This year, the learning path has been designed on a completely new LMS portal. This portal allows you to track your progress as your data science journey continues. We have designed questions and exercises after each module to test your understanding. You will also be able to access the related hackathons / practice problems from the same place.We even have a discussion portal within the learning path where you can share your doubts and queries and even post the awesome projects youre working on!Take a sneak peek below of how the progress tracking looks like:Just a few things to note before you experience our new LMS portal:Below is a summary of the learning path  an overview what you should follow throughout the year. Lets get crackingBy the end of January, youll know what role data science plays in the industry. Youll also be able to answer the burning question  why use Python and how is it useful?Before this month is over, you should have a firm grasp over the basics of statistics. You should also be proficient at exploring the dataset given to you and know the role data visualization plays in this. The budding data scientist is slowly coming out!Time to get into machine learning!By the end of the month, you should have a firm command on the basic machine learning topics like linear and logistic regression, among others. To test what youve learnt so far, we will provide you with two projects to apply your newly acquired data science skills!Continue learning the ML basics and by the end of April, you should know enough to take part in hackathons are secure a decent rank. Also, go in depth into feature engineering  one of the MOST important things in data science.Building models is not enough. The real test of a data scientist comes in explaining the power of the model youve created to non-technical people. By the end of May, you should have structured your thinking and personality as a data scientist to be able to do this.This is a very critical month in your progress. Attempt to get a high ranking in hackathons and competitions all the while learning how to make an impactful presentation of your work. Also, start looking for an internship; you should have enough knowledge by now to secure one.Deep dive into advanced machine learning. With half the year behind you, you should be ready to tackle advanced ML algorithms and time series models.Unfortunately, most real-world data comes in an unstructured format. This month you should get a deeper understanding of how to deal with unstructured data in business scenarios including learning the Natural Language Processing field. At the end of the month, you will be given a few projects to apply your newly learned skills.Here comes one of the hottest data science subjects around  Deep Learning! By the end of August, you should be able to deal with basic neural network problems. As usual, we will provide you with a couple of projects to test your mettle.Practice is the name of the data science game. Keep checking your progress by taking part in competitions.By the end of October, you should also be familiar with topics like recommendation methods, and reinforced learning. This is also when you should start taking up a language like SQL to interact with databases (a truly important skill for a data scientist).If you have seriously followed this plan, you should be able to deal with interview questions. Continue to acquire new skills, delve into Big Data and make sure you stick to your plan!A few pointers to make this learning path (and 2018) super successful for you:2018  here we come! I hope this year our path gets 1,000,000+ visits! We have made sure that we put all our wisdom and experience in creating it. Having said that Is there anything you feel we should have included?Or if you had taken our learning path last year  what was your experience and how do you like the current changes? Let us know your thoughts in the comments section below!",https://www.analyticsvidhya.com/blog/2018/01/ultimate-learning-path-becoming-data-scientist-2018/
IBM using AI to Predict the risk of Psychosis,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|The Ultimate Learning Path to Becoming a Data Scientist in 2018|Facebooks AI team Releases Detectron  A Platform for Object Detection Research|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In another major breakthrough in the healthcare field, IBM researchers have come up with a machine learning system to predict the risk of a patient developing psychosis. For the uninitiated, Wikipedia defines the condition as an abnormal condition of themindthat involves a loss of contact withreality.IBM previously attempted this study in 2015 with limited results. Back then, they had asked patients to talk about themselves for close to an hour, and then analysed the results of those interviews using the Natural Language Processing (NLP) technique. The model, however, had several limitations as IBM mentioned in their blog post:only one cohort, from a single location, and using a single evaluation protocol.This time around, the research team used a different approach for creating the model. To get data for the research, the team analysed the speech patterns of 59 people. Patients were given a story to read and were then asked to talk about what they understood from it. Following that, the transcripts were broken down into different parts and a score was assigned based on how coherent the sentences were.The model was able to predict which patients would develop psychosis with an impressive 83% accuracy. In other words, 19 participants developed a psychotic disorder within 2 years on the study, while 40 did not. The research found that those at a huge risk of developing the illness used far less coherent sentences than healthy people.IBM is also currently researching and developing models for other mental health problems like depression, Parkinsons disease, Alzheimers and chronic pain. Guillermo Cecchi, the author of the post and this study, has also posted the below video:This is a huge step forward inneuropsychiatric assessments. It shows how AI can be an extremely effective tool to assist mental health professionals both inside and outside the hospital. If the risk of psychosis can be predicted years in advance, the patient can be given treatment well ahead of time to prevent the onset of the illness.Also, currently the feeling is that as far as mental health issues go, homeless and poor people struggle to get any sort of medical treatment for it. The hope is that AI will enable these folks to get an equal chance of getting help.",https://www.analyticsvidhya.com/blog/2018/01/ibm-using-machine-learning-predict-risk-psychosis/
Facebooks AI team Releases Detectron  A Platform for Object Detection Research,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|IBM using AI to Predict the risk of Psychosis|Amazon Go Launched: An AI Powered Store that puts an end to Check-Out Lines|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We covered Googles Cloud AutoML Vision last week and, as we predicted, Facebook has already come out with a platform for object detection of its own  Detectron.                                     Source: FacebookDetectron is a software system developed by Facebooks AI Research team (FAIR) that implements state-of the art object detection algorithms. It is written in Python and leverages the Caffee2 deep learning framework underneath.Detectron aims to provide a high quality and industry standard codebase for object detection research. The results it has posted are incredibly accurate. The image above shows the prediction power of the software. The following object related algorithms are embedded in Detectron:Along with the Python code, FAIR has also released performance baselines for over 70 pre-trained models. Once the model(s) is trained, it can be deployed on the cloud and even on mobile devices.You can check all of this out on the Github library for Detectron hereand the official Facebook launch page Googles Cloud AutoML Vision.Detectrons release will help research communities around the world immeasurably. Its open source so you can download the code behind this software and even use the plethora of pre-trained models the team has released. From augmented reality to various computer vision tasks, Detectron has a wide variety of uses in the research community.",https://www.analyticsvidhya.com/blog/2018/01/facebook-launched-detectron-platform-object-detection-research/
Amazon Go Launched: An AI Powered Store that puts an end to Check-Out Lines,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Facebooks AI team Releases Detectron  A Platform for Object Detection Research|Irdeto is Using Machine Learning in the Fight Against Piracy|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Amazon opened a cashier-less store yesterday in Seattle in a bid to do away with long check out lines. Called Amazon Go, the store uses AI powered technology to charge the customers for items they have taken off the shelves.                                    Source: AmazonThe store requires the customers to have the Amazon Go app installed before they enter. They also need to select their payment method in advance. This then generates a QR code which is scanned at the front of the store.According to Amazon, the technology behind this is the same thats being used in self-driving cars: computer vision, sensor fusion and deep learning. Wall mounted cameras and sensors track every item thats taken (or returned) off the shelf and keep track of it in a visual cart. Once the customer is done shopping, they can walk right out of the door. Within a few minutes, Amazon will then send a receipt of the bill and charge their Amazon account.But the technology isnt just limited to this. Amazon needs to understand when to restock items, which items to be placed in which shelf, among many other questions. It has to optimise traffic flow and get every decision spot on (if a customer picks up a block of cheese but then puts it back, the AI has to be absolutely certain not to charge the customer for it). To pull off something as ambitious and audacious as this, you need plenty of data to train the AI and machine learning models. As far as Amazon is concerned, that is not a stumbling block at all.Amazon Go has been in beta testing for almost a year. Amazons employees were the volunteers during that phase and it has been well received throughout. Even though there are no cashiers inside the store, quite a few staff members are still present to help out customers and to prepare the ready-to-go food in the kitchen area.Could this spell the end of most other brick and mortar stores? Will Amazon use this AI and expand its reach into other areas like book stores and full supermarkets? Its a significant effort (not to mention expense) setting up stores with sensors and other equipment to monitor the customer journey. But if Amazons history is anything to go by, they will be planning to go global with this soon. And lets face it, who isnt tired of standing and waiting in long queues? Its a welcome addition to our shopping experience and I, for one, cant wait for Amazon to get this technology in India.",https://www.analyticsvidhya.com/blog/2018/01/amazon-go-ai-powered-store/
Irdeto is Using Machine Learning in the Fight Against Piracy,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Amazon Go Launched: An AI Powered Store that puts an end to Check-Out Lines|Googles Arts and Culture App Matches Your Face With Classical Art|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Anti-piracy laws have come a long way since the advent of online streaming. But the fight against piracy has turned even more desperate in recent years as content, from TV shows to sports matches, is finding illegal ways into millions of households. Look at a couple of common examples, seasons of the TV show Game of Thrones have been pirated billions of times and football matches are streamed illegally to millions every day around the world.                                 Source: IrdetoThe relative ease with which a person can put a stream online is a major issue for these anti-piracy companies. The days of torrents have come and gone; all the person needs is a streaming box and an internet connection.By the time these anti-piracy firms launch their counter-attack and try to take down streams, it is usually always too late. Shutting down all available streams is an impossible task even with huge manpower allotted to the task.This is where Irdeto has stepped in. Instead of throwing away hours of manual labor, Irdeto is using machine learning to fight back against the pirates. They have developed a convolution neural networkthat works through the internet to try to find illegal streams. Currently, the AI looks for broadcaster logos to identify which streams are illegal. The neural network was trained on a sample of more than a million images.However, pirates are savvy folks and they have begun masking or blanking out logos from streams. This makes it incredibly hard to find illegal copies of the content. Irdeto is working on improving its current algorithm by making the machine understand how to distinguish between the kits each team wears. Its also trying to incorporate facial recognition, wherein the machine will tag the face to the athlete/actor and enable a quick shutdown of the illegal course.The firm is also hoping to use the videos watermark to locate where the source of the stream is coming from. In response, the pirates either reduce the quality of the video so the watermark cannot be detected or use various sources to bring the stream together, rendering it almost untraceable.The battle is almost relentless and as technology continues to grow, so will the fight between the anti-privacy firms and the pirate distributors.One wonders if the people pirating the content will also start using advanced machine learning methods to circumnavigate companies like Irdeto? The hope is that more firms like Irdeto take up the fight against illegal streaming. But it goes to show the extent to which pirates will go to distribute the content and the incredible demand that sustains it.",https://www.analyticsvidhya.com/blog/2018/01/irdeto-using-machine-learning-piracy/
Googles Arts and Culture App Matches Your Face With Classical Art,Learn everything about Analytics||Our take on this,"Share this:|Like this:|Related Articles|Irdeto is Using Machine Learning in the Fight Against Piracy|10 Data Science, Machine Learning and AI Podcasts You Must Listen To|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The Arts and Culture app recently released by Google has been making waves on social media. It matches the users face with an art work from collections of museums around the world.Source: Googles Play StoreThe app was released in USA a couple of weeks ago and has also been launched in India this past weekend.In this app, the user uploads their selfie and the app uses facial recognition technology to match it with a classical work of art. The app tries to find the nearest match using various parameters, but its not a perfect fit just yet. Once a match is found, it returns the percentage of visual similarity between the users face and the art.According to Google, the aim behind this app is to engage users in finding out more about the worlds different cultures. Googles database has more than 80,000 works of art curated from more than 6,000 museums in over 70 countries.Google Photos, another application of the companys facial recognition research, has also made leaping strides and can now identify pets with an incredible 99.63% accuracy.This all comes about with training the machine learning algorithm with tons and tons of images, as Google explained in its post on pattern recognition.The technology behind the applicationGoogle is using facial recognition to identify similar patterns between the users uploaded selfie and the art work. Basically, facial recognition detects a face in the image, creates a print of unique characteristics, and then tries to match it with the existing characteristics in the database.These characteristics include all sorts of facial features, like the shape and size of the eyes and nose to the facial hair the user has (among many, many other features).Check out a demo of how the app works in the video below:Facial recognition has been one of the hottest topics on Googles agenda lately. This app has already gone viral and generated a lot of buzz. The advantage for Google here is that it gets millions of selfies to improve its algorithm and predict with greater accuracy. This enhances Googles already formidable database of information. Its safe to predict the likes of Microsoft also rolling out similar apps in the near future to improve the competition.",https://www.analyticsvidhya.com/blog/2018/01/google-arts-and-culture-app-matches-face-art-work/
"10 Data Science, Machine Learning and AI Podcasts You Must Listen To",Learn everything about Analytics|Analytics Vidhyas own DataHack Radio Podcast!|The Data Skeptic|The OReilly Data Show|Concerning AI|Data Stories|Learning Machines 101|Women in Data Science (WiDS) Podcast|Artificial Intelligence in Industry (Dan Faggella)|Talking Machines|This Week in Machine Learning & AI|Linear Digressions|Freakonomics Radio,"Share this:|Related Articles|Googles Arts and Culture App Matches Your Face With Classical Art|Building a FAQ Chatbot in Python  The Future of Information Searching|
Pranav Dar
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With the rapid pace at which technology is driving innovation in machine learning and artificial intelligence, it has become immensely important to keep pace with the ongoing trends in data science. However, it can become challenging to read everything thats out there.Podcasts are a great alternative to keep yourself updated. With the remarkable rise of the data science industry in recent years, enough podcasts have been created for us to geek out over.In this article, we look at 10 such podcasts that we feel any data scientist must listen to:This is Analytics Vidhyas exclusive podcast series which features top leaders and practitioners in the data science and machine learning industry.In fact, the first three episodes are already available! Listen and subscribe now!Its available on SoundCloud, iTunes, Google Podcasts and of course, our own site!Average episode duration:60 minutesTotal Number of Episodes: 23Area(s) of focus:Data science, machine learning, deep learning, artificial intelligenceWith episodes ranging from anywhere between 15 minutes to an hour, the Data Skeptic is a great way to introduce yourself to the world of Data Science podcasts. The topics include interviews with data science practitioners to talk about real world data science challenges, simple academic concepts like feature selection, NLP, decision trees, among may others.If you have to narrow down to one podcast, make sure its this one.Average episode duration: Anywhere between 15-60 minutesTotal Number of Episodes: 198Area(s) of focus:Data science concepts and real-world issuesThis can get technical and quite in-depth at times, but its still a great way of keeping up to date with whats happening in the world of AI and Machine Learning. Its hosted by OReilly Medias Chief Data Scientist, Ben Lorica.Average episode duration: 20-60 minutesTotal Number of Episodes:60Area(s) of focus:Technically driven, deals with current issuesThis podcast offers a slightly different take on AI  it looks at the threats and risks that the growing influence of AI can have on todays society, and what steps we need to take to combat it.Average episode duration: 20-40 minutesTotal Number of Episodes:62Area(s) of focus:Regulations and concerns in the AI worldData Visualization is at the heart of this podcast. Hosts Enrico Bertini andMoritz Stefaner interview folks from various fields every week. Recent topics include Data Pottery, Bitcoin Visualizations and a fascinating episode of Whats Going on with this Graph?.Average episode duration:30-50 minutesTotal Number of Episodes: 112Area(s) of focus:Data visualizationTheir aim is to demystify the field of Artificial Intelligence by explaining fundamental concepts in an entertaining manner. Their topics do tend to get technical sometimes, like How to use Expectation Maximization to Learn Constraint Satisfaction Solutions or How to Use Radial Basis Function Perceptron Software for Supervised Learning. However, some topics are meant for all listeners regardless of technical knowledge.Average episode duration:20-30 minutesTotal Number of Episodes:69Area(s) of focus:Technically driven, intermediate to advanced machine learning conceptsIm a big fan of the Women in Data Science (WiDS) series. In the WiDS podcast, Stanford Professor Margot Gerritsen interviews women across the data science profession as they share their career highlights, advice, and lessons learned along the way.Its quite an inspirational podcast and covers a wide range of domains, from healthcare to seismology to human rights and more.Average episode duration:30-40 minutesTotal Number of Episodes:9Area(s) of focus:Data science, machine learning, data science careerEvery week, Dan Faggella interviews data scientists and AI leaders from the worlds companies to figure out the applications and implications of AI. There are tons of episodes you can listen to from the last couple of years that are still relevant. The latest episode, Will you buy your home or car using AI? is a very pertinent topic in todays society.Average episode duration:30 minutesTotal Number of Episodes:99Area(s) of focus:Interviews with data practitioners, discussions on current topicsIf youre new to data science or are not a fan of the technical stuff, this podcast is for you. Each episode features interviews with industry experts about data science and gives a holistic overview of a few techniques. Most episodes also feature listeners calling in and asking questions.Average episode duration: 60 minutesTotal Number of Episodes:29Area(s) of focus:Basic to intermediate data science concepts, listener Q&A , interviews with industry expertsEpisodes in this podcast are churned out at a fairly regular interval every week. This features interviews with AI /ML experts on a variety of data science topics.Average episode duration: 45 minutesTotal Number of Episodes:111Area(s) of focus:Current topics in ML and AI including projects and companiesThe hosts, Ben Jaffe and Katie Malone, manage to break down complex data science problems and techniques into snippets of information that can be easily digested by the casual listener.Average episode duration: 15 minutesTotal Number of Episodes:164Area(s) of focus:Data science and machine learning concepts applied to real-world issuesDisclaimer: This isnt exclusively a data science podcast. Host Stephen J. Dubner explores a whole host of puzzling issues in the world but it gives the listener a good idea of how data science can be integrated into economics and other global issues.Average episode duration:45 minutesTotal Number of Episodes:310Area(s) of focus:Economics, interviews with social scientistsAre there any other podcasts you listen to? Add them in the comments section below!Note: The total number of episodes mentioned in this article are updated till the publishing date.",https://www.analyticsvidhya.com/blog/2018/01/10-data-science-machine-learning-ai-podcasts-must-listen/
Building a FAQ Chatbot in Python  The Future of Information Searching,Learn everything about Analytics|Introduction|Table of Contents|Chatbots and NLP|Why RASA-NLU?|Building GST FAQ bot architecture|Installations|Server|Client|Engine|Our chatbot in action|End Notes|References,"Steps to build server side of the GST chat bot application:|Share this:|Like this:|Related Articles|10 Data Science, Machine Learning and AI Podcasts You Must Listen To|Japanese Scientists Unveil new AI Technology that can Read our Minds|
Yogesh Kulkarni
|45 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What do we do when we need any information? Simple: We Ask, and Google Tells.But if the answer depends on multiple variables, then the existing Ask-Tell model tends to sputter. State of the art search engines usually cannot handle such requests. We would have to search for information available in bits and pieces and then try to filter and assemble relevant parts together. Sounds time consuming, doesnt it?Source: InbentaThis Ask-Tell model is evolving rapidly with the advent of chatbots (also referred to as just bots).This article talks about the development of a bot for extracting information related to the recently introducedGoods and Services Tax (GST) in India. Being a relatively new concept, a lot of people are still trying to understand how it works. Chatbots can provide such information in a natural and conversational way. This article demonstrates building a chatbot for answering queries related to GST. Lets call this a GST-FAQ Bot!To know more about GST, like how to apply for registrations, tax slabs, etc., companies have posted Frequently Asked Questions (FAQs) on their websites. Going through that amount of information can be a painstaking process. In these situations, chatbots come in handy, effective and thus, have become enormously popular.These days, Natural Language Processing (NLP), especially its component Natural Language Understanding (NLU), has allowed bots to have a greater understanding of language and context. They are becoming more intelligent in understanding the meaning of the search and can return very specific, context-based information.Applications like WhatsApp, Facebook Messenger, Slack, etc. are increasingly being used by businesses. Bots are starting to replace websites-interface as well. From the considerable number of choices available for building a chatbot, this particular implementation uses the RASA-NLU library in Python.Many chatbot platforms are currently available, from rudimentary rule-based AIML (Artificial Intelligence Markup Language), to highly sophisticated AI bots. Some popular chatbot platforms are API.ai, Wit.ai, Facebook APIs, Microsoft LUIS, IBM Watson, etc.RASA-NLU builds a local NLU (Natural Language Understanding) model for extracting intent and entities from a conversation. Its open source, fully local and above all, free! It is also compatible with wit.ai, LUIS, or api.ai, so you can migrate your chat application data into the RASA-NLU model.Below is a demonstration on how to install RASA-NLU and build a simple FAQ bot in Python.A chatbot is a client-server application. In the case of RASA-NLU, even the server can be local. The client is nothing but the chabot UI. The interaction and architecture can be understood by the following diagram:RASA can be installed and configured on a standalone machine. Steps to follow:RASA-NLU is made up of a few components, each doing some specific work (intent detection, entity extraction, etc.). Each component may have some specific dependencies and installations. Options like MITIE (NLP + ML), Spacy and Sklearn are available to choose from. We will be using Spacy-Sklearn here.Client UI can be a web page (using frameworks like Flask in Python) or a mobile app. Flask is simple to code and runs locally. Use pip install flask and follow this tutorial to get a basic understanding of the framework.A RASA-NLU platform needs to be trained before we start using it. We need to supply it with a few sentences and mention which are the intents and entities in it. Intents are the actions/categories of the sentences and entities are the necessary variables needed to fulfil the actions.For example, I wish to book a flight from Mumbai to Pune on 27 March has flight-booking as the intent and Mumbai, Pune and 27 March as the entities. Similarly, many training examples can be used so that the RASA-NLU model is trained on different ways of extracting intents/entities from our domain conversations. This training data is stored in a json, a sample of which can be seen here.It contains many entries. One of the sample entries is shown below:Following is the explanation of some of the fields mentioned in the above code:You can use the below online tool as well, to generate this json file:https://rasahq.github.io/rasa-nlu-trainer/We can now look at the remaining components of our GST FAQ bot.Our chatbot client UI has been built using Flask framework. It uses two html templates to render the UI (i.e. the chat window). The outer UI is built with base.html as shown below:The content and other_footers blocks are defined in home.html as shown below:This is the heart of the chatbot. Based on the intent received from RASA-NLU, it dispatches the entities to the mapped call-back functions. The function in turn, depending on the entities, calls Knowledgebase to get the response. Once the response is received, it is sent back to the UI.Knowledgebase can be as simple as a dictionary of questions and answers, or as sophisticated as one can imagine/require (like databases, internet sources, etc.). This article, being minimalistic for demonstration purposes, fetches pre-coded responses from the dictionary.Lets take a look at the sample dictionary:The engines use of RASA-NLU for intent-entities extraction and dispatching call-backs can be seen below:User text is sent to the RASA-NLU server using http://localhost:5000/parse. Its response contains the intent and the entities. Depending on the intent, functions like gst-info and gst-query are called. Their responses are then sent back to the UI.The source code for this app is available on github.1. Start the RASA-NLU server by executing run_server.bat script. It loads the custom trained model and starts listening to port 50002. Execute the Flash app, by running the localhost at the given port, say 8000.3. Start typing commands in the bottom chat window and click Send.4. Typed messages and their responses appear in the window above, as seen in the adjoining picture.You can view the video demonstration here.This tutorial presents just a small example, demonstrating the potential to develop something full-fledged and practically useful. Our GST Q&A bot can be enhanced on various fronts, such as expansion of knowledgebase (i.e. number of questions and answers), better training to find more intents and entities, Natural Language Generation of the responses to have a human language feel, etc.GST FAQ Bot is just one example of building an intuitive frontend using government information. With the availability of more APIs and open public data, we can build similar (if not better) bots for those databases. Imagine interacting with government departments using a chatty bot!",https://www.analyticsvidhya.com/blog/2018/01/faq-chatbots-the-future-of-information-searching/
Japanese Scientists Unveil new AI Technology that can Read our Minds,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Building a FAQ Chatbot in Python  The Future of Information Searching|The Olympics are adding AI Powered Robots from next months Winter Games|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Weve all seen those movies where the police put the person through a lie detector test to gauge the truth. Those lie detectors read the brain activity and tell the police whether the person is lying or not. All this seemed like a distant thought on the silver screen but thanks to four Japanese scientists at the Kyoto University, machines have taken a significant step in reading the human mind.These scientists have developed a deep neural network to decode our thoughts. The above image shows how the technology works.The research was conducted over a course of 10 months. Three subjects were shows natural images, artificially generated shapes and alphabetical letters, all for differing lengths of time.Brain activity was measured in two phases:The brain activity was then scanned by the machine. It then decoded (or reverse-engineered) the information to visually represent the thoughts. These reconstructed images did resemble the original images a little bit, but most of it looked like a blurred blob.The technology is still in its nascent stages but the advancements are still impressive. Binary pixels are no longer the in thing; the AI can now detect entire objects on its own.If youre interested in how the technology works, you can see the research paper here. The below video also gives a lowdown about how this works:The accuracy of the technology will only improve going forward. The day is not too far away when machines can decipher exactly what we are thinking. Whether thats a good thing or an alarming intrusion of privacy, remains to be seen. Already industry leaders are clamoring to get a lid on how far this technology can be used. Ethics in AI is a genuinely pressing subject and this new breakthrough will only add to the list of topics in that regard.The potential applications of this technology blow your mind away. It could paint images simply based on your thoughts. For people with speaking disorders or paralysis, this could be a dream come true. What if you could wake up and analyse your the dreams you just had? The list is endless.",https://www.analyticsvidhya.com/blog/2018/01/japanese-scientists-unveil-new-ai-technology-read-our-minds/
The Olympics are adding AI Powered Robots from next months Winter Games,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Japanese Scientists Unveil new AI Technology that can Read our Minds|Microsofts New AI Bot can Draw Images Based on Captions|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The South Korean government is planning to have 85 robots at the Winter Olympic Games in Pyeongchang next month.In December 2017, a humanoid robot named HUBO, made history by carrying the Olympic torch in the city of Daejeon. It used a tool attached to its arm to cut off a huge chunk of wall and hand over the torch to its creator, Professor Oh Jun-Ho. He then handed the torch on to another robot, named FX-2.                               Source: YoutubeHUBO, developed a few years ago by the Korean Advanced Institute of Science and Technology, was originally designed for rescue missions and won the DARPA robotics challenge in 2015. The FX-2 on the other hand, is a collaboration between HUBO lab and Rainbow Robotics and was built for the purpose of carrying the Olympic torch. It stands at over 8 feet tall and weights close to 600 pounds.HUBO robots will be used for multiple purposes at the event. They will assist with security with the aim of improving safety amidst the growing fear of terrorist attacks at sporting events. They will also assist the staff with cleaning up the Olympic village and guiding fans and tourists to their destinations.A plethora of robots will roam in and around the event providing information on transportation schedules, game venues, directions, scores, ticketing information, etc. These robots will also paint murals on the walls to decorate the entire venue and add color to the event.You can view the video of HUBO with the Olympic Torch below:This is expected to usher in a new era of robotic technology at all future Olympic events. Tokyo is also mulling leveraging AI at the 2020 Olympics. There will be an entire robot village replete with driverless cars and robots that will translate different languages instantly . It does sound ambitious but would be wonderful to watch. Robots will also help out the judges with the scoring in all the gymnastics events. The 2020 games will be the most futuristic Olympics till date and its only going to get better.",https://www.analyticsvidhya.com/blog/2018/01/olympics-adding-ai-powered-robots-next-months-winter-games/
Microsofts New AI Bot can Draw Images Based on Captions,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|The Olympics are adding AI Powered Robots from next months Winter Games|Kolkata Police to use Analytics with Google Maps to Manage Traffic|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Microsoft has built an AI powered bot that can draw images based on the text it is provided. The below image, published by Microsoft, depicts a yellow black bird that was completely generated by the bot.Source: MicrosoftMicrosoft is simply calling this new technology the drawing bot for now. It can generate images from animals to scenic hillsides, and even outlandish things like flying cars and twisted street lamps. Its basically the AI version of pictionary where youre supposed to draw something based on cue cards. The only difference is you type something for the bot, and it will run its algorithm and give you the image.The most exciting part about the technology is that the images geenrated might not even be of actual real things. The bird created in the above image? It might not even be in existence  theyre just a rendering of the machines imagination of how a bird looks like. Further, each image that is created contains other details that are not provided in the text descriptions.In terms of where this bot will be used once its made available, Microsoft see it being used by painters and interior decorators. It can also be used a voice-activated tool for creating or refining photos (maybe theres a role for Cortana in there).To make the AI understand what words go with which pictures, the drawing bot was trained on pairs of images and captions. The algorithm is divided into two parts:Microsoft has previously released the CaptionBot, which takes images as input and writes captions for them. They followed this up with the SeeingAI tool. Again, it takes images as input and describes whats in them. This is especially targeted towards low-vision and blind people.While Google launched a similar AI last year which could create doodles, Microsofts version is in a different league altogether. Its not perfect yet, but one can imagine the future uses for such technology. The principal researcher in this matter, Xiaodong He, thinks it might even be used to create animated movies (using pre-written scripts). Following Googles AutoML Vision launch yesterday, 2018 is already promising to be a big year in the image recognition field.",https://www.analyticsvidhya.com/blog/2018/01/microsoft-drawing-bot-ai/
Kolkata Police to use Analytics with Google Maps to Manage Traffic,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Microsofts New AI Bot can Draw Images Based on Captions|10 Audio Processing Tasks to get you started with Deep Learning Applications (with Case Studies)|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Traffic jams could be a thing of the past. In a first in India, the Kolkata traffic police is planning to leverage the use of analytics using data from Google Maps to deal with the kilometers long traffic jams plaguing the city.Additional CP Vineet Goel said that the Kolkata police have proposed an algorithm to deal with the traffic. The algorithm will take the Google real-time traffic data as an input, calculate the traffic lines at intersections throughout the city, and ultimately attempt to predict the exact signal cycle. The plan is to optimise the number of cars passing through the intersections to avoid long queues.All these details were revealed at the Urban Road Traffic Management and Safety conference last week.Previously, traffic signals had differing clock timings which made it difficult to keep things organised. This has now been phased out in preference of the new proposed system. Wi-fi systems around the city will connect all the signals to a single location. This has achieved using help from Siemens.In an interview with Times of India, Mr. Goel revealed that the cops had considered using underground induction loops to track the number of vehicles passing each intersection. But that wouldve meant digging throughout the city, massive costs, and led to the one thing theyre trying to solve  traffic jams.Thus the approach using Google Maps was put forward and accepted unanimously. Google Maps updates the traffic on a real-time basis and using satellite imagery, the number of vehicles on the road can be calculated. If the queue stretches beyond a 100-150 metres, an alert will go out to the concerned traffic personnel in that sector.Data will be analysed in real time and the cops at each intersection will be asked to file reports where the traffic had to be managed manually. In addition to this, the nature of vehicles will also be predicted using a software developed for this purpose.This could herald a new phase in traffic management in the country. For decades, traffic jams have caused headaches for commuters throughout the nation. With the Kolkata police spearheading the charge to untangle this mess, the hope is that other metro cities will also use this analytics approach to deal with traffic.",https://www.analyticsvidhya.com/blog/2018/01/kolkata-analytics-google-maps-traffic/
10 Audio Processing Tasks to get you started with Deep Learning Applications (with Case Studies),Learn everything about Analytics|Introduction|1. Audio Classification|2. Audio Fingerprinting|3. Automatic Music Tagging|4. Audio Segmentation|5. Audio Source Separation|6. Beat Tracking|7. Music Recommendation|8. Music Retrieval|9. Music Transcription|10. Onset Detection|End Notes,"Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|Kolkata Police to use Analytics with Google Maps to Manage Traffic|Googles Newly Launched Cloud AutoML Lets You Build Models without Coding|
Faizan Shaikh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine a world where machines understand what you want and how you are feeling when you call at a customer care  if you are unhappy about something, you speak to a person quickly. If you are looking for a specific information, you may not need to talk to a person (unless you want to!).This is going to be the new order of the world  you can already see this happening to a good degree. Check outthe highlights of 2017 in the data science industry. You can see the breakthroughs that deep learning was bringing in a field which were difficult to solve before. One such field that deep learning has a potential to help solving is audio/speech processing, especially due to its unstructured nature and vast impact.So for the curious ones out there, I have compiled a list of tasks that are worth getting your hands dirty when starting out in audio processing. Im sure there would be a few more breakthroughs in time to come using Deep Learning.The article is structured to explain each task and its importance. There is also a research paper that goes in the details of that specific task, along with a case study that would help you get started in solving the task.So lets get cracking!Audio classification is a fundamental problem in the field of audio processing. The task is essentially to extract features from the audio, and then identify which class the audio belongs to. Many useful applications pertaining to audio classification can be found in the wild  such as genre classification, instrument recognition and artist identification.This task is also the most explored topic in audio processing. Plenty of papers were published in this field in the last year. In fact, we have also hosted a practice hackathon for community collaboration for solving this particular task.Whitepaper  http://ieeexplore.ieee.org/document/5664796/?reload=trueA common approach to solve an audio classification task is to pre-process the audio inputs to extract useful features, and then apply a classification algorithm on it. For example, in the case study below we are given a 5 second excerpt of a sound, and the task is to identify which class does it belong to  whether it is a dog barking or a drilling sound. As mentioned in the article, an approach to deal with this is to extract an audio feature called MFCC and then pass it though a neural network to get the appropriate class.Case Study  https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/The aim of audio fingerprinting is to determine the digital summary of the audio. This is done to identify the audio from an audio sample. Shazam is an excellent example of an application of audio fingerprinting. It recognises the music on the basis of the first two to five seconds of a song. However, there are still situations where the system fails, especially where there is a high amount of background noise.Whitepaper  http://www.cs.toronto.edu/~dross/ChandrasekharSharifiRoss_ISMIR2011.pdfTo solve this problem, an approach could be to represent the audio in a different manner, so that it is easily deciphered. Then, we can find out the patterns that differentiate the audio from the background noise. In the case study below, the author converts raw audio to spectrograms and then uses peak finding and fingerprint hashing algorithms to define the fingerprints of that audio file.Case Study  http://willdrevo.com/fingerprinting-and-audio-recognition-with-python/Music Tagging is a more complex version of audio classification. Here, we can have multiple classes that each audio may belong to, aka, a multi-label classification problem. A potential application of this task can be to create metadata for the audio so that it can be searched later on. Deep learning has helped solve this task to a certain extent which can be seen in the case study below.Whitepaper  https://link.springer.com/article/10.1007/s10462-012-9362-yAs seen with most of the tasks, the first step is always to extract features from the audio sample. Then, sort it according to the nuances of the audio (for example, if the audio contains more instrumental noise than the singers voice, the tag could be instrumental). This can be done either by machine learning or deep learning methods. The case study mentioned below uses deep learning to solve the problem, specifically convolution recurrent neural network along with Mel Frequency Extraction.Case Study  https://github.com/keunwoochoi/music-auto_tagging-kerasSegmentation literally means dividing a particular object into parts (or segments) based on a defined set of characteristics. Segmentation, especially for audio data analysis, is an important pre-processing step. This is because we can segment a noisy and lengthy audio signal into short homogeneous segments (handy short sequences of audio) which are used for further processing. An application of the task is heart sound segmentation, i.e. to identify sounds specific to the heart.Whitepaper  http://www.mecs-press.org/ijitcs/ijitcs-v6-n11/IJITCS-V6-N11-1.pdfWe can convert this into a supervised learning problem, where each time stamp can be classified on the basis of the segments required. Then we can apply an audio classification approach to solve the problem. In the case study below, the task is to segment the heart sound into two segments (lub and dub), so that we can identify an anomaly in each segment. It can be solved by using audio feature extraction and then deep learning can be applied for classification.Case Study  https://www.analyticsvidhya.com/blog/2017/11/heart-sound-segmentation-deep-learning/Audio Source Separation consists of isolating one or more source signals from a mixture of signals. One of the most common applications of this is identifying the lyrics from the audiofor simultaneous translation (karaoke, for instance). This is a classic example shown in Andrew Ngs machine learning course where he separates the sound of the speaker from the background music.Whitepaper  http://ijcert.org/ems/ijcert_papers/V3I1103.pdfA typical usage scenario involves:The mask is then multiplied with the spectrogram and the result is converted back to the time domain.Case Study  https://github.com/IoSR-Surrey/untwistAs the name suggests, the goal here is to track the location of each beat in a collection of audio files. Beat tracking can be utilized to automate time-consuming tasks that must be completed in order to synchronize events with music. It is useful in various applications, such as video editing, audio editing, and human-computer improvisation.Whitepaper  https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/01-students/2012_GroschePeter_MusicSignalProcessing_PhD-Thesis.pdfAn approach to solve beat tracking can be to be parse the audio file and use an onset detection algorithm to track the beats. Although the techniques used to for onset detection rely heavily on audio feature engineering and machine learning, deep learning can easily be used here to optimize the results.Case Study  https://github.com/adamstark/BTrackThanks to the internet, we now have millions of songs we can listen to anytime. Ironically, this has made it even harder to discover new music because of the plethora of options out there. Music recommendation systems help deal with this information overload by automatically recommending new music to listeners. Content providers like Spotify and Saavn have developed highly sophisticated music recommendation engines. These models leverage the users past listening history among many other features to build customized recommendation lists.Whitepaper  https://pdfs.semanticscholar.org/7442/c1ebd6c9ceafa8979f683c5b1584d659b728.pdfWe can tackle the challenge of customizing listening preferences by training a regression/deep learning model. This can be used to predict thelatent representationsof songs that were obtained from a collaborative filtering model. This way, we could predict the representation of a song in the collaborative filtering space, even if no usage data was available.Case Study  http://benanne.github.io/2014/08/05/spotify-cnns.htmlOne of the most difficult tasks in audio processing, Music Retrieval essentially aims to build a search engine based on audio. Although we can do this by solving sub-tasks like audio fingerprinting, this task encompasses much more that that. For example, we also have to solve different smaller tasks for different types of music retrieval (timbre detection would be great for gender identification). Currently, there is no other system that has been developed to match the industry expected standards.Whitepaper  http://www.nowpublishers.com/article/Details/INR-042The task of music retrieval is divided into smaller and simpler steps, which include tonal analysis (e.g. melody and harmony) and rhythm or tempo (e.g. beat tracking). Then, on the basis of these individual analysis, information is extracted which is used for retrieval of similar audio samples.Case Study  https://youtu.be/oGGVvTgHMHwMusic Transcription is another challenging audio processing task. It comprises of annotating audio and creating a kind of sheet for generating music from it at a later point of time. The manual effort involved in transcribing music from recordings can be vast. It varies enormously depending on the complexity of the music, how good our listening skills are and how detailed we want ourtranscription to be.Whitepaper  http://ieeexplore.ieee.org/abstract/document/7955698The approach for music transcription is similar to that of speech recognition, where musical notes are transcribed into lyrical excerpts of instruments.Case Study  https://youtu.be/9boJ-Ai6QFMOnset detection is the first step in analysing an audio/music sequence. For most of the tasks mentioned above, it is somewhat necessary to perform onset detection, i.e. detecting the start of an audio event. Onset detection was essentially the first task that researchers intended to solve in audio processing.Whitepaper  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.989&rep=rep1&type=pdfOnset detection is typically done by:Case Study  https://musicinformationretrieval.com/onset_detection.htmlIn this article, I have mentioned a few tasks that can be looked at when solving audio processing problems. I hope you find the article insightful in dealing with audio/speech related projects.",https://www.analyticsvidhya.com/blog/2018/01/10-audio-processing-projects-applications/
Googles Newly Launched Cloud AutoML Lets You Build Models without Coding,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|10 Audio Processing Tasks to get you started with Deep Learning Applications (with Case Studies)|Online Learning Guide with Text Classification using Vowpal Wabbit (VW)|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Google yesterday announced the the launch of their Cloud AutoML platform. Its primary aim is to help businesses with limited resources and expertise, streamline and build high-quality machine learning models.Currently, very limited organizations around the world have the capability to handle advanced machine learning and AI applications. Its simply not possible to wait around for people to skill up; technology is advancing too quickly for companies to sit on their laurels and hope that people catch up.Source: YoutubeKeeping this in mind, Google has developed and made this platform available to businesses. It hopes to bridge the gap between the current AI capabilities and the vast potential this field has to offer. Google believes this will provide opportunities for less-skilled engineers to build powerful AI systems and make AI experts even more productive and efficient.Its first product launch, as part of the Cloud AutoML portfolio, is Cloud AutoML Vision. This service will make it simpler to train image recognition models.It has a drag-and-drop interface that lets the user upload images, train the model, and then deploy those models directly on Google Cloud.Cloud AutoML Vision is built on Googles transfer learning and neural architecture searchtechnologies(among others). Disney has already started using the technology to annotate their products to improve the customers experience on their shopDisney site. The Zoological Society of London is also using AutoML Vision to recognise and track wildlife in order to understand their disturibtion and how humans are impacting the species.You can watch Googles Cloud AutoML introduction here:You can sign up for it here.There are already automated machine learning tools out there, but Googles newest addition is sure to create waves. The entire Cloud AutoML platform will help businesses scale up their AI capabilities without requiring advanced machine learning expertise. It opens the door for non-technical people to understand and utilise machine learning to improve not only their portfolio, but their companys as well. Google has promise more products under the Cloud AutoML umbrella so those who dont like coding can enjoy this latest renaissance in the ML and AI fields.",https://www.analyticsvidhya.com/blog/2018/01/google-cloud-automl-platform-build-models-without-coding/
Online Learning Guide with Text Classification using Vowpal Wabbit (VW),Learn everything about Analytics|Introduction|Table of Contents|Online Learning Vs Batch Learning|Vowpal Wabbit  A Framework for Online Learning|End Notes,"Input Format|VW for text classification|Training|Testing|n-grams|Model Interpretability|Regularization|Share this:|Like this:|Related Articles|Googles Newly Launched Cloud AutoML Lets You Build Models without Coding|Alibabas Neural Network Model Beat the Highest Human Score in Stanfords Reading Test|
Ankit Choudhary
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A large number of E-Commerce and tech companies rely on real time training and predictions for their products. Google predicts real time click-through rates for their ads. This is used as an input to their auction mechanism, apart from a bid from the advertiser to decide which ads to show to the user.Stackoverflow uses real time predictions to automatically tag a question with the correct programming language so that they reach the right asker. An election management team might want to predict real time sentiment using Twitter to assess the impact of their campaign.Such datasets are also characterized by their large size. For training the model, we can always take a sub-sample of the data, but its a rather unreliable method. Theres a good chance we might miss out on a significant amount of information.Is there a solution that tackles both these problems? As it turns out, there is. In this article, we will discuss a comparison of batch learning and online learning. In the second section, well look at an example of text classification using an online learning framework called Vowpal Wabbit (VW).Lets say we want to build a model that identifies spam emails. One way is to download a large corpus of emails, train a model on it and subsequently test it on unseen examples. This is called offline learning. Another way is to take in each email sequentially and continuously update the parameters in a classifier. You guessed it  this is online learning.Going back to our example of spam classification, imagine a situation where the spammers have found a work-around and started bypassing the existing spam classifier. Two teams have been tasked to solve the problem  The Batch Learning Team and the Online Learning team.SkLearn has SGDClassifier and SGDRegressor from sklearn.linear_model. These are nice implementations of SGD/online learning but well focus on Vowpal Wabbit as its superior to sklearns SGD models in many aspects, including computational performance. The Vowpal Wabbit (VW) project is a fast online learning framework sponsored by Microsoft Research and (previously) Yahoo! Research.When it comes to categorical variables, label encoding or one hot encoding are popular methods of dealing with it. These are good, if we are able to store and process the whole dataset. But online learning does not have the liberty of looking at the complete dataset. Also, real data can be volatile and we cannot guarantee that new values of categorical features will not be added at some point. This issue hampers the use of the trained model when some new data is introduced,This is where Vowpal Wabbit comes into the picture.The idea is very simple: convert data into a vector of features. When this is done using hashing, we call the method feature hashing or the hashing trick.Ill explain how it works with a simple example using text as data.Lets say our text is:the great blue whaleWe would like to represent this as a vector. The first thing we need to do is fix the length of the vector, i.e., the number of dimensions we are going to use. For the purpose of this example, we will take 5 dimensions.Once we fix the number of dimensions, we will need a hash function that will take a string and return a number between 0 and n-1 (in our case between 0 and 4). Any good hash function can be used. We will use h(string) mod n to make it return a number between 0 and n-1. h(string) generates a large number based on the number of bits allotted.Ill compute the results for each word in our text:h(the) mod 5 = 0h(great) mod 5 = 1h(blue) mod 5 = 1h(whale) mod 5 = 3Once we have this, we can simply construct our vector as:(1,2,0,1,0)Notice that we just add 1 to the nth dimension of the vector each time our hash function returns that dimension for a word in the text.Vowpal Wabbit is so incredibly fast in part due to the hashing trick. With many features and a small-sized hash, collisions (i.e. when we have the same hash for 2 different features) start occurring. These collisions may influence the results. Often for the worse, but not necessarily: Multiple features sharing the same hash can have a PCA-like effect of dimensionality reduction.If you feel hashing is really hurting the performance of your model, you could increase the number of bits required to produce the hash. Read more about hash functions here.Its friendly to online learning where you can train on a dataset that doesnt fit in memory because you need to see each example only once. One-hot encoding/Label Encoding will not work with online learning because to prepare dictionaries you need to see whole dataset first.Vowpal Wabbit reads data from files or from standard input stream (stdin) assuming the following format:Namespace=String[:Value]
Features=(String[:Value] )*here [] denotes non-mandatory elements, and ()* means some repeats.Consider a setting where a movie production company wants to build a real time IMDB review extraction and prediction system. This is a binary classification problem and the task is to predict whether a particular review is positive or negative. The data provided has only the actual reviews as text. Here, I will show you how easy it is to work with text data using Vowpal Wabbit.Installing Vowpal Wabbit is easy and can be done using the documentation provided at this link.VW can only be used via command line syntax. Help related to all the functions can be seen using the following command:The review dataset is provided in the form of text files at the provided link. The following piece of python code helps to read all the text files and combine it into a single file.Next, we will convert the text to the Vowpal Wabbit format described above.Here, it is difficult to put the words (features) in different namespaces and hence we will just consider all the words separately without considering the interaction features.We have done a very basic removal of all the words with less than three characters (like is, in etc.) because these do not add any valuable information to the model.VW can be used at the terminal or command line. If you prefer using the python notebook, you can use ! before the command to run it like any other python code.Now, lets examine each of the syntax elements in the above command line function.That is quite an impressive accuracy for such a simple model and it only gets better as we stream more data to VW.We have built a decent model by just feeding it the actual words and a label. And we did this in a fraction of a second on an average system! Now, lets try hinge loss as the loss function to check if we can improve the results.Hinge loss did not show any significant improvement. Lets move on to adding more features via n-grams.VW also supports other functionalities specifically for text data.For those new to the Natural Language Processing landscape, n-grams are a continuous sequence of n words inside a text. For example, in the sentence:Hi, I dont like dark placesdont like and dark places are examples of bi-grams (n =2).Let us briefly discuss how n-grams can provide a boost to accuracy. Well take the phrase  not the best experience. This phrase has a definite negative sentiment, but if we dont use n-grams, a positive weight will be learnt owing to the presence of the word best. However, if we include bi-grams, not best will be a separate feature and will have a more negative weight which is appropriate in this case.VW supports n-grams via command line tags. Let us try including bi-grams in our model to see whether they improve the overall accuracy.Indeed, we get a better accuracy and a higher AUC after including the bi-grams in the model. We can also try including tri-grams or higher to see if we can get a further boost in the accuracy.How do we interpret the model we just created? VW has a one line solution for it. We just need to replace -f withinvert_hash and it will produce a model file that is human readable.We can see the weights learnt for various bi-grams in the above screenshot. abominably, being a strictly negative word, produces mostly negative weights except in a few cases where the other word is highly positive, like best.L1 and L2 regularization may be added with the l1 and l2 options respectively.Text data is not the only domain where online learning is useful. It performs really well for CTR (Click-through Rate) predictions and supports many different optimization methods. I plan to cover these in a future article.Meanwhile, please let me know in the comments if you have ever used VW or online learning to solve a problem. Detailed documentation on VW can be viewed here.",https://www.analyticsvidhya.com/blog/2018/01/online-learning-guide-text-classification-vowpal-wabbit-vw/
Alibabas Neural Network Model Beat the Highest Human Score in Stanfords Reading Test,Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Online Learning Guide with Text Classification using Vowpal Wabbit (VW)|IBM, Tommy Hilfiger and FIT Using AI to Collaborate and Reimagine Retail|
Pranav Dar
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machines getting the better of humans is no longer a surprise. It started with IBMs Deep Blue program beating Garry Kasparov in a game of chess more than 20 years ago and with the increasing breakthroughs in the world of machine and deep learning, machines continue to become powerful tools.                                   Source: SFGateYesterday, Alibaba developed a model that beat out any human competition in Stanfords reading comprehension competition. The dataset consists of more than 100,000 questions sourced from more than 500 Wikipedia articles. The purpose of the quiz is to see how long it takes the machine learning models to process all the information, train themselves and then provide precise or accurate answers.Alibaba used a deep learning framework to build a neural network model. Its based on the Hierarchical Attention Network, which according to the company, works by identifying first paragraphs, then sentences and finally words. The underlying technology has been used previously by Alibaba, in its AI-powered chatbot  Dian Xiaomi.Alibaba achieved a score of 82.44, which beat out the human high of 82.304. Microsofts AI achieved a score on 82.650. The website lists that Microsoft submitted their model a couple of days before Alibaba but the team evaluating the models, Squad (StanfordQuestionAnsweringDataset), officially released the results of Alibabas model first, and Microsofts a day later, thus giving Alibaba the unique distinction.The competition leaderboard published by SquadCompanies like Google, Tencent, IBM and Samsung (among many others) have also participated in the competition but Alibaba became the first to beat the human best score.Alibaba have mentioned that they will be sharing the model-building framework with the public in the coming weeks.This just goes to show that machines are now able to answer complex objective questions with remarkable precision. Remember going to museums or historical monuments with a guide? That will be a thing of the past.Customer service is expected to be fully automated in the next few years and Alibaba hope to lead the drive using their Natural Language Processing lab. The human input required for these tasks will be minimal.",https://www.analyticsvidhya.com/blog/2018/01/alibabas-neural-network-model-beat-highest-human-score-stanfords-reading-test/
"IBM, Tommy Hilfiger and FIT Using AI to Collaborate and Reimagine Retail",Learn everything about Analytics|Our take on this,"Share this:|Like this:|Related Articles|Alibabas Neural Network Model Beat the Highest Human Score in Stanfords Reading Test|Amazing New AI Innovations Unveiled at CES 2018 in Las Vegas|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The turnaround time in the fashion retail space is shrinking as the demand for the latest trend reaches fever pitch. Consumers demand the latest style in double quick time and because of the rate at which trends change, speed of delivery has become quintessential to retailers.Source: IBMThis was a design created by one of the FIT students using AICompanies like Zara, H&M and Tommy Hilfiger themselves have had to restructure their entire supply chain processes recently to keep up with the market demands. The big retailers are now able to get the latest apparel into the markets within weeks of production but the majority of items take anywhere between six to twelve months which leads to a lot of lost sales.To combat this, IBM, Tommy Hilfiger and the Fashion Institute of Technology (FIT) have joined hands on a project called Reimagine Retail. According to Steve Laughlin, the general manager of IBM Global Consumer Industries, their aim is to speed up the supply chain process and aid the next generation of retailers with AI powered skills.IBM gave access of their AI facilities to the FIT students for this project; including access to their natural language understanding and computer vision labs as well as several deep learning techniques trained specially with fashion data.All these tools were then applied to 15,000 Tommy Hilfigers product images, along with approximately 600,000 publicly available images (taken from various fashion shows). Close to 100,000 patterns were taken from fabric sites. The resulting model then churned out tons of patterns, trends, silhouettes and prints that enabled the FIT students to create completely new designs by incorporating the trends of other designs into the ones already existing in the Tommy Hilfiger database.The Reimagine Retail project also uses social media listening as a tool to understand how previous products have been received and make changes in upcoming designs. Predicting which items are going to be in style in the coming months (and even years) has become critical for retailers and Tommy Hilfiger is determined to be at the front of the queue to enhance customer satisfaction.This is only the beginning of AI in the retail space. There is a general concern that using tools will lead to the death of creativity but we disagree here. AI gives the designers the tools to reimagine designs, look at tons of patterns and fabrics that have been used previously and to come up with new trends.The human mind cannot comprehend or retain thousands of images which is why this is an unprecedented step in fashion. And all this is done in a matter of minutes, if not seconds. This in turn also speeds up the supply chain process by getting products to the shelves in weeks to keep up with the consumer demands.",https://www.analyticsvidhya.com/blog/2018/01/ibm-tommy-hilfiger-fit-using-ai-collaborate-reimagine-retail/
Amazing New AI Innovations Unveiled at CES 2018 in Las Vegas,Learn everything about Analytics,"Share this:|Like this:|Related Articles|IBM, Tommy Hilfiger and FIT Using AI to Collaborate and Reimagine Retail|A Simple Introduction to ANOVA (with applications in Excel)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"CES 2018 concluded last week in Las Vegas where there were tons of new and improved gadgets, innovations and technologies on show from some of the worlds leading tech companies. From advancements in 5G technology to updated digital assistants, from virtual reality headsets to retrofuturistic robots, this years show did not disappoint. In this article, we take a look at some of the most interesting highlights from the event:The future of Healthcare
AI and Machine Learning are leading the way in the field of healthcare. Presenters at the event claimed that personalised medicine (using gene sequencing) would be introduced very soon and Machine Learning will help healthcare providers reach out and treat patients in remote places, hence cutting out a lot of the tedious paperwork.                          Source: NewatlasHumox Hex, a watch wearable developed by a team at MIT, goes deeper than any wearable device till date. Rather than monitoring just the heart rate, it calculates oxygen consumption in the muscles to make real-time recommendations to the user. This reduces injury, optimises exercises and monitors the recovery process.LOreals Thumbnail-sized Sensor                            Source: LOralSurprised to see LOreal in this list? Youre not the only one. But theyve been active participants at CES events in recent years and this week unveiled their own innovation  UV Sense Patch. Its so small that you can attach it to your thumbnail and itll measure your exposure to sunlight, thus helping you limit the risk of skin cancer. Its less than two millimetres thick and battery-free so has to be replaced every two weeks.Hyundais Digital Assistant
Hyundai made a splash at this years event when they announced that a new voice-enabled bot was in advanced development (expected in 2019). Most of us are acutely aware of the dangers of texting while driving and Hyundai wants to solve that problem. This digital assistant will be fitted in the car and you can talk to it to text, make phone calls, switch on the AC, play a song or open the radio, and even get directions.Co-developed by Silicon Valley based SoundHound Inc., it will also recognise multiple commands in the same sentence. For example, if you tell it call my home and also tell me what the weather forecast is for tomorrow, it will recognise those are two separate commands.Cocoon Cam Clarity                             Source: CESThis baby monitor will send you real time data on the infants breathing and send you other alerts as well. Their latest version, showcased at the event, has added sensors for temperature, humidity and sleep analytics (like sleep and wake patterns, trends, growth behaviors , etc.).Rinseed Snap                         Source: RinseedStill a concept rather than a reality, Rinseed is developing a car model (called Snap) that looks like a skateboard pod. The idea behind this is to move people in urban areas quicker and they can step on and off it easily if they dont like how the AI powering the car is driving.Toyotas e-Palette Concept Car                         Source: SlashgearToyotas response to the whole driverless cars debate  the e-Palette. Its a fully autonomous electric vehicle with eight wheels. It looks like a home in a box  it can contain furniture, TVs, can be used to deliver parcels, among other things. According to the Toyota CEO, Akio Toyoda, it can even be used as a temporary accommodation. A driverless hotel room!A software called MSPF (Mobility Services Platform) is at the core of the e-Palette design.Google Assistant is taking on Amazons Alexa, in a BIG wayIts no secret that Google wants to be your go-to everything  home, car, device. At CES 2018, Google brought its AI assistant to smart displays  basically, speakers with displays built in! Its a smart bet that smart displays will outnumber smart speakers very quickly. For example, if you ask the smart display for directions, itll also send those directions to your phone without being prompted. Amazing.                         Source: EngadgetIn cars, Google Assistant will be incorporated into Android Auto to improve your in-car experience. Ask it to pick out a song, check the score, recite a recipe; anything you can think of in a car, Google Assistant wants to help you get that information.Youtubes Recommendations Keep Getting BetterYoutubes Chief Product Officer, Neal Mohan, said in a CES panel discussion, that their ever improving recommendation AI has taken another leap forward. According to him, the average session duration on a mobile device is almost an hour! The user is lured into a loop by the excellent recommendations that are shown next to each video. Using various ML techniques (which they prefer keeping under wraps), they expect this number to drive up even further this year.Our take on thisThis years event saw a lot of focus on digital assistants. If last years event was all about Amazons Alexa, this year Googles virtual assistant was the star of the show. We can expect to see it being integrated into our everyday lives very soon, with Google insistent on having it available in cars and in your house (its already available on the phone). Alexa and Google Assistant will be at the front of the next advertising wave.Driverless cars were also at the forefront of the event with Lyft, Rinseed, Ford, Intel and Toyota leading the chase. A lot of these are still concept cars but the general consensus is that these should start rolling out by 2020.In other categories, healthcare saw a lot of innovation  wearables, already a trending topic among the general public, is seeing a ton of upgrades. Features like sleep, oxygen consumption, sunscreen exposure are getting embedded deeper into these devices, thus granting us a far more integral view of how our bodies behave in different circumstances.Virtual reality headsets were introduced as well by Facebook and Lenovo but failed to gain any real traction. There is a lot of scope for improvement in this field and next years event could potentially see a leap forward in the technology.You can check out the full range of technologies and gadgets that were showcased on the CES site.",https://www.analyticsvidhya.com/blog/2018/01/amazing-new-innovations-unveiled-ces-2018-las-vegas/
A Simple Introduction to ANOVA (with applications in Excel),Learn everything about Analytics|Introduction|Table of Contents||Introduction to ANOVA|Terminologies related to ANOVA you need to know|One Way ANOVA|Two-Way ANOVA:|Multi-variate ANOVA (MANOVA):|End Notes,"Limitations of one-way ANOVA||Steps to perform one-way ANOVA with post-hoc test in Excel 2013||                          |Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|Amazing New AI Innovations Unveiled at CES 2018 in Las Vegas|Intermediate Tableau guide for data science and business intelligence professionals|
Gurchetan Singh
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Buying a new product or testing a new technique but not sure how it stacks up against the alternatives? Its an all too familiar situation for most of us. Most of the options sound similar to each other so picking the best out of the lot is a challenge.Consider a scenario where we have three medical treatments to apply on patients with similar diseases.Once we have the test results, one approach is to assume that the treatment which took the least time to cure the patients is the best among them. What if some of these patients had already been partially cured, or if any other medication was already working on them?In order to make a confident and reliable decision, we will need evidence to support our approach. This is where the concept of ANOVA comes into play.Source: MegapixlIn this article, Ill introduce you to the different ANOVA techniques used for making the best decisions. Well take a few cases and try to understand the techniques for getting the results. We will also be leveraging the use of Excel to understand these concepts.You must know the basics of statistics to understand this topic. Knowledge of t-tests and Hypothesis testing would be an additional benefit.And we believe the best way to learn statistics is by doing. Thats the way we follow in the Introduction to Data Science course where we provide a comprehensive introduction to statistics  both descriptive and inferential.So, lets begin!A common approach to figure out a reliable treatment method would be to analyse the days it took the patients to be cured. We can use a statistical technique which can compare these three treatment samples and depict how different these samples are from one another. Such a technique, which compares the samples on the basis of their means, is called ANOVA.Analysis of variance (ANOVA) is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. ANOVA checks the impact of one or more factors by comparing the means of different samples.We can use ANOVA to prove/disprove if all the medication treatments were equally effective or not.Source: QuestionproAnother measure to compare the samples is called a t-test. When we have only two samples, t-test and ANOVA give the same results. However, using a t-test would not be reliable in cases where there are more than 2 samples. If we conduct multiple t-tests for comparing more than two samples, it will have a compounded effect on the error rate of the result.Before we get started with the applications of ANOVA, I would like to introduce some common terminologies used in the technique.Grand MeanMean is a simple or arithmetic average of a range of values. There are two kinds of means that we use in ANOVA calculations, which are separate sample means and the grand mean . The grand mean is the mean of sample means or the mean of all observations combined, irrespective of the sample.HypothesisConsidering our above medication example, we can assume that there are 2 possible cases  either the medication will have an effect on the patients or it wont. These statements are called Hypothesis. A hypothesis is an educated guess about something in the world around us. It should be testable either by experiment or observation.Just like any other kind of hypothesis that you might have studied in statistics, ANOVA also uses a Null hypothesis and an Alternate hypothesis. The Null hypothesis in ANOVA is valid when all the sample means are equal, or they dont have any significant difference. Thus, they can be considered as a part of a larger set of the population. On the other hand, the alternate hypothesis is valid when at least one of the sample means is different from the rest of the sample means. In mathematical form, they can be represented as:wherebelong to any two sample means out of all the samples considered for the test. In other words, the null hypothesis states that all the sample means are equal or the factor did not have any significant effect on the results. Whereas, the alternate hypothesis states that at least one of the sample means is different from another. But we still cant tell which one specifically. For that, we will use other methods that we will discuss later in this article.Between Group VariabilityConsider the distributions of the below two samples. As these samples overlap, their individual means wont differ by a great margin. Hence the difference between their individual means and grand mean wont be significant enough.Now consider these two sample distributions. As the samples differ from each other by a big margin, their individual means would also differ. The difference between the individual means and grand mean would therefore also be significant.Such variability between the distributions calledBetween-group variability.It refers to variations between the distributions of individual groups (or levels) as the values within each group are different.Each sample is looked at and the difference between its mean and grand mean is calculated to calculate the variability. If the distributions overlap or are close, the grand mean will be similar to the individual means whereas if the distributions are far apart, difference between means and grand mean would be large.Source: Psychstat  Missouri StateWe will calculate Between Group Variability just as we calculate the standard deviation.Given the sample means and Grand mean, we can calculate it as:Source: UdacityWe also want to weigh each squared deviation by the size of the sample. In other words, a deviation is given greater weight if its from a larger sample. Hence, well multiply each squared deviation by each sample size and add them up. This is called thesum-of-squares for between-group variabilityTheres one more thing we have to do to derive a good measure of between-group variability. Again, recall how we calculate the sample standard deviation.We find the sum of each squared deviation and divide it by the degrees of freedom. For our between-group variability, we will find each squared deviation, weigh them by their sample size, sum them up, and divide by the degrees of freedom (), which in the case of between-group variability is the number of sample means (k) minus 1.Within Group VariabilityConsider the given distributions of three samples. As the spread (variability) of each sample is increased, their distributions overlap and they become part of a big population.Now consider another distribution of the same three samples but with less variability. Although the means of samples are similar to the samples in the above image, they seem to belong to different populations.Source: TurntheWheelsandBoxSuch variations within a sample are denoted by Within-group variation.It refers to variations caused by differences within individual groups (or levels) as not all the values within each group are the same.Each sample is looked at on its own and variability between the individual points in the sample is calculated. In other words,no interactions between samples are considered.We can measure Within-group variability by looking at how much each value in each sample differs from its respective sample mean. So first, well take the squared deviation of each value from its respective sample mean and add them up. This is thesum of squares for within-group variability.Source: TurntheWheelsandBoxLike between-group variability, we then divide the sum of squared deviations by thedegrees of freedomto find a less-biased estimator for the average squared deviation (essentially, the average-sized square from the figure above). Again, this quotient is called the mean square, but for within-group variability:. This time, the degrees of freedom is the sum of the sample sizes (N) minus the number of samples (k). Another way to look at degrees of freedom is that we have the total number of values (N), and subtract 1 for each sample:F-StatisticThe statistic which measures if the means of different samples are significantly different or not is called the F-Ratio. Lower the F-Ratio, more similar are the sample means. In that case, we cannot reject the null hypothesis.F = Between group variability / Within group variabilityThis above formula is pretty intuitive. The numerator term in the F-statistic calculation defines the between-group variability. As we read earlier, as between group variability increases, sample means grow further apart from each other. In other words, the samples are more probable to be belonging to totally different populations.This F-statistic calculated here is compared with the F-critical value for making a conclusion. In terms of our medication example, if the value of the calculated F-statistic is more than the F-critical value (for a specific /significance level), then we reject the null hypothesis and can say that the treatment had a significant effect.Source: Dr. Asims Anatomy CafeUnlike the z and t-distributions, the F-distribution does not have any negative values because between and within-group variability are always positive due to squaring each deviation.Source: Statistics How ToTherefore, there is only one critical region, in the right tail (shown as the blue shaded region above). If the F-statistic lands in the critical region, we can conclude that the means are significantly different and we reject the null hypothesis. Again, we have to find the critical value to determine the cut-off for the critical region. Well use theF-tablefor this purpose.We need to look at different F-values for each alpha/significance level because the F-critical value is a function of two things: and.As we now understand the basic terminologies behind ANOVA, lets dive deep into its implementation using a few examples.A recent study claims that using music in a class enhances the concentration and consequently helps students absorb more information. As a teacher, your first reaction would be skepticism.What if it affected the results of the students in a negative way? Or what kind of music would be a good choice for this? Considering all this, it would be immensely helpful to have some proof that it actually works.To figure this out, we decided to implement it on a smaller group of randomly selected students from three different classes. The idea is similar to conducting a survey. We take three different groups of ten randomly selected students (all of the same age) from three different classrooms. Each classroom was provided with a different environment for students to study. Classroom A had constant music being played in the background, classroom B had variable music being played and classroom C was a regular class with no music playing. After one month, we conducted a test for all the three groups and collected their test scores. The test scores that we obtained were as follows:Now, we will calculate the means and the Grand mean.So, in our case,Looking at the above table, we might assume that the mean score of students from Group A is definitely greater than the other two groups, so the treatment must be helpful. Maybe its true, but there is also a slight chance that we happened to select the best students from class A, which resulted in better test scores (remember, the selection was done at random). This leads to a few questions, like:To answer all these questions, first we will calculate the F-statistic which can be expressed as the ratio of Between Group variability and Within Group Variability.Lets complete the ANOVA test for our example with= 0.05.A one-way ANOVA tells us that at least two groups are different from each other. But it wont tell us which groups are different. If our test returns a significant f-statistic, we may need to run a post-hoc test to tell us exactly which groups have a difference in means. Below I have mentioned the steps to perform one-way ANOVA in Excel along with a post-hoc test.Step 1: Input your data into columns or rows in Excel. For example, if three groups of students for music treatment are being tested, spread the data into three columns.Step 2:Click the Data tab and then click Data Analysis. If you dont see Data Analysis,load the Data Analysis Toolpak add-in.Step 3:Click ANOVA Single Factor and then click OK.Step 4:Type an input range into the Input Range box. For example, if the data is in cells A1 to C10, type A1:C10 into the box. Check the Labels in first row if we have column headers, and select the Rows radio button if the data is in rows.Step 5:Select an output range. For example, click the New Worksheet radio button.Step 6:Choose an alpha level. For mosthypothesis tests, 0.05 is standard.Step 7:Click OK. The results from ANOVA will appear in the worksheet.Results for our example look like this:Here, we can see that the F-value is greater than the F-critical value for the alpha level selected (0.05). Therefore, we have evidence to reject the null hypothesis and say that at least one of the three samples have significantly different means and thus belong to an entirely different population.Another measure for ANOVA is the p-value. If the p-value is less than the alpha level selected (which it is, in our case), we reject the Null Hypothesis.There are various methods for finding out which are the samples that represent two different populations.Ill list some for you:We wont be covering all of these here in this article but I suggestyou go through them. Now to check which samples had different means we will take the Bonferroni approach and perform the post hoc test in Excel.Step 8: Again, click on Data Analysis in the Data tab and select t-Test: Two-Sample Assuming Equal Variances and click OK.Step 9: Input the range of Class A column in Variable 1 Range box, and range of Class B column in Variable 2 Range box. Check the Labels if you have column headers in the first row.Step 10: Select an output range. For example, click the New Worksheet radio button.Step 11: Perform the same steps (Step 8 to step 10) for Columns of Class B  Class C and Class A  Class C.The results will look like this:Here, we can see that the p-value of (A vs B) and (A vs C) is less than the alpha level selected (alpha = 0.05). This means that groups A and B & groups A and C have less than 5% chance of belonging to the same population. Whereas for (B vs C) it is much greater than the significance level. This means that B and C belong to the same population. So, it is clear that A (constant music group) belongs to an entirely different population. Or we can say that the constant music had a significant effect on the performance of students.Voila! The music experiment actually helped in improving the results of the students.Another effect size measure for one-way ANOVA is called Eta squared. It works in the same way as R2 for t-tests. It is used to calculate how much proportion of the variability between the samples is due to the between group difference. It is calculated as:For the above example:Hence 60% of the difference between the scores is because of the approach that was used. Rest 40% is unknown. Hence Eta square helps us conclude whether the independent variable is really having an impact on the dependent variable or the difference is due to chance or any other factor.There are commonly two types of ANOVA tests for univariate analysis  One-Way ANOVA and Two-Way ANOVA. One-way ANOVA is used when we are interested in studying the effect of one independent variable (IDV)/factor on a population, whereas Two-way ANOVA is used for studying the effects of two factors on a population at the same time. For multivariate analysis, such a technique is called MANOVA or Multi-variate ANOVA.Using one-way ANOVA, we found out that the music treatment was helpful in improving the test results of our students. But this treatment was conducted on students of the same age. What if the treatment was to affect different age groups of students in different ways? Or maybe the treatment had varying effects depending upon the teacher who taught the class.Moreover, how can we be sure as to which factor(s) is affecting the results of the students more? Maybe the age group is a more dominant factor responsible for a students performance than the music treatment.For such cases, when the outcome or dependent variable (in our case the test scores) is affected by two independent variables/factors we use a slightly modified technique called two-way ANOVA.In the one-way ANOVA test, we found out that the group subjected to variable music and no music at all performed more or less equally. It means that the variable music treatment did not have any significant effect on the students. So, while performing two-way ANOVA we will not consider the variable music treatment for simplicity of calculation. Rather a new factor, age, will be introduced to find out how the treatment performs when applied to students of different age groups. This time our dataset looks like this:Here, there are two factors  class group and age group with two and three levels respectively. So we now have six different groups of students based on different permutations of class groups and age groups and each different group has a sample size of 5 students.A few questions that two-way ANOVA can answer about this dataset are:Two-way ANOVA tells us about the main effect and the interaction effect. The main effect is similar to a one-way ANOVA where the effect of music and age would be measured separately. Whereas, the interaction effect is the one where both music and age are considered at the same time.Thats why a two-way ANOVA can have up to three hypotheses, which are as follows:Two null hypotheses will be tested if we have placed only one observation in each cell. For this example, those hypotheses will be:
H1: All the music treatment groups have equal mean score.
H2: All the age groups have equal mean score.For multiple observations in cells, we would also be testing a third hypothesis:
H3: The factors are independentorthe interaction effect does not exist.AnF-statisticis computed for each hypothesis we are testing.Before we proceed with the calculation, have a look at the image below. It will help us better understand the terms used in the formulas.The table shown above is known as acontingency table. Here, represents the total of the samples based only on factor 1, andrepresents the total of sample based only on factor 2. We will see in some time that these two are responsible for the main effect produced. Also, a term is introduced which represents the subtotal of factor 1 and factor 2. This term will be responsible for the interaction effect produced when both the factors are considered at the same time. And we are already familiar with the  , which is the sum of all the observations (test scores), irrespective of the factors.We have calculated all the means  sound class mean, age group mean and mean of every group combination in the above table.Now, calculate the sum of squares (SS) and degrees of freedom (df) for sound class, age group and interaction between factor and levels.We already know how to calculateSS (within)/df (within)in ourone-way ANOVAsection, but in two-way ANOVA the formula is different. Lets look at the calculation of two-way ANOVA:
In two-way ANOVA, we also calculateSSinteractionand dfinteractionwhich defines the combined effect of the two factors.Since we have more than one source of variation (main effects and interaction effects), it is obvious that we will have more than one F-statistic also.Now using these variances, we compute the value of F-statistic for the main and interaction effect. So, the values of f-statistic are,F1 = 12.16F2 = 15.98F12 = 0.36We can see the critical values from the tableFcrit1 = 4.25Fcrit2 = 3.40Fcrit12 = 3.40If, for a particular effect, its F value is greater than its respective F-critical value (calculated using the F-Table), then we reject the null hypothesis for that particular effect.Steps to perform two-way ANOVA in Excel 2013:Step 1: Click the Data tab and then click Data Analysis. If you dont see the Data analysis option, install the Data Analysis Toolpak.Step 2: Click ANOVA two factor with replication and then click OK. The two-way ANOVA window will open.Step 3: Type an Input Range into the Input Range box. For example, if your data is in cells A1 to A25, type A1:A25 into the Input Range box. Make sure you include all of your data, including headers and group names.Step 4: Type a number in the Rows per sample box. Rows per sample is actually a bit misleading. What this is asking you is how many individuals are in each group. For example, if you have 5 individuals in each age group, you would type 5 into the Rows per Sample box.Step 5: Select an Output Range. For example, click the new worksheet radio button to display the data in a new worksheet.Step 6: Select an alpha level. In most cases, an alpha level of 0.05 (5 percent) works for most tests.Step 7: Click OK to run the two-way ANOVA. The data will be returned in your specified output range.Step 8: Read the results. To figure out if you are going to reject the null hypothesis or not, youll basically be looking at two factors:And you are done!
Note: We dont only have to have two variables to run a two-way ANOVA in Excel 2013. We can also use the same function for three, four, five or more number of variables.The results for two-way ANOVA test on our example look like this:As you can see in the highlighted cells in the image above, the F-value for sample and column, i.e. factor 1 (music) and factor 2 (age) respectively, are higher than their F-critical values. This means that the factors have a significant effect on the results of the students and thus we can reject the null hypothesis for the factors.Also, the F-value for interaction effect is quite less than its F-critical value, so we can conclude that music and age did not have any combined effect on the population.Until now, we were making conclusions on the performance of students based on just one test. Could there be a possibility that the music treatment helped improve the results of a subject like mathematics but would affect the results adversely for a theoretical subject like history?How can we be sure that the treatment wont be biased in such a case? So again, we take two groups of randomly selected students from a class and subject each group to one kind of music environment, i.e., constant music and no music. But now we thought of conducting two tests (maths and history), instead of just one. This way we can be sure about how the treatment would work for different kind of subjects.We can say that one IDV/factor (music) will be affecting two dependent variables (maths scores and history scores) now. This kind of a problem comes under a multivariate case and the technique we will use to solve it is known as MANOVA. Here, we will be working on a specific case called one factor MANOVA. Let us now see how our data looks:Here we have one factor, music, with 2 levels. This factor is going to affect our two dependent variables, i.e., the test scores of maths and history. Denoting this information in terms of variables, we can say that we have L = 2 (2 different music treatment groups) and P = 2 (maths and history scores).A MANOVA test also takes into consideration a null hypothesis and an alternate hypothesis.:The Calculations of MANOVA are too complex for this article so if you want to further read about it, check this paper.We will implement MANOVA in Excel using the RealStats Add-ins. It can be downloaded from here.Steps to perform MANOVA in Excel 2013:Step 1: Download the RealStats add-in from the link mentioned aboveStep 2: Press control+m to open RealStats windowStep 3: Select Analysis of varianceStep 4: Select MANOVA: single factor Step 5: Type an Input Range into the Input Range box. For example, if your data is in cells A1 to A25, type A1:A25 into the Input Range box. Make sure you include all of your data, including headers and group names.Step 6: Select Significance analysis, Group Means and Multiple Anova.Step 7: Select an Output Range. Step 8: Select an alpha level. In most cases, an alpha level of 0.05 (5 percent) works for most tests.Step 9: Click OK to run. The data will be returned in your specified output range.Step 10: Read the results. To figure out if you are going to reject the null hypothesis or not, youll basically be looking at two factors:And you are done!RealStats add-on shows us the results by different methods. Each one of them denotes the same p-value. As the p-value is less than the alpha value, we will reject the null hypothesis. Or in simpler terms, it means that the music treatment did have a significant effect on the test results of students. But we still cannot tell which subject was affected by the treatment and which was not. This is one of the limitations of MANOVA; even if it tells us whether the effect of a factor on a population was significant or not, it does not tell us which dependent variable was actually affected by the factor introduced.For this purpose, we will see the Multiple ANOVA table to generate a helpful summary about it. The result will look like this:Here, we can see that the P value for history lies in a significant region (since P value less than 0.025) while for maths it does not. This means that the music treatment had a significant effect in improving the performance of students in history but did not have any significant effect in improving their performance in maths.Based on this, we might consider picking and choosing subjects where this music approach can be used.I hope this article was helpful and now youd be comfortable in solving similar problems using Analysis of Variance. I suggest you take different kinds of problem statements and take your time to solve them using the above-mentioned techniques.You should also check out the below two resources to give your data science journey a huge boost:Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
Intermediate Tableau guide for data science and business intelligence professionals,Learn everything about Analytics|Introduction|Table of Contents|1. Dealing with different data sources|2. Conditional Combination of Data|3. Calculated Field|4. Parameter Control|5. End Note,"1.1 Joins|1.2 Data Blending|2.1 Groups|2.2 Sets|Learn,Engage,Compete&Get Hired|Share this:|Like this:|Related Articles|A Simple Introduction to ANOVA (with applications in Excel)|Introductory Guide  Factorization Machines & their application on huge datasets (with codes in Python)|
Pavleen Kaur
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The greatest value of a picture is when it forces us to notice what we never expected to see. John W. TukeyLets assume that you have some data with you and you wish to garner some insights from it. Coding is not your forte and you dont know how to get started. Let me tell you this  youcan make something as descriptive / insightful as the image below, with gestures as simple as drag and drop. And it doesnt even require a single ounce of coding. Now that is the power of Tableau for you!For all those reading this who have been acquainted with Tableau, can plot a few basic charts on it, and wish to learn more about its wide horizons, this article is meant for you. As for those who have yet to be introduced to the beauty and simplicity of Tableau, quickly go through Tableau for Beginnersfirst.Practice making a few simple visualisations and then rush back here!In this article we are going to discuss a few core functionalities of Tableau which help in making really dynamic graphs. So lets quickly get started!Its not practical to store all data in a single table. In order to avoid anomalies related to updates, data is almost always distributed in multiple tables that have some relation with each other. Lets understand the same with an example.Consider the situation where aSuperstore, on the verge of expansion, perceives that thenumber of Returned orders has been increasing by day. To ascertain the analysis and come up with the right action plan, they plotted the following chart to understand the products that were being returned :As can be observed,Binders have the maximum number of items being returned. But judging by the color of the bars, Machines and Tableshave the highest percent of return ( returned / bought ) :This seems pretty similar to plotting just another chart, but the trick here was that it was created by using the combined data of two tables : Orders and Returns. This is known as aJoin.Lets try making the same to get a better understanding :The dataset is made up of 3 tables: Orders, People and Returns, and the ones that we are interested in at the moment are Orders and Returns.For two tables to be joined, there has to be the presence of at least one common field.Here, Tableau automatically Inner Joined the two tables, based on the commonality of the columnOrder ID. By way of inner join, the combined data only consists of those rows that have the same Order ID in both the tables.You can change the Join Type as well as the Joining Field in Tableau, but you need to ensure that its sensible.Changing the Join type (Default Inner to Right) :Changing the Join field :See how I tried to join the two tables based on Row ID of Orders and Order ID of Returns? Since the two are not compatible, we dont see any records, and plus that red mark near the circles showsan error.Lets get back to working on the chart now :Here we have used Inner Join, but you can always choose between Inner, Right, Left and Full Outer based on your requirements.Data blending is quite similar to Joins, with the difference being that Joining requires the data to be from the same data source. In the above example, we used different tables from the same Excel file. But Data Blending comes into the picture when you are working with different Data Sources. Lets understand the same with an example.The Superstore has another vertical in the form of a Coffeechain which is spread across as many states as the Superstore is. But they are considering shutting down some of the branches after observing the following plot :As can be seen there are some branches that are doing equally well as the Superstore such as California and New York, while many are not, such as Iowa and New Mexico. Just like in Joins, here the trick is that both the datasets, dealing with the two verticals, were stored in different Data Sources, an Excel file and a TDE database.Why not we plot the same as well to get a better understanding? We will begin by blending theSuperstore data and theSample  CoffeeChain database. You can find the data for the latterhere as well:Here you must have observed a few things; lets take them up one by one :Lets rectify these null values by interchanging Steps 3and 4 :Now that your data is all ready, blended or joined, lets start making some interesting dashboards. From here on out, we will be using only the Superstore data: Orders + Returns (Left-Joined).Lets start off by considering the example of a Survey analysis. In a survey of Food Consumption, under the section ofFood Preferences, instead of Low Fat, you may have LF, or instead of Regular, you may have reg.In such cases, during data visualisation, you face issues like the following:As you can see, due to different nomenclatures, this visualisation is not ideal. So, one possible solution to this isGroupingwhere you can place LFand Low Fatin one group, and reg and Regular in another :Lets understand this a bit better with the help of the following Dashboard :The above is the Returns Analysis across Categories and their Sub Categories. Although, it cannot be seen, in thebargraph, Copiers have the maximum Return percentage, followed by Furnishings : As far as the Line Chart goes, it seems that the Sales Team had been right all along. The Returns were in fact increasing quite rapidly, but luckily from what we can see, the rise is slowly receding.From the pie charts, you can clearly analyse the Returns of each Category. As can be observed,Technology suffered the maximum number of Returns.Another analysis, which I am going to leave for you to make, could be the Return distribution across the various States.Once you finish learning how the above graphs were made, you can easily make this too. So, lets get started :This step automatically segregated the Sales of each Sub Category based on whether the Orders had the Null or Yes value under Returns or not.The remaining steps are merely customisation. Lets do those as well :Also, in the Legends, you will most likely see In/Out as the aliases. You can change this as per your requirements, by right clicking on the In/Out blue pill in the Marks Pane, and choosing Edit Alias.Lets shift to the pie chart that we had made. We are going to apply the same ReturnedOrNot group to this as well. First create two duplicates of this sheet, and work on one of them : Its equally easy making the Line Chart :All that is left now, is combining the above Worksheets into one Dashboard.Why dont you try makingthe chart for State wise Returns distribution as well?After taking note of the Returns analysis, your organisation decided that the increase in Returns was not that alarming, and that it should not be constituted as a reason for non  expansion.But the Superstore is only going to expand in those States where the Sales and Profit both have crossed a certain margin, for example,40,000 and 10,000 respectively :So, Sets, as are created above, are really similar to groups.In Set, you group data that fulfils a particular set condition.Another interpretation could be: Groups help you attain a higher level hierarchy, as we had seen in the previous example, whereas Sets help you attain a lower level granularity.Lets understand this better by creating the above Dashboard :This step joins the above two conditions for Sales and Profit, to get the requisite combined computation. To view the results :You can always customise the above chart by changing the colour, adding labels etc.The line chart is as easy to make as the one we had made previously for Trend of Return. Here we have excluded the States that belong to theNo Expansiongroup, like we had excluded the Not Returned there.The Superstore dataset is pretty comprehensive. It offers quite a lot of information and field sets. But like all data, there is always the possibility of extracting more features. Calculated Fields help you do exactly that while alsoallowingyou to carry out both simple and complex calculations on the data.Sowhat is a Calculated Field?
To put it in simple words, its a formula that you apply to your data, where the various Measures act as the variables.How to create one?
Just simply go to Analysis, click on Create Calculated Field and something like this will pop up :Here is where you write your formulas, and as you can see, Tableau provides you with the various syntaxes too, so that you never feel lost! You can also apply If-Else conditions, Case conditions (as we shall see next) and of course the usual mathematical computations too, which we will explore now.So what Calculation to start with? Letsbegin with something simple, that is, Average Sales associated with the Orders.The most apt formula for the same would be Total Sales / Total Number of Orders. To convert this formula in Tableau terms, Total Sales implies the SUM of Sales, whereas Total Number of Orders means their COUNT.Lets get to the application then :So what you have basically accomplished with the Calculated Field is create a Measure of your own, which you can use just like Sales and Profits.Obviously this was just a gist of what Calculated Fields can do. They can be used for various complex calculations as well, and the glimpse of one such instance can be seen in the following section.Filters such as the following are a great way of interacting with the visualisations on Tableau :Just like filtersare measures through which you can look at various aspects of your data, a Parameter is another great feature. Itcan be used in place of Filters, and can exhibit itsown dynamic property too.Sowhat are Parameters? These act as variables of an equation, which you can change to get different results each time.Lets try and understand this with the help of an example. So far we have been making separate graphs for separate Measures. Whenever we had to analyse Sales, Profits, Quantity or Discount of the various Categories of Products, we had to make different charts, everything being the same BUT the Measures.With the power of Parameter Control, there is actually a pretty easy way of going about this, without the repetitions.Consider this for Sales :And the following for Profit : Just with a simple click on the MeasureValue list, you are getting cumulative results across various Dimensions. Here,the MeasureValue is our Parameter, since we are able to change its value.Lets get to making one of our own now. Parameters rely heavily on Calculated Fields, so you are going to get a nice chance to practice what you learned above:Do not expect the drop down list to start magically creating graphs on its own. There are still some computations to be done.So far we have only allocated the names to the Parameter, but not the Values that they are supposed to take up. So for this purpose, we are going to create a Calculated Field. There you go! Go ahead and try changing the MeasureValues now.Now onto the chart we have created in the Dashboard. To see the lines for the individual Categories, simply drag that Dimensionon top of the chart :For creating other charts, do as you normally would, but instead of dragging the individual Measures onto the Rows / Columns, drag the NameOfMeasure parameter instead.That brings us to the end of this article. But dont worry, I will be back again with another article on Tableau!Meanwhile, I think its customary to give you a Dashboard to make :You may face a tad bit of difficulty in the beginning, but if you truly apply yourself, you are bound to get it. And of course if there are ever any doubts, or if you wish for me to cover any concept in the next article, do leave them as comments.All the best to you Data Explorers!",https://www.analyticsvidhya.com/blog/2018/01/tableau-for-intermediate-data-science/
Introductory Guide  Factorization Machines & their application on huge datasets (with codes in Python),Learn everything about Analytics|Introduction|Table of Contents|Intuition behind Factorization|How Factorization Machines trump Polynomial and linear models?|Field-Aware Factorization Machines|Implementation using xLearn Library in Python|End Notes,"FM to the rescue|Example:Demonstration of how FM is better than POLY2|Important note on numerical features|xLearn|Share this:|Like this:|Related Articles|Intermediate Tableau guide for data science and business intelligence professionals|Key Highlights in Data Science / Deep Learning / Machine Learning 2017 and What can we Expect in 2018?|
Ankit Choudhary
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I still remember my first encounter with a Click prediction problem. Before this, I had been learning data science and I was feeling good about my progress. I had started to build my confidence in ML hackathons and I was determined to do well in several challenges.In order to do well, I had even procured a machine with 16 GB RAM and i7 processor. But the first look at the dataset gave me jitters. The data when unzipped was over 50 GB  I had no clue how to predict a click on such a dataset. Thankfully Factorization machines came to my rescue.Anyone who has worked on a Click Prediction problem or Recommendation systems would have faced a similar situation. Since the datasets are huge, doing predictions for these datasets becomes challenging with limited computation resources. However, in most cases these datasets are sparse (only a few variables for each training example are non zero) due to which there are several features which are not important for prediction, this is where factorization helps to extract the most important latent or hidden features from the existing raw ones.Factorization helps in representing approximately the same relationship between the target and predictors using a lower dimension dense matrix. In this article, I discuss Factorization Machines(FM) and Field Aware Factorization Machines(FFM) which allows us to take advantage of factorization in a regression/classification problem with an implementation using python.To get an intuitive understanding of matrix factorization, Let us consider an example: Suppose we have a user-movie matrix of ratings(1-5) where each value of the matrix represents rating (1-5) given by the user to the movie.We observe from the table above that some of the ratings are missing and we would like to devise a method to predict these missing ratings. The intuition behind using matrix factorization to solve this problem is that there should be some latent features that determines how a user rates a movie. For example  users A and B would rate an Al Pacino movie highly if both of them are fans of actor Al Pacino, here a preference towards a particular actor would be a hidden feature since we are not explicitly including it in the rating matrix.Suppose we want to compute K hidden or latent features. Our task is to find out the matrices P(U x K) and Q(D x K) (U  Users, D  Movies) such that P x QT approximates R which is the rating matrix.  Now, each row of P will represent strength of association between user and the feature while each row of Q represents the same strength w.r.t. the movie. To get the rating of a movie dj rated by user ui, we can calculate the dot product of 2 vectors corresponding to ui and dj  All we need to do now is calculate P and Q matrices. We use gradient descent algorithm for doing this. The objective is to minimize the squared error between the actual rating and the one estimated by P and Q. The squared error is given by the following equation.Now, we need to define an update rule for pik and qkj. The update rule in gradient descent is defined by the gradient of the error to be minimized. Having obtained the gradient, we can now formulate the update rules for both pik and qkjHere,  is the learning rate which can control the size of updates. Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.The above solution is simple and often leads to overfitting where the existing ratings are predicted accurately but it does not generalize well on unseen data. To tackle this we can introduce a regularizationparameter  which will control the user-feature and movie-feature vectors in P and Q respectively and give a good approximation for the ratings.For anyone interested in python implementation and exact details of the same may go to thislink. Once we have calculated P and Q using the above methodology, we get the approximate rating matrix as:Notice how we are able to regenerate the existing ratings, moreover we are now able to get a fair approximation to the unknown rating values.Let us consider a couple of training examples from a click prediction dataset. The dataset is click through related sports news website (publisher) and sports gear firms (advertiser).When we talk about FMs or FFMs, each column (Publisher, Advertiser) in the dataset would be referred to as a field and each value (ESPN, Nike.) would be referred to as a feature.A linear or a logistic modeling technique is great and does well in a variety of problems but the drawback is that the model only learns the effect of all variables or features individually rather than in combination.Where w0, wESPN etc. represent the parameters and xESPN, xNikerepresent the individual features in the dataset. By minimizing the log-loss for the above function we get logistic regression. One way to capture the feature interactions is a polynomial function that learns a separate parameter for the product of each pair of features treating each product as a separate variable.This can also be referred to as Poly2 model as we are only considering combination of 2 features for a term.FM solves the problem of considering pairwise feature interactions. It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model. FM also allows us to do this in an efficient way both in terms of time and space complexity. It models pairwise feature interactions as the dot product of low dimensional vectors(length = k). This is illustrated with the following equation for a degree = 2 factorization machine:  Each parameter in FMs (k=3) can be described as follows:  Here, for each term we have calculated the dot product of the 2 latent factors of size 3 corresponding to the 2 features.From a modeling perspective, this is powerful because each feature ends up transformed to a space where similar features are embedded near one another. In simple words, the dot product basically represents similarity of the hidden features and it is higher when the features are in the neighborhood.The cosine function is 1 (maximum) when theta is 0 and decreases to -1 when theta is 180 degrees. It is clear that the similarity is maximum when theta approaches 0.Another big advantage of FMs is that we are able to compute the term that models all pairwise interactions in linear time complexityusing a simple mathematical manipulation to the above equation. If you want to have a look at the exact steps required for this, please refer to the original Factorization Machines research paper at this link.Consider the following artificial Click Through Rate (CTR) data:This is a dataset comprising of sports websites as publishers and sports gear brands as publishers. The ad appears as a popup and the user has an option of clicking (clicks)the ad or closing it (unclicks).In order to understand FFMs, we need to realize the meaning of field. Field is typically the broader category which contains a particular feature. In the above training example, the fields are Publisher (P), Advertiser (A) and Gender(G).FFMs have proved to be vital for winning the first prize of three CTR (Click through Rate) competitions hosted by Criteo, Avazu, Outbrain, it also won the third prize of RecSys Challenge 2015. Datasets for the CTR can be accessed fromKaggle.Some of the most popular libraries for its implementation in Python are as follows:For using FMs on datasets, it needs to be converted to a specific format called the libSVM format. The format of training and testing data file is:<label> <feature1>:<value1> <feature2>:<value2> .In case of a categorical field, the feature is uniquely encoded and a value of 1 is assigned to it. In the above figure ESPN is represented by code 1, Nike is represented by code 2 and so on. Each line contains an equivalent training example and is ended by a \n or a new line character.Similarly for FFMs, the data needs to be transformed to a libffm format. Here, we also need to encode the field since ffm requires the information of field for learning. The format for the same is:<label> <field1>:<feature1>:<value1> <field2>:<feature2>:<value2> ..Numerical features either need to be discretized (transformed to categorical features by breaking the entire range of a particular numerical feature into smaller ranges and label encoding each range separately) and then converted to libffm format as described above. Another possibility is to add a dummy field which is the same as feature value will be numeric feature for that particular row (For example a feature with value 45.3 can be transformed to 1:1:45.3). However, the dummy fields may not be informative because they are merely duplicates of features.Recently launched xLearn library provides a fast solution to implementing FM and FFM models on a variety of datasets. It is much faster than libfm and libffm libraries and provide a better functionality for model testing and tuning.Here, we will illustrate with an example of FFM for the loan prediction dataset which can be accessed at the Loan Predictionpractice problem.The problem to identify the customers segments eligible for loan amount so that they can specifically target these customers based on some demographic and credit history variables.First step is to import necessary librariesFor simplicity we will just take a few variables here:Next we will create a test set for testing the ffm modelNext, we need to convert the dataset to libffm format which is necessary for xLearn to fit the model. Following function does the job of converting dataset in standard dataframe format to libffm format. df = Dataframe to be converted to ffm formatType = Train / Test/ ValidNumerics = list of all numeric fieldsCategories = list of all categorical fieldsFeatures = list of all features except the Label and Id xLearn can handle csv as well as libsvm format for implementation of FMs while we necessarily need to convert it to libffm format for using FFM.
Once we have the dataset in libffm format, we could train the model using the xLearn library.xLearn can automatically performs early stopping using the validation/test logloss and we can also declare another metric and monitor on the validation set for each iteration of the stochastic gradient descent.The following python script could be used for training and tuning hyperparameters of FFM model using xlearn on a dataset in ffm format. More options and complete documentation is given here.The library also allows us to use cross-validation using the cv() function:Predictions can be done on the test set with the following code snippet:In this article we have demonstrated the usage of factorization for normal classification/Regression problems. Please let us know how this algorithm performed for your problem. Detailed documentation for xlearn is given at this linkand has regular support by its contributors.",https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/
